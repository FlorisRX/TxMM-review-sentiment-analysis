GPU model;Stars;Confidence;Comment
RTX 4090;4;0.3005101978778839;Nog wat reviews om te bekijken: En filmpjes uiteraard. Digital Foundry: Hardware Unboxed: GamersNexus: TechTesters: LinusTechTips:
RTX 4090;2;0.3331948518753052;DerBauer heeft een interessante video review online gezet. Daarin is hij met de power target setting gaan spelen en de conclusie daaruit is dat als je de power target omlaag zet naar 60%, je nauwelijks performance verliest in games en toch een enorme afname van het verbruikte vermogen ziet (bijvoorbeeld 10% minder performance in 3dMark Time Spy Extreme met een opname vermindering van 33%). Engelse link: (power target sectie start rond 14:35) Duitse link: (power target sectie start rond 15:35) Zeer interessant als je toch al een frame rate hebt in een spel waar je 5 tot 10% niet eens zult merken.
RTX 4090;4;0.3243623971939087;De review van computerbase laat vergelijkbare resultaten zien. 450W naar 300W geeft 90% van de performance. Bedankt voor de linkjes!
RTX 4090;5;0.7334117293357849;Zeer interessant! Dat zou betekenen dat deze kaarten meer plaats krijgen in de wereld van hoge energieprijzen. 90% met 300 watt is zeer netjes te noemen. Dan nog de prijzen iets omlaag en je houdt een prachtige generatie kaarten over. Mooie wetenswaardigheid om te horen zo
RTX 4090;2;0.35737302899360657;Dat is een fijne bijkomstigheid, maar de fabrikant vraagt dan wel 2x zoveel geld voor iets waarvan je 10% niet gaat gebruiken. Dat lijkt mij geen topdeal....
RTX 4090;2;0.2965300679206848;"netjes dat je met al die links komt ik zag zojuist de review op gamers nexus en die 4090 is meer dan 2x zo snel als mijn rtx3080 kaartje welke nu ineens midrange is geworden ( dat heb je met vooruitgang ). ik ben erg onder de indruk van deze kaarten en als het klopt dat de rtx 4090 zo ' n 450 watt lust valt me dat nog mee voor hetgeen wat je krijgt. mijn rtx 3080 lust ook dik 300 watt, dus dat valt nog wel te compenseren. ik heb een 4k monitor dus zou dit ( volgens nexus ) een hele vooruitgang zijn. ik ga hier absoluut niet zielig doen of jaloers zijn want ik kan zo ' n 4090 met gemak kopen zonder er iets aan in te boeten. dat is zo geregeld, maar er is iets anders dat me nu nog tegen houdt : hoe vaak game ik nu echt en hoe vaak heb ik nood aan zulke hoge prestaties? ik heb een eigen groothandel en als ik in de avond ga gamen dan zal dat zo ' n 1 a 1, 5 uur zijn. dit is genoeg en ik ben dik tevreden met mijn huidige systeem. desalniettemin ben ik wel degelijk verheugt te zien dat er nog zoveel rek zit in de gpu markt en zoals iemand hier riep ( die heeft al een - 1 ) wat een waardeloos stuk electronica ; daar ben ik het zeker niet mee eens. de sprongen tussen de generaties worden steeds groter en als ik weer zover ben om iets nieuws te kopen dan heb ik tegen die tijd weer nieuwe games die deze power echt wel gaan gebruiken. de prijs zal voor vele wel te hoog zijn, maar er komen ook andere varianten uit ( 4060 - 4070 - 4080 ) die wel te betalen zijn en alsnog een stuk rapper zijn dan hun broertjes uit de vorige generatie. met zulke kaarten loont het om te kijken wat je echt nodig hebt en er is keus genoeg op de gpu markt. wil je echt 4k op strak 60 + fps en rtx op high gaan spelen dan zul je er iets voor over moeten hebben en kost je dat geld. dat is altijd al zo geweest en erover klagen is niet fair want dat zal altijd wel zo blijven."
RTX 4090;3;0.3241809010505676;Ik ben het helemaal met je eens, en ook ik krijg beetje FOMO van hoe goed de 4090 nu al lijkt te zijn. Maar laten we een 3080 absoluut geen midrange kaart noemen toch? Zelfs al zijn andere kaarten sneller, dat maakt deze kaart niet langzaam. Een formule 1 auto is natuurlijk het snelste, maar dat maakt een lamborghini niet een 'midrange' auto, hoewel hij niet het snelste is (mag die vergelijking?) Je noemt zelf ook al dat je geen nood hebt voor zulke prestaties, maar ik wilde daar nog bij zeggen, dat we nu niet al moeten zeggen dat de vorige generatie mid-range is
RTX 4090;4;0.3536972403526306;En toch zal het midrange worden al geef ik daar niet veel om. Je vergelijk klopt en de 3080 is nog steeds een beul van een kaart, dat zeker. Ik weet nog de discussie met mijn neef over de gtx1080 die ik destijds had. Mijn neef zei dat de gtx1080 overkill was voor 1080p. Daar was ik het niet helemaal mee eens. Ik had toen een 1440p scherm en ik kon die gtx1080 echt wel op z'n knieën krijgen. Ook met 1080p is dat me gelukt. Ray Tracing is prachtig, maar die luxe kan ik me niet altijd veroorloven met mijn rtx3080. In CP77 moet ik best wel wat concessies doen en ik RDR2 ook. Gelukkig geeft DLSS me een hele lange rek en kan ik alsnog dik gamen op 4K. Ik vond de uitvinding van DLSS de beste die ik ooit gezien had. In plaats van de resolutie naar beneden bij te stellen schakel je DLSS in. Dan heb je ook geen andere vorm van anti aliasing nodig. DLSS geeft een lange adem en ik koop wel een 40 serie kaart als dat nodig is. Ik kan het betalen <> doordat ik geen geld over de balk gooi. Om nu een rtx4090 te kopen vind ik een impulsaankoop en ik wacht rustig af
RTX 4090;1;0.41412442922592163;DLSS zou een lange adem kunnen geven als Nvidia het niet artificiëel segmenteerd voor hun nieuwe kaarten. De 3090 kan al geen DLSS3.0. Uit eerdere generaties is ook gebleken dat ze zeer gemakkelijk nieuwe DLSS versies konden runnen door de nieuwe binary over de oude te pasten maar Nvidia ze gewoon niet bij de driver update gaf en pretendeerde dat het niet kon.
RTX 4090;1;0.4816563129425049;Je gaat 'DLSS' 3.0 niet missen als input latency er voor jou toe doet. Er is vanochtend een review van Hardware Unboxed uitgebracht over DLSS 3.0 waarin duidelijk wordt dat de input latency fors trager is dan met DLSS 2.0 en dat 3.0 tevens meer merkbare artifacting heeft t.o.v. 2.0. Na het kijken van de video kan ik niet anders dan concluderen dat deze techniek de goede DLSS naam onwaardig is. Hier de link: Hardware Unboxed DLSS 3.0 review :
RTX 4090;2;0.387876033782959;Die gtx1080 was blijkbaar overkill in periode dat hij waarschijnlijk zei. En rtx2080 misschien net was of niet en rt en dlss niet relevant was. En cb77 was er toen ook niet. Al zou top 3 ruk games die idioot hoge eisen niet als de standaard nemen maar 75% van alle moderne games. Als oude dx11 game ruk loopt ligt dat meer aan de devs. Tenzij het je favo game is of grijs speelt, zou ik daar niet aankopen op baseren. Tenzij die in rtx30 tijdperk zei. Al ben ik wel met hem eens. Het is overkill als gemiddelde game neemt en niet competatief gamed dus 60fps locked aan 60hz shermpje. En dat in 2019
RTX 4090;1;0.5231994986534119;Misschien heb je hier onbewust wel een punt gemaakt. Er is amper een persoon die deze kaart gaat kopen, maar in de benchmarks geeft het jouw het gevoel dat jouw 3080 nog maar midden segment is. Laat jij nou ook bij die kleine groep horen die een 3080 of hoger heeft gekocht, dat betekend dat het grootste gedeelte van de videokaart eigenaren dus eigenlijk nu het idee krijgt een 'minder dan gemiddelde' kaart te hebben. Ik denk dat dat het eigenlijke idee achter de 4090 is. De rest van de kaarten tot middelmatigheid verdrukken. Iedereen het gevoel geven dat zijn kaart zwaar verouderd is en niet meer mee kan. En dat terwijl get gros van de gebruikers niet veel snellere kaarten draait dan een 2070ti/2080 misschien en jij nog een flink boven gemiddelde kaart in gebruik hebt. Ontwikkelaars maken games voor het publiek. Dat straks 1,5% van de gamers de komende jaren een 4090 heeft betekend niet dat games hiervoor massaal gemaakt gaan worden. Ze gaan gemaakt worden om nog goed speelbaar te zijn op die 2070ti/2080 generatie. Deze kaart is niet bedoeld om een serieus verkoopbaar product de markt in te zetten, dit is puur bedoeld om jou ontevreden te maken over je huidige hardware en je er toe aan te zetten de portemonnee te trekken voor hardware wat amper nut gaat hebben.
RTX 4090;5;0.5190790295600891;Juist! En daar zit hem de knevel. Nu trap ik hier niet zo snel in en aan de andere kant heeft Nvidia niet in de gaten dat het gros (zowat iedereen) deze kaart niet kan betalen en al helemaal niet nu alles duurder aan het worden is. Maar toch is het leuk om te zien wat er allemaal nog mogelijk is op videokaarten gebied. Het is er, het biedt goede garanties voor de toekomst en je 'kunt' upgraden. Ik ben heel benieuwd hoe de zaken er over een paar maanden voorstaan. Zal het percentage 40-serie bezitters nu flink gaan groeien of haalt de Steam-survey nog steeds een grote groep met mindere kaarten en zelfs nog de gtx1650 of de gtx1060 over een tijdje. We gaan het zien
RTX 4090;2;0.4328869581222534;Omdat de wereld sterk budget beperkt is en liever 1080p op 40 tot 60fps voor lief nemen ipm geld te smijten ook in goede tijden. En dat halo sku volk op tech site zwaar over vertegend woordig zijn. Naast dat niet iedereen op steam ook die zware games spelen maar ook meer die leuke casual games. Die de tech amper pushen. Dan is midrange van 3 gen terug snel zat. Aan de steam survey zal er niet veel veranderen. 1060 zal krimpen of ook de 2060 en 3060 krimpen geen idee.
RTX 4090;2;0.39987507462501526;Ik snap je reactie volkomen en deels ben ik het er ook mee eens. Ik probeer hiermee te denken vanuit het oog van AAA-titels games. Het is en blijft veel geld en het merendeel van de mensen hebben die tech niet nodig, true. Maar dit wil niet zeggen dat ik het eens ben met Nvidia. Ik vind dat het bedrijf meer naar hun eigen belang kijkt dan die van de burger. Hun winst is gebaseerd op een hele flinke winstmarge en ik probeer daar een argument op te bouwen. Er zullen vast meer mensen naar console gaming gaat kijken want dat is een pak goedkoper en ik denk dat Nvidia dit aan het vergeten is. Het bedrijf verdiend zeker een tik op hun vingers en het mag wat mij betreft wel wat menselijker. Vandaar dat je zegt: Dit punt deel ik zeker met je. Daarmee suggereer je dat de kaarten (of heel de PC gaming markt) way to overpriced is. Die Steam survey vind ik wel interessant want het geeft ook een beetje weer waar het geld hem zit en veel mensen hebben genoeg aan een goedkope kaart of een APU. Voor candy crush heb je geen 40 serie nodig. Ik poog te zeggen dat Nvidia een prachtige oplossing biedt voor PC gamers die AAA-titels willen spelen, maar dat de prijs die ze ervoor vragen gewoonweg over the top is. Dit kan hoe dan ook een stuk minder
RTX 4090;1;0.6641159057617188;Ik denk dat dat het eigenlijke idee achter de 4090 is. De rest van de kaarten tot middelmatigheid verdrukken. Iedereen het gevoel geven dat zijn kaart zwaar verouderd is en niet meer mee kan. Dit lijkt me nogal een persoonlijke opvatting. Dit soort halo producten hebben altijd al bestaan en zijn voor de meesten niet meer dan een showcase van vette tech voor een ernstige prijs. Het gros van de mensen zal een 4060 of een 4070 kopen en daar erg blij mee zijn, flagship be damned.
RTX 4090;2;0.5198088884353638;Behalve dat wat we nu dus een middenmotor kaart noemen door het bestaan van het halo product de 4090, eigenlijk gewoon een high end kaart is en ook voor een prijs en energie consumptie van iets wat we nog niet heel lang geleden een productiviteitskaart voor proffessioneel gebruik zouden noemen. Dit terwijl dat halo product eigenlijk niet echt een reeel product is, 450 watt voor een grafische kaart gaat nergens meer over, moet ik zometeen de totale powerdraw van mijn systeem in de kilo-watt gaan meten? Waar houdt dit op? Vanuit een meer principieel perspectief vind ik het het onacceptabel dat we in een tijd als dit vermogensbudgetten voor grafische kaarten en processoren iedere generatie maar blijven oprekken, voor pure entertainment doeleinden, net wat mooiere games, net wat hogere resolutie. Gewoon niet ok in een wereld die van fossiele brandstoffen probeert af te kicken.
RTX 4090;1;0.34960880875587463;Maarja. je weet dat ze er dik winst op maken. Zelf met 150% van de prijs af.
RTX 4090;1;0.32017284631729126;Dat zeker en Nvidia zal zichzelf in de vingers snijden als ze deze prijzen aanhouden. Inflatie, energieprijs, 2 kernwoorden van vandaag en Nvidia zal zelf moeten ondervinden dat de afname van deze kaarten minder snel gaat dan als ze een redelijke prijs aan zouden houden. Ik denk dat als Nvidia 40% van de prijs afhaalt dat er meer dan 50% van de mensen deze kaart zal gaan kopen. Ik denk dat ze wel bij zullen draaien als de verkoop tegen zou gaan vallen. Ik run een eigen bedrijf, dus ik ben wel thuis in dit soort zaken. We gaan het zien
RTX 4090;2;0.3321402668952942;Je snapt wel dat de prijzen nooit 40% gaan zakken en dat door inflatie juist alles duurder wordt, toch?
RTX 4090;1;0.5171750783920288;Ik begrijp dat je nooit bedrijfskunde hebt gehad en dat geeft ook niet, maar Nvidia verkoopt hun kaarten met een hele flinke marge. Aan de andere kant zou Nvidia er ook voor kunnen kiezen om minder winst te maken en zo een groter aantal klanten (kopers) te krijgen zodat de winst alsnog op het oude peil terecht komt. Dat betekend dat als Nvidia de prijzen met 40% omlaag doet dat er meer dan 2 keer zoveel mensen hun producten kopen. Dan draai je 120% afzet en als je dan alsnog 20% in R&D stopt, heb je dezelfde winst en dat is te investeren in meer productie. Er komt gewoon veel meer geld binnen en je hebt tevreden klanten. Nu gaat AMD zich wel achter de oren krabben want met zo'n beleid kun je Nvidia flink aftroeven en dwing je Nvidia min of meer te concurreren. Flinke winst afdingen zorgt ervoor dat het aantal afnemers zal dalen. Snap je
RTX 4090;1;0.4872293472290039;3090 TI daalde toch ook insane? nieuws: Nvidia verlaagt prijzen RTX 3090 Ti, RTX 3090 en RTX 3080 Ti in Europa
RTX 4090;1;0.5326505303382874;Hele andere situatie, de vraag kakte compleet in nadat de prijzen door veel vraag en weinig aanbod opgeblazen waren..
RTX 4090;3;0.3602420687675476;Het zou geweldig zijn als dat nu ook met de RTX 4080 en RTX 4090 zou gebeuren. Al lijkt het me sterk dat ze dat gaan doen met die hoge inflatie…
RTX 4090;3;0.4803854525089264;Ik zie het verschil niet zo met 1 maand terug.. Maar goed, time will tell
RTX 4090;5;0.45359039306640625;"En niet te vergeten onze ""eigen"" @Foritain > Vind ik toch ook wel een vermelding waard!"
RTX 4090;1;0.3978974223136902;Het maken van dat filmpje heeft al €100 aan stroom gekost! Ik heb nu een 3080ti die al flink is ge-undervolt net als mijn 5950x. En nog schrik ik soms van het verbruik na een flinke sessie gamen. Wie had ooit gedacht dat stroomverbruik en de kosten die daar aan hangen zo'n grote factor zouden spelen? In mijn geval wacht ik wel op een nieuwere generatie die hopelijk meer winst boekt qua efficiëntie dan qua prestaties. Want net als @jimh307 hieronder ergens zegt zijn de prestaties niet echt nodig. Zelfs niet voor de meeste VR headsets en games.
RTX 4090;4;0.2927435636520386;"Volgens Hardware Unboxed, de 4090 is veel ""efficiënter"" dan de 3090 TI."
RTX 4090;5;0.37939924001693726;Juist, omdat de kaart maximaal zoveel power kan vreten wil nog niet zeggen dat die dat ook altijd doet.
RTX 4090;3;0.4152259826660156;Ik ben eigenlijk wel benieuwd hoeveel DLSS 3.0 scheelt als je een spel limiteert op 60 Hz. Dan zou je verwachten dat de video kaart minder hoeft te doen als normaal omdat hij dan maar 30 echte frames hoeft te genereren en 30 DLSS 3.0 frames
RTX 4090;2;0.38234707713127136;Ruim 450 watt verbruik is echt wel flink hoor.. alleen al voor de gpu. Kan te gek ook, dure grap
RTX 4090;2;0.4662214517593384;Maar dat is dus op volle toeren, doorsnee verbruik ligt natuurlijk veel lager. Maargoed, als je al deze kaart wilt kopen dan maak je je waarschijnlijk ook echt niet zo druk over de stroomkosten, en een hobby mag best wat kosten. Voor mij vind ik dit al zo'n 1500 euro teveel als dat ik maximaal wil betalen voor ern GPU.
RTX 4090;2;0.34841278195381165;Een hobby zou niet ten koste mogen gaan van de levenskwaliteit van toekomstige generaties. Wat mij betreft wordt een streep getrokken: wettelijk max 100 Watt voor een videokaart.
RTX 4090;2;0.5275425314903259;Ligt eraan hoe het stroom wordt opgewekt. De een koopt een jetski of motor de ander gamed. Als we alle hobbies(bijna elke hobby is milieu belastend) opzij zitten voor toekomstige generaties wordt het leven saai. Is er ook niets voor om te leven. En ja de kaarten mogen een stuk zuiniger. Maar dat komt wel met de tijd. De eerste autos waren ook niet zuinig.
RTX 4090;2;0.35624516010284424;Nou, laten we dan meteen maar formule 1 en andere races mee kappen en ook concerten etc. Want ook dat zijn allemaal grote stroomvreters/vervuilers. Ook maar bijwonen van voetbalwedstrijden afschaffen. Ofwel dat beetje extra stroom wordt de levenskwaliteit van toekomstige generaties niet minder op. Hell, juist deze nieuwe chips zorgen dat ze er beter op worden want de technology gaat zo tenminste hard vooruit.
RTX 4090;2;0.573782205581665;Ik denk dat we sowieso te veel vasthouden aan alles houden zoals het was, in het licht van de klimaatproblemen. Laten we het ook hebben over vliegtuigen die leeg vliegen in verband met de slots. Er zijn op heel veel vlakken maatregelen te nemen. Dat dat met vliegtuigen nog niet gebeurd is, betekent niet dat videokaartvermogen limiteren geen bijdrage levert.
RTX 4090;1;0.5442375540733337;Ja, dat van leegvliegen ivm slots is echt te krankzinnig voor woorden, dat is gewoon onnodige kapitaalvernietiging naast onnodig milieuvervuilend. Maar waarom zou je een videokaartvermogen limiteren? Er zijn gewoon use case scenario's waarbij die extra kracht gewoon 'nodig' is en zonder dat extra vermogen is dat niet haalbaar, op dit moment. Niemand verplicht je die kaarten te kopen, en in principe wordt een relatief groot deel van de energie inmiddels gewoon 'schoon' geproduceerd, probleem zit em vooral in de kartelvorming/prijsafspraken, want waarom is electriciteit nog steeds gekoppeld aan gas?
RTX 4090;3;0.5181148648262024;Goed verhaal, helaas zijn we in Nederland kampioen in het selectief winkelen naar CO2 reductie. Een beperking van energieverbruik met computeren/gamen zou voor de hand liggen, immers betekent het vaak veel gebruik CO2 productie. In de winter kom je er misschien nog een beetje mee weg ivm dat het huis deels wordt verwarmd. In de zomer staat regelmatig aan de ene kant een PC te draaien en aan de andere kant de airconditioning. Dat is te gek voor woorden. Hoe problematisch is het voor de gemiddelde computer enthousiast als de focus zich verplaatst van hoge prestaties naar hogere efficiëntie? Ik denk nauwelijks.
RTX 4090;5;0.4074605107307434;40ct/uur blijft goedkoper dan een GameState.
RTX 4090;1;0.2515706717967987;en water uit de kraan blijft ook goedkoper dan spa blauw op een terras.
RTX 4090;1;0.45850345492362976;"Mijn ""40ct/uur"" is ""Spa blauw"", jij haalt er azijn erbij Je had gelijk als ik de 4090 had afgekraakt."
RTX 4090;4;0.2668676972389221;Je dient ook te berekenen hoelang het duurt om je nieuwe kaart terug te verdienen in verbruik als je zo denkt, spoiler: dan koop je heen nieuwe kaart...
RTX 4090;1;0.3733999729156494;Ik zocht op hun eigen website maar kon die nog niet vinden helaas.
RTX 4090;1;0.36506912112236023;ah, die 2 namen had ik nog niet in mijn hoofd verbonden.
RTX 4090;5;0.2602563202381134;EposVox
RTX 4090;3;0.2596018314361572;Hierbij nog het filmpje van Paul
RTX 4090;3;0.5422529578208923;Toch wel enigszins teleurstellend. Met 50% meer cores en een kloksnelheid die dik 33% hoger ligt had ik grotere verschillen verwacht met de 3090Ti. Geeft ook maar weer aan dat kloksnelheden verhogen maar een marginale verbetering geeft tegenover meer cores.
RTX 4090;2;0.47339507937431335;50% meer cores hoeft zeker niet meteen 50% meer prestaties te betekenen. Dat ligt aan of men een korte of lange pijplijn heeft qua core opbouw, kortere pijplijn betekend hogere kloksnelheid, maar ook minder werk verricht per klok-tik, lange pijplijn, lagere klok, maar meer werk per tik. Dat zijn allemaal keuzes die men maakt om de beste performance per Euro en/of Watt te behalen. Dat is net zoiets als bij auto motoren, het ene ontwerp haalt +150pk uit een 1L motor met hoge toeren, een ander ontwerp haalt net 100pk uit een 2L, er zijn dus een hoop verschillen qua ontwerp en ontwerp doelen.
RTX 4090;2;0.4703260362148285;Absoluut, daar ben ik van op de hoogte, maar gaf zojuist hierboven dit ook al aan: Als we nu kijken naar Ampere vs Lovelace, dan zien we dat het verschil tussen de 3070 en 3090Ti (circa 97% krachtiger) ongeveer 60% is qua performance. Dat is Ampere vs Ampere. Bij Ampere vs Lovelace is het verschil met de 4090 (circa 106% krachtiger tegenover de 3090Ti) lager dan Ampere vs Ampere, namelijk rond de 50%. Dat is wat mij betreft teleurstellend.
RTX 4090;2;0.47052666544914246;Waar baseer je die 97% op? PF32 performance? Ruwe rekenkracht =/= gamingprestaties. Anders was een Vega 64 in staat geweest om (even buiten scenario's waarin 8GB VRAM een beperking vormt) een 2080Ti te verslaan en zou Ampère het verpletterend moeten winnen van RDNA2. In realiteit is een Vega 64 vergelijkbaar met een 1080 en zijn Ampère en RDNA2 aan elkaar gewaagd, en wint RDNA2 het zowel op prijs per frame en energieverbruik per frame. Gamen draait niet om brute force rekenwerk maar om een hele zooi onvoorspelbare berekeningen die altijd tot inefficiënt gebruik van aanwezige rekenkracht zal leiden. Door die inefficiëncy te verminderen kun je een groter of kleiner deel van de aanwezige rekenkracht tot uitdrukking zien komen in gameprestaties, en er bestaan daarbij enorme verschillen tussen ongeveer even snelle GPU's in compute performance en ook voorbeelden waarbij qua compute performance vergelijkbare GPU's enorm veel verschillen in gameprestaties.
RTX 4090;3;0.39014774560928345;Dat is het verschil op papier tussen de 3070 en 3090Ti. Met de nadruk op 'op papier'. Dat weet ik, hiermee geef je eigenlijk precies aan wat ik bedoel. Vega 64 en Radeon VII waren immers de koningen van de teraflops, terwijl nieuwere GPU's, die op papier veel minder krachtig waren deze GPU's, veel beter presteerden. Nu is de vergelijking AMD vs NVidia niet helemaal eerlijk, omdat er een groot verschil in architectuur zit. Het punt is dat we bijvoorbeeld in het verleden een 2070 kaart zagen qua specificaties onder zou moeten doen voor de 1080, maar wél beter presteerde. Nu zien we juist het tegenovergestelde, althans, dat doen deze benchmarks vermoeden. Het is nog even afwachten hoe de wat mindere krachtigere kaarten presteren (die minder met bottlenecks te maken zullen hebben), maar het lijkt er op dat Ampere 100% naar Ampere 200% procentueel een grotere prestatieverbetering laat zien dan Ampere 100% naar Lovelace 200%. Als Ampere 100% naar Ampere 200% een 60% prestatieverbetering laat zien, dan verwacht ik van de nieuwste architectuur dat die prestatieverbetering hoger is dan die 60% tegenover de oude architectuur. Die sprong is de afgelopen generaties bijna altijd gemaakt. Enkel Turing naar Ampere is wat lastig te beoordelen omdat ze van FP + INT naar FP + INT/FP zijn gegaan.
RTX 4090;1;0.38036179542541504;Even een vraag, want ik snap wel wat je bedoelt maar dit zit echt op de grens van m'n begrip. Wordt in games veel gebruik gemaakt van integers ipv. floating point vergeleken met compute workloads?
RTX 4090;3;0.37933486700057983;Als we naar Turing > Ampere kijken dan lijkt het er op dat de FP/INT shading units niet voor extreem veel extra prestatie zorgen, althans, niet zoveel als de genoemde TFLOPs doen vermoeden. Omdat games niet altijd de gehele core kunnen voor FP kunnen gebruiken. Of Integer veel wordt gebruikt hangt af van de game engine en wordt gebruikt om bepaalde taken die voorheen richting de CPU gingen op conto komen van de GPU. Volgens mij was het zo dat wanneer er INT berekend wordt, de hele core cluster geen FP kan berekenen. Het is dan ook een beetje een farce dat Nvidia destijds deze shading units heeft meegerekend bij de opgegeven FP32 Teraflops, om zo de kaarten er beter laten uit te zien dan ze waren. Pascal bijvoorbeeld een architectuur waarbij alle shading units FP/INT waren en dus konden 'flippen', bij Turing zijn ze juist afzonderlijk gemaakt en vanaf Ampere zijn er naast de dedicated FP shading units nu weer de 'flippende' FP/INT SU's toegevoegd. Je kan de kaarten wel enigszins vergelijken omdat sommige Ampere kaarten 2x zoveel shading units hebben dan Turing kaarten. Zo is heeft 2080 2944 FP shading units en 2944 INT, en de 3070 heeft 2944 FP en 2944 FP/INT. De 3070 presteert ongeveer 25% beter dan de 2080. Uiteraard is het verschil per game anders. De vraag is of die prestatieverbetering vooral komt vanuit de extra beschikbare shading units of vanwege de prestaties per instructie zijn verbeterd. Die 25% komt namelijk ook behoorlijk overeen met Pascal naar Turing.
RTX 4090;1;0.6184923052787781;Sorry wat? Heb je überhaupt wel naar de resultaten gekeken? We hebben het op 4k over een ruime 50% verbetering. In welke wereld is dat teleurstellend?
RTX 4090;2;0.528998851776123;Omdat het verschil op papier een stuk groter lijkt dan het daadwerkelijk is. En natuurlijk is dat geen verrassing, een verdubbeling qua performance was nooit reëel, maar in het verleden zag je bijvoorbeeld bij Pascal naar Turing dat de Turing kaarten die op papier zwakker waren dan de Pascal kaarten alsnog een stuk beter presteerden. Als we nu kijken naar Ampere vs Lovelace, dan zien we dat het verschil tussen de 3070 en 3090Ti (circa 97% krachtiger) ongeveer 60% is qua performance. Dat is Ampere vs Ampere. Bij Ampere vs Lovelace is het verschil met de 4090 (circa 106% krachtiger tegenover de 3090Ti) lager dan Ampere vs Ampere, namelijk rond de 50%. Dat is wat mij betreft teleurstellend.
RTX 4090;3;0.4094255268573761;Het kan zijn dat spellen en drivers nog meer geoptimaliseerd moeten worden om al die cores aan het werk te zetten.
RTX 4090;3;0.5721980929374695;Schaalbaarheid van GPU-cores/prestatie in games zal wel steeds beter worden, omdat multi chip modules de norm gaan worden en je dus steeds minder bijkomende kosten hebt om rekenkracht uit te breiden. Je zit dan alleen met de langere latency die het gebruik van een chiplet interconnect, veroorzaakt.
RTX 4090;3;0.5378029346466064;Nou dat ben ik wel met hem eens. 52% meer cores en een zo'n 35% hogere kloksnelheid en dan toch maar 50% hogere prestaties vind ik wel teleurstellend. Dan had je wel 100% mogen verwachten
RTX 4090;2;0.5198894143104553;Het vergroten van de rekencapaciteit middels meer cores en hogere kloksnelheden is een goed begin om de prestaties te vergroten. Helaas ontstaan er dan vanzelf flessenhalzen op andere vlakken. Zo zie je dat in sommige omstandigheden de extra rekenkracht volledig tot uiting komt en ontstaat er in andere situaties een cpu-bottleneck.
RTX 4090;1;0.6182306408882141;Zo werkt het niet, aangezien de cores ook anders in elkaar zitten. Lovelace is geen 2.0 versie van Ampère, zoals bijvoorbeeld RDNA2 dat wél is van RDNA.
RTX 4090;2;0.5395121574401855;Je denkt in veel te simpele termen over cores en kloksnelheden. Een auto met een 2x zo grote motor gaat ook niet 2x zo snel. De theoretische maximale performance (in FLOPS) is wel 2x zo hoog als een 3090 Ti, maar de architectuur is anders. Je kunt niet simpelweg dezelfde architectuur pakken en de kloksnelheid wat opvoeren, want je zit met fysieke limitaties. Om een snellere GPU te maken moet de hele architectuur op de schop. Denk aan de interne functies van de cores, gpu cache, maar ook het aantal threads per core, de werking van de thread scheduler, thread blocks, memory sharing tussen threads, noem maar op. Er komen zoveel variabelen bij kijken... En dan hebben we het nog niet eens gehad over mogelijke bottlenecks aan de CPU-kant. Ik vind 50% extra performance heel indrukwekkend, en allesbehalve teleurstellend!
RTX 4090;5;0.42257779836654663;grappig. Hoe jullie die berekeningen maken.
RTX 4090;3;0.5319740772247314;Het valt me op dat de resultaten in de testen van Tweakers.net aardig achterblijven bij die van andere testers. Kijk maar eens de video van LTT. Daar zie je veel grotere verschillen in de resultaten. In sommige gevallen (zonder RTX) bijvoorbeeld 4x de performance van de 3090TI in sommige testen. Ik vraag me dus een beetje af of tweakers.net wel de testen optimaal heeft uitgevoerd, of dat er ergens iets mis is gegaan. Wellicht dat een andere CPU al andere resultaten geeft.
RTX 4090;2;0.3928024470806122;Het is al vaker bewezen in gaming benchmarks dat een cpu met heel veel cores averechts werkt bij games. Tweakers zou voor gaming een 5800X3D moeten gebruiken, of 12900K.
RTX 4090;5;0.4878436326980591;Ben ik met je eens En ik mis ook MSFS2020, (ook al is dit meer cpu gebaseerd), maar ik ben toch zeer benieuwd wat de prestaties zijn van de 4090 icm 5800X3D
RTX 4090;3;0.49565330147743225;Mij vielen die verschillen ook al op. Tweakers mag ray tracing wel aan zetten wat mij betreft. Het is niet echt ultra settings als je selectief een setting weg laat
RTX 4090;3;0.4965614378452301;Ray tracing is echter typisch een setting die je los wilt testen, omdat het in veel spellen niet aanwezig is en je niet goed met AMD en Intel kunt vergelijken. Ik zou ze alletwee testen, met en zonder
RTX 4090;3;0.2732832729816437;Ik ben het met je eens wanneer je over mid range kaarten praat, een deel van de gebruikers zal RT uit schakelen in dat segment. RT aan/uit tests zijn in die context zinvol. Maar moderne, mega dure, high-end kaarten moeten er gewoon mee overweg kunnen. Geen excuus meer. Ik zou de Radeon RX 7000 serie er in elk geval op af rekenen als ze wederom falen op dat vlak. Aan de andere kant zou ik serieus een RX 7800/7900 overwegen als ze wél mee kunnen dit keer en goed geprijsd zijn.
RTX 4090;1;0.39631351828575134;Ik vermoed dat je verwijst naar de Cyberpunk 2077 benchmark van LTT? Ze hebben ondertussen aangegeven dat er daar iets is misgelopen. Door problemen met de testbench werd FidelityFX Upscaling automatisch aangeschakeld, wat ervoor zorgt dat de resultaten te hoog zijn. Zie de pinned comment op hun video.
RTX 4090;2;0.4095977544784546;zelden is een op papier 50 % snellere gpu ook bij alle games 50 % sneller zal zijn. de games die ze testen zijn nooit voor deze kaart bestemd. in deze gpu ' s zitten tensor cores en ray tracing cores. voordat je dat in game benut moet je wachten tot er een update komt of tot nieuwe spel uitkomt. dat de chip sneller is bestaat geen twijfel over. dat zie je aan de renderbenchmarks. bij octanebench ( die die cores wel benut ) zie je een toename van 83 % en bij blender 3. 3 lts 61 % en ook bij f1 2022 zie je een toenalme van + 67 % fps indien je raytracing inschakelt. sommige mensen vinden raytracing maar niets maar het maak het beeld wel mooier. al moet ik toegeven dat in de huidige games waar raytracing achteraf is toegevoegd de meerwaarde beperkt is. het ziet er allemaal wat fake uit. alsof alles moet reflecteren. een spel waar raytracing helemaal tot uiting komt vraagt meer ontwikkeltijd omdat je elk elment moet bewerken om het realistisch te maken. nvidia doet dat natuurlijk heel goed in de demo ' s. je voelt echt het materiaal. menselijke huid is bijvoorbeeld complex omdat licht door de eerste huidlaag gaat. door gewoon uw 3d model door een raytracing engine te draaien bereik je niets. nvidia heeft bijvoorbeeld rtx remix uitgebracht. om modders te helpen oudere games nieuw leven in te blazen. ai gaat zelfs objecten herkennen, textures upgraden, bumb maps met ai een gokje wagen welk materiaal het is. ( katoen, papier, staal.. ) misschien bestaan deze tools achter de schermen ook voor games studio ' s die eerst een low def wereld willen ontwikkelen om daarna de wereld te upgraden. geen idee... het potentieel van het te verwachten visueel spektakel is wel gigantisch. de vraag is enkel hoe snel games gaan omschakelen. het goede nieuws is dat je fps hit kan je met dlss bijna volledig wegwerken. je kan teleurgesteld zijn maar ik zie wel een hele nieuwe wereld opengaan. sommige willen enkel meer fps maar ik wil een mooier beeld.
RTX 4090;3;0.3008725345134735;Nvidia komt waarschijnlijk volgend jaar met een RTX 4090 Ti, die zal dan naar verwachting meer dan 50% sneller zijn dan de RTX 3090 Ti.
RTX 4090;3;0.2844354510307312;Kan ook nog aan de cpus liggen helemaal met een 5950x(begrijp me niet verkeerd) een 5950x is een leuke cpu maar had toch liever een 12900k gezien in de test. En als ik het vergelijk met andere test systemen en reviews dan zie ik altijd een verschil met Tweakers. (Ik neem Tweakers dan ook al een tijdje niet meer serieus in testen) Maar met een eventuele cpu upgrade in de toekomst is het mogelijk dat je meer uit de 4090 gaat halen en dat de verdubbeling dan wel geldt. Want een cpu kan nog steeds een bottleneck vormen op welke resolutie dan ook, betreft efficiëntie berekeningen.(Architectuur)
RTX 4090;5;0.7846249938011169;Een 5800X3D zul je bedoelen. Dát is namelijk de krachtigste momenteel verkrijgbare gaming CPU.
RTX 4090;3;0.3650893568992615;Erbij was een toegevoegde waarde geweest het had zeker iets meer gezegd.
RTX 4090;3;0.3490920066833496;Ik bedacht me net ook dat je ook niet echt anders kunt of je moet alle concurrerende GPU's overnieuw testen. Je wilt toch appels met appels vergelijken.
RTX 4090;2;0.4896301329135895;Prima, maar waarom doet Techspot dit dan wel en Tweakers niet, ik snap je punt. Maar de gpu's zijn nu zo bloedje snel dat een snelle cpu dan ook wel mag, desnoods moet je dan helemaal opnieuw beginnen het zij zo. Een 5950x kan je toch niet serieus nemen( terwijl een 12900k hem al eruit performed) en dan gaan ze ook nog eens een 4000 serie kaart erop benchen wat gewoon zonde van de tijd is geweest. Want je wilt immers de bottleneck zoveel mogelijk beperken en dat is hun keuze dus juist niet geweest.
RTX 4090;2;0.3479948341846466;Als iemand die zich bezig houdt met GPGPU werk kan ik je vertellen dat het aantal cores maar 1 van de factoren is die de performance bepalen. Ik ben op dit moment aan het proberen om een algoritme te optimalizeren (samen met collega's) welk op een RTX5000 een performance heeft van 1x. Als ik hetzelfde algoritme run op een A100 (een kaart met meer dan 3x de cores) dan heeft die daar een performance van 0.98x. Er is dus een andere bottleneck dan het aantal cores!
RTX 4090;3;0.38035932183265686;Voor productivity workloads is dit een zinnige upgrade... bijna 40% kortere rendertijden in apps als blender... sign me up! Want renderen - hoe snel het ook gaat op mijn 3080ti - blijft in het geval van animaties een erg lange activiteit. Ik heb inmiddels ook een punt bereik waarop mijn VRAM in blender vaak tegen de limiet aanloopt. (realtime raytracing previews in de viewport icm high res textures en adaptive subdivision vreten als snel je vram op). Dus die 24 gig is ook welkom. Voor games is het echter niet bepaald de upgrade waar je op zit te wachten imho.
RTX 4090;3;0.434666246175766;Waarom niet? Dit is de eerste kaart waarbij 4k 144hz gaming een realistisch scenario is. Lijkt mij prachtig op een grote OLED monitor 42/48 inch ofzo. Je betaald er wel de volle mep voor nu, of dat het waard is hangt van je budget af. Op 4k 144 Hz kun je niet beter ontspannen dan op een 1080p 60hz. Maar het is wél mooier.
RTX 4090;3;0.4237385094165802;"Ik vraag me af of die DLSS 3 framerate verdubbelaar nu wel echt gaat werken. Als het extra frame wordt geinterpoleerd door het vorige en het volgende frame te combineren, dan introduceer je extra latency, wat me onwenselijk lijkt. Als je het extra frame probeert te schatten door bewegingen ""voort te zetten"" vanuit het vorige gerenderde frame, dan kan je rare artifacten krijgen bij voorwerpen die continue van richting veranderen (zoals shooters). Eigenlijk wil je dus WEL de CPU werkzaamheden hebben gedaan (en dus kwa tijd wat verder in het spel zitten dan het vorige frame) en dus objecten hebben op nieuwe locaties, maar tegelijkertijd wil je NIET een volle rendercyclus doen, maar iets slims doen met vorige frame. Ik heb nog geen reviews gelezen die specifiek op dit punt induiken. Daarnaast moet je game wel DLSS 3 ondersteunen, wat nog best een tijd gaat duren. Los van DLSS 3, zie je wel een flinke vooruitgang in de rauwe performance van de kaart."
RTX 4090;4;0.43711695075035095;DLSS3 introduceert zeker extra latency, dit is dan ook meteen de reden waarom Reflex automatisch geactiveerd wordt zodra je DLSS3 aanzet. Digital Foundry benoemde het al in hun preview video. Je kunt DLSS3 redelijk goed vergelijken met het renderen in SLI/AFR. Inplaats van GPU1->GPU2->GPU1 is het nu GPU1->Interpolatie -> GPU1. Hopelijk zullen reviewers binnenkort testen hoe effectief Reflex is in het tegengaan van de extra latency.
RTX 4090;1;0.4788450598716736;Er wordt nieuwe data bij verzonnen. Hoe fancy het algoritme dat dat doet ook is, er is geen garantie dat die pixels ook kloppen. De vraag is alleen of je het verschil gaat zien. Denk het eigenlijk niet.
RTX 4090;1;0.7134602069854736;Volgens Techpowerup zijn er met bepaalde settings artifacts te zien in screenshots maar zijn deze absoluut niet te zien in het bewegende beeld. En al helemaal niet als als je ook nog eens gefocust bent op je spel. Oftwel, als gebruiker ga je er niets van merken dat je DLSS 3 gebruikt. Althans, daar lijkt het op nu.
RTX 4090;3;0.48908084630966187;Volgensmij zie ik het dan juist alleen maar sneller. Maar goed, eerst zien, dan geloven
RTX 4090;5;0.6600351333618164;Je hersenen zijn enorm goed in het wegfilteren van informatie die niet kan kloppen. Daarnaast ook niet vergeten dat als je framerate hoger is dan de refreshrate van je scherm je ook niet eens elk frame op je scherm gaat krijgen.
RTX 4090;2;0.4449751377105713;Screen tearing, ghosting en andere artefacten zijn voor mij ook aardig duidelijk te zien, afhankelijk van de inplementatie van DLSS 3.0 kan dat net zo erg zijn. Als ik al 100 fps heb dan zou het ook niet echt meer nodig zijn om DLSS 3.0 aan te zetten, bij 60 of zelfs 30 fps is die essentie er dan meer, maar dan zou ik nog wel onder mn 165 Hz kunnen blijven. Bovendien weet je ook niet of je dan juist de mooie, of juist de slechte frames te zien krijgt
RTX 4090;2;0.2946964204311371;"In de spiderman voorbeelden in de video van digital foundry zag het er voor mij uit als ""480p"" artefacten op de randen tussen snel bewegende voorgrond en langzaam bewegende achtergrond. Let op, dat is dus een heel subjectieve ervaring."
RTX 4090;5;0.3817693889141083;Daar ben ik dus ook heel benieuwd na. Lijkt mij echt de ultieme marketing tool om je product te verkopen maar is het ook echt zo goed als native? komende weken/ maanden zal daar wel veel onderzoek naar gedaan worden. Maar ik blijf nog even sceptisch.
RTX 4090;3;0.43433624505996704;Ik vraag het mij af, ik speel liever native dan dlss 2.0 quality mode. Verschil is echt wel merkbaar.
RTX 4090;3;0.3196393847465515;Kan je na gaan 3.0 doet nog meer trucjes om een hogere FPS te behalen.
RTX 4090;3;0.47089138627052307;Ligt vooral aan het spel dan dlss zelf. Er zijn zat spellen waarbij het daadwerkelijk fantastisch werkt en er zijn slellen waar je artifacts zoals flickering krijgt en blurred textures. Bijvoorbeeld heeft RDR2 met nieuwe dlss een rare flickering en over sharpness. Zodra je aangepaste dlss dll gebruikt waar betere configuraties in staan is dit een verschil van dag en nacht.
RTX 4090;3;0.3309376537799835;+1 Native Rendering FTW!!! Ik wil dan ook niks weten van al die upscaling trucjes, maar er zijn helaas teveel mensen die daar zo verliefd op zijn dat je meteen als een weirdo in een hoekje wordt geduwd als je dat zegt...
RTX 4090;4;0.4979925751686096;Digital Foundry heeft precies waar je naar refereert een goede technische video. Bij DLSS 3.0 is ook Nvidia Reflex verplicht en zie je dat de latency eigenlijk niet toeneemt.
RTX 4090;1;0.611487865447998;Tweakers, waarom testen jullie de RTX 4090 met zo een oude CPU? Dat is in 1440 en zelfs 4k nu al een bottleneck! Test het dan minimaal met een 5800 X3D of een 12900K of 7950x. Ik begrijp dit niet helemaal. Want op deze manier toon je totaal niet de kracht van de 4090. Want die is nog beter met een goede CPU. Want de CPU is nu heel snel een grote botleneck met zoveel power. Een grote gemiste kans en eigenlijk is hierdoor de review niet correct en eigenlijk moet je die zelfs negeren. Heel erg jammer!
RTX 4090;3;0.27774614095687866;De RTX 4090 is een echte 4K-gamingkaart en in deze review heb ik de focus vrijwel volledig op die resolutie gelegd, uiteraard met daaronder ook 1440p en 1080p-resultaten weergegeven. Als ik even onze review van de Ryzen 9 7950X erbij pak en kijk naar de resultaten van twee games die we zowel op 1080p als op 1440p testen in cpu-reviews (Metro Exodus en Red Dead Redemption 2), dan is goed te zien dat op 1440p Ultra er amper verschillen zitten tussen de processors, laat staan dat dat op 4K Ultra het geval is. Nu zijn die cpu-tests met een RX 6950 XT uitgevoerd, en uiteraard is de RTX 4090 nog een stuk sneller, maar als we de RTX 4090-resultaten van deze review naast de 1080p-resultaten van de 7950X-review houden, dan toont de cpu-review hogere framerates in die games. Dat betekent dat onze Ryzen 9 5950X, die voor gpu-tests bovendien overgeklokt is én van sneller geheugen is voorzien, niet de bottleneck vormt in de door ons geteste games op 4K. Zoals @rsnubje al zegt gaan we binnenkort naar nieuwe gpu-testplatformen kijken om te waarborgen dat dit in de toekomst ook zo blijft.
RTX 4090;3;0.4749833643436432;Laat maar eens zien dat een gpu op 1440p (met name ultra) en op 4K een bottleneck heeft van een cpu(welke overigens op 4.65GHz draait en daarmee prima mee komt). Gaat wel wat ver om te zeggen de review maar meteen te negeren, beetje lomp. Maar goed, de tijd was al krap genoeg met al die launches achter elkaar. Wanneer Intel's 13th gen uit is, gaan we uitgebreid kijken welk platform de beste keuze is om in de toekomst GPU's mee te testen.
RTX 4090;1;0.5337867736816406;Kijk maart eens naar de review van hardwareunboxed op YouTube. Ze geven aan dat zelfs de 5800 X3D op 1440 vaak al CPU limited is en laten dat zien met data! De CPU kan gewoon de GPU niet meer bijhouden en daarom moet je gewoon de snelste CPU erin zetten! Deze review heeft op sommige vlakken gewoon meer dan 15% verschil met FPS op 1440! Dat is op zijn zacht gezegd gewoon niet goed. Let maar op daar gaan binnenkort veel meer review over uitkomen over de CPU bottleneck omdat deze nieuwe 4090 dus zo belachelijk snel is. Je komt gewoon niet weg met een 5950X met een kleine overclock terwijl zowel dee 12900K als de 7950X en de 5800X 3D gewoon een 10% gemiddeld sneller zijn dan deze CPU. Je gaat toch ook niet een Bugatti Veyron testen met banden die tot 300 km/h kunnen ipv de speciale die tot zo een 500 km/h kunnen en dan zeggen dat de top 300 is omdat we niet sneller kunnen door de banden. Dat is een beetje wat Tweakers hier nu doet. Je mag toch verwachten van een Review waarin zo een achterlijk snelle kaart wordt gebruikt je zorgt dat je alle performance die erin zit er ook daadwerkelijk uithaalt? Wat heeft het dan voor zin om tabellen met prestaties erin te zetten die op sommige vlakken behoorlijk afwijken met de concurrentie? Naar mijn mening is deze test daarom gewoon niet valide en heb ik niet eens meer de moeite genomen om alles door te lezen nadat ik heb gezien wat ze als testverantwoording gebruiken. Ik denk dat vele mensen hier het erg onderschatten de kracht van een CPU om extra performance te krijgen. van gebruikers kan je dat verwachten maar niet van tech sites. Kijk ook op Paul'sHardware op Youtube. Er staan een redelijk aantal spellen erbij die vooral op 1440 ongeveer 10% meer FPS hebben dan op Tweakers in combinatie met een 7950X. Als jij 10% geen verschil vind dan weet ik het ook niet meer. Dan is gewoon enorm.
RTX 4090;2;0.2778075933456421;Games 1 op 1 vergelijken tussen andere reviewers zou ik sowieso mee uitkijken, maar dat terzijde. Ondanks dat de 12900K al wat ouder is, is de 7950X dat niet en kwam die uit terwijl we al gpu's aan het hertesten waren. Daarnaast, voor de echte upgrade naar een nieuw platform voor gpu's wilden we op Intel's 13th gen wachten om een goed gewogen afweging te maken, ipv nu direct op wat nieuws springen (of ouds zoals de 5800X3D, maar die is locked dus valt imo af). Je mag daarom ook verwachten dat de andere 40 serie kaarten gewoon op een nieuw platform getest zullen worden en volgt er tegelijkertijd dus ook een hertest van de meeste kaarten die nu ook getest zijn. We gaan hoe dan ook kijken met welke processor we de beste resultaten krijgen, hoogstwaarschijnlijk icm een mooie overklok. Overigens is die 10% die je noemt wel heel game afhankelijk en als je met een 4090 alleen maar op WQHD wilt gamen kun je beter een goedkopere kaart nemen imo.
RTX 4090;2;0.3430160582065582;het vergelijken tussen andere reviews is juist een goede stap om een eerlijk overzicht te krijgen. dat de 5800x locked is maakt toch niets uit? die is veel sneller dan de 5950x op 4. 65 all core. dat is toch geen valide reden om die niet mee te nemen? en op 26 - 9 - 22 kwam de 7950x uit samen met jullie review dat is ruim 2 weken om met de nieuwe kaarten te testen. ik begrijp dat het niet altijd makkelijk is om alles opnieuw te testen maar de 12900k en 12900ks zijn al een tijd uit en zijn ook gewoon een stuk sneller dan de 5950x. kom op tweakers dat is een ruim 2 jaar oude cpu. in tijden dat alles wat 1 maand jong is oud is in de computer wereld is dat gewoon wederom geen valide reden. voor mijn part zetten jullie dan een extra teest met een van deze cpu ' s erbij in de review waarin je aantoont wat het verschil tussen de oude en nieuwe cpu ' s doen voor zulke zware kaarten. dus echt niet enkel op 1440 of 1080. en of veel mensen dan wel of niet gebruik daarvan maken doet er mijn inziens niet toe. meten is weten. en als je half meet weet je dus half. ik kijk en kom graag op tweakers al heel veel jaren. en vroeger waren jullie een van de meest betrouwbare sites. dat is nu gewoon niet meer zo. en daarom is het altijd goed om naar veel meer sites te kijken. dan krijg je een algehele indruk over iets. ik zeg niet dat hier bewust iets wordt gedaan om cijfers anders te presenteren. maar het is gewoon een gemiste kans. als iemand een dure rtx 4090 koopt zorgt degene ook dat die de beste cpu, een goede case met airflow heeft een een goede voeding. je mag dus verwachten dat tweakers weet wat de verschillen zijn. een gebruiker hieronder beweert zelfs dat een 5e gen i7 geen bottleneck heeft op 1440 met een 3080ti... dan zit je er toch wel flink naast. jammer dat mensen dan gaan minnen omdat ze zelf blijkbaar niet goed weten hoe het echt is.
RTX 4090;3;0.4038723111152649;Je kan nog wel hele lappen tekst schrijven, maar ik heb mijn uitleg gedaan. Daar laat ik het bij. Ik begrijp echt wel het concept van een bottleneck en welke cpu er wel en niet goed is, maar het heeft weinig zin om daar nu op door te beuken.
RTX 4090;1;0.34934788942337036;"Ik hoop wel wanneer jullie de AIB modellen testen welke ik begrijp jullie einde deze week te verwachten je dan hopelijk 1 van deze cpu's ook gebruikt om deze als ""extra"" erbij te zetten mits jullie met de oude 5950X blijven testen. Dat lijkt mij niet zoveel moeite en je hoeft dan niet alle GPU's opnieuw te testen. Wanneer stappen jullie over op een nieuwe CPU? Bij uitkomen vaan de 13900K of Ryzen 7950 X 3D die ook gaat komen? Gewoon uit nieuwsgierigheid."
RTX 4090;3;0.3175021708011627;De komende tests zullen nog op de 5950X zijn, maar dat is allemaal 4K Ultra, dus ik betwijfel of dat uit gaat maken. Ik weet niet of er in deze korte tijd, tijd is om een extra cpu te doen. De Ryzen 7000 3D is nog onbekend wanneer die gaat komen, dus hebben we besloten daar niet op te wachten. Het zal dan eerder gaan om een opgevoerde 7950X of 13900K.
RTX 4090;2;0.46631231904029846;De 7950X en 13900K gaan sowieso sneller zijn met een OC dan de 5800X3D, want dat is de 7950X op stock al praktisch. En aangezien we een testplatform voor een langere tijd willen gebruiken, moet dat gewoon goed zijn en niet op stel op sprong een andere even snel, omdat de 4090 misschien bottleneckt op een resolutie waarvoor ie niet bedoelt is.
RTX 4090;1;0.5290279388427734;DF stelt dat je de kaart niet moet gebruiken op 1080 of 1440p aangezien je dan tegen cpu bottlenecks aanloopt en de frametimes stuk gaan. 4 of 8k is de enige use case voor dit ding.
RTX 4090;1;0.3423156440258026;En geef ze eens ongelijk Dat is natuurlijk ook waar je zo'n kaart voor koopt, of dikke sim setups met 3 schermen.
RTX 4090;2;0.3664570748806;3 x 1920x1200 oftewel 5760x1200 is inderdaad veel leuker dan zo'n saai enkel Ultra HD scherm en al dat HiDPI scaling gedoe!
RTX 4090;3;0.4334803521633148;3x WQHD 144Hz hier Maar 3090 trekt al die sims nog prima, omdat de meeste behoorlijk oud zijn, dus grafisch niet zwaar. Wellicht dat Motorsport 8 daar verandering in brengt.
RTX 4090;5;0.4564419090747833;NICE!!! Dan heb je inderdaad wat meer powerrr nodig!
RTX 4090;2;0.5127195715904236;Volgens mij schat je de benodigde CPU kracht echt vele malen te hoog in. Heb zelf in mijn oude PC (8 jaar oud, i7-5820K CPU) de 3080Ti gehangen toen ik hem binnen kreeg om te testen of het werkte, had zelfs met die CPU nog geen bottleneck in Cyberpunk op ultrawide 1440p en was het nog steeds de GPU met de bottleneck. Moet wel eerlijk zeggen dat die situatie voor de CPU kritisch werd met 93% verbruik op 6/12 cores/threads. Zit nu op een i7-12800K met die 3080Ti en die CPU doet zo goed als niets in welke game dan ook. Dus met een CPU die toevallig net 1 gereratie oud is ga je echt geen bottleneck mee tegenkomen.
RTX 4090;2;0.4006440341472626;Gegarandeerd als je nu een 5800 X3D, 12900K of 7950X erin zet met dezelfde videokaart je minimaal 20% meer fps haalt op deze resolutie die je aangeeft. Je zit er echt helemaal naast! Je weet toch dat CPU bottleneck op 1080 nagenoeg altijd aanwezig is en 1440 tegenwoordig ook heel erg aanwezig is en zelfs al op 4K. Kijk bv naar hardwareunboxed waar ze meerdere malen al aangeven dat op 1440 de 5800 X 3D die ze gebruiken de bottleneck is Ook bij cyberpunk 2077 is de 4090 al weer een CPU bottleneck op 1440. Veel mensen hebben dit blijkbaar nog niet in de gaten. Er zijn veel spellen die zowel CPU als GPU goed benutten en daarom is het belangrijk om van beide de beste en snelste te hebben. Met je oude I7 laat je belachelijk veel FPS liggen met je 3080 TI! Zet je kaart maar eens in een systeem met de juiste CPU dan ga je dat echt zien! Kijk ook even op Paul's Hardware op YouTube vooral op 1440P testen met een aantal dezelfde games en ultra instellingen haalt de 4090 met een 7950X ongeveer 10% meer FPS op dezelfde instelling. Dat is gewoon belachelijk veel en ik vind het niet acceptabel om met een oude CPU als de 5950X zo een kaart te testen. In meerder games is het verschil dus 10% soms zelfs meer!
RTX 4090;1;0.41339826583862305;Nu wachten op kaarten die eigenlijk dezelfde performance hebben tegen een schappelijke prijs. Zoals rond de 800-ish. Inplaats van de gestoorde prijs die nVidia nu durft erop te plaatsen.
RTX 4090;1;0.6200367212295532;Ik weet nog dat ik 1000 gulden heb neergelegd voor een Geforce 2 GTS heb neergelegd. Dat vond ik toen al een krankzinnig bedrag. /opaverteld
RTX 4090;1;0.5795170068740845;En tel daar nu eens alle inflatie bij? Dan mag je er op 22 jaar al direct 70% bijtellen. Ik heb destijds een Gefore4 Ti 4600 gekocht voor 500 euro, dat zou vandaag bijna 800 euro zijn om deze een jaar (en 2 koelers) later te vervangen voor een Gefore FX 5950 die me 600 euro kostte, of vandaag al meer dan 900 als ik zou corrigeren voor inflatie.
RTX 4090;1;0.4447602927684784;Wow, ik wist niet dat die kaarten zo duur waren. Eigenlijk zijn gpu's niet eens zo extreem veel duurder geworden terwijl de prestatie sprong dit keer vrij hoog is.. (die FX kaarten waren ook nog eens zeer matig in DX9 oid, herinner ik me nog)
RTX 4090;2;0.5457736253738403;Alles word duurder. Het valt nog te bezien of andere fabrikanten significant goedkoper gaan zijn. Tot nu toe heeft AMD iedere prijsverhoging van Nvidia gevolgd en zitten ze er hooguit een paar procent onder. Maar deze prijzen voorspellen alvast niet veel goeds voor de goedkopere modellen.
RTX 4090;1;0.5049338340759277;Ze zullen zien dat verkoopcijfers drastisch zullen dalen, onontkoombaar. Er zijn grenzen.
RTX 4090;2;0.3258015513420105;Laten we het hopen, maar ik ben sceptisch aangezien er genoeg mensen waren die gewoon van de scalpers kochten tegen 3keer de prijs zoals 2000 ah 3000 euro.
RTX 4090;2;0.42283472418785095;Maar toen betaalden mensen een derde per maand aan gas en elektra. Zowel voor miners als gamers niet onbelangrijk.
RTX 4090;2;0.4396124482154846;Niet overal gebruiken ze gas, en niet overal is het zo duur geworden als in Nederland. In de rest van de wereld zullen er nog genoeg zijn die die kaartjes gaan kopen
RTX 4090;3;0.3320760130882263;Klopt, al denk ik wel dat iemand die 2000 euro aan een gpu uitgeeft ook bereid is wat meer stroomkosten te dragen voor zijn of haar hobby.
RTX 4090;2;0.41134533286094666;Tijdens de pandemie was er een enorme vraag naar entertainment en apparatuur voor thuiswerken, en je had ook nog crypto GPU mining. Het kan eigenlijk niet anders dan dat de verkoopcijfers drastisch gaan dalen.
RTX 4090;2;0.30616921186447144;Ik denk dat jij je daar gruwelijk in zult vergissen, mensen betaalde ver over de 2000 euro om zo'n 3090 te bemachtigen, en deze is dus goedkoper maar bijna dubbel zo snel. Ik verwacht dat de 4090 zeer goed zal verkopen, is voor veel contentcreators (rendering etc) zeer interessant, die kosten hebben ze er behoorlijk snel uit.
RTX 4090;3;0.3194791376590729;Adviesprijzen: GeForce GTX 980Ti ( 2015 ): ~€ 750,- GeForce GTX 1080Ti ( 2017 ): ~€ 825,- ( +10% ) GeForce RTX 2080Ti ( 2018 ): ~€ 1259,- ( +53% ) GeForce RTX 3090Ti ( 2022 ): ~€ 2250,- ( +79% ) GeForce RTX 3090 ( 2020 ): ~€ 1549,- GeForce RTX 4090 ( 2022 ): ~€ 1959,- ( +26% ) Ik mag toch hopen dat onze inflatiecijfers niet dergelijke proporties aan gaat nemen…
RTX 4090;2;0.4025745093822479;De inflatie (en waarde van de euro) helpt zeker niet mee, maar dit is een heel ander beest dan de 980 Ti wat gewoon een high-end gaming kaart was. Deze zijn meer voor creatieve doeleinden bedoelt. Daarnaast lijkt chipfabricage momenteel steeds duurder te worden. Ik ben echt benieuwd waar AMD straks mee komt qua concurrentie voor een echte gaming kaart zoals de 4060.
RTX 4090;2;0.4661049246788025;Je hebt flinke infaltie over de dollar, dan super inflatie in de EU ook NL en diepte punt in euro vs dollar. Die stap van modex 240p naar 480p vga was zeer grote stap naar 600p 3dfx sli addon ook 720p was al stuk beter maar 1080p was stuk gangbaar. En met 1440p heb je al duidelijk te maken met deminishing return. 4k is vaak de monitor te klein om volledig van de resolutie baat bij te hebben en met 8K vs 4k is de meerwaarde nihil tot subtiel tot zeer grote monitors waar je wel wat aan kan hebben. Voor goede hoge FPS heb je daarbij de nieuwste displayport standaard voor nodig en dat zal alleen RDNA3 bieden. Al zal ik 1st gaan voor grote 144hz game monitor 4K 48” en minder dan 1 meter afstand. 32” op ruim meter is te klein. Die idiote prijs is stack up van meerdere factoren. Duur N4x x huge-die x $inflatie x NL-infatie x $:eur x marge x hoge bom x hoge koeling. Als er geen marge is dan heeft het ook geen zin om te produceren dan sluit je busnes. Zoals nu hier met BAkkers gaat . Voor de oorlog zou vs alleen inflatie hebben een sterke euro dan zat je op 1600 euro ipv 2000 ook nog veel maar ja die hardware is duurder te produceren en daar komt die marge over.
RTX 4090;3;0.43690767884254456;Helaas heb je wel gelijk ja. Ik blijf gewoon hangen op mijn 2070S voorlopig. Tot er wat meer Sanity in de markt terug keert. En anders dan tja, geef ik mijn droom van 3D artist maar wederom op. En ga ik dan maar wat anders doen.
RTX 4090;3;0.561532735824585;Je hebt niet zo'n hele dikke GPU nodig voor 3D art. Als je met een path tracer wilt renderen zal het zeker helpen maar anders heb je aan zo'n 2070S ruim voldoende.
RTX 4090;3;0.5706079006195068;Voor nu heb ik ruim voldoende ja aan mijn 2070S ook all kan ik dan wel niet echt 4K textures gebruiken. Wat ik eigenlijk wel wil doen.
RTX 4090;2;0.4300563931465149;Dat wel, maar dit geef ik echt niet uit aan een grafische kaart. Je kan voor de prijs iets van 5 verschillende consoles kopen, samen! Ik koop deze kaart simpelweg niet, omdat ik het niet normaal vind, net als een telefoon niet boven de 800 komt hier. Ik heb geen mega prestaties nodig om leuk te kunnen spelen, ook geen 4K, dus kan prima af met veel goedkoper.
RTX 4090;2;0.5254319906234741;Console is niche product die hardware dump prijs heeft. PC koop je de hardware met marge dus 1,5 x console prijs. Er is geen APU voor PC met zware igpu specifiek voor gamen. Je hebt ook niet zeer snel systeem geheugen en direct store is daar optie en geen standaard spec. Met apparte midrange g-kaart zit je ook met hogere BOM kosten van meerdere PCB met als voordeel dat je zelf bepaald of meer xobxS of x of pcie5.0 ssd of pro supper pro of meteen console nextgen of nog gen verder aan performance. Enige voordeel van consoles is stuk minder last van cheaters. PC gamen meer de mooie open werelden en slow pace exploring avonduurlijk gameplay. Waar 60fps zat is. Bij concoles meer competatieve online shooter lijkt mij 120hz support wel belangrijker en gfx boeid dan niet zo.
RTX 4090;1;0.45481377840042114;We kunnen ook gewoon niet kopen. Dat doet meestal wel wat met een prijs.
RTX 4090;3;0.2927318513393402;Dan koopt een ander hem wel.
RTX 4090;3;0.30850034952163696;"Met ""we"" bedoelt hij waarschijnlijk ook ""een ander""."
RTX 4090;1;0.2676580548286438;Zo vaak geroepen de afgelopen jaren, maar de kudde blijft de kudde dus nu zitten we nog heel lang aan deze idiote prijzen vast...
RTX 4090;5;0.2561729848384857;3090 had ook die prijs, 3080 was 800-850 bij release
RTX 4090;1;0.5270258188247681;Mijn 3080 in NL was 719 euro op dag van de release.
RTX 4090;2;0.3378027379512787;"800 euro voor deze performance? Dat wordt dan de 6060 Ti. De 5070 Ti zal nog iets duurder zijn. Op één of andere manier was de prijs niet zo'n groot probleem toen NVidia de topmodellen nog als ""Titan"" verkocht. De rename is vooral bedoeld om de 4080 ""goedkoop in vergelijking"" te laten lijken. En daar trappen nog steeds erg veel consumenten in. Zelfs een 4070 is nog een prima upgrade ten opzichte van de 2080."
RTX 4090;4;0.3529958426952362;Als je het prima vindt om te accepteren dat er tegen die tijd ook een 5090 of 6090 is die het nóg beter doet, is dat een prima strategie natuurlijk. Goeie tip voor tweakers met het hebben-syndroom in dure tijden: koop iets van vorig jaar en lees alleen de reacties onder de aankondiging van dat product
RTX 4090;1;0.34671270847320557;Het grappige is dat 800 euro een paar generaties geleden de 'gestoorde prijs' was.
RTX 4090;1;0.5689340233802795;Dat schijnt dus enorm mee te vallen, een Gefore FX 5950 kostte in 2003 klaarblijkelijk ongeveer 600 euro. (vooruit gerekend naar 2022 is dat veel meer dan 800 euro)
RTX 4090;2;0.36759379506111145;Vooruit gerekend naar nu geeft wellicht een vertekend plaatje, we live in interesting times... Laten we 2020 doen, a 2,1% gemiddelde inflatie kom je dan uit op ~850 euro. Zeker niet 'veel meer dan 800'
RTX 4090;2;0.34156715869903564;Doe de 10% (of meer) inflatie van dit jaar er bij en je komt boven de 900 euro. Hoe het ook zij, 800 is in elk geval geen uitzonderlijke prijs als we de historische prijzen er bij pakken. 2000 euro vind ik overigens wél een uitzonderlijke prijs. (Maar goed als ze die niet meer verhogen dan valt dat met deze inflatie (14,4% in september) en wisselkoers binnen de kortste keren ook mee.)
RTX 4090;1;0.49547338485717773;Dat ligt er dus helemaal aan hoe ver je terugkijkt. We hebben net geconstateerd dat het topmodel in 2003 850-900 inflatie gecorrigeerde euro's kostte. De Titan X uit 2015 kostte 1000 euro en dat was destijds echt buitensporig. Inmiddels zitten we dus op het dubbele in 7 jaar tijd (en dan reken ik buiten de 4090ti die vast nog volgt). Er is lang niet genoeg inflatie geweest om dat goed te praten, ook niet in de afgelopen 9 maanden.
RTX 4090;2;0.46741098165512085;Uiteraard ligt het er aan hoe je het bekijkt, dat geldt voor bijna alle dingen in het leven. Nu is het wel zo dat de performance sprong dit keer ongekend is en de wisselkoers gooit, bovenop de inflatie, nog wat roet in het eten. Maar goed ik geef je geen ongelijk voor wat betreft het uitlopen van de prijzen in zijn algemeenheid. Persoonlijk denk ik er over om een 4080 te nemen en zelfs die is behoorlijk aan de prijs. Helaas kunnen we 'm hier niet voor $899 kopen zoals in de Verenigde Staten.
RTX 4090;1;0.49640506505966187;Nou eigenlijk niet die prijzen hier en nu zit de inflatie op maar lonen werken vertraagd gemiddeld dus is net maatje extremer. 2022 prijzen maar je loon is nog niet inflatie gecompenseerd. Sommigen totaal niet die bokkensprong nada gecompenseerd. Het is helaas hoe de gang van zaken gaan.
RTX 4090;1;0.8088274002075195;Is nog steeds gestoord - inflatie of niet... een videokaart van meer dan 500 euro vind ik al nergers meer over gaan
RTX 4090;1;0.583807647228241;Maar je kan helaas niet zeggen “inflatie of niet”. Inflatie is nu eenmaal inflatie. We gaan richting de 14% dit jaar en 2021 was ook al niet mals.
RTX 4090;1;0.5071422457695007;"""De RTX 4090 verbruikt tijdens belasting bijna 470W"" is dat alleen de RTX 4090, of met de rest van het systeem er bij? Want een Playstation 5 verbruikt +/- 220W. Dus als het alleen de RTX 4090 zou zijn dan is het echt een extreem hoog stroomgebruik."
RTX 4090;1;0.42342105507850647;Dat is enkel de videokaart zelf. We meten op de PEG en PCIe slot.
RTX 4090;3;0.5614733099937439;En is dat piek of average? Bij derbauer komen ze average een stuk lager uit. Zoals bij battlefield 2042 op 329W average. Daarnaast is het energieverbruik met DLSS 3.0 ook lager. (Je rendert op lagere resolutie). Dus helaas vind ik 470W verbruik dmv 1 game en daar een hele videokaart mee reviewen iets te kort door de bocht.
RTX 4090;3;0.4640865623950958;Dat is gemiddeld. Sterker nog, een mediaan. En wat je zegt over kort door de bocht, dan geldt dat toch ook voor Der8auer met 1 game, die blijkbaar de kaart niet goed genoeg belast.
RTX 4090;3;0.2524728775024414;Er wordt naast Battlefield 2142 ook Cyberpunk 2077, Justice en A Plaque Tale getest. Cyberpunk 2077 - 420W en met DLSS 330W Justice - 373W en met DLSS 339W A Plaque Tale - 389 en met DLSS 369W Wat ik dan bedoelde te zeggen is dat het dan daar is om te zeggen dat toevallig in Metro het verbruik hoog is dan te concluderen dat de hele kaart veel stroom verbruikt. Daarnaast zou je in een test de frames kunnen locken op een vast aantal FPS zoals bijvoorbeeld 80fps en dan meten hoeveel power ze gebruiken. Dan kan je verschillende kaarten ook tegen elkaar afzetten.
RTX 4090;1;0.6249719262123108;Dus de enige zware game die hij getest heeft, is Cyberpunk en die verbruikt dus 420W net als zijn timespy test. Ik wil ook even doorverwijzen naar deze reactie anders gaat dadelijk alles 2x. rsnubje in 'Nvidia GeForce RTX 4090 - Olifant in de porseleinkast'
RTX 4090;3;0.39105597138404846;Het schijnt zo te zijn dat ze eerst een target hadden van 600 Watt, en dat is bijgesteld naar beneden. Gelukkig. Maar een volledige PS5 vergelijken met een 4090 is best lastig. Je kunt ook zeggen dat een Nintendo Switch een heeel stuk zuiniger is dan een PS5, en een acceptabele game performance tevoorschijn tovert. Maar da's echt wel appels met peren vergelijken.
RTX 4090;5;0.3844021260738373;Ja natuurlijk is het niet hetzelfde. En met een PC kan je meer dan met een console, is een veelgehoord argument. Maar als het om gamen gaat dan is het met deze prijzen en stroomverbruik in een toch al dure tijd alleen maar aantrekkelijker geworden om console gamer te worden. En ik bedoel niet te klagen en ik ben ook niet zielig, heb een PS5 en een RTX 3080. Maar op deze manier is de lol van PC hardware en het upgraden zoals ik dat in de tijd van Hardware.info deed, toch wel echt definitief voorbij.
RTX 4090;1;0.3978326916694641;Moet wel bij genoemd worden dat wanneer men bijvoorbeeld Cyberpunk speelt met een framerate cap op 90, op 4K ultra settings, deze zelfde 4090 toch echt iets van 40% minder stroom verbruikt dan een 3090ti op gelijkwaardige settings. Net nog in een review voorbij zien komen. Dat een kaart zoveel kán trekken, wil niet zeggen dat je dat moet toelaten in elke mogelijke game of applicatie. Dus ondanks dat het inderdaad behoorlijk belachelijk kan worden, in benchmarks of wanneer álles open getrokken wordt.... hoeft dat natuurlijk niet.
RTX 4090;4;0.39046862721443176;Daar heb je een goed punt. Nu ben ik ook gelijk benieuwd of een cap op 45 of 60fps en dan met DLSS 3.0 dat oprekken naar 90fps zuiniger of minder zuinigen gaat zijn. Interessant aspect
RTX 4090;3;0.44582605361938477;Dat is ook al het geval met DLSS 2.0. Native 60 fps in Spiderman doet gemiddeld 280 watt op mijn 3080 ti. Dlss quality 60 fps in Spiderman doet gemiddeld 235 watt op mijn 3080 ti. Sowieso cap ik veel spellen op 60 fps (4k) want bij single player games vind ik dat prima en bijkomend voordeel is dat de kaart een stuk minder herrie maakt en minder stroom verbruikt.
RTX 4090;2;0.3441486656665802;Maar realistisch gezien, welk percentage gebruikers gaat dat doen? Gemiddelde gamer gaat geen 2000 euro uitgeven, om het op beestje op halve kracht te laten werken natuurlijk....
RTX 4090;1;0.5824704170227051;Sowieso geeft een gemiddelde gamer geen 2000 euro uit aan een videokaart. Gemiddelde gamer en enthousiastelingen zijn twee verschillende groepen consumenten
RTX 4090;1;0.4133663475513458;Vooruit, je hebt gelijk: gemiddelde gamer gaat mensen gaan geen 2000 euro uitgeven, om het op beestje op halve kracht te laten werken natuurlijk....
RTX 4090;4;0.5283040404319763;Inderdaad, goed punt. M'n zoontje speelde een tijdje wat lichtere games en de videokaart stond toch een beetje te blazen in de woonkamer. Ik heb toen geen FPS cap ingesteld, maar een 'stroomcap' in MSI Afterburner. Gewoon de TDP op 70% a 80% gezet bij een RTX 3080 en alles bleef lekker stil. Beetje zonde ergens van de performance, zeker bij zo'n RTX 4090 van 2000 euro. Echter, als het geld je niet veel uitmaakt, zijn de prestaties per watt wel heel goed. Je kan er dan ook een zeer goed presterende ietwat zuinige kaart van maken...
RTX 4090;1;0.3057238459587097;Kies dan voor een undervolt, heb mijn 3080 Ti op 1820 MHz op 785mV draaien. Voluit wordt die nu max 60 graden wat voorheen rustig naar de 82 graden ging.
RTX 4090;3;0.4333517849445343;Je krijgt er natuurlijk wel veel meer performance voor terug dan in een PS5. Als je de performance aanpast van de 4090 krijg je waarschijnlijk dezelfde performance/framerate met minder verbruik dan de PS5.
RTX 4090;2;0.3996839225292206;Mwha lijkt me niet. Ps5 is totaal 220 watt. Gpu, mobo, cpu, disks, fans etc gebruiken onder lage load al wel meer dan 220 watt
RTX 4090;4;0.43422460556030273;Dat is prima te bereiken met een pc. Wanneer je undervolt en met power limits gaat werken hoeft dat zeker geen probleem te zijn.
RTX 4090;2;0.40000176429748535;Een PlayStation 5 heeft, volgens Digital Foundry, het equivalent van een gpu die zich ergens tussen de RTX 2060 en de RTX 2070 bevindt. Dat is met de beste wil in de verste verte niet te vergelijken met een RTX 4090. Sterker nog, het ding is zo ontzettend veel krachtiger dat het me niets zou verbazen dat het per frame (inclusief pc) zelfs zuiniger is dan de ps5. Daar kopen we verder niets voor natuurlijk.
RTX 4090;3;0.4760880172252655;Leuke kaart, maar voor 98% absoluut niet interessant. Afgezien van de enorm hoge prijs is het vebruik ook extreem hoog (net als de RTX 3090 Ti trouwens). Het formaat van de kaart zorgt ervoor dat je goed moet opletten wat voor behuizing je hebt, want ik de meeste kleinere kasten zal die niet eens passen. Afgezien van de hoge aanschafprijs is het idle verbruik ook behorlijk aan de hoge kant, met een steeds hoger idle vebrruik van zowel cpu's als gpu's mag je zelfs als entousiatseling wel oppassen met de huidige electra prijzen wat voor componenten je aanschaft. Ik kijk al lange tijd niet meer naar grafische kaarten, maar de prijs vind ik echt hoog. En ja iedereen kan wel roepen over de invlatie en andere redenen verzinnen waarom, maar zelfs voor een vlaggenschip en wetende dat er over aantal maanden een Ti versie komt, is dit toch best schrikken. De prestaties zijn daarenegen best indrukwekkend. Wel ben ik erg benieuwd naar de mid en low range kaarten. Voor mensen die een computer inclusief electrische verwarming zoeken komt deze kaart natuurlijk als geroepen.
RTX 4090;1;0.39451614022254944;Precies wat je zegt, deze specifieke kaart is voor 98% van de gamers niet interessant om te kopen. Net zoals een dure auto voor de meeste mensen niet interessant is. Toch is er een markt voor beide én Nvidia kan roepen dat zij de snelste zijn dus ‘koop een nvidia!’
RTX 4090;3;0.3494347333908081;Het idle verbruik is inderdaad niet laag, maar nog wel lager dan kaarten van enkele generaties terug. Tijdje terug opgezocht wat mijn vorige kaart, een GTX970, idle verbruikt, en daar zit deze 4090 nog altijd een heel stuk onder. Die verbruikte meer dan 70W idle.
RTX 4090;2;0.2527201473712921;Deze kaart zal op max 250 watt en underclocked en undervolted vermoedelijk nog steeds enorm goed presteren, beter dan een 3090 of 3080 op die settings. Dus staar je niet blind op de max power. Je bent niet verplicht deze kaart op 450 watt te draaien
RTX 4090;3;0.3580429255962372;Er zijn al wel video's te vinden die dit voor je berekend hebben en met een 280-300Watt heb je inderdaad nog steeds een hele goede performance. Maar laten we eerlijk zijn je koop niet zo'n kaart om deze maar op 60% power te gebruiken ook al krijg je daar 80% van de prestaties mee. Wat nog steeds behoorlijk indrukwekkend is hoor, daar gaat het niet om. Maar de prijs is ook extreem hoog. Daarnaast heb je niets aan een kaart als deze als de rest van de hardware er ook niet op afgestemd is. Dat betekend een high-end cpu en moederbord, snel geheugen, etc en ook een 4k monitor met hoge refreshrate. Alles wat dus betekend dat het hele verbruik van de pc sowieso veel hoger ligt, want al die componenten gebruiken in verhouding meer stroom, ook al zijn ze misschien wel efficienter.
RTX 4090;2;0.37491756677627563;Enkel op lagere resolutie heb je een snellere cpu nodig.. niet op 4k
RTX 4090;4;0.3651588261127472;Iemand die een GPU als de RTX 4090 koopt doet dit niet enkel en alleen maar voor gamen. Toch zijn er games die zowel cpu als gpu bound zijn en waar een betere cpu wel degelijk een grote invloed heeft, vooral als je raytracing wilt gebruiken. Als je andere taken gaat uitvoeren met een kaart als de RTX 4090 en die wil je optimaal benutten dan heb je ook een high-end cpu nodig. Voor alleen al het verschil in prijs tussen de 3090 Ti en de 4090 kun je al een high end cpu aanschaffen.
RTX 4090;3;0.27560678124427795;Ik draai zelf een 3090ti op 250 watt. En ja daar koop ik wel zo'n kaart voor.
RTX 4090;2;0.38027307391166687;Dus je gaat rond de €2000,- betalen voor een top of the line kaart en hem dan underclocken en zodoende minder performance krijgt ? Me dunkt dat degene die deze kaart kopen, zich geen moer om de elektra rekening interesseert.
RTX 4090;1;0.560270369052887;Ik heb een 3090 ti waar ik destijds 1500 voor heb betaald en ik heb deze underclocked en undervolted en op max 250 watt gezet.
RTX 4090;2;0.44666650891304016;Ik mis de metingen met betrekking tot het geluidsniveau? @rsnubje ? Heb ervaring met de 3090 FE en die was indrukwekkend goed. De 4090 FE is nog dikker met grotere fans, maar verbruikt ook meer. Nogmaals, ik mis het hoofdstuk over geluidsdruk..
RTX 4090;2;0.47922778129577637;Wilden we eigenlijk nog doen, maar dadelijk bij de AIB's gaan we hem er ook mee vergelijken. Voor deze review niet echt meer gelukt.
RTX 4090;3;0.3499515950679779;Wanneer is de verwachting dat de test met de AIB's beschikbaar is?
RTX 4090;4;0.30233898758888245;Het doel is einde van de week.
RTX 4090;5;0.48121753334999084;Thanks! Houd ik de knip morgen nog even stevig dicht..
RTX 4090;5;0.3482845425605774;Proshop heeft al wat exemplaren beschikbaar zag ik. Je kunt nu al bestellen en uitlevering is dan vanaf morgen (duurt dan nog wel even voordat het product in NL is is een clubje in Scandinavië)
RTX 4090;3;0.4379798471927643;Die paar dagen wachten kunnen er nog wel af. Deze FE uitvoering ziet er prachtig uit en presteert ook prima qua geluidsdruk en koeling. Maar ik wacht geduldig ook de AIB's af die de komende dagen beschikbaar komen in reviews.
RTX 4090;3;0.5101696848869324;Hij verbruikt dus MINDER in realiteit als de 3090TI en heeft betere koeling. Kortom hij is stiller.
RTX 4090;3;0.4652618169784546;Misschien ben ik een leek maar kan een videokaart niet op gegeven moment te groot worden voor een moederbord? Lijkt me lastig te koelen ook.
RTX 4090;2;0.39546769857406616;Nee hoor, een video kaart kan nooit te groot worden voor het moederbord. Wel natuurlijk de kast waar hij in moet. Je hebt alleen restricties in je X en Y as maar de Z as van je ruimte kan volledig benut worden. Daarnaast is dit probleem alleen aanwezig voor de flagship kaarten, de 3060 en waarschijnlijk de 4060 zijn helemaal niet zo groot en kunnen toch behoorlijk wat !
RTX 4090;3;0.4821445941925049;Als je de 4090 met een itx moederbord gebruikt is de verhouding wel gek. De uitbreidingskaart is dan vele malen groter en zwaarder dan het moederbord... maar technisch geen enkel probleem mits de videokaart goed is vastgezet. Misschien als je dat niet doet dat dan het PCI-E slot afbreekt oid
RTX 4090;3;0.35450223088264465;Is het een bewuste keuze om de prestaties per watt alleen voor metro exodus te meten ?
RTX 4090;3;0.5782590508460999;Ja, omdat we anders in meerdere games stroom zouden moeten testen. Metro is onze game waarmee we stressen voor het stroomverbruik. Over het algemeen werkt dat overigens prima en is de game geloof ik redelijk representatief.
RTX 4090;3;0.39766398072242737;DerBauer is het daar niet helemaal mee eens. Die haalt in veel games niet eens de 400w. En heeft ook meteen getest hoeveel het uitmaakt als je de power target aanpast. Wat blijkt: bij 50% powertarget heeft de kaart de beste performance per watt (0.42fps/W in Time Spy Extreme, vs 0.15fps/W voor de 3090ti) terwijl bij 60% je in game iets van 5% fps verliest, terwijl je 20-35% minder stroom verbruikt (afhankelijk van de game).
RTX 4090;3;0.41086530685424805;Tja, dan zijn we het daarover oneens. Dit is toch echt wat we zien bij goede belasting. [EDIT] net even de video van Der8auer bekeken en die haalt op max load gewoon 422W en in Cyberpunk is de 330W op 100% PL die hij meet met DLSS, dus natuurlijk gaat het verbruik dan omlaag, want dan hoeft die kaart niet zo hard te werken.
RTX 4090;4;0.5545194745063782;Leuke video review! Zat veel enthousiasme in. Vind het mooi dat deze man echt gepassioneerd is over GPU's en dat zonder dat het overdreven over komt. Conclusie is imo gewoon dat dit het beste van het beste is op dit moment, de vraag is of je het er financieel voor over hebt. Ik ga nog even door met mijn MSI 3080. Scheelt dat ik vooral 1440P game en daar de framerate krijg die ik wil hebben. Ik hoop dat de 5000 serie de performance heeft van de 4000 serie of liefst nog wat sneller maar vooral dat er dan technieken zijn waardoor de 5000 serie zuiniger is. Deze 4090 verbruikt mij teveel, hij zal dus ook goed gekoeld moeten worden en dat kan je dus ook horen in veeleisende games. Ik wil mooi uitziende games, tegelijkertijd moet mijn PC nog enigszins stil blijven. Hierin ben ik benieuwd wat de aftermarket modellen van de 4000 serie laten zien. Zou MSI hem weer zowel stil als koel kunnen krijgen?
RTX 4090;2;0.4596160054206848;Er is altijd nog de optie te undervolten en/of de stroomtoevoer te dempen. Dat zal een flink effect hebben op het verbruik.
RTX 4090;3;0.3042219579219818;Inderdaad. Ik ben benieuwd hoe de 4090 underclocked en undervolted op 250 watt presteert tov de 3090 op 250 watt. Mijn 3090 is op die setting echt een stuk efficienter en krachter dan een 3080 op dat wattage.
RTX 4090;2;0.3775438368320465;Als de framerate zo enorm toeneemt, dan begin ik mij af te vragen of DLSS 3.0 nog wel dezelfde beeldkwaliteit levert als DLSS 2.0. Hebben jullie nog een vergelijking gedaan?
RTX 4090;2;0.2639262080192566;Het antwoord is sowieso nee. DLSS 3.0 is een combinatie van DLSS 2.1 met een verbeterde vorm van frame interpolation zoals gevonden op iedere TV van de afgelopen 10 jaar. Frame interpolation komt altijd met beeldfouten omdat d.m.v. algoritme tussenbeelden worden ingeschat en bijgetekend. Dit kan nooit foutloos en gaat altijd gepaard met beeldfouten.
RTX 4090;2;0.466218501329422;"Interpolatie gaat altijd gepaard met beeldfouten, maar een te oud frame laten staan is óók een beeldfout. En het is niet triviaal welke van de twee beeldfouten het meeste opvalt. Je visuele systeem is nogal complex, en dat is toch echt het enige wat telt. Die ""click-to-photon"" testen zijn geen echte vervanging van ""click-to-neuron""."
RTX 4090;5;0.4490198791027069;Ruim een 50% verbetering t.o.v. de vorige generatie, wanneer is de laatste keer dat we dat hebben gezien.
RTX 4090;5;0.34958508610725403;2080 super ti->3090 was 41% winst dus vergelijkbaar
RTX 4090;1;0.5779789090156555;Waarom is er voor gekozen om deze kaart niet te testen op FS2020? Heb een 3090ti, 12900K & 64GB DDR5 en FS2020 geeft gewoon doodleuk aan op hi-res dat m'n GPU de bottleneck is. FPS stort ook in met alles op max. Zou leuk zijn als een grafisch monster als FS2020 meegenomen zou worden door tweakers.
RTX 4090;3;0.28073155879974365;(onder 17. DLSS3)
RTX 4090;1;0.3933556079864502;Of eentje van X-Plane 12, die net uit is, en qua schoonheid niet onderdoet voor MSFS.
RTX 4090;3;0.47715574502944946;Ontzettend krachtige kaart! uber gaaf! Maar helaas is dit 'de kaart'' Dat ik toch effe ermee wacht, RTX 3090 is voorlopig wel prima voor mij voor aantal jaren, Aangezien dat we in de crysis zitten lijkt me niet zo handig om dat kaart mee te gamen met extreme hoger gebruik, Gelukkig heeft RTX 3000 serie ook prima goed werkende DLSS 2.0+ die ook onzettend goede toevoeging heeft en natuurlijk niet te vergeten Amd fsr 2.0, Dus ik en andere mensen kunnen lang uitzingen voor 2 generatie ofzo, totdat kaart betaaldbaar zijn en stroomverbruik ook acceptable zijn.
RTX 4090;1;0.6050765514373779;Ik ben überhaupt nog maar een paar games tegengekomen waar DLSS 2.0 geïmplementeerd is. Wat mij betreft is 2.0 al overrated laat staat 3.0
RTX 4090;3;0.4754706025123596;Grappig om te zien, de verkleining van procedes loopt vaak in steeds kleinere stappen. Maar zij hebben het in dit geval lineair lopen qua generaties, van 16 nm voor de 1080, naar 12, naar 8 en nu dus 4nm voor de 4090. Ik vind het gewoon bijna jammer dat de 5xxx serie dit mooie rijtje gaat breken want 0nm zit er gewoon niet in lijkt mij (of gaan we naar 0.49999 nm wat je dus naar 0 zou kunnen afronden 😅).
RTX 4090;2;0.3673674166202545;Procentueel groeit het dan weer wel exponentieel. En na nanometer gaan we weer door in picometer
RTX 4090;3;0.45709723234176636;"Dat zegt toch niet heel veel aangezien het aantal ""nanometer"" tegenwoordig vooral door de marketing-afdeling van chipbakkers wordt bepaald, en niet door de technische afdeling."
RTX 4090;3;0.37500494718551636;Ik ben vooral benieuwd als je deze kaart begrenst op 250W max wat dan het resultaat is. Ik denk dat je dan niet eens zoveel performance kwijt bent en hij wel een stuk stiller en koeler is. De prestaties zullen dan nog steeds beter zijn dan de 3090 TI vermoed ik. 250W is met maximale wat ik in mijn kast wil pompen en deze 4090 zou dan een erg leuke optie zijn. Alleen die prijzen.....
RTX 4090;5;0.2865567207336426;Zie de Der Bauer video op YouTube van vandaag. Er valt inderdaad heel veel te winnen op het vlak van energieverbruik, ten opzichte van minimaal performantieverlies. De titel van de video is dan ook zijn conclusie, en die klopt. Net als jouw vermoeden. De kaart kan makkelijk op 300W gezet worden en dan blaast 'ie nog de 3090 Ti eruit.
RTX 4090;5;0.42134642601013184;Dat is precies ook hoe ik mijn 3090 ti draai. Underclocked en undervolted op 250 watt. Super efficient. En mijn kast kan wel meer aan maar dat hoeft voor mij niet. Mensen staren zich teveel blind op de max power settings, terwijl je niet verplicht bent de kaart zo te gebruiken.
RTX 4090;3;0.4337051510810852;DLSS3 verdubbelt je latency voor dubbel de frames als basisregel. Ik kan me voorstellen dat dit net zoals ray tracing en DLSS niet een automatische altijd aan setting is voor iedereen. Zeker bij lagere framerates zou de hogere latency kwalijker kunnen zijn dan een gehalveerde framerate.
RTX 4090;3;0.5490564107894897;Dat zou goed kunnen. Ik ben ook benieuwd of er media outlets zijn die click-to-photon latency tests gaan doen. Lijkt mij wel een leuke technologie die dergelijke resultaten flink kunnen verzieken, aangezien je een 50% kans hebt (bij frame rate verdubbeling) dat je daadwerkelijk echt of nep beeld ziet. Motion vector interpolatie is leuk, maar dat kan alleen op basis van oude data. Als je een shooter hebt die een animatie afspeelt op jouw gun trigger click, dan start die animatie soms 1 frame te laat, en soms niet. Lijkt mij funest voor snelle oog-hand-coordinatie of simpelweg een stuttery/wazige gameplay experience als je op die details gaat letten. Misschien is het beter bij cinematic titles, zoals flight simulator..
RTX 4090;2;0.4476304352283478;enkele reviews gelezen, snap ik eigenlijk niet zo goed waarom sommige reviewers de stap in prestaties extra er uit lichten ten op zichte van andere generaties. de 2080ti is 50 % sneller dan de 1080ti @ 1440p ultra. de 3090 is 45 % sneller dan de 2080ti @ 4k ultra, en de 3090ti is zo ' n 50 - 60 % sneller dan de 2080ti @ 4k ultra. de 4090 is zo ' n 45 - 50 % sneller dan de 3090ti @ 4k ultra. die verbetering ligt compleet in lijn wat we de afgelopen generaties gezien hebben. het is zelfs nog zo dat een 3090ti > nu < ( daarmee reken ik dus inflatieproblematiek in mee ) voor zo ' n ~ 1300eur weg gaat. deze kaart kost 1950eur voor 50 % meer prestaties. 1300 + 50 % = 1950eur. we zien dus exact dezelfde fps / euro. mensen waren toen veel meer enthusiast over de 3080, en niet zo over de 3090, vanwege het prijskaartje. wellicht interessant voor andere kaarten.. maar deze kaart is al zo bloedsnel dat de framerates enkel maar boven de display standaarden uit stijgen. dp1. 4 met dsc is leuk, maar het is een optional onderdeel van de spec. een next - gen dp2. 0 tv zou er makkelijk voor kunnen kiezen dat te skippen, want dat is wel beetje tv - domijn eigen, en dan zou zo ' n overkill videokaart daar z ' n beeldsignaal niet op kwijt kunnen. evenmin als pcie5. het lijkt er op dat hedt echt overleden is op de desktop. 20 pcie lanes moeten we het mee doen. een videokaart die altijd 16 lanes op een oudere spec bezet moet houden voor optimale prestaties ( mits dat zo is ) is gewoon een gemiste kans.. extra m. 2 drives, hdmi capture card, enz. kunnen allemaal profiteren van pcie bifurcation bij de power user. daarnaast zou een entry - level sciencist of ml applicaties kunnen profiteren van de extra bandbreedte. voor dit geld zou je die feature wel verwachten.
RTX 4090;1;0.5266607403755188;Ik ga er vanuit dat AMD wél met PCIe5 komt in zijn volgende Radeons, het zou namelijk bijzonder slechte coördinatie zijn het wel in de processor te bouwen en dan zorgen dat er geen videokaarten zijn. Kortom, vooral een misser van Nvidia. Je kunt altijd twisten hoe nodig iets nu wel of niets is, maar zowel Intel als AMD hebben PCIe5 in hun processoren dus je had je planning erop kunnen afstemmen. En wat HEDT betreft... ik weet dat een HEDT-PC een wat hoger budget heeft, maar of de HEDT-koper nu 2 miel voor een videokaart neer wil leggen... die koopt toch liever een dikkere processor schat ik zo.
RTX 4090;1;0.24564428627490997;Er zullen altijd nog allerlei fora buiten GoT overblijven waarop je die dikke GPU ook i.c.m. die dikke HEDT CPU zal zien en dan ook nog eens inclusief een uitgebreide DIY waterkoeling set erbij ongeacht de prijs!
RTX 4090;1;0.7338342070579529;Wat ik vooral weer erg pijnlijk vind is het feit dat Nividia dus weet te presteren om hier geen DisplayPort 2.0 op te zetten. Waarmee je dus nu een beperking hebt op 4K @ 120hz.Terwijl de kaart in staat is in bepaalde games om meer te halen. Puur weer kosten besparing geweest om toch voor de DP 1.4 te gaan werken. Belachelijk op een kaart van bijna 2000 euro!
RTX 4090;1;0.5854412913322449;Dat is inderdaad een bizar feit! Gelukkig komen de RX 7000 series wel met port display 2.0 uit. Deze kaart heeft nu een max fps van 120 op 4K terwijl die dus vaker daaroverheen kan gaan. Dat is gewoon niet normaal en eigenlijk absurd dat het kan. Besparen op een kaart die 2K kost. Als dat geen trap tussen de k***en is wet ik het ook niet meer. Begrijp van die keuze niets. Ook het feit dat ze wederom PCI-E 4 gen zijn ipv 5. Ook als ik mij niet vergis komt AMD daarmee uit met de 7000 serie. Wat dat betreft is de 7000 serie van AMD veel meer futureproof. Er zijn 240 hz monitoren momenteel op 4K. Dus zulke monitoren kan je dan niet met je RTX 4090 gebruiken...lekker gedaan zeg!
RTX 4090;2;0.3514474928379059;@Trygve In de conclusie zeg je het volgende: Hoewel de efficiëntie van de snelste Ada Lovelace beter is dan die van de RTX 30-serie komt de kaart op dit vlak gelijk uit met AMD’s Radeon RX 6950 XT. Ik vraag mij af of dit wel klopt. In de videokaart BBG van afgelopen meimaand, staat ook de efficiëntie. Het enige RTX 30-model dat daar minder efficiënt is dan de RX 6950 XT (en dus ook de RTX 4090, want die scoort hier hetzelfde) is de 3090 Ti. Alle andere modellen uit de RTX 30-serie zijn daar wel degelijk efficiënter, en zeker de oorspronkelijke modellen uit het rode kamp (RX 6800 en 6600 om de meest efficiënte te noemen). Ik vind het ook vreemd dat de resultaten van de efficiëntie zoveel verschillen in nog geen half jaar tijd, kan je mij vertellen wat daar de oorzaak van kan zijn?
RTX 4090;4;0.6172879934310913;Goede vraag! Voor de GPU BBG van afgelopen mei hebben we als RX 6950 XT MSI's Gaming X Trio gebruikt, die we hadden geleend voor de initiële review aangezien we geen referentiekaart voor het reviewembargo konden bemachtigen. Dat model heeft een veel hogere power limit dan de 335W die AMD hanteert, wat helaas niet viel aan te passen met de persdriver. Inmiddels hebben we wel een referentie RX 6950 XT in handen, en die komt met zijn lagere/normale power limit dus veel gunstiger naar voren in de prestaties-per-watt.
RTX 4090;2;0.3701022267341614;1600 dollar in the us of a. 1900 hier. Het lelijke gezicht van wisselkoersen en geïmporteerde inflatie. Als de efficiëntie verder in de serie ook omhoog gaat heb ik wel oren naar een mid range model.
RTX 4090;3;0.2932451665401459;de wisselkoers is ongeveer 1, de btw is het verschil de us prijs is zonder
RTX 4090;4;0.6264168620109558;Erg indrukwekkend, 50% sneller op het zelfde vermogen en amper prijs verhoging (op msrp dan). Toch heb ik het gevoel dat de lagere kaarten (met stukken minder cuda cores) het echt niet zoveel beter doen dan de huidige kaarten, zeker met die enorm prijsverhoging...
RTX 4090;3;0.4578062891960144;"Indrukwekkende resultaten. Helaas het stroomverbruik even indrukwekkend, dus toch wat minder interessant. Heb in mijn huidige systeem een 750w voeding zitten voor mijn 5950x, maar die zou ik dan moeten vervangen; daar heb ik niet echt trek in."
RTX 4090;2;0.38340088725090027;Ik denk dat als je 2k voor een gpu over heb er best 150 bij kan voor een voeding. en zo niet gewoon vast zetten op 2300 mhz of 70% power budget.
RTX 4090;1;0.34641802310943604;Underclocked en undervolten en power van de gpu op max 250 watt zetten.
RTX 4090;2;0.3751063942909241;En dan vergeet die het belangrijkste te melden, dit ding past niet in de meeste kasten
RTX 4090;5;0.5173116326332092;Dat komt dan ook alleen maar omdat iedereen de laatste jaren van die kleine systemen is gaan bouwen, maar de rest van ons die gewoon hun Big Tower trouw zijn gebleven hebben daar geen enkel probleem mee!
RTX 4090;3;0.5983100533485413;De efficientie vooral met RT is beter dan ik had verwacht. Maar nog steeds niet super. Ze pushen de GPU best hard. Een 350 watt versie had denk ik 90% van de prestaties kunnen leveren. Daarnaast weten we ook al dat er een 4090 Ti of Titan gaat komen want dit is niet de volledig actieve chip. Ik ben heel benieuwd wat AMD gaat doen vooral op het gebied van RT. Want met dit soort framerates begint het eindelijk interessant te worden (qua efficiëntie en stroom verbruik niet maar dat is nog even een ander verhaal). Ten tijde van de RTX2000 generatie heb ik vaak genoeg gezegd dat het nog 2-3 generaties zou gaan duren voordat de RT performance goed genoeg zou zijn om het echt aan te kunnen zetten zonder dat je FPS te laag wordt. Daar lijken we nu bijna te zijn. Zeker als DLSS2.0 gebruikt (was toen geen optie want DLSS1.0 was een te grote achteruitgang qua beeld kwaliteit met hoe DLSS nu is is dat geen issue meer) kan je goede framerates halen. En ik denk dat een RTX5000 het ook Native 4K wel voor elkaar zal gaan krijgen (mits dit wel een nieuwe 3nm GPU is en niet een Lovelace refresh).
RTX 4090;1;0.530759871006012;Ongeveer 10% is uitgeschakeld. Wellicht nog een kaart met GDDR7 chips als geheugen. Maar 10 tot 15% sneller gaat het niet meer worden in deze generatie. Wacht ook geduldig de undervolt reviews even af. Die waren ook zeer leuk bij de 3090!
RTX 4090;3;0.4085961580276489;Ik zeg ook niet dat het meer wordt maar er gaat nog wel een snellere kaart komen. Of RDNA3 moet de 4090 niet eens kunnen evenaren maar dat zou wel een flop zijn gezien hoe RDNA2 het deed. Dus ik verwacht dat ze die 4090 Ti ook wel nodig gaan hebben net als met de 3090 Ti.
RTX 4090;1;0.4293316900730133;Yikes dat idle verbruik is echt onacceptabel hoog. Verder wil ik wel zien wat deze kaart presteert en verbruikt als de clocks gelockt worden op 2500, 2400, 2300, 2200 Mhz Dat scheelt bij mijn 6900XT echt enorm veel power draw (45% voor 5% prestatieverlies).
RTX 4090;3;0.4186372458934784;Ik ben wel benieuwd naar de beschikbaarheid van de diverse modellen de komende weken... Dat was door de omstandigheden een paar jaar geleden bij de Nividia30x0 launch dramatisch.
RTX 4090;5;0.8178735971450806;Wow wat een prestatie verbetering in vergelijking met de 3090 Ti ! Vooral op 4K (deze kaart ga je ook niet gebruiken voor 1080p gaming). En ik vind het stroomverbruik nog meevallen als je ziet dat hij hetzelfde verbruikt als een (stock) 3090 Ti, als je hem undervolt is hij dus nog efficiënter
RTX 4090;5;0.3305724561214447;Jawel, wie liefhebber(1080p) van monitor boven 200 hz heeft...
RTX 4090;3;0.3418823778629303;Iedereen heeft het alleen maar over de ultra setting maar nog niemand opgevallen dat bij Cyberpunk de 6950XT op medium setting het dik wint van de 4090?? Weird? Vanuitgaande dat dit klopt Ook in andere titels op medium weet de 6950XT in sommige gevallen nog aardig bij te houden. Dus als je geen behoefte heb om alles op ultra wilt spelen is dat wellicht een goede keus? Helemaal nu het kleine broertje van de 6050Xt de 6900 nu voor rond de 730 over de toonbank gaat?
RTX 4090;4;0.34171655774116516;Ik denk niet dat mensen dit type kaart kopen om op medium te gamen... ik vind zonder ray tracing benchmarken al een stretch eerlijk gezegd. Voor dit geld wil je gewoon ''all the bells and whistles''. Sterker nog, ik speel met mijn 3080 niet eens op medium eerlijk gezegd... Ben overigens erg benieuwd naar wat AMD op de mat gaat leggen met de RX 7800/7900, als men mee kan komem en de ray tracing prestaties dit keer wel acceptabel zijn ga ik die kaarten zeker meenemen in mijn overwegingen.
RTX 4090;1;0.3749605715274811;Waarom worden er geen VR benchmarks uitgevoerd?
RTX 4090;1;0.6566275954246521;2000€ is gewoon veel te veel. Schande. Ben voor een 3080 gegaan en ik kan alles spelen in 1440p, meer moet dit op dit moment niet zijn voor mij.
RTX 4090;1;0.4921809434890747;Komt de 40 series van NVIDIA zelf (FE) uberhaubt te koop in NL? Bij de 30 series was dat namelijk niet het geval.
RTX 4090;2;0.3743593096733093;Waarom zijn er in de vergelijking niet meer AMD kaarten meegenomen? Nu is het meer een review van Nvidia tegenover Nvidia... Alleen de 6950 in het rijtje vind ik niet echt representatief.
RTX 4090;5;0.2826813757419586;Ik gok dat sommigen van ons dat helemaal niet erg vinden : - Eyefinity is door AMD gesloopt dus dan maar aan de Surround Gaming - De games die ik speel zijn in feite Nvidia titels en doen het beter i.c.m. een groene GPU - De Nvidia Control Panel zonder Geforce Experience is vele malen fijner en overzichtelijker dan wat AMD gebruikt sinds 2016/2017 toen CCC verdween!
RTX 4090;2;0.45928725600242615;Paar opmerkingen die ik toch ook wel mis. 1. Geen dp2.0. Is dat heel erg nou ja en nee. Voor toekomst gebruik zou het wel fijn zijn. Maar nog kan je hiermee gewoon nog 4k 240 bv halen zie de G85NB monitor b.v. die is ook gewoon dp1.4. Ja moet wel met DSC dan. 2. Kennelijk zijn ze nu milder met overklokken want zelfs deze FE kan je dus +33% powerlimit zetten. Niet dat het veel brengt maar je ziet her en der toch dat ze over de 3ghz heen gaan. 3. Hoe zit het met ondervolten/power limit omlaag brengen? der8auer had de zijne op -33%! power gezet en miste maar 5% performence, Hij vraagt zich zelf echt af hoe ze nu bij 450W zijn gekomen als je met 33% minder al zoveel performence krijgt. Dan hadden ze minder gezeur/grote koelers gehad ect.
RTX 4090;1;0.4174334704875946;Heb ik eroverheen gelezen of staat er geen grafiek met de maximum temperatuur en lawaai?
RTX 4090;4;0.4776279926300049;Mooie sprong in prestaties en dat is weer lang geleden, vergelijkbaar met de GTX 1080(ti). Er zijn wel een aantal min puntjes. Geen DP 2.0, maar 1 HDMI 2.1 aansluiting, te groot voor veel behuizingen, nieuwe voeding voor velen en een hoog prijskaartje. Het valt mij ook op dat de kaart wederom pas tot zijn recht komt op 4k net als met high end Ampere.
RTX 4090;3;0.45780953764915466;Persoonlijk vind ik het verbruik erg meevallen na al die doemscenario's die de revue zijn gepasseerd. Als je het frame voor frame vergelijkt is de 4090 zelfs fors zuiniger dan vorige generaties. Indrukwekkend!... Alleen jammer dat ie zo duur is. Ben benieuwd naar de RTX 4080 / RX 7900. Hopelijk heeft AMD wat aan de achterstand op het gebied van ray tracing gedaan.
RTX 4090;2;0.3110950291156769;En ik denk dat Intel ook nog niet weggecijferd moet worden met hun nieuwe GPU's. Lijkt erop dat ze van low end naar high end aan het uitbrengen zijn, kan me in ieder geval niet voorstellen dat het bij de A770 blijft. Ontwikkelingen gaan tegenwoordig immers hard. Ik ben benieuwd wat dat nog met de toekomstige prijzen en prestaties gaat doen en of Intel zich een beetje kan mengen met de gevestigde orde van Nvidia en AMD.
RTX 4090;3;0.3527827858924866;Indrukwekkende prestaties, maar dat geldt helaas ook voor de prijs.. Ik ga echt niet rond de €2000,- betalen voor een videokaart.. Nog niet zo heel lang geleden kocht je voor dat geld een complete high-end PC, nu alleen een videokaart.. Echt idioot wat er met die prijzen gebeurd is de laatste jaren.. Ik heb zelf een RTX 3080 die ik in december 2020 gekocht heb voor €849,- wat ik toen al op het randje vond van wat ik ervoor over heb, en die gelukkig nog wel even mee kan.. RTX 4080 ben je straks ook minimaal meer dan €1100,- voor kwijt..
RTX 4090;2;0.3690098822116852;Ik was wel hyped voor deze kaart, maar eerlijk gezegd heb ik nu toch een beetje m'n bedenkingen. Ik laat amd graag eerst nog even aan zet, want met het einde van PoW Ethereum mining en een overschot aan 30xx kaarten heb ik zo het vermoeden dat de prijzen die Nvidia nu hanteert niet echt de 4090's als warme broodjes over de toonbank gaan doen vliegen. De crypto bullrun van 21 was gewoon een uitzonderlijke situatie voor de Gpu markt, ik heb het idee alsof ze dat hebben geextrapoleerd naar 2022 qua voorraad en prijs, maar de wereld staat er iets anders voor in minder dan 12 maanden. En met deze energietarieven denk ik dat weinig mensen hier nu echt 2 rug aan gaan uitgeven. Ben iig erg benieuwd naar de verkoopcijfers over een aantal maanden. Het hele 30xx scalping snapte ik nog, want vorig jaar had je je 3090 in een aantal maanden terugverdiend op het hoogtepunt van de mining craze. En als je mazzel had kon je je gpu ook nog eens met belachelijk winst doorverkopen. Maar in deze markt..dunno. Al vrees ik dat ik voor December al wel gewoon zo'n kreng in m'n pc heb zitten, maar een beetje hypocrisie is mij niet onbekend
RTX 4090;4;0.5995581746101379;Indrukwekkende resultaten.....alleen jammer dat DLSS 3 alleen voor RTX40 kaarten is, voel je je wel een beetje bekocht als consument....
RTX 4090;1;0.38733169436454773;Nouja ze hebben niet geadverteerd net DLSS 3.0 bij eerdere kaarten. Al zou het zeker gaaf zijn op een RTX 30xx kaart.
RTX 4090;2;0.3925243020057678;Nou ben er wel even uit.... Net een PC gebouwd en even getwijfeld aan welke GPU wachten of niet. Had een 3080 maar die kan nog terug nu voor 50,- meer een 3080Ti FE kunnen bestellen en dat is voorlopig wel even genoeg voor de zeker komende 2 jaar op 1440P zou ik zeggen. Kijk ik daarna wel weer voor een andere eventueel. Of AMD moet met iets komen. Begrijp me niet verkeerd, monster kaart! maar 4090 is niet voor mij simpelweg omdat ik nog niet zo nodig naar 4k hoef en ik 1999,- wel veel geld vind voor een GPU. En 1500,- voor een 4080 (de echte voor 1440P gamen wat ook nog eens vrij beperkt is voor mij met eigenlijk alleen Destiny 2 en Forza games is ondanks dat het natuurlijk vet is gewoon onnodig vooralsnog. Zolang mn games om en nabij 100fps zitten op 1440P met max settings ben ik aardig gelukkig zolang ik me kan inhouden want heb wel graag de nieuwste dingen haha. Oftwel ik wacht nog even rustig af met mn schamele 3080Ti
RTX 4090;1;0.599937379360199;"""This video is private"" :\ Edit: nu niet meer"
RTX 4090;1;0.4441734254360199;Dit heeft te maken met regeltjes vanuit de fabrikant waarop je niet eerder dan strikt het afgesproken tijdstip het een en ander mag publiceren. Houd je je hier niet aan, krijg je ook niet meer die kaartjes in de toekomst. Youtube kun je instellen vanaf wanneer een video gepubliceerd mag worden, dit loopt soms nogal een beetje achter.
RTX 4090;3;0.40293920040130615;Jammer dat van de oudere kaarten enkel de **80 versie getest wordt terwijl de meeste mensen toch een **60 of **70 hebben.
RTX 4090;2;0.4535773694515228;Het topmodel (cq het vroegere Titan-model) afzetten tegen een budget-model is gewoon niet zo nuttig.
RTX 4090;3;0.439453661441803;Maar hoe weet je nu hoeveel winst in fps je kan maken door te upgraden tenzij door op andere sites te gaan kijken? Tweakers faalt hier een beetje in mijn ogen. Edit: ik vermoed dat ze de cijfers zelfs al hebben van vorige benchmarks.
RTX 4090;3;0.34374773502349854;Je wil de vergelijkingstesten van de 3080's wél doen met de drivers van 2022. Dat gezegd hebbende, als je nu een 3060 hebt dan zou ik adviseren om te wachten. Niet op de 4060, maar op de 5060. Bij NVidia kun je prima generaties overslaan.
RTX 4090;1;0.7303728461265564;Bij de 5090 krijg je een gratis schuur en dieselgenerator. Walchelijk, het pad waar videokaarten op zitten. Groter en meer power is een doodlopend pad.
RTX 4090;3;0.5874267816543579;Performance/watt is beter. De kaarten in de 4000serie zijn dus efficiënter. Van een topmodel mag je verwachten dat die tot het uiterste gedreven wordt.
RTX 4090;1;0.8480448722839355;Efficienter op welke manier? Ik vind ze juist enorm inefficient. Meer dan 60FPS heeft niemand nodig. Als die kaart 120 FPS produceert, dan doet ie er dus 60 voor niks, maar staat wel voor bijna 500 watt te verstoken. Hoe bedoel je efficient? En jij merkt en ziet er niks van. Er is een revolultie nodig. De GPU evolutie staat op een dood spoor.
RTX 4090;2;0.4606362581253052;Perf/watt is simpelweg beter. Zo zie ik het . Als die trend doorzet naar de lagere modellen zijn dat prima kaarten. In die zin is deze kaart een Ferarri, meeste mensen kopen gewoon echter gewoon een Renault met een 0.9L motor. Overigens het meer dan 60fps debat wil ik me niet aan wagen, maar zelf heb ik er 120 en heb ik wel het idee dat dat verschil het waard is. Ben wel benieuwd wat jij dan onder revolutie zou willen zien?
RTX 4090;1;0.3814586400985718;Zou jij de kraan laten lopen als je bad vol is? Want je bad is 60FPS, alles wat daarna komt is onnodig en domweg verspilling. Dat je het gevoel hebt dat het een verschil uitmaakt komt zeer waarschijnlijk voort uit het feit dat je anders moet toegeven aan jezelf dat je veel te veel geld hebt uitgegeven. Het zit in de aard van de mens om onze beslissingen goed te praten (daarom zijn er ook nog steeds Trump aanhangers). Revolutie is een totaal nieuwe architectuur voor GPUs die hoge prestaties levert bij een laag energie verbruik. Ik ben geen Appelaar, maar Apple doet dit heel erg goed voor zowel hun CPUs en GPUs. Dat kan op PC ook. Je kan niet tot in de eeuwigheid dingen maar groter blijven maken en meer energie laten verslinden. Zeker niet als het bad al ruim vol is. (PS - ik geloof best dat je een verschil voelt op hogere FPS, maar de impact is gewoon niet groot genoeg om er meer CO2 voor de lucht in te stuwen)
RTX 4090;2;0.46532925963401794;Ach, laat je iets minder opfokken door het mediaspektakel en je kan bij de concurrentie voor een tiende van de prijs en ruim minder dan een vijfde van het energiegebruik ook heel netjes een spelletje spelen. Het zal allemaal wel mooie technologie zijn, maar voor mij hoeft het niet in 8k met magische-stralen™ en volgende-machine-geleerd-truukje-met-marketing-saus.
RTX 4090;1;0.6057580709457397;Foutje artikel: Vanaf het moment dat de GeForce RTX 4080 in ons testlab arriveerde. 4080 ipv 4090.
RTX 4090;1;0.5337935090065002;Dat of een slip of the tongue... hebben ze stiekem ook al een 4080 in huis
RTX 4090;5;0.4704676866531372;Dat gaat het leuke titanengevecht worden met de 8950XT van AMD. Ik kijk er nu al naar uit.
RTX 4090;3;0.3261072337627411;Typo Rx 7950 ?
RTX 4090;1;0.41143226623535156;Als ik me niet vergis gaan ze nu de oneven duizendtallen voor CPUs gebruiken en de even duizendtallen voor GPUs, maar ik kan het mis hebben. In ieder geval 'dat GPU dat komende maand uit komt'.
RTX 4090;2;0.4578653573989868;Oneven voor desktop CPUs en even voor laptop CPUs (afgelopen generatie). Staat los van de GPUs.
RTX 4090;1;0.3820298910140991;Bij mij staat het Wattage bij het stroomverbruik er niet bij, hebben meerdere mensen dat? Bij mij staat Idle er wel bij... Nu wel -_-
RTX 4090;5;0.4554812014102936;Waarom is Hell Let Loose niet als test gebruikt? Dat is een van de zwaardere games momenteel.
RTX 4090;2;0.6175749897956848;Multiplayer games fluctueren teveel qua performance en zijn afhankelijk van een goede en stabiele netwerkverbinding met de server (waarbij de server fps ook stabiel moet zijn). Kortom teveel variabelen voor een betrouwbare score. Voor zover ik weet kan je ook geen demo's opnemen bij HLL.
RTX 4090;1;0.5357500314712524;zwaarder of slechter geoptimaliseerd ?
RTX 4090;2;0.49038493633270264;Slecht geoptimaliseerd natuurlijk. De game zie er ook niet zo heel goed uit.
RTX 4090;5;0.31212398409843445;Kunnen ze dan metro exodus mee vervangen
RTX 4090;5;0.2756214439868927;R.I.P. mini-ITX.
RTX 4090;3;0.33977028727531433;Misschien komt er nog een compacte singlefan versie?
RTX 4090;2;0.2859500050544739;Ik vraag me stiekem wel eens af hoe krachtig een passief gekoelde single-slot low-profile grafische kaart zou kunnen zijn met alle moderne techniek. ( Als NVIDIA / AMD zich er ècht op toe zouden leggen ) Gewoon hypothetisch. Als ik kijk naar de AMD Rembrandt APUs of de SteamDeck dan sta ik al verbaasd hoeveel performance daar al uit gehaald wordt. Als je daar de CPU componenten uit zou halen en vervangen met extra CUs en InfinityCache en dan de kloksnelheid terugschroeven tot je het passief kunt koelen. Lijkt me hilarisch om te zien welke top-kaarten van weleer je daarmee voorbij zou gaan…
RTX 4090;2;0.32585957646369934;Volgens de berichten is het PCB nagenoeg gelijk aan de huidige RTX3000 series, als je toch al mini-ITX deed met de RTX3000 series kan dat met een RTX4000 series ook.
RTX 4090;2;0.37632203102111816;Neem aan dat je watergekoeld bedoeld, anders gaat het niet passen daar ze allemaal 3+ slot zijn. Bovendien is er geen SFX PSU die deze kaarten kan trekken, op die van Cooler Master na wellicht.
RTX 4090;3;0.4938955307006836;Toevallig heb ik hem inderdaad wel watergekoeld, maar wel in een kast die 4 slot GPU's aankan (SSUPD Meshlicious). Dus het zou zonder waterkoeling ook moeten passen (let op ik heb niet de exacte maten er bij gepakt maar puur naar de slots gekeken). Daarnaast zie ik ook geen probleem om met een Corsair SF750 een 4090 te draaien, beetje afhankelijk van de rest van het systeem natuurlijk en alles zonder overclock. Op basis van de testwaarden hierboven van Tweakers zit dat ruim binnen de marges van de voeding. Veel mensen schatten het benodigde vermogen veel te hoog in. Wordt het minder efficiënt? Zeker.
RTX 4090;5;0.6208630800247192;Come to papa!
RTX 4090;5;0.570141613483429;Ik ben al een aardig tijdje uit het modden van PCs etc maar 2K voor een videokaart?! Daar had je vroeger een compleet high-end watergekoelde gamebak voor. Tijden veranderen lol Ik speel tevens WOW op mijn HP werk laptopje met ingebouwde Intel UDH videokaart, werkt prima voor mij.
RTX 4090;1;0.5723201632499695;Zie te zien blijf ik nog lekker bij m’n 6900xt op 1440p. Nul komma nul verbetering en dan die achterlijke prijzen nog 😂
RTX 4090;2;0.3796655535697937;inderdaad ja, ik ook met mijn zuinige 6800xt, dan maar even geen fancy RT.
RTX 4090;1;0.40154582262039185;Mijn twijfels gaat uit tussen RTX 4090 of een 4080 16GB. De reden van upgraden bij mij, omdat ik op een GIGABYTE M32U 32-inch 4K 144Hz monitor speel en met mijn huidige videokaart een MSI VENTUS OC GeForce RTX 2080 8GB de instellingen steeds terug moet zetten op medium/low-settings om over de 100fps te kunnen op 4K resolutie. De RTX 4090 is daar zelfs dus snel genoeg voor om 4K op 144Hz op Ultra te draaien? 3 uur gamen per dag €200,- per jaar aan stroom dat is ook behoorlijk. Nu game ik niet 3 uur per dag.
RTX 4090;5;0.3531164526939392;Wat gaat het mij kosten om 35fps meer te halen dan mijn huidige RX6900XT in RDR2 1440p
RTX 4090;1;0.6071211099624634;Meer dan 2000 euro
RTX 4090;2;0.4099411964416504;Grappig, dat Nvidia nu ineens met een volledige naam komt, i.p.v. slechts de achternaam. Bij Ampère en Turin namen ze die moeite niet. Ik begrijp het wel, hoor. Al denk ik niet dat veel mensen uit hun doelgroep de illustere Linda (nog) zullen kennen.
RTX 4090;2;0.45474159717559814;Hmm zonder dp2.0 is die kaart dus limited op 120hz bij 4k? Beetje jammer…. En kansloos voor deze prijs. Zoveel +120hz monitoren met 4k zijn er ook niet….
RTX 4090;2;0.5458172559738159;Jammer van deze matige review Tweakers! Echt een gemiste kans! Wanneer je een Ryzen 9 5950X gebruikt zelfs met een all core overclock heb je gewon vaak een CPU bottleneck op 1440 en uiteraard op 1080. Zelfs al soms op 4K. Hier had echt een 7950X , 12900K of een Ryzen 7 5800 X 3D in gemoeten. Vaak heb je dan op 1440 een 10% of meer FPS doordat je dus minder snel een CPU bottleneck hebt. Helaas zijn er teveel sites die met oudere CPU's testen, niet enkel Tweakers, maar gelukkig zijn er nog genoeg die met een 12900k/s of een 7950X of 5800X 3D testen en de verschillen zijn dus behoorlijk. Gek als je het mij vraagt terwijl we nog niet lang geleden de review van dee 7950X hebben gezien met de verhoogde FPS in alle games zeker op 1440 tov de 5950X. Waarom grijpen jullie dan terug naar deze test? Want dan kan je beter nog enkel 4K testen omdat de 4090 zelfs op 1440 soms langzamer is dan bv een 3090tio terwijl dat niet met een betere CPU is. En het is niet zo dat deze CPU's niet beschikbaar zijn. Straks met de 13900K zal het verschil enkel groter worden tov de 5950X en ook met de 7000 series X 3D die begin volgend jaar uitkomen. Tijd om een recente CPU te gebruiken voor de test om niet teveel afwijking te krijgen zoals je nu dus hebt!
RTX 4090;1;0.5149574875831604;Gestest met een 5950X laat je toch behoorlijk at performance liggen in deze review had toch echt ryzen 7000 of de 5800X3D gepakt of anders de 12900k je wil toch echt wel laten zien in een review wat de maximale prestaties zijn? zag iemand in een post zeggen dat de 5800X3D locked is dus af valt ( totale nonsence ) en dat we zouden moeten wachten op intel RL ....Nvdia koos vandaag als release dag.
RTX 4090;1;0.7872778177261353;Nou Tweakers, succes met de BBG opstellen dan Jullie zijn altijd al zo bizar zuinig voor een site voor hobbyisten, en nu is de videokaart zelf al (veel!) duurder dan jullie duurste complete systeem. Misschien toch maar eens erkennen dat de BBG mee moet gaan met inflatiecijfers enzo?
RTX 4090;1;0.395110547542572;Maar is de 4090 dan een bust buy? Hoeveel mensen hebben zo een kaart nu echt nodig?
RTX 4090;1;0.34355685114860535;Gamers die op 4k willen gamen op 144Hz of hoger, mensen die streamen, mensen die professioneel video bewerken, mensen die 3D programma's gebruiken, mensen die het uiterste vragen van WebGL. Zelfs mensen die de door Tweakers al jarenlang aangeraden 4k-schermen kopen hebben nu eindelijk een videokaart die boven de 60Hz uitkomt. Mensen die nu op 1440p en 144Hz gamen zullen er ook bij alle games baat bij hebben. 1080p Gamers zijn er bijna niet meer, en de paar die dat nog doen zijn vaak professionals die op 240Hz of hoger gamen (dus ook deze GPU nodig gaan hebben), of het zijn mensen die inderdaad de onderste outliers zijn en genoeg hebben aan een GTX-970 uit 2014 Dus ja, het is absoluut de beste op dit moment. Als Radeon zometeen met een nieuwe lichting komt zullen ze iets onder de prijs gaan zitten, maar dan heb je het alsnog over €1400 tot €1800 per kaart. En dat is nog altijd duurder dan het duurste game-systeem uit de laatste BBG van Tweakers.
RTX 4090;5;0.3451150059700012;ik weet niet of streamen, video bewerken deze kaart nu nodig hebben gamen ja, mensen die met hoge resolutie vr willen doen oke
RTX 4090;1;0.4308326542377472;"Erm, meer dan 70% van steam-spelers zit nog op 1080p (60%) of daaronder. De enige andere resolutie met een aandeel van meer dan 5% is 1440p met 11,25%. Het aandeel spelers met een 4k primary display zit onder de 2,5%. Bron: Steam Hardware Survey September 2022 Mijn conclusie is dat ""gamers"" eerder staan te springen om een nette 1080p videokaart in de prijsklasse onder €300 dan een kaart met een prijs hoger dan €500, ongeacht de prestaties van die duurdere optie."
RTX 4090;5;0.5855225920677185;Ik zit ook op 4k, 144hz met een gewone RX6800xt die dat uitstekend doet.
RTX 4090;1;0.29512733221054077;Het is niet omdat iets de allerbeste is, dat het ook de beste koop is. Dat het de beste waarde voor je geld geeft. Er kunnen andere, goedkoepre kaarten zijn waarbij de prestaties meer dan aanvaardbaar genoeg zijn voor de meeste mensen. Dan is dat een veel betere koop dan gewoon de allerbeste die buiten budget van de meeste mensen ligt. Een best buy guide moet een aankoopadvies voor de massa zijn. Voor hen die het beste van het beste willen, ongeacht de kost, heb je geen BBG nodig, dan kan je gewoon even rondvragen naar wat het beste is dat er te koop is.
RTX 4090;2;0.3063505291938782;Ik heb niet eens de ruimte om 3 slots op te offeren. Dan moet ik mijn tweede PCI-e opofferen en daar zit nu een M2 ssd in.
RTX 4090;1;0.4406466484069824;Wat een geld he! Ik heb tijdje geleden mijn Suprim 3090 TI voor 1050 verkocht en ben nu geld bij elkaar aan het harken voor de rest. Maar mijn god wat heb ik het er voor over zeg! Eindelijk Cyberpunk alles op Ultra en dan standaard meer dan 60 FPS op 4k! Ongelofelijk die jump in FPS.
RTX 4090;3;0.41875073313713074;Indrukwekkende toffe kaart! Maar niet voor 2000€… Houd het wel ff bij mijn 3090 FE
RTX 4090;1;0.4196166396141052;Krijgen we ook energiesubsidie bij deze kaart?
RTX 4090;2;0.29420480132102966;Wat mij opvalt is dat NVIDIA serieus bezig is. Het verschil tussen rtx 3090 en de rtx 4090 is enorm. Ik kan mij niet herinneren wanneer Nvidia voor het laatst een dergelijke upgrade op de markt bracht. They really stepped up te game. Niet alleen in de gpu markt ook in de cpu markt zie je nu dat intel (en min of meer amd) geen genoegen nemen met een kleine boost ten opzichte van de vorige generatie.
RTX 4090;2;0.5264143347740173;Misschien een mooie kaart voor velen, maar ik ben zwaar bedroefd dat na al deze tijd er met een nieuwe generatie kaarten er nog steeds geen nieuwere generatie NVDEC codec is. Ik ben volgens mij echt niet de enigste die al lang roept om HEVC 4:2:2 10bit support. Camera's als de Canon R5 gebruiken die specifieke codec en de video's zullen dus ook gigantisch stotteren met de RTX 4090 tenzij je hem met de nieuwste Intel processors gebruikt waar de IGPU van is aangezet (als die al überhaupt aanwezig is). Dit maakt het dat de RTX 4090 voor mij persoonlijk nog steeds waardeloos is (als upgrade van mijn iet wat oudere video edit pc).
RTX 4090;1;0.9716764092445374;Wat een dooddoener en onzinnige propaganda zeg! Dit model gaat gewoon verkocht worden en veel meer dan jullie denken ook! Elke keer worden er dit soort dingen over topmodellen geroepen en elke keer verschenen ze gewoon in verschillende high end watercooled builds en in sommige gevallen zelfs in Dual of Quad SLI configuratie! Echt... Hou op schei uit en besef dat niet iedereen zich iets van die onzin aantrekt!
RTX 4080;1;0.5249890685081482;Wat een prijsverschil van RTX 3080 FE voor €719,00 naar een RTX4080 FE voor €1469,00, dit is niet best, ook niet met deze prestaties. Ik ben heel benieuwd naar de prijzen voor de RTX 4070 en RTX 4060. Ik denk dat je nog prima af bent (sweet spot) met een RTX 3080 of RTX 3090, de consoles zitten al aan hun max, dus heel veel games die op PC en consoles beide uitkomen zullen niet persé nog beter geoptimaliseerd worden voor een RTX 40XX serie GPU.
RTX 4080;1;0.7588775753974915;Ik ben eerlijk gezegd helemaal niet benieuwd naar de prijzen voor de 4070 en de 4060. Als je kijkt naar wat de 4080 en 4090 kosten dan kan je wel een gokje wagen, 4070: 1000 euro, 4060, 700 euro. Oftewel, een midrange chip probeert Nvidia te verkopen voor 1000 euro. Ze zijn totaal de weg kwijt daar.
RTX 4080;1;0.47704046964645386;Ze zijn niet de weg kwijt. Er zijn nog teveel idioten op de wereld die dat gewoon betalen. Mij niet gezien. Mijn max voor een gpu is 500 a 600. Voor mij mag een gpu niet meer kosten dan een heel console. Dat Nvidia idioot te duur is, is zeker.
RTX 4080;1;0.39780348539352417;Ik heb net een 3080 voor € 600 gevonden via vraag & aanbod, een 4080 voor 1.470 euro is gekkenwerk. Altijd fan van nieuwe tech, maar buiten proportie prijzig moet voorbehouden blijven aan de absolute top van x090 modellen, de x080 mag onder de duizend euro of anders niet. Nuchter stemmen met de portemonnee werkt wonderen, laat de prijsdalingen maar komen volgend jaar. NVidia zal dat gezegd hebbende, vrees ik alsnog prima verkopen richting kerst: overpriced Christmas gifts are the American way
RTX 4080;1;0.643597424030304;Tja, of ze de weg kwijt zijn dat is maar de vraag. Dat zal echt afhangen van hoeveel winst ze hiermee maken, niet wat wij een eerlijke prijs vinden. En kennelijk hebben hun marktanalisten bedacht dat dit meer winst oplevert voor ze. Ze zijn geen goed doel dat ervoor staat om PC-gaming op ultra mogelijk te maken voor iedereen. Mijn 2060 draait ook nog steeds alles, maar niet met alles op max. Voor de mensen die dat wél willen schat Nvidia nu in dat ze daar flink voor willen betalen. Simpel kapitalisme.
RTX 4080;2;0.4563852846622467;Ik snap die analyse van ze echter echt totaal niet. Tijdens de 30 series gingen veruit de meeste kaarten naar crypto mining. Die lui zijn 100% bezig met kosten/baten analyses en in het huide crypto klimaat is die erg negatief als je deze kaarten voor MSRP koopt. Daarnaas had je corona hike op alles wat in-house entertainment was. Het KAN natuurlijk dat de productie van 40 series kaarten gewoon ook veel lager is dan die van 30 series, en dat ze denken ze alsnog te kunnen verkopen voor deze prijs omdat ze gewoon minder kunnen leveren. Maar no way dat de verkoop cijfers vergelijkbaar zullen zijn met 30 series. Puur omdat een groot deel van de markt weg gevallen is. De tijd zal het leren, maar ik verwacht dat de 40 series een vergelijkbaar lot als de 20 series tegemoet gaat op deze manier. Een serie die bijna exclusief door gamers opgepakt werd als deze echt een nieuwe kaart nodig hadden, maar niet iets waar je doelbewust voor gaat upgraden tenzij nodig.
RTX 4080;2;0.44695353507995605;Nee dat werkt niet zo die productie ligt al vast. Die was namelijk al bij TSMC gereserveerd en daar betalen ze toch voor dus die chips worden gemaakt hoe dan ook. Ze kunnen er kunstmatig minder leveren aan AIB partners waardoor het aanbod beperkt is maar de productie die ze hebben afgenomen gaan ze hoe dan ook krijgen. Ergens liggen dan dus een hele berg chips opgeslagen.
RTX 4080;3;0.44714292883872986;Daar heb je gelijk in ik vind die prijzen wel heel erg veel, maar dat komt mede ook dat nvidia op eenzame hoogte staat wat kwaliteit betreft van de videokaarten.Waargoed dat wilt niet zeggen dat ze kunnen vragen wat ze willen, maar dat doen ze ondertussen wel.Nee ik denk dat ik wat meer highend voor intel ga kiezen. Puur en alleen wat die prijzen betrefd zijn ze een stuk beter, nvidia is me ondertussen veel te duur geworden.Nu effe wachten dat ze beter afgesteld worden wat wattage beterfd.
RTX 4080;2;0.4104422628879547;Ik vind het persoonlijk wel meevallen dat Nvidia op eenzame hoogte staat kwa kwaliteit. Op het gebied van raytracing ja. Maar als je daarbuiten kijkt dan was de 6900XT vorige generatie slechts een paar procent langzamer dan de 3090 met een 20% lager stroomverbruik. En ook nog eens 600 euro goedkoper. Ik verwacht dat amd dit keer ook weer een topkaart ergens tussen de 4080 en 4090 kan neerzetten met een veel minder extreme prijs dan Nvidia met zijn 4000 line up.
RTX 4080;2;0.4436098337173462;Helaas niet alleen met Raytracing. Was AMD maar op het zelfde level voor professioneel gebruik, maar helaas is Nvidia de enige echte optie. Als gamer zou ik zeer waarschijnlijk een AMD kaart halen, maar helaas gebruik ik de kaart ook voor Blender e.d, en dan is AMD geen optie.
RTX 4080;3;0.46121373772621155;Ah oke ja daar weet ik weinig van af. Ik gebruik enkel de kaarten voor gamen.
RTX 4080;5;0.44447067379951477;Precies. Dan is de 4080 wel een stuk beter dan de 3080: 40% sneller dan een 3090 ti, 4-6 GB videogeheugen meer voor bv textures én dat bij minder verbruik. In de praktijk ook nog eens ruim 1000 euro minder qua prijs dan een 4090. Dus voor blender qua prijs/prestatie momenteel de beste kaart. AMD heeft hier vooralsnog geen antwoord op.
RTX 4080;2;0.4699285328388214;Ik vind dat amd heel erg goed bezig is en net zoals je schrijft hebben nvidia daar weinig in te brengen. Tevens zijn de kaarten van amd veel stukken goedekoper en misschien nog beter van kwaliteit. Het is niet alleen nvidia wat de klok slaat maar zijn wel een stukje duurder dan amd.De kwaliteit zijn bijna hetzelfde.De prijzen niet ,daar is amd beter in.Wat die topkaart betrefd ik hoop ook dat ze daar in de toekomst kort bij zitten.
RTX 4080;3;0.4287988543510437;De 3080 was/is niet te vinden voor 719 terwijl ik verwacht dat de 4080 wel wat lager dan 1469 zou gaan zitten.
RTX 4080;1;0.5314698219299316;"Jawel hoor. Moest je wel geluk hebben om een van de eerste tien kaarten te kopen die een webshop bij launch beschikbaar had... als ze al niet buiten het oog van het webshop voorraad systeem waren gereserveerd (ofwel aan vaste klanten). Het prijskaartje gaat echt werkelijk nergens meer over. De FPS scores bij gelijk vermogen laten een zeer standaard generationele stap zien van om en nabij 45% of slechter. Het enige positieve performance pluspuntje is dat eindelijk FPS/watt gestegen is, daar waar vorige generaties (2000 en 3000 series) meer verbruikten dan hun voorgangers voor proportionele prestatiesverschillen. De RTX4090 waren reviewers lovend over de geweldige grote stap in FPS; maar vergaten even het verbruik. Niets laat mij zien om buitengewoon onder de indruk te zijn van deze kaart. Het zal vast een prima dikke kaart zijn, maar het is een verlengstuk van vorige generaties, dus dat rechtvaardigt het prijskaartje niet. We hadden bij 2000 serie al flink prijsstap gezien... de 3000 serie leek aanvankelijk mee te vallen totdat er flinke tekorten waren. Ik zou idd ook verwachten dat de 4080s wat minder lopen dan de 3080s vanwege dit prijskaartje. Maar waar komen we straks gemiddeld op uit? 1200 euro voor een high-end kaart? Dat is nog krankzinnig geld als je er over na denkt dat je daar bijna de 3 vlaggenschip consoles van merk N, S en X kan kopen."
RTX 4080;1;0.4394975006580353;Zoals wie? Reviewers die niet naar het verbruik en te leveren Wattage hebben gekeken, niet opgemerkt hebben dat er speciale aansluitingen nodig zijn om het ding te voeden, zijn geen knip voor de neus waard. Het is/was sowieso vrij normaal dat meer rekenkracht meer verbruikt. En de RTX4080 presteert t.o.v. de 3080 beduidend beter met hetzelfde verbruik. Het is me totaal niet duidelijk waarom je dit zou aanhalen om te onderbouwen dat de prijs te hoog is aangezien dit eigenlijk het enige pluspunt is waarom de kaart misschien iets meer dan de 3080 zou kunnen/mogen kosten. Het enige dat de idioot hoge adviesprijs zou kunnen rechtvaardigen is als de productiekosten enorm gestegen zouden zijn. Aangezien daar blijkbaar niemand het fijne van weet behalve Nvidia zelf heeft het geen nut om daarover te speculeren. Als doorsnee consument heb je daar normaal ook geen boodschap aan. De verwachting is dat er nieuwe ontwikkelingen zijn en dat de nieuwe generatie weer wat sneller is dan de laatste voor ongeveer dezelfde prijs tenzij er drastische verschillen zijn die een meerprijs waard zijn. Net zoals we dat zien bij CPU's, geheugen, etc.. Afgezien van het feit dat zelfs een verdubbeling van FPS geen verdubbeling van de prijs waard is zit de grootste wrevel gewoon in het feit dat alle producten onderhevig zijn aan een gevoelsmatige prijs. Wat heb ik er voor over. Wat mag het kosten. Dat is een van de grootste redenen waarom bijv. PC games nog steeds rond de €50 slingeren. Die prijs is niet gebaseerd op daadwerkelijke productiekosten maar op wat ze denken dat de consument wil betalen. Jouw voorbeeld van de 3 vlaggenschip consoles is ook niet helemaal terecht omdat volgens hetzelfde principe (plus concurrentie) die consoles onder kostprijs worden verkocht. Verreweg de meeste consumenten zijn het er op dit moment over eens dat de adviesprijzen voor de 40xx-serie buitensporig zijn.
RTX 4080;2;0.4419045150279999;"Veel grote bekende YT reviewers die de ""70%"" prestatie verbetering t.o.v. de 3090Ti als bovengemiddeld benoemden en daarmee concludeerden dat het een indrukwekkende kaart en generatie sprong is. Dat terwijl als je het afzet tegen gestegen vermogen dit op een gemiddelde architecturele prestatiesprong lijkt. Hierin hadden ze het hoge vermogen wel waargenomen, maar niet de koppeling gemaakt met prestaties. Dat is dus geen rechtvaardiging voor de hoge prijs. Je zou zelfs kunnen zeggen dat hogere verbruikskosten de kaart iets lagere marktwaarde moet geven. Er gaan wel geruchten dat productiekosten gestegen zijn omdat wafers van nieuwere silicon processen onevenredig duurder worden. Hoewel Moore's law nog steeds doordendert met 2x zoveel transistors op dezelfde area elke ~2 jaar, worden die transistors niet 2x zo goedkoop meer. Maar ze worden nog steeds wel goedkoper. De CEO van NVIDIA wilt ons graag overtuigen dat Moore's law echt dood is; en dus dat we niet meer hoeven te verwachten dat elke generatie sprong bvb die +45% prestaties voor hetzelfde geld en stroomverbruik haalt. Maar dat zou er op duiden dat die 45% prestaties altijd puur te danken waren aan processtechnologie innovaties. Maar wat heeft de chip ontwerper dan toegevoegd? Enkel copy-paste? Sja, daarin ga je de strijd niet winnen. Daarom zeg ik dat de kaart te duur is; omdat het gepresenteerde blaadje van cijfers en feiten IMO niet eerlijk is. Ik ben het met je eens dat de prijzen nergens op slaan, hoor."
RTX 4080;2;0.42306363582611084;Dat kwam/komt eerder door te hoge vraag en productie uitdagingen dan door intentie/adviesprijs. Op dit moment zou je een 3080 voor €850 kunnen krijgen want inderdaad hoger is dan de €759 dat de huidige adviesprijs is. Als je kijkt naar de adviesprijs van de 2080/1080/980 waren de stappen klein(wellicht gelijk met inflatie). Nu is het letterlijk dubbel. En ze zullen hun best doen om fabrikanten zich boven die prijs te laten zitten want zoveel macht oefent Nvidia uit op hun partners. Voor mij persoonlijk word dit een generatie om over te slaan en ik hoop dat de consument(en competitie) met hun portemonnee stemt en duidelijk maakt dat dit niet de juiste richting is.
RTX 4080;3;0.4656217098236084;Uiteindelijk hangt het ook af van de prijsontwikkeling. Zodra de markt deze kaart links laat liggen door de slechte bang for buck verhouding, zal de prijs op den duur wel gaan dalen, zeker nu dat de beschikbaarheid van chips in het algemeen weer iets beter aan het worden is.
RTX 4080;1;0.6129008531570435;Die beschikbaarheid wordt door NVidia nu kunstmatig op het huidige niveau gehouden. Ze zitten nog met bergen onverkochte voorraad en ze willen niet dat de 3x00 kaarten verder in prijs dalen en de marges daarop afnemen. Ze hebben dit zelf zo aangekondigd zie hier een breakdown van een investeerderscall met de CEO. Het dalen van de prijs gaat helemaal afhangen van hoe goed AMD het gaat doen met de nieuwe kaarten en of ze dat bij NVidia echt gaan voelen in termen van marktaandeel
RTX 4080;3;0.626824676990509;"Goed punt, maar ik kan me niet voorstellen dat ze alle moeite nemen om een kaart als de 4080 te ontwikkelen en er vervolgens maar weinig van laten produceren om van ""old stock"" af te komen die inherent minder oplevert. Het zal idd van de concurrentie afhangen maar daarom is het ook wel goed om te zien dat Intel nu ook een serieuze gooi doet naar goede kaarten. Niet dat Intel nou zo'n geweldig bedrijf is maar in dit geval is het wel wenselijk dat ze zich in de strijd mengen."
RTX 4080;2;0.42751356959342957;Zou het kunnen dat Nvidia realiseert dat er na de 4090 mogelijk niet veel meer performance nodig is? Dus dat we vanaf nu in een soort iphone markt terecht komen waarbij elke upgrade eigenlijk maar een heel klein stapje is en voor de meeste gamers niet de moeite waard om over te stappen? Een 4090 kan in dat geval misschien wel 10 jaar mee. Als dat zo is dan zou ik de prijs ook omhoog gooien en de komende jaren langzaam af gaan bouwen. De komende generaties dan meer focus op energieverbruik.
RTX 4080;2;0.38878703117370605;Ik denk dat we als gamers altijd wel een stapje er boven op willen. Als je bijv. kijkt naar wat een blizzard als cutscenes maakt zouden we er van dromen dat we op zo'n grafisch niveau kunnen gamen. Raytracing heeft de mogelijkheid om realistischere beelden te creëren maar grafische kaarten zijn nog lang niet ver genoeg om een game volledig op die manier te renderen. Met meer GPU kracht komt meer mogelijkheden en ik denk dat dit nog jaren lang touw-trek competitie word van games en gpu's om steeds een stap vooruit te zetten.
RTX 4080;2;0.416001558303833;Dat had andere oorzaken en is niet relevant. De MSRP was dat, niet anders.
RTX 4080;1;0.8110505938529968;jawel hoor net zeoals de gewobne 3080 voor 800 te koop was, deze prijzen zijn absurd
RTX 4080;5;0.4017084240913391;Had de 3080RTX voor 719 bij launch direct en uiteindelijk heeft mijn gehele bubbel een 3080 of 3070 weten te bemachtigen voor niet al te krankzinnige prijzen.
RTX 4080;1;0.584033191204071;Ik had er ook 1 voor 719 op launchdag. Voor dat geld was/is het al 2 jaar een topkaart geweest, maar de prijzen die ze nu vragen zijn absurd.
RTX 4080;1;0.6043664813041687;Ik heb die van mij (FE) voor 719 gekocht en geen cent meer.
RTX 4080;2;0.40691882371902466;Dat is twee keer zo duur met EEn type aanduiding hoger.
RTX 4080;2;0.4915505051612854;niet echt gebeurd
RTX 4080;1;0.6520329117774963;Nee hoor. Er word nog lang niet alles uit die hardware geperst. De meeste ontwikkelaars moeten hun eerste game nog lanceren op deze generatie en pas bij de tweede (als dat nog een ding is met de ontwikkelingstijden van nu) halen ze echt het onderste uit de kan.
RTX 4080;3;0.4645318388938904;Ik denk dat je daar wat te veel van verwacht. Game engines worden beter en daardoor zien latere games er beter uit. Maar datzelfde gebeurt op de pc. Daar zien latere games op equivalente hardware van de consoles (6650XT) er ook beter uit, zonder dat ze slechter gaan draaien. Zo is FSR 2 zojuist toegevoegd aan Cyberpunk op de consoles, waardoor de beeldkwaliteit flink omhoog is geschroefd. Ik verwacht weer zo'n verbetering als FSR 3 met frame generation beschikbaar komt. Als RDNA 2 dat aan kan tenminste.
RTX 4080;3;0.41914522647857666;"Citaat: ""Game engines worden beter en daardoor zien latere games er beter uit"" Het verschil tussen BF3 (2011) en BF 2042 is niet veel."
RTX 4080;2;0.2686510980129242;Juist naarmate een console generatie langer bestaat wordt er geen rekening meer gehouden met backwards compatibiliteit en dus worden de nieuwe console games zwaarder met meer upscaling en lagere fps cijfers, waardoor een zwaardere desktop juist beter tot z'n recht komt, omdat je daar wel nog 120hz met 4k kunt combineren.
RTX 4080;1;0.8794568777084351;Stemmen met je portemonnee, doet hopelijk wonderen, zeker als dit hele grote getallen zijn. Gewoon niet kopen..
RTX 4080;2;0.38377487659454346;De consoles zitten nog niet aan hun max, daarvoor hoef je alleen maar naar de Matrix demo te kijken. Dat soort graphics zien we het komende jaar niet in commerciële releases, ook niet op pc. De echte sweet spot is tegenwoordig een console, voor max 550 euro krijg je in een best case scenario 3070-achtige prestaties. PC gaming maakt voor de 2e generatie op rij geen enkele kans als het op prijs/prestatie aankomt.
RTX 4080;3;0.31268855929374695;"Nou, ""3070-achtige graphics""... 6650XT- a 6700XT-achtige graphics krijg je op de XSX. Die kaarten gaan voor 350 of 450 euro, terwijl de 3070 bij een kleine 600 euro begint. Nog steeds een goede deal aangezien de XSX voor 500 euro gaat. De PS5 heeft echter meer exclusives en staat voor 800 euro in de pricewatch en heeft een 15% tragere GPU, oftewel die 6650XT van 350 euro. Als je dan al een pc hebt staan, wordt pc gaming ineens goedkoper. Je hebt tenslotte ook een tv nodig voor een console."
RTX 4080;2;0.38534674048423767;De MSRP van de PS5 is € 549 (disc) en € 449 (digital). Die 800 euro van de pricewatch voor een digitale versie is geen redelijke prijs. In de praktijk worden er door de lage beschikbaarheid veel bundels verkocht en zul je een telegram-kanaal of iets dergelijks in de gaten moeten houden om er 1tje te scoren.
RTX 4080;1;0.5953916907310486;Een console biedt echt geen 3070 oftewel 2080ti prestaties. Ik weet niet hoe je daar bij komt. Je kan vast wel een voorbeeld van 1 game vinden waar het zo is maar dan heb je het over een outlier. Niet over de gemiddelde usecase. Daarnaast zijn games veel duurder op consoles. En moet je extra betalen voor allemaal zaken als online multiplayer.
RTX 4080;1;0.3314254879951477;Digital Foundry. En ik noemde ook een best case scenario. Games kunnen ook goedkoop worden als ze op disc verkrijgbaar zijn, er zijn sites waarop je dat kan tracken. Laatst lag diablo 3 nog voor 3 euro bij de lokale Intertoys.
RTX 4080;1;0.48665469884872437;Tja, en dan heb je het nog maar over de adviesprijs van 1469 euro. De daadwerkelijke prijs zal weer hoger liggen. Straks bij scalper-winkel Megekko vanaf €1849,-
RTX 4080;3;0.675595223903656;Er zijn redelijk wat games waar ik de RTX 4090 voor nodig ben op 4k 120hz om boven de 60fps te blijven. De RTX 3080 die ik hier voor had heeft wel moeite met native 4k met raytracing aan. Ik verwacht in de toekomt ook meer games met raytracing. Zelf vind ik raytracing ook echt wat toevoegen maar dat geldt niet bij iedere game waar je het verschil goed kunt zien.
RTX 4080;1;0.3908911347389221;Ik ben echt bang om dit te weten, volgens mij: 4070: 899€ (de geannulleerde 4080 12GB) 4060 ti: 699€ 4060: 599€ Just a wild guess maar komaan, deze prijzen...
RTX 4080;2;0.47996652126312256;"Geweldig snelle kaart, maar slecht prijskaartje. Als ik hier bij techspot in de ""cost per frame"" grafiek kijk (iets wat tweakers ook wel zou mogen toevoegen aan de reviews, gezien we de pricewatch als prijsbron hebben) dan zie ik dat de 4080 per frame nog steeds verschrikkelijk duur is. Niet dat ik verbaasd ben overigens, want het is nog steeds een nvidia product. Een 6900XT bijvoorbeeld is gewoon nog steeds veel betere value met de ""recente"" prijsdalingen. Wat me ook tegenvalt is dat de cost per frame eigenlijk gewoon identiek is aan de 4090. Je krijgt dus gewoon 75% van de 4090 voor 75% van de prijs. Normaal gesproken is iedere stap omhoog in tier een stap omlaag in value. Ik hoop niet dat ze dit gaan aanhouden, want dan is de 4070 straks 999 USD... Stock performance per watt is ook nog steeds dramatisch slecht (vind ik, aan de hand van de 4090 undervolt tests) omdat deze kaart zo te zien precies zo overdreven hoog staat afgesteld als de 4090. Hopelijk is hij dan ook net zo undervoltbaar... Ik vind het raar dat de 4080 op precies dezelfde performance per watt is afgesteld. Dat de 4090 voor de performance kroon gaat, dat snap ik, maar de 4080? Ze hadden hem wel ietsje terug mogen schroeven. Overigens, stukje persoonlijke mening: Ik ga echt never, never, nooit 1200 euro uitgeven aan een videokaart. Al helemaal niet als dat dan niet de snelste videokaart is van het moment. Mijn highscore is de 770 euro die ik voor mijn GTX 1080 betaalde net na launch en ik was niet van plan om ooit over de 800 te gaan. Ik ben benieuwd wat “de meute” gaat doen. Gaan we deze 4080 bovenaan de steam hardware survey zien rond waar vorige 80’s altijd stonden?"
RTX 4080;3;0.33398985862731934;De 4090 schaalt van 75% naar 100% in prijs, maar heeft ook alle specs 33% meer of beter, dus de 4090 gaat op de lange termijn gewoonweg de betere kaart zijn. Als je een 4080 kunt betalen, kun je beter doorsparen voor een 4090. Desnoods zet je de laatste dan op een powerlimit van 60-80% en dan behoud je 95% of meer van de fps, met 275-300W verbruik.
RTX 4080;2;0.4897025227546692;Meer betalen voor future proofing zou ik niet aanraden. Dat heb ik ooit gedaan met een 980 Ti en dat was overkill (voor mijn monitor) tot Nvidia met allerlei nieuwe features kwam die de Maxwell kaarten niet hadden. Je weet dus niet of je echt future proof bent of dat er straks een DirectX13 aan zit te komen of iets dergelijks.
RTX 4080;3;0.34394630789756775;Ik zou inderdaad niet te veel aan future proofing doen. Een 40xx kaart heeft overigens in het hier en nu wel een meerwaarde als je graag alle toeters en bellen aan zet, inclusief ray tracing, icm een snel 4k scherm.
RTX 4080;1;0.364291250705719;dat wat je ten tweede noemt is heden en niet future proef inderdaad. Straks komen ze met een dlss4.0 exclusief voor bijvoorbeeld de 50xx met bijhorend toeters en bellen. dan heb je nog zoveel betaald voor een hedendaagse kaart, future proof was hij dan niet. mocht het voorkomen, wat mij niet onmogelijk lijkt
RTX 4080;3;0.5541576147079468;Inderdaad, de 3080 loopt nog steeds uitstekend met de nieuwste games. Het verschil in prijs is dan dusdanig dat ik niet echt snap dat mensen nog zoveel meer willen leggen voor deze nieuwe generatie. Sneller dan snel heeft niet echt veel nut meer.
RTX 4080;2;0.4689153730869293;Het is meer de keuze tussen een 4080 en 4090. Je betaalt dan €500 (600?) meer, terwijl het nog te bezien valt of een 4090 langer mee gaat. Over het algemeen is het toch de featureset waar zoiets stuk loopt. Maar de 3080 is ook nog een keuze inderdaad. Die lijkt bijna goedkoop met z'n €800 Het zou goed kunnen dat je daar de rest van de generatie ook nog voldoende aan hebt.
RTX 4080;1;0.3462139666080475;Tuurlijk, er zijn denk ik maar weinig mensen die overstappen van een 3080. Maar als je nu net een nieuwe pc gaat bouwen of je vervangt een 2060 en je hebt het geld... dat is meer de doelgroep lijkt me?
RTX 4080;2;0.45808354020118713;Mogelijk, maar als ik nu een nieuwe pc bouw, ook al heb ik het geld, dan spendeer ik dat liever aan andere leuke dingen voor de pc dan aan het verschil tussen de 3080 en 4080, het voelt gewoon niet alsof dat waard voor je geld is.
RTX 4080;3;0.44495123624801636;Er zijn allerlei situaties waar meer kracht nut heeft: - MSFS - VR - 4K op hogere refresh rates
RTX 4080;4;0.4208104610443115;4K op hoge refresh rates is overrated, zeker als je op een afstand zit. VR kan prima met een 3080 als ook MSFS, als MSFS gewoon draait op 50fps oid dan is dat zat.
RTX 4080;1;0.33023232221603394;Dat is allemaal jouw subjectieve mening en er zijn behoorlijk wat mensen die dat niet vinden.
RTX 4080;3;0.2711087465286255;"Dit. Het is veel verstandiger om nu een 6700xt of 6800xt te kopen en dan de rest van je budget over 2 of 3 jaar uit te geven. Zeker als je eerst je oude gpu verkoopt dan heb je dan een veel betere pc dan iemand die nu een ""futureproof"" 4080 koopt en hem dan voor 5 jaar houdt. En laten we eerlijk zijn, je moet een 4k 120hz monitor hebben en willen spelen op ultra om baat te hebben bij een 4080. Echter, de meeste games zien er op medium ook nog prima uit en dan kun je prima met iets minders de 120 fps halen."
RTX 4080;5;0.37971192598342896;"Ik heb vorig jaar (na een half jaar met de nieuwe pc met een gtx970 e hebben moeten doen) via AMD zelf een 6900xt kunnen vinden. Ik speel op een 120hz monitor op 1440p, vind ik persoonlijk meer dan prima. De volgende kaart wordt dus waarschijnlijk een rx 7080 ofzo ;-D"
RTX 4080;2;0.47036799788475037;Daarom is 'future proof' meestal bij dezelfde afdeling te vinden als 'buyer's remorse'. Het streven naar future proof resulteert vaak dat je of oneindig je aankoop uitstelt, of dat je nu bergen met geld spendeert voor wat nu het beste is op de markt (in de hoop dat je de volgende generatie kan overslaan), en beide koop-strategiën maken je zelden gelukkig. M.i.: tenzij je weet dat er een nieuwe generatie van X heel snel aan zit te komen, koop de huidige huidige generatie en daarvan de klasse die voor jou het meeste waar voor je euro geeft. Moet ook gezegd worden dit wikken en wegen minder zou voorkomen als videokaarten niet zulke hoge premium zouden hebben. Nog niet zo heel lang geleden was het heel simpel: nieuwe generatie komt uit -> koop een upper midrange kaart voor 600€. Nu moet men met gemak het dubbele spenderen... maar je kaart gaat niet 2x zolang mee. Dan snap ik wel dat koopvrees toeneemt.
RTX 4080;2;0.3792494535446167;Ja ik vind dat dus ook apart. Vroegah kocht je een midrange of upper midrange kaart omdat je dan meer waar voor je geld kreeg. Verreweg de meesten kochten gewoon elke 2 of 3 generaties weer een X60 of X70 kaart en hadden zo altijd een goed meekomende pc voor een goede prijs. Op deze manier loont doorsparen dus áltijd, wat nvidia vast leuk vindt omdat er dan meer mensen gaan doorsparen. Maar feitelijk worden de midrange shoppers dus gewoon genaaid. RX7000 please don’t suck. Ik wil een nieuwe videokaart, maar niet dit.
RTX 4080;1;0.5526576042175293;Ik zie geen reden om door te sparen? Als ik een auto ga kopen en er een nodig heb met 4 zitplaatsen en 120pk Ga ik toch ook niet doorsparen voor een auto met 6 zitplaatsen en 200pk ookal hebben ze hetzelfde verbruik/km? Je geeft gewoon 500/600 euro extra uit aan je pc. Ja het is mogelijk dat je pc langer meegaat maar hij verbruikt ook meer. En hoe langer je wacht met het verkopen van je tweedehands pc hoe minder de onderdelen waard worden. Ja je kan gaan undervolten en andere onzin. Maar je koopt toch geen top kaart om dan op medium settings te gaan spelen? Ik koop nog steeds liever elke 2/3 jaar nieuw dan dat ik twee keer zoveel ga betalen om de beste onderdelen in mijn pc te hebben en dan na 3 jaar ingehaald worden door midrange spul.
RTX 4080;3;0.390596866607666;Je gaat alleen nu niet 2x zoveel betalen. Je betaalt 1:1 extra voor 1:1 dezelfde performance, maar met het verschil dat alle extra specs van de 4090 zich op lange termijn juist beter gaan houden (geheugen en brandbreedte bv) en dus relatief gaat die kaart dus juist langer mee en zal normaal ook relatief meer waard blijven.
RTX 4080;3;0.42387819290161133;Maar wat je nu niet uitgeeft kan je in de toekomst wel doen aan een nieuwe generatie welke weer een heel pak sneller is.
RTX 4080;3;0.39860299229621887;Dat is alleen zo als de prijs/performance van de nieuwe generatie een stuk beter is, maar Nvidia stapt daar met deze generatie van af. De 4080 is dan wel sneller, maar kost nu veel meer dan de 3080 bij de introductie.
RTX 4080;2;0.38476043939590454;Dat is met de prijsverhogingen van TSMC nu helaas de realiteit.
RTX 4080;2;0.47157248854637146;Dat lijkt me extreem onwaarschijnlijk, want de chip in de 4080 is ongeveer 60% zo groot als de chip van de 3080. Dus waarom is die dan $500 duurder, terwijl de 4090, die een chip heeft die bijna even groot is als de chip van de 3090, maar $100 duurder is? Dan zijn er twee mogelijkheden. Als de half zo grote chip van de 4080 echt $500 duurder is, dan zou de 4090 chip dus $800-1000 duurder moeten zijn. Dus dan maakt Nvidia nu $800-1000 minder winst op de 4090. Of dit is gewoon niet waar en de meerprijs van de 4080 komt niet door de hogere kosten van TSMC. Mij maak je in elk geval niet wijs dat Nvidia nu zoveel minder winst zou maken op de 4090.
RTX 4080;1;0.3675006031990051;Omdat de 3080 door Samsung op een andere procede gefabriceerd werd volgens contracten waar toen over onderhandeld was. Dat is iets heel anders dan wat TSMC nu voor zijn 4nm procede vraagt. TSMC heeft meerdere verhogingen doorgevoerd en gaat in 2023 nog eens 9% doorvoeren.
RTX 4080;2;0.39276573061943054;Het is een beetje frustrerend dat je niet lijkt te begrijpen dat ik heb aangetoond dat de veel grotere prijsstijging van de 4080 ten opzichte van de 4090 niet te verklaren is door een prijsverhoging van TSMC, hoe hoog die stijging ook is. Zowel de 3090 als de 3080 zijn een 628 mm2 chip die bij Samsung werd gemaakt, terwijl de 4080 een veel kleinere chip is dan de 4090. Dus de extra kosten voor de chip in de 4080 moeten altijd veel lager zijn dan de extra kosten voor de chip in de 4090. Dus het kan gewoon niet zo zijn dat de 4090 dan $100 duurder is geworden om te produceren, maar de 4080 dan $500 duurder is geworden. Het zijn dezelfde wafers, maar voor de 4080 heb je zo'n 60% van de wafer nodig, dus als de meerprijs voor de 4090 echt $100 dollar is, zou je voor de 4080 chip een meerprijs van hoogstens $60 verwachten. Dus waar komt die andere $440 dan vandaan ($500 minus $60)?
RTX 4080;2;0.3614051938056946;Wat jij niet begrijpt is dat het niet linear is naast de prij sook af hangt van de andere componenten waaronder de uitzonderlijk prijzige heatsink welke 1 op 1 is overgenomen van de 4090RTX. Daarnaast is een chip niet alleen de size het is ook de yield, wafer kosten enzo. Bij de 3080 versus 3090/3080TI zat er een verschil in het gehele PCB.
RTX 4080;2;0.44987815618515015;Het verschil in kosten tussen de heatsinks komt uiteraard niet in de buurt van de $400. Bovendien is de keuze voor een onnodig grote heatsink alsnog te wijten aan Nvidia. Ze hadden die kaart net zo goed even groot kunnen maken als de 3080. Het is zeker geen rechtvaardiging voor $500 extra en kost ook hoogstens wat tientjes extra. De yield van een kleinere chip is uiteraard substantieel hoger, dus dat is juist een reden waarom de prijs van de 4080 minder omhoog zou moeten gaan dan de prijs van de 4090. Maar je bent echt aan het reageren als een extreme fanboy, zonder enige logica, om het gedrag van Nvidia maar goed te praten. Dus ik ga verder niet meer reageren.
RTX 4080;1;0.34996598958969116;Hangt van de supplpy chain af. De heatsink is volgens AIB's welke Nvidia maakte vele malen duurder dan die zij zelf inzetten overigens. Iets wat je ook direct merkt als je een recente FE in je handen hebt gehouden. Het is een extreem prijzige heatsink en heb daar zelf ook de voorkeur voor. AIB heatsinks en shrouds is echt als goedkope meuk gemaakt tegenwoordig. De yield zal hoger zijn maar het verschil weten wij niet, jij gaat uit van hele simpele aannames terwijl het een heel complex prijskaartje is. Boehoe, meteen de aanname dat ik een Nvidia fanboy zou zijn. Ik geef gewoon aan met gedegen argumenten waarom het niet zo simpel is als dat jij projecteert. Can't stand the heat, get out of the kitchen! De prijzen zijn niet alleen een chip, het is de PCB welke vrijwel identiek is aan de 4090 qua traces en formaat, board components welke op een aantal fases identiek zijn aan de 4090, de heatsink welke identiek is aan de 4090. Moeilijk he?
RTX 4080;4;0.2979139983654022;zo werkt dat niet je kan voor die aankoop strategie kiezen, als meer consumer bent die x060 of x070 tier koopt zal de next ook die tier zijn. dan koop je vaker een g - kaart sommigen elke gen. dat kan ook met een x080 tot x090 ti. maar met hogere tier kan je makkelijk gen of 2 overslaan. en game je begin periode heel luxe late periode nog steeds voldoende maar uiersard wat minder. vaak kan je dan makkelijk upgrade uitstellen ipv 2gen overslaan wegens personlijke omstandigheden tot wel 5. gen zoals huidige 1080ti gebruikers naast 3060 gebruikers in heden als niet in die 4k 8k of rt of vr lixe streven zit en meer gewoon van gameplay geniet met ook goede gfx ervaring. kan met x070 ook paar gen overslaan. het is 4k wat veel performance vreet. zo ook rt maar dat zijn uitzonderlijke games. gros van games doet er niet veel mee omdat massa meer vanaf 3gen terug x060 en ook amd ouwere midrange zijn. gamedev maken games voor de mainstream. waar bunnyhop tdm esport of competive je snel in competatieve beleving valt en die 144 + fps belangrijker is die rt als mind noise en elk ander gfx wegvalt en puur in gameplay competitive flow zit. kan hoge detail setting juist tegenstanders verhullen. ik speel dat soort games liever op console waar er meer hardware gelijkheid is 60 a 120fps wij nog steeds op ps4 60fps max. dan op pc met cheaters en volk 360fps monitoren met halo sku g - kaart daar speel ik liever singleplayer sandbox exploring games op. en als markt door covid of crypto of oorlog inflatie crisis verpest dan upgraden in betere tijden. het blijft luxe ding, die ouwe vega56 is wat traag maar daar moet ik het dan mee doen en dan wat meer op de ps4.
RTX 4080;3;0.4379937946796417;natuurlijk zal jij de kaart relatief sneller vervangen. maar het prijs verschil is 9 van de 10x niet gelijk aan de extra levensduur. Een 3090 heeft marginaal maar een iets langere levensduur dan een 3080 (is maar 10% sneller) maar uiteindelijk koste deze veel meer en zal je ongeveer net zo snel moeten ontvangen. Heb je welleens naar Steam hardware surveys gekeken, het percentage van high-end machines is zeer laag. Daarnaast kom ik zelden cheaters tegen, wel een hoop mensen met hackusations.
RTX 4080;4;0.39003098011016846;Ik vind technieken zoals raytracing super gaaf. Met RT en een snel 4k scherm levert zo’n 4080 of 4090 echt wel een meerwaarde. Ik heb maar weinig vrije tijd, maar als ik dan vrij ben vind ik het ontspannend om op te gaan in een atmosferische game met alle toeters en bellen aan. Vind het ook erg leuk om een beetje te klooien met de raytracing effecten enzo. Je moet voor jezelf nagaan of de prijs het waard is. In mijn geval dus wel (en schijnbaar dus ook voor veel anderen gezien het marktaandeel). Overigens ben ik het er mee eens dat de prijzen exorbitant hoog zijn. Iedere gek zijn gebrek zullen we maar zeggen. (P.S. ik vind dat sommige reaguurders in deze comment section wel heel erg tekeer gaan tegen mensen die het ‘t wél waard vinden om een 40xx kaart te nemen. Dat je het duur vindt is prima (en dat klopt ook), maar laat mensen gewoon hun eigen overwegingen maken aub. Dit is een tech forum, natuurlijk zitten daar hobbyisten die veel geld uitgeven aan hun hobby (zoals bij elke hobby) )
RTX 4080;5;0.2792103886604309;daar heb je gelijk in
RTX 4080;2;0.242172971367836;Ik koop 80 en 90 series kaarten omdat ze teruggeklokt en undervolted en op max 250 watt een stuk efficienter en stiller zijn dan 60 en 70 series kaarten.
RTX 4080;2;0.31504473090171814;Je hebt met videokaarten niet te maken met een maximale toegestane snelheid op de weg of verschil in resolutie/verversingssnelheid, daarnaast verbruikt een 200PK brandstofmotor in de regel meer brandstof dan een auto van 120pk. Als ik zo gauw kijk op Autoweek carbase: Dan gaan de meeste auto's met 120PK tussen de 160-200km/u. Dan gaan de meeste auto's met 200PK tussen de 160-240km/u. Stel jij rijdt 20 dagen in de maand 200km per dag in totaal voor werk en je kan op maximale snelheid op een rechte weg doorrijden zonder verkeer of stops: 240km/u = 16 uur, 39 minuten en 59 seconden. 200km/u = 20 uur Dat scheelt je toch wel even 3 uur, 20 minuten en 1 seconde per maand of ongeveer 40 uur per jaar aan reistijd. Nu zal dat niet voor iedereen de moeite waard zijn, zeker omdat dit op een dag genomen om 2x5 minuten gaat maar goed voor het verschil met Ultra en High in spellen kan je hetzelfde stellen. Als je bij wijze van twee auto's hebt gebouwd met dezelfde materialen, zelfde vorm alleen het (laad)gewicht is anders dan zal je daarin verschil merken. Zwaarder gamen (4k ultra / 100+ verversingssnelheid) = Zwaarder geladen auto Datzelfde principe zie je terug bij videokaarten, als je een lagere resolutie en verversingssnelheid aanhoudt dan kan je prima vertoeven met een budget/midrange videokaart. Speel jij op een hogere resolutie (2-4k) met een hogere verversingssnelheid dan heb je minimaal een mid-range kaart nodig om daar fatsoenlijk gebruik van te kunnen maken. Eens, met uitzondering van competitive games uiteraard. Dat zou toch een reden kunnen zijn om te sparen? Ik koop normaliter om de 2-3 jaar een nieuwe computer met zo'n beetje de beste beschikbare onderdelen. Nu zit ik even wat langer met mijn huidige computer vanwege de prijzen, maar normaliter had ik dus een PC met een 4090 gekocht. Nu is het wachten op prijsverlagingen of zelfs de 5xxx-kaarten.
RTX 4080;5;0.4722936153411865;Een 200pk kan door downsizing euro6 mogelijk bij 120km/u op idealere kruis snelheid zitten in 2000. rpm 6gant dan 80pk 4000rpm die gast ook 120km/u moet halen ver over zijn kruis snelheid met zijn Euro4 5 bak.
RTX 4080;2;0.510304868221283;Nee, niet de power limit inschakelen. Dan blijft de gpu te hoge voltages krijgen, en zakt je framerate gewoon eerder in. Je moet de gpu undervolten, dan zakt het stroomverbruik zonder dat de prestaties eronder lijden.
RTX 4080;4;0.2846148908138275;
RTX 4080;1;0.6085959076881409;Ik cap de power limit, klok terug en undervolt.
RTX 4080;2;0.4436337649822235;indeed, undervolten.. betere termals, minder verbruik zelfde performance
RTX 4080;2;0.4145662188529968;De kaart blijft toch de voltage/frequentie curve volgen? Als je de power capt zou je zeggen dat het voltage en de clock allebei zakken als je over de power cap heen dreigt te gaan.
RTX 4080;3;0.40766045451164246;Bij een undervolt laat je de kaart bij een bepaalde frequentie op een lager voltage draaien. Dan verbruikt hij minder.
RTX 4080;1;0.46498581767082214;Of hij crasht
RTX 4080;5;0.3384183943271637;En dat is wat Nvidia wil :-P Zo dat mensen 4080 en 4090 vergelijken en denken, hee ik kan beter 4090 kopen dan 4080 dus meer geld :-)
RTX 4080;1;0.48257943987846375;Niks is future proof...beter NIET kopen is mijn advies. Meesten onder jullie kochten toen de 2 serie { hoge prijs } toen al. toen kwam de 3 serie heel snel na de 2 serie....en ik dacht toen al oei oei die mensen met de 2 serie voelen zich genaaid. Ik game nog op 1080 en maybe naar 2k gaming op 1070ti ...niks mis mee. Heb totaal nu nog niet/rede om andere kaart te kopen. Voor mij zijn de kaarten te duur en veel te veel TDP. De power consumption is veel te hoog...daar moet wat aan gebeuren.
RTX 4080;1;0.5811396241188049;Een nieuwe generatie kaarten die per frame meer kost dan de generatie 2 jaar ervoor. Belachelijke prijzen. De 3080 had gewoon de laagste cost-per-frame bij introductie:
RTX 4080;2;0.33105891942977905;Bedoel je dan de NVidia suggestie, of de prijs waar je'm feitelijk voor kon krijgen? Want dat waren heel verschillende prijzen.
RTX 4080;1;0.343781441450119;Hier een 3080 asus tuf voor 810euro op amazon.nl gescoord. Oudere 1080ti verkocht voor 400euro
RTX 4080;1;0.35104691982269287;Had lekker een 6900XT gekocht voor dat geld en je hebt nog geluk dat iemand de 1080ti voor €400 heeft gekocht! Zie ze regelmatig voor €250-300
RTX 4080;1;0.40073347091674805;Ik moet van mijn leven geen amd grafische kaart meer . De 3080 was er eerst en ik wilde persé upgraden. Verder zie ik dat deze kaarten gewoon hetzelfde performen maar dan heb je geen Rtx, Dlss en gsync bij de amd kaart. Ook waren er te korten met de pandemie ik heb destijds 1080ti's zien gaan voor 600+
RTX 4080;5;0.2728523015975952;Ah nog in de crisis verkocht dus. De 6900XT is echt sneller hoor Maar ieder zijn eigen ding! Ik wacht nog even op de release van de 7000serie van AMD en dan kijken of de prijzen nog iets lager zijn voor de 6900XT
RTX 4080;3;0.4697277545928955;Zit in hetzelfde schuitje, ben echter wel benieuwd of die prijzen van een 6900XT daadwerkelijk nog gaan dalen. Al wel gebeurd in de VS, maar ik ben bang dat black friday en december eerder prijsdalingen gaan opleveren voor de iets lagere kaarten, 6600-6750XT bijvoorbeeld. Hopen doe ik echter ook
RTX 4080;5;0.4869750142097473;In de winkel ben ik ook bang dat er misschien max nog €50 vanaf gaat maar ik hou liever V&A in de gaten. Gewoon lekker 2e hands zijn de prijzen vaak perfect en nog ruim voldoende garantie op die kaarten
RTX 4080;1;0.5070409178733826;eerste google hit zegt dat naarmate de resolutie omhoog gaat de 6900XT flink verliest van de 3080
RTX 4080;5;0.5168951153755188;Altijd even hardware unboxed kijken op youtube met hun 50 game benchmark. Ze zijn het meest te vertrouwen dat ze de testen goed en nauwkeurig uitvoeren. Dan zie je toch een ander beeld.
RTX 4080;5;0.2793402671813965;Rtx, dlss en gsync heeft amd gewoon of een ander benaamde variant voor
RTX 4080;1;0.3116712272167206;Tuurlijk! Moet je voor amd gaan dan.
RTX 4080;5;0.6893698573112488;Nvidia markerting, mag weer een bonus krijgen als ik jouw post lees . Ongeinformeerde consumenten zijn de beste consumenten!
RTX 4080;5;0.3607653081417084;Ik heb begrepen dat AMD volledig nieuwe drivers heeft geschreven. Dat heeft de performance en stabiliteit flink verbeterd. Misschien weer een kans geven?
RTX 4080;3;0.36575600504875183;Nah. Ik ben goed met de 3080. Heb 0 problemen, drivers up te date, toffe features. Als het goed is mag het ook gezegd worden zeker
RTX 4080;2;0.3907388746738434;850 en 400 voor mijn 1080 ti 2 maand later, was redelijk normaal rond die tijd omdat er gewoonweg zoveel schaarste was
RTX 4080;1;0.4172654151916504;810 euro is eigenlijk nog veels te veel.
RTX 4080;1;0.30506864190101624;Ik denk dat nvidia earner statements heeft afgeven toen de cryptogekte nog gaande was. Men wil niet dat het aandeel onderuit knalt, dus zetten ze de nieuwe kaarten zo hoog in de markt dat ze toch nog aan hun beoogde omzet proberen te komen. Dat zag je ook al aan hun houding tegenover retailers die gedwongen werden de vorige serie te blijven verkopen ook na de crash. Alleen verspelen ze hiermee wel heel veel goodwill, eerst beweerden ze immers dat er niet speciaal voor crypto werdt geproduceerd wat dus duidelijk een leugen was en extra hoge prijzen voor games betekende ne ze hun kaarten niet meer kwijt kunnen proberen ze die zelfde gamers te bewegen de extra dure kaarten te betalen om hun verlies te beperken. Zo raak je wel mooi je hele klanten circel kwijt.
RTX 4080;1;0.5088955760002136;Nou nee is de VS hebben niks met de euro crisis te maken daarbij is vraag naar 4090 hoger dan aanbod. Dus markt werking maakt g-kaarten duur. Daarbij zitten wij nu in crisis regio en de bedrijven bieden hun producten wereld wijd. En blijkbaar is die vraag prijs voor nV haalbaar. Genoeg die toch kopen je wordt mede genaait door je mede gamer hier en internationaal die deze prijzen accepteren door toch te kopen. Tja en voor American is het ook duurder maar niet zo extreem duurder.
RTX 4080;3;0.267863929271698;Wie heeft het over euro? Ik heb het over crypto & beurswaarde. (Aandeel houders tevreden stellen)
RTX 4080;3;0.348202109336853;Fixed that for ya
RTX 4080;3;0.39151397347450256;Cost per frame of zoals GN het zegt: Als je bij bij elk procent dat de kaart beter is, de prijs ook met een procent verhoogt, dan is het niet tof meer. Tegenwoordig met CPU's gaat dit juist wel goed vanwege de AMD vs Intel rivaliteit. Daar gaat de prijs omlaag terwijl de prestaties sterk verbeteren.
RTX 4080;1;0.4058011770248413;De meute gaat waar de value zit. De 1060 is na 6 jaar nog steeds de meest gebruikte kaart, alle 'dure' versies (*80 en hoger) bij elkaar halen net aan 10% van het totale aantal GPU's in de survey. En daar zitten de 4000 kaarten nog niet bij. De 3090, die qua prijs ongeveer op dat niveau zit, heeft nog geen halve %. AMD heeft hier een uitgelezen kans om de meute binnen te halen. Als de 7600-7700-7800 goed presteren en geen fortuin kosten zal het marktaandeel van nvidia flink gaan zakken denk ik zo.
RTX 4080;1;0.5401960015296936;"""AMD heeft hier een uitgelezen kans om de meute binnen te halen. Als de 7600-7700-7800 goed presteren en geen fortuin kosten zal het marktaandeel van nvidia flink gaan zakken denk ik zo"" Als we de geruchten kunnen geloven presteert de 7900xtx zo'n 15 % minder dan een 4090,en gaat 30% minder kosten. . Time will tell...."
RTX 4080;1;0.7594802379608154;Ik heb ooit 420 euro betaald voor een RTX2070 en dát vond ik al extreem, want de duurste GPU die ik ooit aangeschaft had. Nu weliswaar een kneitersnelle laptop, maar dat is een ander verhaal en reden. Gevoelsmatig zou ik nooit over de 500 heen willen gaan voor een GPU. Echt bizarre prijzen.
RTX 4080;1;0.29059314727783203;Ik heb nu een 3080 voor 719 op launch maar ik ga kijken of ik een 7900xtx kan bemachtigen. Voor het eerst dat ik een AMD kaart ga nemen. Imo jaagt nVidia hun klanten gewoon weg op deze manier. 1500eu voor een 80 kaart, ze zijn helemaal gek geworden.
RTX 4080;2;0.4134153425693512;mijn 3070 voor 800,- doet het nog primaaaa, dus vooralsnog geen intentie te switchen, vooral omdat ik gewoon op 1080p speel en nog geen 1440p ga halen op dit moment.
RTX 4080;2;0.33875301480293274;"Reactie op je persoonlijke mening: ""ik was niet van plan om ooit over de 800 te gaan"". Als principe een mooie baseline alleen wat velen hier lijken te vergeten is dat je de afname euro/usd niet moet onderschatten (nvidia is een US genoteerd bedrijf). Daarnaast is er natuurlijk inflatie over 6.5 jaar. Dat bij elkaar ga je al gauw over die 1200 euro heen. Dus ja in absolute termen heb je gelijk maar relatief gezien is het prijsniveau imo verdedigbaar. Zeker als we van mening zijn dat 800 voor een 1080 in 2016 ook oke was. En laat ik het zo zeggen, mijn loon is ook niet meer gelijk aan die van begin 2016... Dat wil natuurlijk niet zeggen dat de consoles, danwel kaarten van AMD niet een betere bang for buck keuze zijn."
RTX 4080;1;0.5392390489578247;GTX 1080 msrp was 599. Inflatie van 6,5 jaar -> 745. Dat is bij lange na geen 1200. De 4080 is 1200 Heel erg natte vinger werk: 957 voor mijn 1080 met dezelfde inflatie. Ook nog steeds lang geen 1200 en zéker geen 1400 De 4080 staat ook met die dingen meegerekend in geen verhouding tot de 1080 bij launch
RTX 4080;1;0.5419943332672119;"Als je mijn bericht goed leest dan zie je wat je mist: - Google eens op ""compound inflation since 2016"". Dat is 24.7% (in de VS, EU is hoger). - En google vervolgens eens op de usd/eur koers mid 2016. Die was 1.19:1. Die is momenteel 1:1. Delta 19%. De rekensom is dan: 1.19 * 1.24 = 1.48 Dan terug naar de OP: ""800€ was oke in 2016"" maar hij ""zou nu nooit 1200 betalen"". Mijn punt is dat die 800€ in 2016 (in dollars omgerekend) nu 800*1.48 = 1184€ is (in dollars omgerekend). Dan is ruim 1400€ voor een 4080 natuurlijk wel wat duurder maar zo'n absurd verschil is het niet. Zeker als je de PPI mee gaat nemen (gestegen inkoop/grondprijskosten voor Nvidia). Die ligt namelijk hoger dan de inflatie. Met dank aan frau Ursula zijn wij in Europa tov de rest van de wereld gewoon een stuk armer geworden. Voor de headlines roepen we echter liever massaal dat het aan de corporate greed van Nvidia ligt, zo ook hier op Tweakers. De waarheid is imo genuanceerder."
RTX 4080;3;0.418476402759552;Ah, fair. Maar zoals je zegt, dan haal je nog steeds die 1400+ niet. Bovendien is met inflatie rekenen reuze leuk voor economen, maar mijn inkomen is niet evenredig met de inflatie gestegen, dus bij een prijs van 1200 euro, zoals je hebt berekend, geef ik procentueel gezien dus wel veel meer uit. Als consument moet je rekenen met je eigen koopkracht, niet met een theoretisch getal voor de hele samenleving
RTX 4080;2;0.3600110411643982;"Nou, ""De enige echte 4080""... Deze 4080 ligt qua shader percentage ten opzichte van de 4090 dichter in de buurt van een 4070 dan een 4080, gezien de vorige generatie: 4090 - 16384 shaders - 100% 4080 - 9728 shaders - 59% 3090 - 10496 shaders - 100% 3080 - 8704 shaders - 83% 3070 - 5888 shaders - 56%"
RTX 4080;3;0.43977251648902893;Nou ja, als je eerdere generaties vergelijkt dan lijkt de xx80 non-Ti vaker op de xx70 dan op de xx90 (/Titan, in ieder geval het exorbitante topmodel van de generatie).
RTX 4080;1;0.7342421412467957;Het was inderdaad zo dat de 3080 een buitengewoon genereuze kaart was, maar de 4080 is dan weer het tegenovergestelde. Relatief slechte prestaties en een gigantisch hoge prijs.
RTX 4080;2;0.5132030844688416;Oh ja inderdaad, dat valt me tegen, ik had inderdaad het idee 'de 3080 is bijna een 3090' in mijn hoofd, van de vorige generatie.. Misschien toch maar even de kat uit de boom kijken dan.
RTX 4080;1;0.9147653579711914;Volledig belachelijke prijzen. Waar je vroeger een overpriced over the top flagship kon kopen voor 830 euro (8800Ultra) Mag je nu blij zijn als je voor 1000 euro een instapkaart kan kopen. Ik wacht nog wel even wat langer met mijn 1060. Ik zou graag upgraden voor VR games, en omdat ik eigenlijk een nieuw scherm wil met een hogere resolutie, maar ik vertik het om meer dan 500 euro voor een videokaart neer te leggen.
RTX 4080;1;0.2959344685077667;In Nvidia's natte dromen betalen we bij de RTX5000 serie 1200 euro voor midrange, 2000 voor highend en 4000 voor enthusiast kaarten. En je hebt nog aardig kans dat dit de waarheid gaat worden. Je zou verwachten dat hun grip op de markt op losse schroeven komt als je deze prijsstrategieën lang genoeg aanhoudt. Als OEM bouwers dan ook nog eens zouden switchen naar AMD krijg je snel genoeg een domino effect en draaien de marktaandeel rollen om. Triest uitwringen van de markt.
RTX 4080;1;0.3010469079017639;Als je een upgrade voor je 1060 wil is tweedehands kopen geen slecht idee. Voor 400 euro vind je nl gemakkelijk een RTX 3070 wat een gigantische upgrade zou zijn tov je 1060. Onlangs nog op vraag & aanbod een rx 5700 xt gekocht als upgrade vr mn rx 480 en er mn fps exact mee verdubbeld.
RTX 4080;1;0.6506401300430298;Ik ben ook een VR enthousiasteling, en heb me er al een lange tijd bij neergelegd dat als we echt van VR willen genieten kaartjes van 500 euro sowieso waardeloos zijn. Mijn 2080TI smelt al half m'n case wanneer ik GTAV in VR speel, ik wil niet weten wat een 4090 met m'n case zou doen (al wil ik wel doodgraag weten hoe het eruit ziet met zo een kaartje) dus ik wacht nog even tot de power issues wat getackled zijn. Maar ik vrees dat onder de 500 euro je niet veel gaat kunnen doen in VR.
RTX 4080;1;0.49101752042770386;"Waarom gaat het halve bericht over het niet doorzetten van gelekte specificaties van een alternatieve 4080? ""Ze wilde iets doms doen, maar hebben ze toch niet gedaan, maar we willen nog wel even zeggen dat het dom was""?"
RTX 4080;4;0.24296890199184418;Het geeft belangrijke context voor de onwaarschijnlijke prijsstijging van de '80-serie. Dit model was oorspronkelijk bedoeld om de andere 4080 goedkoop te laten lijken. Daar was zoveel kritiek op dat dat plan is laten varen, maar daarom zitten we nu met een tactisch overprijsde 4080 die eigenlijk geen zinnige plek inneemt in de GPU-lineup van Nvidia. Of je dat een probleem vindt of niet is een losse vraag, maar het is achtergrondinformatie die je nodig hebt om deze release in het totaalplaatje te kunnen plaatsen.
RTX 4080;2;0.43900516629219055;"Het grote probleem met die stelling is dat deze 4080 de AD103 chip heeft, als enige. De ontwikkeling daarvan is alles behalve gratis; NVidia moet verwacht hebben dat ze er genoeg van gaan verkopen. Anders hadden ze een AD102 genomen en daar CUDA-cores uitgeschakeld. Dat scheelt een design (geen AD103 meer nodig) maar heeft iets hogere kosten per stuk (de AD102 is een stuk groter, maar de yields zijn niet zo'n probleem als je maar 70% can de cores nodig hebt). We kunnen dus aannemen dat de prijsbesparing van de kleinere AD103 die maal de verwachte aantallen 4080's de one-off kosten van de AD103 goed maakt, en dat betekent dat die aantallen toch redelijk significant moeten zijn. (Orde-grootte van de 4090)"
RTX 4080;2;0.35569271445274353;Dat is aangenomen dat deze prijsstelling het plan was vanaf de startblokken van het designproces, toch? Het is voor zover ik weet ook goed mogelijk dat de AD103 pas na (een gedeelte van) het ontwikkelingsproces werd gedelegeerd tot de zijlijnen op basis van een veranderde kosten-batenafweging. Dat zou ook verklaren waarom het oorspronkelijke plan van Nvidia afwijkt van de opzet van de vorige generaties.
RTX 4080;1;0.42539477348327637;Dit zijn geen gelekte specs maar is daadwerkelijk door Nvidia gepresenteerd. Als het niet in deze review werd besproken, waar dan wel? Of had Nvidia hier wat jou betreft gewoon mee weg kunnen komen?
RTX 4080;2;0.2706584632396698;Het komt zo'n beetje elk 4080/4070 bericht langs. Het is wat vermoeiend om dat verhaal telkens opnieuw te lezen. Nvidia greep in, paste de naam aan en ging verder met hun leven. Dat zouden meer mensen moeten doen.
RTX 4080;3;0.3355900049209595;Waar komen ze wat jou betreft dan mee weg? Ze presenteren iets, daar is commentaar op en ze passen het aan. Einde verhaal dan toch? Waarom nu nog bashen op iets wat al gecorrigeerd is?
RTX 4080;1;0.37357765436172485;Het was een poging om net te doen alsof ze de prijzen niet massaal hadden verhoogd, door een kaart met x070 of misschien zelfs x060 Ti specs te verkopen als een 4080.
RTX 4080;1;0.5163772702217102;Het is dus een vrij normale sprong in prestaties die we al veel vaker gezien hebben tussen generaties, alleen is de prijs nu wel verdubbeld.. Waar een RTX 3080 zo tussen de €750-850 kostte mag je nu doodleuk tussen de €1450-1550 neerleggen voor een RTX 4080.. Sorry, maar dat vind ik echt onacceptabel.. Een prijsverhoging met misschien 200 euro ofzo gezien de omstandigheden in de wereld, ok, maar een verdubbeling in prijs? Nee, dat is gewoon ronduit schandalig..
RTX 4080;1;0.4389326572418213;Niet meer ter betalen voor de modale gamer.
RTX 4080;2;0.5062273740768433;Een 4080+ is niet voor de modale gamer. Kijkend naar de Steam Hardware Survey kun je stellen dat die kaarten voor maar een select percentage gamers is. Het gros van de mensen heeft een 1060 (7,4%), 2060 (5,9%), 1650 (5,4%), 3060 (5,3%), 1050 (4,3%) en 3060 laptop (3,3%), de eerste 70 versie komt pas op plek 7 (2,6%) in de vorm van een 2070. De eerste 80 versie komt met de 2080 op de 15e plaats met een aandeel van 1,7%. Een 3090 staat er niets eens tussen en valt binnen de categorie overig. Al met al dus verre van gemiddeld/modaal te noemen. Dit lijkt me dus gewoon een tactiek om de mensen die het geld over hebben voor het ‘beste wat er is’ verder uit te knijpen na gezien te hebben dat mensen het er tijdens de tekorten voor over hadden om absurde prijzen neer te leggen. Een videokaart lijkt hiermee nog meer een prijsstelling te krijgen gebaseerd op wat mensen er voor overhebben ipv wat het werkelijk waard is na productie + een fair marge.
RTX 4080;2;0.44181546568870544;"Tsja ik ga bewust voor een 60 reeks, aangezien ik weinig game. Dit is met zijn ups en downs om het zo te zeggen, maar 5% a 10% van de tijd maximaal die ik een pc gebruik (8 a 10u per dag). Dus dan is een 60 qua stroomverbruik zeker wel te ver. Ik kan gerust een 80/90 en TI aanschaffen maar dat vind ik persoonlijk overkill. Ik heb nu nog een 2060 super maar op 4k is dat het niet. Recent scherm upgrade gedaan, maar mits wat settings naar beneden te halen, lukt het allemaal wel. Dus ga er ook niet van wakker liggen. En de vraag is, moet je alles op ultra quality kunnen draaien om te spelen ? Met de huidige prijzen zie ik gewoon mensen meer en meer shiften naar een playstation of xbox. En hier op tweakers zitten veel mensen die in de IT werken, die hebben altijd net iets meer loon. Maar de gemiddelde burger kan geen 2500 uitgeven alleen aan een videokaart. Dat is maximaal 1500 euro aan een pc, zelfs nog minder dikwijls. Het onderste en middenste segment is gewoon hetgene dat het beste verkoopt. Enja hier op tweakers ligt dat anders, maar er bestaan nog mensen buiten ""tweakers"" en ik ken er zo genoeg. Als de huidige prijzen voor graphics zo blijven stijgen dan zetten ze hunzelf buiten spel en worden kaarten alleen verkocht als er eentje kapot gaat bij het merendeel"
RTX 4080;1;0.37963131070137024;Precies, voorheen kon ik met mijn vakantiegeld een flinke game pc kopen. 2 jaar geleden met mazzel de RTX3080 kunnen bestellen op release Dag voor €720 (al stond ik nog plek 10) 2 weken later het ding in huis. Een maand later een 5600x en toebehoren in huis gehaald. Dus voor €2000 had ik een mooie pc. Nu voor dat geld haal je het nu jammer genoeg verreweg niet.
RTX 4080;2;0.38071516156196594;Het hangt er maar vanaf waar je prioriteiten liggen. Als je die kaart gedurende jaren alle dagen gaat gebruiken om te gamen en het kost evenveel als een of andere optie op een auto die je ook maar enkele jaren gebruikt… Sommige mensen roken op een jaar vlotjes voor 1500€ aan sigaretten of draaien het er door voor één weekje verlof. Mijn prioriteiten lagen dit jaar bij zonnepanelen en reversibele airco’s en ik ben blij dat ik voor 512€ een 3060ti heb kunnen kopen, voor de komende jaren is dit voldoende om me te vermaken op 1440p.
RTX 4080;3;0.282988578081131;Ik bedoelde eigenlijk modale inkomen. Maar ik begrijp je punt.
RTX 4080;2;0.44362789392471313;Dat is een kwestie van prioriteiten. Voor veel modale inkomens zou een 1080 het geld destijds ook niet waard geweest. Nu zullen er vast mensen met een modaal inkomen die sparen voor een dure kaart. Het is duur maar het is niet onbereikbaar.
RTX 4080;3;0.3577062785625458;Er is nogal een verschil tussen een kaart van 600 en 1400 euro..
RTX 4080;1;0.4920113682746887;Dat staat buiten kijf. Ik heb het er ook niet voor over.
RTX 4080;2;0.38616153597831726;Voor een FE, als je die kan krijgen, de prijzen van de 4080 gaan ongeveer tussen de 1700 en 2100 liggen.
RTX 4080;1;0.8701992034912109;"Volkomen belachelijk.. Lijkt wel Apple prijsstrategie.. Het ""gewone"" model al zo enorm duur maken dat meer mensen geneigd zijn nog iets meer uit te geven voor het topmodel.. Want als je al zoveel voor een 4080 moet betalen dan maakt die paar honderd euro meer voor een 4090 ook niet meer uit zullen velen denken.. Ik hoop echt dat deze generatie qua verkoopcijfers hard flopt hierdoor, dat dit niet de nieuwe norm wordt.. AMD zal hierdoor ook alleen maar duurder worden helaas.. Die durven inmiddels ook al ruim meer dan €1200,- te vragen voor hun topmodel.. Echt ongehoord nog niet zo lang geleden.."
RTX 4080;1;0.6439080834388733;Waar kijk je dan naar EU prijzen komt door de oorlog. Dollar>Euro Dollar prijzen ook in de VS is er inflatie zooi andere factoren en uiteraard zullen de lonen daar in de vs ook deels inflatie gecompenseerd worden en dat zijn kosten. Die uiteraard door berekend worden. Dat maakt hele tracject van resources wafers bom transport import vaste lasten hoger voor us firma dan krijg de slechte euro en dan krijg je idioot hoge prijzen. Zijn wij niet gewent maar zuid amerika afrika turkey zuid europa hebben veel meer ervaring met inflatie. Het is deze tijd van energie crisis inflatie en recesie niet de tijd om luxe producten aan te schaffen. Ipv commerciele bedrijven aan te vallen of prijs redelijk is of niet zou ik eerder kijken naar de oorzaak van die inflatie er zijn genoeg mensen die wereld wijd ook in NL nog steeds financieel goed hebben dus als 4090 blijft verkopen is er markt voor. Nv amd intel gaan niet de dollarprijs aanpassen zodat wij hier in nl een 700 euro 4090 kunnen krijgen. Dit jaar en volgendjaar zal ik niet upgraden. Zen1 en vega56.hier en daar moet ik het mee doen. En PS4 dus er zal flink wat gegamed worden.
RTX 4080;2;0.38094958662986755;Maar de 7900 XTX krijgt geen hogere MSRP en de 4090 is 'maar' $100 duurder dan de 3090. Dus de $500 verhoging van de 4080 is wel extreem en niet verklaarbaar door algehele inflatie.
RTX 4080;1;0.6358956098556519;Wat een prijzen zeg....echt niet dat ik dit ooit voor een kaart ga uitgeven 🙋‍♂️
RTX 4080;2;0.3837200701236725;Heb jaaaren geleden 600 uitgegeven aan een 980, daar wel heel lang mee gedaan en was achteraf een super aankoop, maar nu ook niet verder gekomen dan een 3060 Ti voor 700 en dat was meer dan zat. Heb hem wel al een dik jaar en het is een heerlijke kaart voor toen een prima prijs. Deze kaarten zijn voor mij ook totaal onrealistisch, de TUF gaming bijvoorbeeld staat in FR al gelist voor 1969,- euro. Zie wel wat er over 2 jaar voor iets van 600 te krijgen is, wellicht een 4070 of een 5060, wie weet.
RTX 4080;3;0.2799505591392517;Of de 6900 xt die al regelmatig naar de 700 euro zakt en een hoop rasterization performance biedt als je geen RT hoeft, en ik verwacht dat dat doorzet na de release van de 7900.
RTX 4080;2;0.2783793807029724;Ik heb nu al bijna 2j een 6900XT, en ik heb er nog geen moment spijt van gehad. Komende van een 1080Ti is een 6900XT een zeer mooie step, en de prijs was ook OK zeker in vergelijking met alles wat toen van Nvidia beschikbaar was. Als non-miner maakte het niet uit voor mij van welke kamp de kaart kwam. En er zijn zoveel meer titels die profijt hebben van goede rasterization prestaties als je dat vergelijkt met welke games daadwerkelijk beter zijn met RT. Ik ben nu eindelijk aan CP77 begonnen, en op 1440p haal ik zonder RT ongeveer 120fps (met alles op Ultra) en met RT aan zit ik aan 45fps. En ik moet bekennen dat ik het verschil tijdens het spelen eigenlijk nauwelijks merk. Edit: typo
RTX 4080;1;0.594376266002655;Ik heb exact de zelfde stap gemaakt, geen seconde spijt van gehad, een Saphire nitro+ 6900XT op de kop kunnen tikken voor ~1000 euro. Wat betreft RT ben ik het ook helemaal met je eens ik zie het eigenlijk niet als ik het spel speel en er veel actie plaats vind dan moet je echt stil gaan staan en er specifiek op gaan letten.
RTX 4080;3;0.36998021602630615;Het enige is dat de nieuwe Nvidia kaarten wel een betere performance / Watt weten te bieden. Maar daar betaal je dan wel weer de hoofdprijs voor.
RTX 4080;2;0.4311472177505493;Bizar toch. Je zou toch verwachten dat je voor elke fps meer en meer geld zou moeten betalen. Maar deze kaart is rond de 33 procent trager en ook 'maar' 33 procent goedkoper.
RTX 4080;1;0.4626512825489044;Precies, 2x zo duur als een 3080, en maar 45% sneller in 4K. Ik denk dat ik wacht op de R7900 reviews, en die vind ik eigenlijk zelfs al te duur...
RTX 4080;2;0.36044690012931824;Nu maar hopen dat de meeste potentiele klanten er zo over denken. Voor mij is het €1000 max voor een 80 serie (ook al lijkt dit meer op een70) en anders tuf ik gewoon door op de 2080 zolang het kan.
RTX 4080;2;0.35754552483558655;"Ondertussen vergeten mensen ook vaak dat de keuze ""niet upgraden"" ook een optie is Ik werk al jaren met de RTX 2070 Super en die volstaat nog meer dan voldoende voor de games die ik speel. Competitive games zoals Overwatch 2 en F1 2022 doen het uitermate goed met enorm goede visuals op een 4k en 144Hz scherm, waarbij de FPS altijd boven de 100fps uitkomt. Meer dan prima. Scheelt weer een maand huur Ik wacht wel op AMD en ik ga pas upgraden zodra ik games tegenkom die gewoon niet meer fijn werken op mijn RTX 2070 Super."
RTX 4080;2;0.25705042481422424;Ach en mensen zoals jij vergeten ook weer dat sommige mensen al een tijdje zitten te wachten op een upgrade. Mijn GTX 1070 heeft het erg zwaar in veel spellen en ik moet regelmatig ver terug in kwaliteit op die 60fps te behouden. Om nog maar te zwijgen over de R7 570X van mijn vriendin. Afhankelijk van de 7000 serie prijzen natuurlijk, maar de 6900XT gaat vaak voor 699. Dus dan moet ik die maar aanschaffen. Maar met de 40 serie hebben ze het wat mij betreft dusdanig bont gemaakt dat ik me heel erg afvraag of ze zichzelf niet in de voet schieten op lange termijn. Met deze prijzen ben ik bijvoorbeeld best wel geneigd om dan maar een console te kopen. Ik de vraag is of ik dan ooit nog terug keer naar de PC wat gamen betreft. Ik gok dat ik vast niet de enige ben.
RTX 4080;2;0.4416755139827728;RX560 hier... Ik zou graag naar een wat oudere kaart upgraden, maar zolang de nieuwe GPU's zo gigantisch duur zijn, zakken de oudere kaarten niet in prijs.
RTX 4080;3;0.45922917127609253;Ik zie 1080ti’s voor rond de 200 dat is redelijk volgensmij, anders rx 580 wellicht?
RTX 4080;2;0.427388459444046;Het is niet omdat de markt gek is dat je het evengoed doodnormaal vindt om 3 generaties/features/efficiëntie achter te zitten. De 10-serie is van 2016, de 1080Ti kwam in maart 2017 uit. De RX560 kwam in juli 2017 uit. Ook al is het een rebrand van een jaar eerder én sneller, echt technologische vooruitgang kan je dit nauwelijks noemen. Zelfde voor RX580. Als Benji daar dan weer 5 jaar mee wil doen kan het best dat de driver-updates/support ergens in die tijd gewoon stoppen. Vijf jaar later wil je gewoon ook een efficiëntieslag maken. Die RX560 kan zonder PCI-E connector werken. Die 1080Ti zuipt 250W. Een RX6600 zou op dat vlak een vooruitgang zijn (maar is ook alweer last-gen). Pakweg 3x sneller aan 130W, binnen 5-10% van die 1080Ti en is recent ruim gehalveerd in prijs naar een nu bijna geloofwaardige 270 euro. Misschien nog even wachten en die RX6600 zit daadwerkelijk dicht genoeg bij de 200 euro om het te overwegen.
RTX 4080;2;0.4845584034919739;Vaak zitten mensen ook gewoon al langer dan jijzelf te wachten op een beschikbare, redelijk geprijsde kaart. Ik zit zelf nog met een midrangekaart van 2017 en moet vaststellen dat ik m'n budget gewoon ga moeten verhogen. Nu, ruim vijf jaar later en na bijna gehalveerde prijzen na de laatste boom, kan ik voor iets meer dan prijs van toen... een verdubbeling in performance krijgen. Joepie Lange tijd hadden we zo'n trend elke 2-3 jaar.
RTX 4080;3;0.35283976793289185;Vroeger had je 2e hands voor 150 euro gewoon een goede midranger. Dat kan je tegenwoordig wel vergeten
RTX 4080;1;0.40857771039009094;Tja ik zit met een 3090 FE te gamen op triple 1440p. Zijn die schermen 144hz krijg ik maximale FPS van 70-80. Ik ben echt wel toe aan een flinke upgrade, maar niet voor dit geld...
RTX 4080;2;0.4489275813102722;Ik zit met MSFS 2020 nog steeds aan te klooien met mijn 3080 Ti echter In 2D heb ik 4K met high/ultra settings en prima framerate, maar met VR heb ik op 2K resoluties al een slechte framerate op low settings en dat ziet er niet uit. VR is echt heel zwaar en dat is voor mij zeker een reden om te upgraden.
RTX 4080;1;0.49037545919418335;Maar als je een single player bent en je wilt games als RDR2 of Metro in 4K spelen, wil je ook de mooiste beelden is dit een dooddoener zulke prijzen. Erg jammer is het.
RTX 4080;3;0.2680090069770813;Mijn 980 is op zich wel eens aan een upgrade toe, maar ik weet niet of het een Nvidia gaat worden, met deze prijzen.
RTX 4080;2;0.3614922761917114;Wat is gamen toch duur geworden... Ja, dit is high end... maar vroeger (lees 5 a 10 jaar geleden) was het naar mijn gevoel echt veel en veel goedkoper.Intel Core i9-13900K BoxedASUS PRIME Z690-AFractal Design Meshify 2 Compact Black SolidNoctua NH-D15 BruinKingston Fury Beast KF560C40BBK2-32Seasonic Focus GX-1000Ongeveer: €1600,- Een nieuwe videokaart van de huidige generatie..4080 € 1500,-4090 € 2500,-7900xtx € 1200,-Ongeveer: €1350,- Een mooi 4k scherm* Ongeveer: €580,- Nog wat randapparatuur*toetsenbord € 90,-muis € 70,-koptelefoon € 200,-Ongeveer: € 360,- Dan gaat je PC zo naar de € 4000,- *(Gemiddelde van Pricewatch top 10)
RTX 4080;3;0.4178540110588074;Klopt. Maar daarbij komt wel dat de mogelijkheden van high-end enorm zijn gegroeid. Waar je vroeger een dergelijk bedrag kwijt was om op 1440p 144hz te gamen, is dat tegenwoordig al voor veel minder geld haalbaar. Er is als het ware een segment boven de traditionele high-end gekomen. Uiteindelijk is de gamer die €1500 bereid is uit te geven er wel op vooruitgegaan. Alleen mensen die zich uit principe verbinden aan het (op een na) duurste, zijn flink meer kwijt.
RTX 4080;2;0.43579956889152527;Maar dat is toch niet hoe het werkt? Er was een tijd dat 1080p gaming alleen voor high end kaarten was. Dit verschuift gewoon. Ooit was 720p en lager normaal, nu is 1440p vrij normaal en schuift het meer en meer door naar 4k.
RTX 4080;3;0.5070674419403076;Kwa temps wil ik hem wel! Mijn 3080Ti FE vind ik maar een lawaai ding….. Maar de prijs…… 500,- bij leggen (minimaal) voor 20% en minder lawaai….. Krijg dat niet meer uitgelegd aan de vrouw ben ik bang haha. Ik wil wel een snellere kaart voor 4K maar ga AMD ook nog even afwachten. Ze doen het wel slim want de 4090 lijkt nu een koopje vergeleken met de 4080
RTX 4080;1;0.5401217937469482;4090 een koopje? Weet niet wat jij gesnoven hebt...
RTX 4080;3;0.43215325474739075;Afhankelijk van je inkomen kan het best meevallen of als je ervoor hebt gespaard.
RTX 4080;1;0.4610483646392822;Dan is het alsnog een heleboel geld voor een videokaart. Een 4090 als koopje wegzetten vind ik wel heel ver gezocht.
RTX 4080;3;0.31805095076560974;Zit wat in ja
RTX 4080;2;0.5002738833427429;Het stond er als in verhouding met de veel te dure 4080 he Vind alles boven de 1100,- te duur. Dat is wat ik voor de 3080Ti betaald heb wat voor toen al aardig de max was. Maar wellicht is dit ook wel het begin van het einde van het PC gamen want zie het niet als alles in deze wereld ook niet meer zomaar goedkoper worden en ik zou het wel willen betalen voor de hobby maar kan dit simpelweg niet meer betalen op deze manier helaas. Ja langer sparen generatie overslaan is dan de optie natuurlijk. Wat natuurlijk ook gewoon kan maar ben wel aardig verkocht aan 4k helaas en dan is een upgrade nog wel de moeite...... maar niet tegen elke prijs. Had ik maar bij 1440P gebleven dan was dit niet zon ''probleem''
RTX 4080;5;0.4405302107334137;Het bonnetje van mijn 3080Ti
RTX 4080;5;0.3737713098526001;Ik vind de 3080Ti FE zeer meevallen qua lawaai eigenlijk. Heb 'm zelf ook. Zitten je fans/koelblokken niet vol met stof ofzo? Ik heb liever deze dan een third party bakbeest, dit is een mooie compacte kaart.
RTX 4080;2;0.3408530652523041;Er zit dus dezelfde voedingskabel op die bij de 4090 er ook op zit? Dus weer kans op smeltende connectoren?
RTX 4080;1;0.36673906445503235;In plaats van 4 verloopjes, 3 geloof ik. Tijd zal het leren of deze ook gaan smelten.
RTX 4080;3;0.4153367877006531;Smelten is sterk afhankelijk van de warmteontwikkeling in de kabel, wat weer een directe functie is van de stroom en dus van het verbruk van de aangesloten kaart. Dus nee, zelfs als de kabel identiek is, dan nog is de kans op een smeltende kabel véél lager.
RTX 4080;3;0.4201289117336273;Gemiddeld dus 40% sneller als de 3080, maar 2 keer zo duur
RTX 4080;5;0.28280550241470337;En dat terwijl we gewend zijn dat elke nieuwe tech-generatie voor ongeveer de prijs van de vorige generatie verkocht wordt. Op die manier kregen we elke paar jaar meer performance voor hetzelfde geld. Nvidia denkt daar anders over. De 4080/4090 lossen niet de 3080/3090 af, maar zijn gewoon als nieuwe prijsklasse boven de oude kaarten gepositioneerd.
RTX 4080;1;0.40102770924568176;Gogo AMD! Geen AMD shill, maar ik hoop dat RX7000 serie goeie concurrentie wordt voor Nvidia en ze een reality-check krijgen. Maarja, de coronapandemie heeft maar bewezen dat mensen bereid zijn dit geld neer te leggen voor een videokaart, dus 'who blames Nvidia'?
RTX 4080;3;0.24358540773391724;Nvidia zit nog steeds met en behoorlijke voorraad van de 3000 serie geloof ik. Persoonlijk zie ik de corona pandemie maar als een tijdelijke increase in vraag naar videokaarten, nu dit voor het grootste gedeelte voorbij is geeft men weer meer geld uit aan restaurants, festivals, kroegen en andere uitstapjes en zijn mensen denk ik niet zo bereid meer om zoveel geld uit te geven aan een videokaart. Ik denk dat dit zichzelf wel snel corrigeert wanneer de verkopen tegen gaan vallen, helemaal als AMD wel relatief normale prijzen blijft hanteren (waar het wel op lijkt nu).
RTX 4080;2;0.37779054045677185;Het kwam niet zozeer door corona, maar door de winstgevendheid van het minen van Ethereum. Dat is nu over.
RTX 4080;3;0.3403870165348053;Iedereen hoopt dat. Maar een goede concurrent betekend dat hij ~10% betere performance per euro heeft. Niet dat hij de helft van de prijs kost van equivalente Nvidia kaart.
RTX 4080;3;0.4982442855834961;Hangt er een beetje vanaf wat AMD wilt bereiken. Als ze vooral marktaandeel willen dan kunnen ze Nvidia wat verder undercutten (zoals we ook zagen in de mobiele wereld met de eerste OnePlus en Xiaomi Poco telefoons). Daarnaast móet AMD wel een betere prijs prestatie verhouding hebben, al helemaal als we het hebben over traditioneel rasterized rendering. Nvidia heeft het voordeel in Raytracing en DLSS. Een RX6700XT is soms hetzelfde geprijsd als een 3060, toch kopen mensen vaak de 3060.
RTX 4080;1;0.44023972749710083;Weet er iemand waar de FE te koop zal zijn in België en Nederland?
RTX 4080;5;0.4390193521976471;forumtopic: [NVIDIA GeForce RTX 40XX] Levertijden & Prijzen
RTX 4080;2;0.3009641766548157;Benieuwd of de 3000 serie prijzen wel gaan zakken dan als de 4000 serie zoveel moet kosten en dus lang niet voor iedereen is. Dan zullen heel veel mensen nog wel lekker op hun 3000 blijven zitten.
RTX 4080;2;0.419926255941391;Zou ik niet op rekenen, gebeurde ook niet bij de stap van 2k naar 3k. Helaas......
RTX 4080;1;0.5194051265716553;"Het is echt absurd hoe groot die dingen zijn. Ik herinner me nog dat ik eind jaren 90 een Speedstar Pro en een Monster 3D in mijn toenmalige PC had zitten. Dat voelde al als een hele constructie maar die 2 bij elkaar is waarschijnlijk nog geen fractie van het gewicht en de omvang van een 4080. Het is een beetje ""too much"". En vooral qua energieverbruik zijn kaarten als de 4080 niet echt van deze tijd zou ik zeggen."
RTX 4080;3;0.4805821180343628;Ik vind de grootte van die 4080 nog wel meevallen. Moet je eens kijken hoe groot de 4090 is:
RTX 4080;2;0.34727126359939575;Voor mij waren die prijzen de nagel aan de doodkist voor PC gaming. Ik heb nog 1 keer een game laptop met een GTX1070 gekocht, maar dat was het niet voor mij. Toen de XSX uitkwam met 12TFLOPS (bijna vergelijkbaar met een GTX2080) ben ik een console peasant geworden, 4K gamen met 60FPS voor 500 euries. Was even wennen die controller dat geef ik toe, maar er zijn ook inmiddels keyboard opties. Kortom, voor GTX4080 koop je: - 1 PS5 (als je 'm kan vinden scoren) - 1 XSX - 1 XSS voor op de slaapkamer
RTX 4080;2;0.5860316157341003;Of gewoon een PS5 + 120hz OLED TV. Kan dat argument dat je ook nog een TV nodig hebt ook meteen van tafel En ja, de consoles hebben nog nooit zo een goede performance én value geboden als deze generatie. Spekkopers zijn het en ook ik vind het zeer verlokkelijk. Helaas ondersteunen de meeste spellen geen muis / toetsenbord en komen veel spellen er simpelweg niet voor uit. Er zijn teveel toffe niet-AAA games op de PC die een overstap erg lastig maken. Wat dat betreft hoop ik vooral op enige vooruitgang in de PC markt die er voor zorgt dat we tzt een 500€ GPU hebben die tenminste sneller is dan wat we in de huidige consoles hebben. Dat zou al flink winst zijn ten opzichte van nu.
RTX 4080;5;0.4895126521587372;Kudo's voor de inleidende paragraaf 'Verleiding en misleiding', die was echt briljant Desondanks, prachtige kaart die ik vast een keer ga halen wanneer de prijzen ooit gaat zakken, of eerst maar mijn 3080 verkopen
RTX 4080;3;0.3653776943683624;Ik heb zelf een 3080, maar met deze prijzen sla ik die kaart wel over, en misschien de volgende generatie ook.
RTX 4080;3;0.5208445191383362;Ik upgrade vaak pas als het de moeite waard is, vaak pas een verdubbeling of meer ten opzichte wat ik op dat moment heb.
RTX 4080;4;0.38318777084350586;Snap ik, maar ik doe ook aan VR gaming en dan kan wat extra power geen kwaad op zich Hoewel er zeker ook nog ruimte is om eerst mijn processor te upgraden (een ryzen 5 2600)
RTX 4080;1;0.6969272494316101;Volgens mij is jouw zwakste schakel de CPU, niet de videokaart. De AMD prijzen zijn gedaald, kijk eens voor een Ryzen 5800X. Je kan nog redelijk goedkoop upgraden.
RTX 4080;5;0.4578208923339844;Lijkt mij een goede idd.. zeker met Black Friday in de aantocht is dit een prachtig moment om mijn CPU te upgraden
RTX 4080;1;0.501185417175293;Ergens hoop ik dat nVIDIA snel kopje onder gaat vanwege hun arrogantie en greed.... nVIDIA komt er bij niet meer in, ondanks diverse verschillende kaarten de afgelopen decenia
RTX 4080;2;0.36194777488708496;Mooie review met eindelijk een 13900K Tweakers 😀 zo hoort het zoveel mogelijk de CPU bound eruit halen waar mogelijk. De 5950X kon echt niet meer. En ja wat moet je zeggen van deze Nvidia kaart 4080 die ongeveer een 15% sneller is dan de 3090ti is eigenlijk veel te duur. Het lijkt of ze performance steeds meer schalen met de absolute prijs. Deze moet niet 1199€ en ongetwijfeld een stuk meer nog zijn maar zou voor 699 of max 799€ in de schappen moeten liggen. Dan krijg je echt waar voor je geld. Nu is het weer iets meer dan voorheen maar vooral Nvidia die zijn zakken verder vult. Ik hoop dat de AMD RX7900 XTX de 4080 kan voorbij gaan met prestaties en niet boven de 1000€ uitkomt in NL wat natuurlijk ook wel weer 1200 zal zijn. Als AMD die kaart voor 799€ zou kunnen aanbieden heb je echt een winnaar. Vrees dat AMD ook het maximale eruit zal willen halen. Het verschil tussen de 4080 en 4090 is wel enorm in alles. En dan is de prijs gewoon niet goed. Dan zou zelfs de achterlijk dure 4090 een betere value zijn. Op deze manier prijzen Nvidia en AMD zich echt uit de markt. Met de hoge inflatie wereldwijd en enorm toegenomen energieprijzen kan dit niet ongelimiteerd doorgaan.
RTX 4080;1;0.6373254060745239;Dat is wel heel onrealistisch, verwachten dat amd de 7900xtx voor 1000 euro gaat aanbieden in NL. Laat staan je de 799 die je aan haalt 😂. De 4080 kost 1199$ en dat is in Nederland in euros al ruim 1469. De 7900xtx kost 999$, dan kun je geen 1000 euro verwachten. Zelf denk ik eerder zo rond de 1200-1250 euro. Men zegt allen wel te willen wachten op amd, maar willen uit eindelijk ook helemaal geen 1200 betalen. Dus dan kun je beter bij een 6900xt of 3080 gaan kijken en niet een maand wachten op de release..
RTX 4080;1;0.8700403571128845;Mijn geld gaan ze in ieder geval niet krijgen en mag hopen dat vele anderen er net zo over denken met deze prijzen... Echt te belachelijk voor woorden. Voor dit geld kocht je een paar jaar geleden nog een leuk compleet systeem.
RTX 4080;1;0.9236724972724915;wat een belachelijke prijzen zeg. Ik koop wel een 6800 tot die wat zakt.
RTX 4080;1;0.681251049041748;Nvidia toont eens te meer aan dat ze geen enkel respect hebben voor de consument én AIB's. En nu prijzen ze zich volkomen uit de markt. Belachelijk. Ik ben wel te spreken over de strenge(re) toon van Tweakers in deze review. Het kan echt geen kwaad om zo nu en dan eens een wat scherpere kritiek te leveren op dit soort haantjesgedrag. Ik vrees dat Nvidia altijd de randjes zal opzoeken en dan een miniem stapje terug gaat zetten als de backlash te groot wordt, en aan onze kant mogen we prima wat harder terugduwen om ervoor te zorgen dat we onszelf iets beter er tegen te beschermen.
RTX 4080;2;0.3452061414718628;Bij wat internationale webshops staan al prijzen online. Het blijft speculeren, maar de prijzen tot nu toe zitten nog flink boven de adviesprijs. Zie hier: Coolmod Rue Du Commerce Ik zou de prijzen bij Rue Du Commerce nog met een korreltje zout nemen, aangezien die waarschijnlijk nog worden gewijzigd. Edit/aanvulling: de goedkoopste die ik tot nu toe heb gezien is deze.
RTX 4080;1;0.49773094058036804;Sorry maar ik vind het echt nergens meer over gaan allemaal, bah. Het zou toch een feestje moeten zijn om een nieuwe generatie GPU te kopen? Lekker uitzoeken, reviews lezen, specs en benchmarks vergelijken, voorpret, en als je dan de keuze hebt gemaakt gewoon bestellen en de volgende dag in huis, toch? Nope , de prijzen zijn echt belachelijk en als je al iets wilt kopen moet je een gemiddeld maandsalaris meenemen en met bots, scripts, alerts, telegramkanalen, distill aan de gang en middagen F5'en, heel veel tijd en energie er in steken, nee, ik, vind er weinig meer aan.
RTX 4080;3;0.4708532691001892;Nvidia bekijkt het maar lekker, AMD heeft op papier een veel beter aanbod met hun 7000 serie kaarten.
RTX 4080;2;0.23435796797275543;Ik zit al sinds de Viper 550 op NVidia, maar met de huidige scrupules overweeg ik dus echt over te stappen naar een Radeon, voor de komende generatie. Het feit dat je AMD nodig hebt om eerlijk te blijven....
RTX 4080;3;0.2951434254646301;Conclusie: De echte bang for buck zit momenteel nog bij de RTX 3080/3090. Ik heb een 3090 FE en de enige serieuze upgrade die ik overweeg is een 4090. Maar niet voor 2000 Eur.. Ik denk dat ik voor het eerst in 2 jaar toch maar een generatie achter ga lopen in het high end segment van GPU's.
RTX 4080;3;0.4280090630054474;Misschien komt er een 4080ti voor een betere prijs. We moeten hopen op steviger concurrentie van AMD.
RTX 4080;1;0.46736618876457214;Of je stemt ook met je portemonnee en koopt een AMD. Een grote middelvinge is wat NVidia kan krijgen op het moment. Ik heb een 1080ti en die loopt nog steeds prima, kostte toen €800,- en vond dat al veel geld voor een GPU. Voor een vergelikbaar bedrag ga ik nog wel eens mijn opvolger kopen. No way dat ik meer dan €1000,- ga neerleggen voor een GPU. Aan de andere kant is het briljant van NVidia, met deze prjijzen houden ze de 30 serie relevant. Mensen kopen nu een 30serie kaart vanwege de prijs/prestatie. Als NVidia de prijzen van de 4080 laat zakken naar +/- €800,- denkt iedereen, wow wat een koopje, en vliegen ze de deur uit.
RTX 4080;1;0.6618049740791321;Deze hele serie van nvidia zou negatieve sterren moeten krijgen op prijs gebied
RTX 4080;1;0.4629265069961548;Mja voor die prijzen mogen ze hun RTX4Xxx houden. Als ik het goed begrijp vragen ze 1500€ voor een xx80 kaart die al sowieso extreem overpriced is maar zijn volledige capaciteit niet eens gebruikt, dat doet me dan vermoeden dat er ruimte is voor een RTX4080 Ti met een volledige ingeschakelde chip voor NOG meer €€. De afgeschreven 12GB zal uiteindelijk een 4070 of een 4070Ti worden maar ook voor +1000€ verkocht worden? En waarschijnlijk niet volledig ingeschakeld zoals de geruchten rond gaan dus met andere woorden een gestripte 4070 12 gb en een volledige ingeschakelde chip als 4070 Ti voor... +1000€ ... Voor een XX70 kaart??? Dan kijk ik even in mijn glazen bol en voorspel ik een 4060 voor 700/800€. Mja die zijn inderdaad de grenzen aan het aftasten. Als ze met deze serie niet veel klanten verliezen dan vrees ik dat de RTX5xxx enkel voor de happy fiew zal worden. Ik zit al een paar jaar te gamen op een 2080 Ti maar als die de geest geeft zal het toch geen RTX4XXX worden, misschien zelfs helemaal geen Nvidia om eerlijk te zijn.
RTX 4080;3;0.5191207528114319;goede review, echter zie ik de 3080ti FE wel genoemd in de te benchmarken kaarten, echter zie ik deze kaart niet terug in de resultaten. Kan deze nog worden toegevoegd?
RTX 4080;1;0.5558364987373352;Fantastische specs, gruwelijke prijs. Voor mijzelf als casual gamer, gemiddelde van een uurtje per dag, zijn dat echt belachelijke bedragen. Gelukkig zijn er genoeg gamers die er anders over denken en die nu hun 20 / 30 cards gaan dumpen. Mooi om die nu op te pikken.
RTX 4080;2;0.5262278318405151;Met deze launch is één ding zeker voor mij: Zeker geen 4080 en 4090 voor mij. De prijs mbt prestaties is het niet waard.
RTX 4080;3;0.47274476289749146;Op de review van Tweakers is niets aan te merken dus een prima review. Toch blijf ik mij wel nog steeds verbazen om zowel de prijs als ook het verbruik van zo'n high-end videokaarten. Dat een x80 kaart op een hoger prijsniveau ligt kan ik nog wel volgen maar bijna 1500 euro vind ik toch fors geld. Vooral ook wanneer je het dan gaat vergelijken met voorgaande series die de helft waren qua adviesprijs. Voor mij blijft dit in elk geval absoluut geen kandidaat voor een eventuele upgrade maar nu spreek ik wel vanuit een niet-gamer. Wel doe ik veel aan video en foto-editing waarvoor een externe videokaart eigenlijk onmisbaar is. Maar bij vooral video-edit is het meer een tijdskwestie dan dat je betere resultaten kan behalen zoals bij gaming.
RTX 4080;1;0.804412841796875;Mijn volgende videokaart gaat een AMD worden. Nvidia is echt mensen hard geld aan het aftroggelen, de bedragen zijn gewoon bizar.
RTX 4080;2;0.31654632091522217;Misschien een idee om naast 'prestaties per watt' ook 'prestaties per euro' weer te geven. Een kaart mag zo goed zijn als die wilt als de 'prestaties per euro' de pan uit swingen mogen ze hem houden.
RTX 4080;1;0.40519633889198303;Opa hier.... vroegah kocht ik voor 1450 een complete game pc met alles erop en eraan. Ja inflatie enzo, maar damn gasten, ik denk dat er stiekem toch echt ingespeeld wordt op FOMO en artificial schaarste om zo de prijzen idioot hoog te krijgen.
RTX 4080;1;0.5721285343170166;"Ik wil hem nogsteeds hebben, als ik er eentje kan vinden die in mijn meshilicious past zonder dat ik hem hoef te verbouwen.. (4 slots kan maar dat is niet ideaal). vind de prijzen belachelijk, maar wetend dat websites als Megekko nog eens een paar honderd euro daar bovenop gaan gooien omdat ze weten dat er genoeg vraag is.. dat maakt mij erg verdrietig. En voor mensen reageren met ""mja, hun goed recht"", ik weet wat een commercieel bedrijf is, maar ik denk dat er ook grenzen moeten zijn want Megekko maakt echt genoeg winst met of zonder die inflatie."
RTX 4080;1;0.42187830805778503;Wanneer komen de sub 300 euro kaarten uit?
RTX 4080;3;0.4557999074459076;Hmmja, dan toch nog maar een jaartje extra wachten voordat ik ga upgraden. Vind het toch wel jammer eigenlijk, want als ik nu met mijn GTX 1080 probeer een potje Diablo 2: resurrected te spelen dan lukt dat wel prima, maar gaat mijn videokaart echt behoorlijk blazen en aangezien we tegenwoordig klein wonen heeft mijn vriendin (die dan in slaap wil komen) daar last van!
RTX 4080;2;0.38967880606651306;"Gezien Nvidia heeft afgestaan van het concept ""goedkopere kaart met dezelfde modelnaam maar mindere specs"" zou ik denken dat de prijs van de enkel overgebleven 4080 ook goedkoper geprijst moet zijn, gezien de productie op één kaart is gefocust."
RTX 4080;1;0.45440784096717834;"Kan iemand mij uitleggen waarom er zo'n groot verschil zit tussen de prijs in euro's en dollars. Momenteel is de euro 1,03 dollar. Waarom betalen wij als consument dan meer? Logischerwijs zou je dan toch zeggen dat de prijs ""eurotechnisch"" lager moet liggen dan 1200?"
RTX 4080;1;0.596175491809845;Dollar prijzen worden altijd getoond zonder BTW. Euro prijzen met.
RTX 4080;1;0.6768308281898499;Hoe Tweakers met dit artikel de plank misslaat is bijna niet te geloven. Iedereen op internet vind een prijsverdubbeling problematisch maar tweakers hoor je er niet over. In feite vinden ze het blijkbaar een koopje want in de mining hype was het nog duurder. Met zo`n instelling met je geen scalpers meer nodig, de reviewers praten het wel goed. Wederom word de onrealistische advies prijs weer genoemd. Het gaat een wonder zijn als je hem daar voor kopen kunt. Ga gerust uit van 300 meer, dus 1800 voor de 4080 is gewoon realistisch.
RTX 4080;3;0.2878929376602173;Lees de inleiding en conclusie (nog eens?) van begin tot eind door, dan zul je zien dat je kritiek onterecht is.
RTX 4080;1;0.4663652181625366;Gedaan. Ik blijf bij mijn standpunt, tweakers is veel te mild en verkopen het veel te makkelijk. De prijstoename is recht evenredig met de performance. Dit is hoogst ongebruikelijk en als je daar mee door blijft gaan kost een kaart op een gegeven moment tonnen. Dit valt gewoon niet goed te praten.
RTX 4080;1;0.5101667642593384;je moet wel echt iets los hebben wil je 1469 euro gaan betalen voor een xx80 class kaart. dat slaat echt nergens op. wacht op AMD die komt natuurlijk ook met echt te hoge prijzen maar wel beter en sws betere performance dan de 4080. bijna 1500 euro zeg, daar kan je letterlijk een hele goeie pc van bouwen. nvidia is de weg kwijt.
RTX 4080;1;0.7135401964187622;Nou, ik weet nog niet zo of die performance beter gaat zijn hoor. 0, maar dan ook echt 0 vergelijkingen tijdens de presentatie met Nvidia. Best vreemd.
RTX 4080;3;0.43809789419174194;De concurrent is de 4080, hoe zie je voor je dat ze met die kaart vergelijken als er nog 0 gegevens van uit waren om het moment dat ze de 7xxx serie aankondigde Van de 6950 weten dat die vaak sneller is als de 3090(ti) en daar wordt mee vergeleken, de rest kan je met een beetje gezond verstand wel bedenken
RTX 4080;3;0.4156958758831024;met een gemiddelde van 60% ish meer performance volgens amd voor 1000MSRP zal de 7900xtx goedkoper zijn dan de 4080 en beter.
RTX 4080;1;0.5191663503646851;Je gaat hem daar echt niet voor krijgen. Het is de bekende tweakers omreken methode. Tel er maar 300 bij op om een realistiche prijs te krijgen.
RTX 4080;3;0.35412275791168213;Blij met mn xbox, ik kan prima 2 uurtjes warzone gamen en graphics maken me niet uit in Warzone:) . Kga wel upgraden naar de nieuwste xbox en nieuwe monitor(voor de 120 fps), maar zelfs dan betaal ik maar de helft van deze kaart.
RTX 4080;1;0.7118975520133972;1469? Mijn 3080 heb ik destijds voor 720 gekocht.. Belachelijk
RTX 4080;2;0.613492488861084;Uiteindelijk zal dit er toe leiden dat nog meer mensen op de console gaan gamen. Niet dat de performance met elkaar te vergelijken valt, maar omdat het prijsverschil steeds groter wordt. Jammer.
RTX 4080;3;0.39982855319976807;Ik kan mij wel vinden in deze video review. Wil je nu een sterke GPU dan spaar je door of je legt wat bij voor de 4090 en anders nog even de kat uit de boom kijken met betrekking tot wat AMD straks aanbied. Leuk om te zien hoe de prestatie sprongen waren tussen de kaarten. O.a van 980 naar 1080 etc.. Zelf heb ik een half jaar geleden een mooie sprong gemaakt. Van 980 naar 3080. Die kaart gaat mij nog wel een tijdje tevreden houden. En dan wordt de volgende waarschijnlijk de 5k serie. Dan zal het wel weer kriebellen Als ik nu nog een 980 had zou ik voor de 4090 gaan.
RTX 4080;3;0.6085929274559021;Had graag wat fotos gezien van de kaart in de test opstelling. Beetje beeldvorming hoe groot het ding nu echt is.
RTX 4080;3;0.6452508568763733;Ik vind het wel mooi dat deze veel snellere kaart toch minder verbruikt en ook een stuk efficienter is dan de RTX 3080 ! Alleen erg jammer van het prijskaartje wat er aan komt te hangen....
RTX 4080;1;0.5344531536102295;De FE kwam toch morgen pas?? Ik zie dat hij nu alweer op uitverkocht staat Vervelend. Ik had er wel eentje willen hebben.
RTX 4080;4;0.37383097410202026;Ik ben en blijf helemaal tevreden met mijn 3080. 2160p is ook volledige overkill in mijn bescheiden mening. Mijn CX staat op mijn pc ingesteld op 1800p waardoor je veel aan prestaties wint en het visuele verschil is voor mij toch niet te zien. Digital Foundry analyses ter zijde, ik zie het toch niet. De enige reden waarom ik hem zou wien is de RT verbetering.
RTX 4080;1;0.679340124130249;Veel te duur. 600 euro is max wat ik voor een GPU wil uitgeven.
RTX 4080;1;0.3078273832798004;Komt de RTX 4070 kaart nog uit?
RTX 4080;3;0.3358355164527893;Mooi stukje techniek, hele mooie stap in efficiëntie ook en wat gpu´s tegenwoordig op 4k aan fps kunnen presteren is echt serieus indrukwekkend. Maar elk product kan je verzieken door het maar duur genoeg te maken en dat hebben ze wat mij betreft met deze generatie videokaarten gedaan bij Nvidia. Als je kijkt naar de prijstoename t.o.v. de 3080 dan gaan we er eigenlijk gewoon geen zak op voorruit in termen van prestaties per euro. Ik hoop dat AMD het beter gaat doen met hun kaarten.
RTX 4080;1;0.4291470944881439;Ik mis eigenlijk de relevantie van de 4080.plus lijn. Feitelijk ken ik geen spel dat (ik speel) dat deze kracht nodig heeft. Of ik nu op 100 fps of 144 fps speel...het allebei prachtig. Komen er spellen uit die een dergelijk investering rechtvaardigen?
RTX 4080;3;0.4631638526916504;Momenteel is de 4080 dikke overkill, maar ik denk wel dat het een uitstekende future proof kaart is die 5+ jaar mee kan gaan. Dat kan waarschijnlijk ook wel met een 4070 of 3080, maar dan zit je na de overstap naar Unreal Engine 5 misschien toch veel te kloten met je settings vanwege een gebrek aan VRAM ofzo. Ik zou wel interesse hebben als we een soortgelijke prijs als de Amerikanen hadden: 1100 euro.
RTX 4080;2;0.3758748173713684;Ik heb een 4K 144hz monitor. Mijn 3080 tikt nog lang niet stabiel 100fps/hz aan en deze kaart is daar ook nog niet helemaal. Wel als je settings lager zet, Zoals de recentelijke cod mw2, met mijn settings draai ik zo rond de 80fps. Opzich prima, als deze kaart voor dezelfde prijs als de 3080 wegging (850 euro oid) dan had ik er al een besteld. Nu wacht ik even af wat AMD gaat doen en wat er gaat gebeuren met de prijzen. Want 80fps is al best speelbaar, maar dan geen 140fps nog.
RTX 4080;4;0.45650917291641235;Dank voor je/jullie reactie: 4k, dat zal het dan zijn idd. Heb zelf de x38 qhd+, mis net de hoogte qua pixels en dit bewust gedaan. Breedbeeld speelt lekker[der] vind ik en scheelt toch best wat pixels om te renderen. Hierdoor komt een 3080 prima mee voor nu. COD was best mooi om te spelen idd.
RTX 4080;2;0.36328575015068054;Dikke overkill? Op 4K ultra met RT valt het mee hoor. Ligt er net aan hoeveel eye candy je wil
RTX 4080;5;0.3121575713157654;op deze manier prijst Nvidia zichzelf compleet de markt uit.. het is dat er een flinke fanboy-base is anders hadden ze nu echt een probleem, maar.. wellicht hebben ze dat alsnog wel! er komt een recessie aan, de 3xxx kaarten zijn nog lang niet allemaal verkocht en AMD heeft een veel betere Prijs/prestatie verhouding lijkt het! ben zeer benieuwd waar dit heen gaat voor mij in ieder gevaal NIET naar nvidia om daar een veelste dure kaart op te pikken
RTX 4080;3;0.49675628542900085;Ben wel benieuwd wat voor marge er op de 4080 zit bij de huidige adviesprijs, en dus hoe ver de prijs nog naar beneden zou kunnen. Want de verkoopaantallen zullen toch niet enorm worden zo.
RTX 4080;3;0.3027458190917969;Ik zit nog met een 2080 en wacht lekker nog een generatie. Ik hoop van harte dat de prijzen dan enigszins gaan zakken... Of wellicht AMD, we gaan het zien.
RTX 4080;1;0.5735418796539307;Ondanks dat je maximaal vooruit gaat op je vorige kaarten? Verschrikkelijke prijzen man. Chips, inflatie, Corona, Ukraine .. Het zal allemaal wel. Ik ben van plan een gpu te huren, zit weinig anders op. Sparen? Nee, die kosten gaan naar leefgenot (huis). Iemand andere suggesties?
RTX 4080;1;0.4111078977584839;Dat zijn toch straffe prijzen! Heb een tweedehands 3070 gekocht voor 370€.. en zelf dat deed een beetje pijn..
RTX 4080;1;0.5351104140281677;Ik hoop echt zo dat AMD met een fatsoenlijk antwoord gaat komen. Helemaal klaar met Nvidia's ethiek.
RTX 4080;1;0.5734114646911621;Van de verwachte €800 naar maar liefst €1.469 euro Tut Tut, Nvidia.
RTX 4080;1;0.510161817073822;Deze prijs gaat werkelijk waar nergens meer over. Dit zal impact hebben op hun verkoop
RTX 4080;5;0.45123913884162903;Ik blijf nog eventjes doorgaan met mijn 3080. Die voldoet nog prima. Het gevoel, om met de tijd over te stappen naar de AMD wordt wel steeds groter en groter. Het is niet dat ik die 1469 niet kan betalen maar dit is echt serieus veel geld.
RTX 4080;4;0.3233773112297058;Ach, Nvidia stunt en doet wat. Zodra AMD kaarten beschikbaar komen zullen de prijzen zichzelf weer aanpassen. Met zulke premiums is het bijna makkelijk om een betere prijs/performance kaart uit te brengen voor AMD. Ze hoeven maar 80% van de performance voor enigzins normale prijzen neer te zetten en ze hebben een winnaar.
RTX 4080;2;0.47473347187042236;"Kortom: De 4080 is m.b.t tot de prijs/kwaliteit helemaal niet goed en wil je een stap omhoog dan beginnen de prijzen (momenteel) vanaf € 2.317,95 voor een 4090... en dat is voor de meeste mensen toch echt een enorme aanslag op het budget. Een stap omlaag is ook geen optie omdat je dan bij die ""4060TI/4070"" komt die ze onlangs als een 4080 in de markt hebben proberen te zetten voor wederom een absurde prijs. Je mag er namelijk wel vanuit gaan dat die 3070 ook ergens rond de 1000 euro gaat kosten. Die nieuwe AMD kaart begint voor mij heel interessant te worden. En het zou mij niet verbazen als die als zoete broodjes over de toonbank gaan, ondanks dat ze wat betreft features nog wat achterlopen op Nvidia. En dan nog maar te zwijgen over het stroomverbruik van de 4090 etc..."
RTX 4080;3;0.3576767146587372;kan met gemak € 1.800 neertellen voor de 4080 daar ik geen andere hobby/interesse dus onkosten heb maar mijn zes maanden oude 6700xt gaat niet weg. Deze doet wat het moet doen. Jeugdsentiment Intel/nVidia is hier al lang passé. Wel interesse in de volgende generatie van AMD.
RTX 4080;2;0.4101146459579468;Ach ja, hoe hard hebben we tegenwoordig nog een nieuwe kaart nodig? Spellen groeien grafisch gezien lang niet meer zo snel als tijdens 2000-2010 Vroeger had je een nieuwe kaart nodig omdat je nieuwe spellen anders niet meer kon spelen. 60 FPS was het hoogste doel. Nu kan je met een midrange kaart van twee generaties geleden alles op high spelen met prima FPS. Mensen willen nu 144 fps ipv 60, en ook nog eens op veel hogere resoluties. De techniek is veul geadvanceerder geworden, en de kaarten veel groter met grotere koeling. De Geforce 2 Ultra was 22 jaar geleden ook al 500 euri MSRP. En ook al schieten de prijzen nu wel erg snel omhoog, ik vind het bijzonder dat kaarten zo lang zo goedkoop zijn gebleven. Dat gezegd hebbende, ik heb mijn 2080 Ti ingewisseld voor een 4090, omdat hij 2-3 keer zo snel is en ik het kan lappen. Tja.
RTX 4080;3;0.5290609002113342;Wat me opvalt is dat de geheugenbandbreedte van de 4080 lager ligt dan de 3080. Gebruikt de 4080 wellicht (betere) compressie? Dat de effectieve bandbreedte hoger ligt? Zo niet zou het interessant kunnen zijn om het geheugen over te klokken en te kijken naar de prestatiewinst. Gezien het feit dat de chip zelf wel een stuk vlotter is dan de 3080.
RTX 4080;1;0.7553615570068359;Helaas is mn reactie voor de 2de keer verwijderd... Grappig hoe je tegenwoordig geen kip meer mag zeggen op online forums...Kan beter in de 2de kamer gaan zitten daar mag ik teminste roepen wat ik wil......... Nu naar het verband van videokaarten en waarop mijn ongebreidelde reacties zijn gestoeld: Kost een intern geheugen stickje ook ineens 800 euro? Nee. Kost een harde schijf, sata, m2 etc etc ook ineens 1000 euro? Nee. Kost een moederbord ook ineens 800 euro? Nee. Kost een processor ook ineens 1500 euro? Nee. Kost een voeding ook ineens 680 euro? Nee. Kost een kast, scherm, of andere randappartuur ineens resp. 10x wat je er 2 generaties terug voor betaalde? Nee. Heb ik nog iets gemist? Ik geloof het niet he. Heb de complete bouw van een pc behandeld. M.a.w. De videokaarten zijn de enige component die belachelijk zijn. Niet mijn reactie. Die prijzen.
RTX 4080;1;0.44578295946121216;"Tja, zit je dan met je 3080 Ti. Volgend jaar maar over naar AMD denk ik maar, één keer € 1,200,- prima, begreep het best door Corona, inflatie en tekorten etc. Maar nu? Ik snap er niks meer van en volgens mij slaat NVIDIA een beetje door in de prijzen. Ik heb € 875,- betaald voor een GTX 1080 Ti in 2016.. Ja ja, hele andere tijden maar dan nog? Wat geeft ze het idee om ineens een RTX 4090 voor € 2,000,- + of een 4080 voor € 1,100,- op de markt te brengen? De instelling ""Het wordt toch wel gekocht en het is maar net wat de gek er voor geeft""? Dan zijn er een boel gekken als dit mogelijk is. Iedereen weet wel beter toch? Verder een goede review van een prima overpriced GPU jongens, bedankt!"
RTX 3090 Ti;1;0.4311341345310211;"@Trygve @Tomas Hochstenbach Is het idee om onder het tabje Stroomverbruik ook iets met prestaties per watt te doen zoals jullie met CPUs al wel doen? In de huidige tijd met prijzen van €0,50 per kWh vind ik dit aspect belangrijker dan ooit te voren We zien nu dat de 3090Ti ~35% meer stroom nodig heeft voor een 8% hogere gemiddelde prestatie score in games. Dat is qua prestaties per Watt natuurlijk compleet absurd wat Nvidia hier laat zien; dit is zelfs slechter dan de complete RTX2000 series line-up. Techpowerup presenteert de data wél op deze manier als aanvulling; waardoor het in één oogopslag duidelijk is hoe weinig prestaties je krijgt voor het stroomverbruik:"
RTX 3090 Ti;2;0.4381764233112335;Beetje overdreven? Wat maakt het verbruik nu uit voor een kaart van €2000? Het gaat hier over pakweg 100watt extra tegenover de gewone 3090. 0.05cent per game uur. Omdat verschil te voelen moet je je echt al dag en nacht gamen. Ook in totalen valt het mee: Als je 2u gemiddeld per dag gamed aan 500watt = max €50cent/kwh = 365kwh/jaar = €182/jaar voor de meest badd ass overclockte grafische kaart van meer dan €2000. Wil je echt sparen, koop dan een 1080p scherm + midrange setup en zet de verwarming een graadje lager.
RTX 3090 Ti;1;0.44312962889671326;Tis maar wat je 'bad ass' wilt noemen. Het zou pas badass zijn als Nvidia een killer kaart op de markt brengt met een TDP van 250W en 3090 prestaties. Dit soort producten moet je als particuliere consument links laten liggen. Het verbruik van deze kaart is volstrekt krankzinnig, als je dan vervolgens met een kWh rekensommetje komt aanzetten, dan heb je de moraal van het verhaal volledig gemist.
RTX 3090 Ti;1;0.45645880699157715;Ik zie echt niet in waarom dit een slechte kaart is. Paar maanden terug wilde ik een 3080ti kopen. Toen €1800 a €2000 puur om een VR bril te kunnen aansturen. Je hebt dan ook liefst 60fps per oog om je niet ziek te voelen als je met je hoofd draait. Andere willen dan weer 4K 120fps. 3080ti kan VR maar in Msfs2020 lag dat moeilijk met een 3090ti gaat het net wat vlotter. Deze 3090ti is net daar voor gebouwd. Dure kaart, relatief veel $ voor weinig extra fps maar soms is het nuttig. Hetzelfde met overclocken. Wat is daar het moraal van het verhaal? Het verbruikt meer en is soms nuttig. Maar moet om nu al een oordeel te vellen over deze kaart puur op efficiëntie? Kwh som is maar om aan te tonen dat het extra vermogen waar iedereen hier in een kramp van schiet eigenlijk peanuts is in vergelijking met uw systeemvermogen en totale kost. Ook als je het met een 3080ti vergelijkt. Moraal van het verhaal is dat je geen moraalridders mag zijn en je niet moet zeggen wat een ander mag kopen en wat niet puur op basis van uw moraal dat gevoelsmatig wordt opgedragen. De volgende keer als je een vliegtuigticket besteld, denk dan maar aan deze post. Eens kijken hoe het met je moraal gesteld is want met die hoeveel energie in kwh ka je uw leven lang gamen met de dikste kaart.
RTX 3090 Ti;1;0.756777822971344;Slecht is weer het andere uiterste natuurlijk. Ik heb ook nergens aangegeven dat het een slechte kaart is. Maar als je nu serieus kijkt naar wat er om je heen gebeurt en dan met name in de energiewereld, dan moet je naar mijn idee als leverancier je toch achter de oren krabben als je nu met een powerhouse van 600W TDP komt aanzetten met een netto fps winst van 9%. Steek dan je resources in het ontwikkelen van een kaart die rond die snelheid zit met de helft TDP. Als we de verwachtingen mogen geloven dan gaat de 4000-serie hier nog dunnetjes overheen. Slechte ontwikkeling als je het mij vraagt.
RTX 3090 Ti;1;0.4605198800563812;goh, een wagen van 20.000€ zal ook wel 220 km/u kunnen een wagen die 400 kan of 420 ... kan snel enkele miljoenen zijn . voor wat? een verdubbeling van snelheid maar een factor 100 van prijs? omdat het kan EN er mensen zijn die graag betalen om de snelste of eerste te zijn
RTX 3090 Ti;1;0.7562335133552551;Geld is niet het enige argument heh, het gaat om verkwisting van energie voor zeer weinig winst. Dat is gewoon niet meer te verantwoorden in 2022.
RTX 3090 Ti;2;0.3188471794128418;Verkwisting is een lastige uitspraak. Want wat is verkwisting? Die laatste fps er uitpersen om toch maar met VR bril deftig te kunnen spelen om om 120fps op 4k te halen? Moeten we overclocken dan verbieden? Of gewoon de bios clocks locken want het is verkwisting? Want dan met andere grootverbruikers? Zoals auto’s en vliegtuigen of fossiele brandstoffen die we ook gebruiken in onze vrijetijd. Is met het vliegtuig reizen verkwisting van energie? Verkwisting tegenover jezelf of verkwisting tegenover de planeet?
RTX 3090 Ti;1;0.4970431327819824;Ja zo kan je alles relativeren, als je niet kan zien dat dit verkwisting van energie is, dan wil je het niet zien. Daar doet je whataboutism niks aan af.
RTX 3090 Ti;3;0.23256731033325195;Als je zo redeneert kun je volgens mij geen enkele kaart boven de AMD 6800 en Nvidia 3070 Ti nog verantwoord noemen
RTX 3090 Ti;3;0.5487755537033081;En dat is een prima redenering natuurlijk. Die moet je voor je eigen uitmaken. En dat geldt voor veel van die milieukwesties, maar als geheel, als maatschappij of community vind ik het zelf niet verkeerd om er op zijn minst over te discussiëren. Laat de consument kiezen, maar laat ze wel bewust wezen dat deze kaart stroom zuipt. Net zoals de consument ook hoort te weten dat er nogal een verschil is tussen pakweg frosties en iets gezond.
RTX 3090 Ti;2;0.36456143856048584;Ik vind de discussie op zich ook terecht, maar denk ook wel bij sommige reacties dat het nu even gebruikt word om weer Nvidia te bashen. Terwijl dit natuurlijk een kaart is die door zo'n gruwelijk klein select groepje gekocht gaat worden, dat die allang weten wat dit voor kaart is, en die zich niet druk maken om milieu, verbruik en al wat nog meer. Daarnaast zijn er ook denk ook wel bij die nu moord en brand schreeuwen maar zelf wel hun CPU en gpu en ram zo hoog mogelijk overklokken om maar net dat laatste frame er uit te pushen.
RTX 3090 Ti;2;0.42541027069091797;Eigenlijk zouden we als consument momenteel alle videokaarten die in bovenstaande grafiek onder de 120% procent scoren, consequent links moeten laten liggen. Alleen op die manier zetten we de fabrikanten aan om zuinige kaarten te ontwerpen. De keuze wordt dan wel wat minder, maar er blijven voldoende aan high- en mid-end kaarten over. Van nVidia echter enkel de 3060 Ti en de 3070, waaruit blijkt dat nVidia wel wat meer inspanning mag doen. Zeker de 3070 Ti, de 3090 en 3090 Ti scoren erg slecht (minder dan 105%).
RTX 3090 Ti;1;0.515803337097168;Nee. Sommigen willen gewoon het snelste en voorbode mensen is deze kaart pond zamelt gebracht. Er zijn genoeg zuinige kaarten om uit te kiezen mocht je dat willen.
RTX 3090 Ti;3;0.4223881661891937;"Mensen willen wel meer. Het is prima mogelijk om met regelgeving te zorgen dat mensen nog steeds het snelste van het snelste kunnen kopen, zonder dat dat bijna 500W verbruikt tijdens gamen. Ik heb geen problemen met mensen die honderden euro's meer uitgeven aan een GPU die een paar procent sneller is, een compleet irrationele aankoop natuurlijk, maar wat mensen met hun geld doen, moeten ze lekker zelf weten. Ik heb wel een probleem bij de gedachte dat NVidia een 1kW voeding adviseert voor deze GPU, omdat het ding krankzinnige hoeveelheden stroom verbruikt. En waarom? Voor een paar fps meer? Als ""we"" van het Russische gas af moeten en onze energiefootprint moeten verkleinen, hoe kun je dan een RTX3090 Ti verantwoorden?"
RTX 3090 Ti;1;0.7349295616149902;Als iemand zonnepanelen op zijn dak zet en de eigen opgewekte stroom graag gebruikt om met zijn 1000 Watt PC te gamen, wat is dan het probleem? Je hebt het over 'we' maar die 'we' bestaat helemaal niet. Je hebt mensen die graag de hele wereld rondvliegen, meerdere keren per jaar. Er zijn ook mensen die dat niet doen. Er zijn mensen die elektrisch rijden en jaarlijks belachelijk veel stroom gebruiken, je hebt ook mensen die de fiets pakken. De een woont in een huis met energie label A en die merkt weinig van de gas tekorten, een ander woont in een huis met energie label F en die stookt zich letterlijk arm. Iedereen is zo verschillend, dus nogmaals, die 'we' bestaat helemaal niet.
RTX 3090 Ti;1;0.449688583612442;"Dat die zonnepanelen geen stroom opwekken als je 's avonds zit te gamen Je kunt het allemaal proberen goed te praten, maar ergens is het gewoon ziekelijk om zo'n hoeveelheid energie te verstoken om een paar fps meer. En natuurlijk gaat nu iemand reageren met ""maar er kan een accu achter die panelen zitten"". Yeah right, dat heeft nog bijna niemand in Nederland en dan nog zal die accu in de winter meestal leeg zijn. Maar dan nog is het een onzinnige discussie: het gaat om het energieverbruik van dit soort kaarten: dat wordt iedere generatie erger en is echt niet nodig. Je ziet dat AMD het al beter doet, maar het lijkt NVidia gewoon niet te interesseren. Dat we verschillende levens leiden, wil niet zeggen dat de een zich kan onttrekken aan een maatschappelijke verantwoordelijkheid en de ander niet. De mensen in een huis met energielabel F worden gestimuleerd (met subsidie e.d.) om hun huis te verduurzamen. Om de redenen die ik hierboven al gaf, zal er de komende jaren flink ingezet worden in het gasloos verwarmen van woningen en bedrijfspanden, of op z'n minst op het flink reduceren van ons gasverbruik. De mensen die de hele wereld rondvliegen voor de lol, worden met hogere taksen ook gecorrigeerd. Als we dat allemaal vanuit onze eigen goedheid zouden laten, was de opmerking van @LievenD niet begonnen met het woordje ""Eigenlijk"". Dus waarom pakken we de automobilist, de tourist, etc allemaal wel aan, maar niet de fabrikant van een kaart als deze? Doe er desnoods een milieutaks overheen... Zelfs luchtvaartmaatschappijen zelf komen er inmiddels eerlijk voor uit dat vliegen eigenlijk toch wel beperkt zou moeten worden."
RTX 3090 Ti;3;0.5684263110160828;gelukkig hebben we door Nederland zelf gemaakte groene stroom die van windmolens afkomt en van zonnepanelen.Deze kaart is echt top of the bill en ook natuurlijk wat de kosten met zich meebrengt. ik vind het een mooie kaart maar als hij alleen al in idle status al 30 watt gebruikt vind ik een beetje absurd. Wat dat betreft heeft amd dat beter voor elkaar.en ik denk net zoals jullie schrijven dat je minimaal een 1000 watt voeding moet hebben voor dit beest aan de praat te krijgen.En zodat dat hij het daadwerkelijk goed gaat doen zonder je hele pc uitvalt omdat de voeding niet voldoende wattage niet kan leveren wat de kaart en het systeem vraagt.
RTX 3090 Ti;1;0.5434574484825134;"Blijkbaar komt 33% van de Nederlandse elektriciteit op het moment uiit ""hernieuwbaar"". Daarbij wordt biomassa echter meegerekend wat zeer discutabel is. Die reken ik liever niet mee, dus neem anders 25%, ruwweg de grafiek bekijkend. En da's dan niet op één moment gerekend.. Nee,da's opbrengst over een heel jaar. Als jij zit te gamen 's avonds schijnt de zon niet meer terwijl dat toch weeral de helft van die hernieuwbare elektriciteit vertegenwoordigt. We zitten misschien al aan 12%. Bekijk anders even op 20:00 gisterenavond. Deze URL laat geen datapicking toe, maar ik zie 17% renewable staan. TPU geeft hierboven een gemiddelde van 482W tijdens gaming. 17% of 82W daarvan is dus hernieuwbaar. De resterende 400W komt van kolen, wind, gas en een snuifje nucleair. Ik zou maar niet te hoog van de daken roepen dat je op hernieuwbare elektriciteit aan het gamen bent. Tenzij je een serieuze vloot zonnepanelen hebt en een stevige huisbatterij om als buffer te dienen. En het geen winter is, want dan is die batterij gewoon leeg."
RTX 3090 Ti;3;0.46385905146598816;We wonen in een flat maar afgelopen 2 jaar hebben ze zonnepanelen hier op het dak geplaatst. Hoeveel percentage daar vanaf komt weten we niet precies, misschien weet de provider van de stroom wat het dus aan onze flat levert misschien wel.Maar we zijn idd ook blij als we een goede zomer hebben, des te minder we betalen.Het gaat om een paar tientjes soms 100 euro per jaar wat we dan minder betalen.Dat zonnen energie word of per maand betaald en we krijgen elk jaar een afrekening ook van de zonnepanelen. De energie van de zonnepanelen worden op het eind van het jaar erg woon vanaf getrokken zodat we niet zo een hoge eindafrekening hebben.Plus hebben we de twee winters gehad waar veel regen gevallen is en dus erg bewolkt was en dus niet 100 % van de zonnepanelen kunnen aftrekken. Maar het maakt wel degelijk iets uit en zijn wel blij met de stand van zaken en ook van onze bouwvereniging.Als je meer plaatst zal zelfs de meter terug gaan lopen en zullen ze je echt moeten gaan uitbetalen.Btw dat is een mooie map wat je laat zien.
RTX 3090 Ti;2;0.5354511737823486;Deze zin leest niet helemaal goed... Verder is dat 'willen' nu net het probleem, mensen willen het grootste, snelste,... maar wat ze daar allemaal voor opofferen, daar staan ze zelden bij stil. Net nu dat wel eens het verschil zou kunnen maken...
RTX 3090 Ti;3;0.3044448494911194;Nou voor die enkeling. Ten 1st moet die al in klasse van 1kwatt voeding regelen. Iemand die dot gedrocht wil zal de vorige halo kaart ook al hebben en die daarvoor. 1080ti 2080 2080ti 3090 3090ti 4090 4090ti heeft waarschijnlijk al een OC custom game rig met WC en 1200 a 1500 watt voeding. En zal voor later ook rekening moeten houden met wat 4090ti gaat trekken. Aantal van dit volk is klein. Maar zou kunnen groot genoeg dan nv kan leveren. De meest zitten nu op 600 a 850 watt voeding en dan wordt de vraag ga je er in mee. Een deel van de halo sku target zal hierin ook niet mee gaan. Je zal altijd volk hebben die V12 of hummer of huge pickup moeten hebben. En early adopter groenen die Prius reden. Nu vol EV zijn. Er markt voor de status of pats consument. En daarmee is onvermijdelijk dat Halo producten buiten proportioneel duur kunnen zijn. Voor e-wast en energie zijn de aantallen laag als 1 3090ti vs 10000 3070 verhouding is zuinigheid van een 3070 belangrijker. Energie footprint.
RTX 3090 Ti;1;0.26980483531951904;Ik kijk ondertussen naar mijn 5950X + 2080TI met custom loop en 750 watt voeding. Als ik een stress test draai is het totaal misschien 450 watt verbruik
RTX 3090 Ti;1;0.44495269656181335;"Afaik zijn er diverse modellen te koop die (veel) minder verbruiken,zoals je zelf al aangeeft, het is simpelweg een keuze die je als consument kunt maken aan de hand van jouw prioriteiten. Niet alle modellen hoeven te voldoen aan alle eisen van alle gebruikers. Ow, en maak je niet druk over ""consequent links moeten laten liggen"" van de 3090Ti, dat zal het overgrote deel van de consument doen, simpelweg omdat het niet binnen hun budget of eisen valt."
RTX 3090 Ti;1;0.49296244978904724;De kaart kan niet zuiniger met dezelfde performance, en fabrikanten ontwerpen al zuinigere kaarten. Dit model is specifiek bedoeld om de max mogelijk met de huidige technologie te behalen (binnen consumenten specificaties dan). Natuurlijk zijn zuinigere kaarten altijd mogelijk, maar consumenten willen juist sneller. Als niemand dit zou kopen, zou deze nooit op de markt zijn gekomen. Zo werkt commercie. Fabrikant maakt waar vraag naar is, of meer zelfs fabrikanten die een succesvol product maken overleven. Je moet dit dus niet helemaal bij de fabrikant neerleggen. Het is het hele systeem.
RTX 3090 Ti;2;0.2891140580177307;"Het is heus geen onwil om zuiniger kaarten te maken. Videokaarten voor de consumenten markt zijn in wezen varianten van kaarten die worden ingezet in data- en reken centers, en in die sector is een gunstige verhouding tussen vermogen en performance cruciaal. Als fabrikanten zuiniger kaarten zou kunnen maken dan zouden ze dat echt wel doen. Dat met name de 3090Ti zoveel meer verbruikt voor een kleine toename in snelheid - vergeleken met oudere generaties; GTX1080Ti verbruikt ook ~35% meer dan de 1080 non-Ti maar dan wel met ~25% snelheidswinst - dat komt omdat fabrikanten de afgelopen ~15 jaar steeds meer tegen limieten van de huidige halfgeleidertechnologie aan lopen: Procedé verkleining kost steeds meer tijd (en steeds meer geld) én er worden steeds meer transistors op een chip gezet die bij elkaar dus steeds meer stroom/vermogen verbruiken. Maar intussen verwacht de markt wel steeds weer meer snelheid van de volgende generatie kaarten. Fabrikanten die aan die vraag willen voldoen (NVIDIA wat mer dan AMD) ontkomen er niet aan om steeds meer de grenzen van halfgeleidertechnologie op te zoeken, en krijgen daarbij onvermijdelijk te maken met ""diminishing returns"". En consumenten krijgen daar dus ook mee te maken: steeds ongunstiger performance-per-Watt én steeds hogere prijzen (over de prijzen van de RTX20-serie (2018) werd al geklaagd vóórdat Corona uitbrak). NVIDIA doet het niet slechter dan AMD: AMD's top kaarten zijn wel zuiniger maar ook minder snel en hebben geen gespecialiseerde hardware voor raytracing en image sharpening. Appels met appels vergeleken (vergelijkbare performance en features, dus zonder RTX en DLSS) zitten ze qua performance-per-Watt niet ver uit elkaar."
RTX 3090 Ti;2;0.46223771572113037;"Het lastige van performance/watt lijkt me dat de winst heel erg subjectief is. Het is doorgaans niet zo dat je e.o.a. vaste taak aanbiedt aan de videokaart en de kosten voor die taak dan belangrijker zijn dan de tijd dat het duurt om tot het eindresultaat te komen. (en ja, er zijn uiteraard wel usecases waar dit voor geldt) Als je zo'n dikke kaart overweegt, lijkt me dat je dat vooral doet om net dat mooiere beeld te krijgen en/of op maximale refreshrate van je (4k) monitor ook echt een nieuw gerenderd beeld te kunnen krijgen. Los van wat rendertaken heeft zo'n 3090Ti (of zelfs de 3090) eigenlijk alleen nut als je zulke hoge eisen stelt aan het resultaat van het renderen. Ik heb zelf nu een RTX 2070. Volgens jouw grafiek kan ik beter overstappen op een RTX 3090 omdat die net wat efficiënter is... Maar het totale stroomverbruik op ""het mooiste beeld dat ie kan maken, met een fatsoenlijke fps"" is uiteraard wel een stuk hoger"
RTX 3090 Ti;3;0.5059656500816345;Tja, ik denk dat het er natuurlijk ook aan ligt hóe je je hardware gebruikt. Je kunt er natuurlijk prima voor kiezen om je hardware flink te undervolten en/of underclocken om zo het optimale 'performance/watt' punt te bereiken. In vele gevallen zal het mij ook totaal niet verbazen als een 'krachtige' kaart die teruggeschroefd is minder verbruikt dan een minder krachtige kaart die voor gelijke performance op volle toeren moet draaien.. Beetje hetzelfde idee als met een 1L A-segment 'city car' constant op de snelweg 130km/u rijden om vervolgens meer benzine te slurpen dan wanneer een dikkere auto met grotere motor hetzelfde doet.
RTX 3090 Ti;3;0.3908061683177948;Het punt is denk ik vooral dat het bij je voorbeeld van auto's allemaal getest en berekend wordt en bij videokaart vergelijkingen het niet eens standaard wordt genoemd. Dat zou best interessant kunnen zijn, bijvoorbeeld om een interactieve tabel te hebben, waarbij consumenten zelf een periode kunnen kiezen en vervolgens het aantal uur dat ze bijvoorbeeld gamen (hoge belasting, performance) en thuiswerken (lage belasting, office bijv.) om te kijken welke kaart dan het goedkoopste is in de situatie van de consument. Dat het verbruik daarmee afneemt en dit ook goed is voor het milieu en het elektriciteitsnet is voor de consument mogelijk bijzaak (of niet), maar ontstaat dan wel. Ik denk dat ACM een goed punt maakt, over het algemeen kopen consumenten een betere kaart om daar meer performance uit te halen. Maar ik heb bijvoorbeeld ook voor een kleinere auto gekozen, omdat deze een lager verbruik heeft. Deze haalt prima de maximum snelheid, maar ik heb inderdaad naar mijn gemiddelde gebruik gekeken. Als ik zelf een minimum fps kan kiezen op de resolutie die ik gebruik, dan kan het misschien wel voordeliger zijn om een kaart van een stapje hoger te kopen en dat over 3 jaar terug te verdienen in lagere energiekosten. Om vervolgens ook nog eens een hogere restwaarde over te houden. Ik heb bij de laatste aanschaf niet eens naar het verbruik gekeken, terwijl ik wel fps/€ vergelijking heb mee laten wegen in mijn besluit. Terwijl verbruik over de jaren toch wel een grote invloed heeft in de totaalkosten, zeker nu.
RTX 3090 Ti;1;0.4060925841331482;Inderdaad voor 10 frames meer in de meeste spellen op 4k ultra verstook je 250 watt meer, absurd dus.
RTX 3090 Ti;1;0.5031810402870178;Ik moet zeggen dat ik in die context ~25% slechtere efficiency tov de naaste concurrent (de 6900XT) eigenlijk niet schikbarend vind. Heb je de verschillen in andere categorieën wel eens bekeken? Autos, koelkasten, verwarmingen, aircos om maar een paar voorbeelden te noemen in die een verschil van slechts 50% tussen de minst en meest efficiënte optie echt een utopie is. En jij hebt het hier over 35% meer stroom voor 8% meer FPS (ik neem aan tov de 3090), dat lijkt zich dus totaal niet te bevestigen in die grafiek die jij zelf linkt.
RTX 3090 Ti;4;0.3786701261997223;EVGA GeForce RTX 3090 Ti FTW3 Ultra powerspikes(20ms) van 570w 530w piekverbruik in gamen edit: MSI GeForce RTX 3090 Ti Suprim X is niet veel beter edit ll: All RTX 3090 Ti cards come bundled with a 3x 8-pin to 12-pin adapter (the four small sense pins aren't used). While technically the ATX specification says that if the four small sense pins aren't connected, the card should limit itself to 150 W, NVIDIA made sure even the 3x 8-pin adapter scenario, which can provide 525 W total (3x 150 W + 75 W from the slot), runs the card at optimal settings.
RTX 3090 Ti;1;0.5214574337005615;570W, Dat is toch volkomen krankzinnig.
RTX 3090 Ti;2;0.4713311791419983;Mwa. Het lijkt veel, maar het is allemaal relatief. Als je kijkt naar andere energieverbruikers in huis: boiler, CV, stofzuiger, koelkast. Of bijv de auto: en motor met een vermogen van 120kW=120000W. Ik neem aan dat je niet van plan bent om je videokaart 24/7 op vol vermogen te laten blazen? Natuurlijk was het vroeger allemaal minder, tenminste in de commerciële markt. Professionele apparatuur had al wel dit soort vermogensverbruik. Je kan het ook zien als dat professionele specs nu beschikbaar komt voor consumenten.
RTX 3090 Ti;1;0.5425221920013428;Niet mee eens, jouw vergelijking gaat ook mank, want die apparaten zijn alleen maar zuiniger geworden over de jaren Dit is gewoon achterlijk veel, hoe je het ook went of keert... zeker als je de minimale prestatie vooruitgang ook nog mee rekent Dit is al jaren het geval, dus dat vind ik echt een zwak excuus
RTX 3090 Ti;5;0.4161742627620697;Voor stofzuigers zijn recentelijk nog regels opgesteld voor een maximum wattage. Zouden ze voor videokaarten/computers ook wel mogen doen. Energieverbruik is het grootste struikelblok voor de mensheid om de komende 100 jaar te gaan overleven.
RTX 3090 Ti;1;0.4894876778125763;Dat is piek die merk je niet op rekening, waar wel merkt als PSU strak heb gekozen dat net voldoet aan optelsom aan opgegeven tdp/tgp van ke PC configuratie. Voor deze kaart moet je sowieso ook wat marge hebben voor elk ander component kwa piek vermogen en tel je dus minstens een 600watt piek erbij voor deze 450watt tgp.
RTX 3090 Ti;2;0.5529037117958069;Uiteraard is té strak kopen van een voeding met voldoende capaciteit nooit handig maar die imho vrij extreme pieken is toch iets van de laatste generatie kaarten als ik de grafiek van Techpowerup zo zie. Slaat natuurlijk nergens op dat je een PSU zó ruim moet kopen om een 20ms piek op te vangen. En je merkt korte pieken wel niet op je rekening maar een PSU die de halve dag aanstaat en idle 5~10% van het vermogen van de PSU gebruikt is niet bepaald efficiënt.
RTX 3090 Ti;3;0.5096290111541748;GTX 1080 had een TGP van ±180 W als ik me niet vergis, maar prestaties en verbruik naast elkaar gezet lijkt er niet echt veel vooruitgang te zijn geboekt.
RTX 3090 Ti;2;0.33359867334365845;Sluit wellicht aan op de doelgroep. Mensen met geld. Daarnaast bijv. thuiswonende jongeren die een vakkenvulbaantje bekleden om te sparen voor deze GPU zal het vast niet zoveel schelen dat hun ouders de energierekening betalen
RTX 3090 Ti;1;0.39038026332855225;Ik denk dat als dit in mijn jeugd beschikbaar was en ik het aangeschaft had zonder dat mijn ouders dat zouden weten, mijn vader na constatering van een dubbel zo hoge energie rekening de pc vakkundig uit het raam van mijn slaapkamer had geslingerd.
RTX 3090 Ti;1;0.391000360250473;En die arme ouders vragen zich vertwijfeld af waarom de energierekening de laatste tijd toch zo omhoog ging, ondanks vervangen van alle verlichting door LED, thermostaat lager, isolatie en noem de voor de hand liggende oplossingen maar op.
RTX 3090 Ti;1;0.7956699132919312;Ik weet niet in wat voor wereld jij leeft, maar voor dit krankzinnige bedrag zal er geen 1 jongere zijn in de supermarkt die deze kaart zal halen hoor. Een PS5 of Xbox Series is al een heleboel geld als je zo uitgeknepen wordt.
RTX 3090 Ti;3;0.45352810621261597;Wel ben ik benieuwd hoe erg deze kaart in trek zal zijn bij de miners. Zit hier ook iets van een restrictie op?
RTX 3090 Ti;4;0.42661088705062866;Geen LHR, redelijke mining efficiency (ongeveer zelfde als 3090) maar belangrijker is dat alle geheugenchips aan de voorkant zitten en dus actief gekoeld worden door de fans. Veel beter design als de 3090 in dat opzicht.
RTX 3090 Ti;2;0.4310852289199829;Ik betwijfel of de kaart in trek zal zijn bij miners, met de huidige koers/stroomkosten zal je nooit je kaart terugverdienen... Die gaan imho toch echt voor betere Watt/prestatie oplossingen kijken met de huidige prijzen...
RTX 3090 Ti;2;0.489228755235672;In bepaalde gevallen inderdaad wel, heb je echter zonnepanelen en overschot kan je nog altijd prima je kaart terugverdienen, al zou ik dan alsnog als gamer die een kaart wil terugverdienen niet voor een kaart als deze gaan. Net als voor landen waar zelfs nu stroom nog relatief goedkoop is. Maar dan nog denk ik niet dat dit de kaart voor miners is, ik verwacht hem te duur en te weinig winst over de andere kaarten in de range.
RTX 3090 Ti;1;0.5337282419204712;Dat overschot is zeer zeker ook geen 'gratis' stroom, naast de initiële investering, kan je ook gewoon die stroom weer terug verkopen aan het netwerk, dus gemiste inkomsten. Voor de prijs van 1 enkele 3090ti kan je voldoende AMD kaarten aanschaffen die dubbel zoveel winst maken en een stuk efficiënter omgaan met stroom...
RTX 3090 Ti;2;0.37834441661834717;Maar dan moet je wel een mining farm willen beginnen, ik heb het eerder over iemand zoals mezelf, koop een gpu (in mijn geval een 6900XT) en mine wanneer je niet aan het gamen bent en verdien zo je kaart terug. De ROI is dan minder van belang, als je maar langzaam terug verdient, de kaart is immers niet gekocht om mee te minen, het is een leuke bijkomstigheid dat je na een jaar of wat een 'gratis' kaart hebt. Dat je die stroom ook kan terug verkopen klopt, maar wat je kan verbruiken, en dus salderen is voor zover ik weet nog altijd gunstiger dan tegen een lager tarief terug verkopen aan de energiemaatschappij. Al kan dat mogelijk nu anders zijn met de hogere prijzen, ik zit nog op oudere tarieven, dus terug lever vergoeding is een paar cent en eigenlijk niet de moeite).
RTX 3090 Ti;4;0.3228055238723755;Als dat inderdaad je use case is, dan is een 3090Ti waarschijnlijk efficiënter dan een 6900XT, het enige waar je rekening mee moet houden is. Dat je voldoende capaciteit hebt om het de rest van de dag te laten draaien (sla je zelf stroom op voor de nacht?) en dat dergelijke kaarten veel warmte produceren, welke je juist moeilijk kwijt kan in de meest productieve periode van het jaar (voor je zonnepanelen, zomer)...
RTX 3090 Ti;1;0.5292410254478455;Dat eerste vroeg ik me inderdaad ook af, op basis waarvan jij een AMD GPU een ROI van dubbel de Nvidia kaarten ziet halen. Als ik mijn stoom prijs in een calculator gooi (die ik niet betaal i.v.m. overschot), dan is een RTX 3090 voor mining bijna 2,5x zo efficient als een 6900XT (ik kan met een 6900XT dan €1,21 per dag minen, met een RTX 3090 €2,95 Stroom opslaan is (nog) niet nodig, dat saldeer je gewoon (zolang dat nog kan in Nederland, wetgeving is weer uitgeteld). In Nederland mag je aan het eind van het jaar alle verbruik tegen alle opbrengst wegstrepen, en wat je overhoud wordt dan uitbetaald tegen het lagere teruglever tarief. Tegen de tijd dat die wetgeving er door is dat dit mogelijk veranderd wordt, is mijn kaart allang terugverdiend.
RTX 3090 Ti;3;0.5895751714706421;Een aantal 'lagere' kaarten van AMD zijn relatief super efficient qua mining, waardoor je met de prijs van een 3090 TI ongeveer 5 van die kaarten kan kopen. Maar aangezien dat niet opgaat voor jou situatie, kan je daar dan moeilijk mee gaan rekenen.
RTX 3090 Ti;5;0.45226141810417175;In SLI(rtx3090Ti also supports sli) 1100W
RTX 3090 Ti;2;0.4981227219104767;Dat hangt van de miner af. Is de Miner een gamer die het beste van het veste wil, dan zal hij als hij dat kan 1 kaart kopen, en te minen naast het gamen. Is het een miner met een farm (groot tot klein) denk het niet, of in elk geval niet veel. Verbruik te groot voor wat het oplevert. Denk dat de mensen die er 1 willen kopen eerder last krijgen van de scalpers die de prijs nog verder gaan opdrijven dan die 2250 die genoemd wordt.
RTX 3090 Ti;4;0.4565379321575165;Genoeg om je slaapkamer ermee te verwarmen.
RTX 3090 Ti;4;0.29253658652305603;Ja: de prestatie/verbruiksverhouding. XD
RTX 3090 Ti;3;0.44018834829330444;Bij miners is ook bang for tha buck , energie/crypto performance./prijs. In landen waar energie goedkoop is is meer crypt performance vs prijs. Down under is meer prijs. Verkrijgbaarheid. De sweets spot is afhankelijk van geolocatie 3070. Waar je per 3 a 100stuks kan vergaren. Waar een 3090ti kwa prijs niet rendabel maakt maar ook de beperkte aantallen. En als orde van 100stuks kijkt is ASIC rig van 5 a 20 K €£€ interessanter kwa rendement.
RTX 3090 Ti;2;0.4443768262863159;De trek bij de miners? Als de koers goed staat en de ROI het waard is zal hij vast op het menu staan, financiële doelen bij die jongens gaan soms absurd ver de pan uit. Het lijkt wel een digitale goldrush.
RTX 3090 Ti;2;0.4208613634109497;Op zich, dat stroomverbruik is niet het grote probleem. Dat wordt vrijwel uitsluitend omgezet in warmte die je verblijf in gaat. Daarvoor hoeft er dus niet verwarmd te worden, en dat scheelt je gas (of electriciteit als je electrisch verwarmd). Op wat ventilatie is je woning immers een gesloten systeem. Alleen in de zomer gaat die vlieger niet op.
RTX 3090 Ti;1;0.6132451891899109;Het is alleen met cop-1 een zeer inefficiënte manier van verwarmen. Dus die vlieger gaat nauwelijks op.
RTX 3090 Ti;1;0.48466169834136963;Mensen die denken dat ze met een GPU hun huis kunnen verwarmen Alleen als het mega warm buiten is merkte ik dat me kamer door me game pc nog warmer werd. Als het koud is buiten voldoet het totaal niet, en moet je gewoon je kachel aan zetten om je huis/kamer te verwarmen
RTX 3090 Ti;3;0.434652715921402;Natuurlijk niet. Een electrische kachel pakt vrolijk 2 tot 3 kiloWatt. Heb je flink wat van die GPUs voor nodig. Maar het helpt wel. Als je electrische kachel op een thermostaat draait, zal die gewoon wat eerder afslaan.
RTX 3090 Ti;3;0.529421865940094;In de winter deed ik altijd een droger slang met een koof gemaakt van karton op de voorkant van mijn kast zetten (2x120mm intake). Slang het raam uit en je kon geweldige die/vrm temps en overclocks halen. Maar het is maar net wat je ermee wilt bereiken. Met een beetje zomer is het een ramp, al is het binnen 25-29 graden dan kan dat net al teveel zijn want dan moet je echt goed gaan ventileren of je pc loopt zo warm en je onderdelen slijten dan stukken harder. Ben laatst nog tot de leuke conclusie gekomen dat een pc case en de achterkant van een pc ook echt heel belangrijk zijn. Je kan je fans op 100% zetten maar als hij geen nieuwe aanvoer van koude lucht krijgt omdat je airflow dus slecht is (pc bijna tegen de muur in een hoek) dan lopen de temperaturen alleen maar hoger en hoger tot het dus een keertje fout gaat.
RTX 3090 Ti;5;0.6866559386253357;Ik heb een tijd in een flat op het westen gewoon. In de zomer werd het daar loeiheet door de zon, dus dan gebruikt ik juist een drogerslang om de warmte van de PC naar buiten te leiden. Dat scheelde makkelijk een paar graden. :-) De case is echt heel belangrijk, ja :-)
RTX 3090 Ti;5;0.2755641043186188;Wow 1000euro duurder, 100w meer verbruik en maar enkele % sneller dan de 3090 in GPU rendering (octane/blender/redshift)!
RTX 3090 Ti;1;0.61512690782547;Financiële zelfmoord in aanschaf en gebruik. Dat is de conclusie. De kleine gamer drukken ze weg naar de game console zoals Xbox, Playstation, enzovoorts. Toepasselijk dat het vroeger toch echt wel allemaal beter was en een stuk betaalbaarder voor iedereen tot de crypto miners eraan kwamen. Had nooit verwacht dat deze de markt zou kapot maken, de hype was al snel voorbij voor mij. Stroomvretende kaartjes die je financieel een poot uit trekken en vervolgens ook nog eens zo warm weg bakken (VRMs) dat ik me zwaar betwijfel of zo'n kaart tijdens redelijk gebruik nog wel werkt buiten het garantietermijn.
RTX 3090 Ti;2;0.3891938030719757;De consument heeft zich helaas zelf weggedrukt door maar te blijven kopen en maar te blijven accepteren wat er gaande is. Groot deel hiervan komt natuurlijk op conto van de miners, die de markt gewoon kapot gemaakt hebben met hun hebzucht. Wij als simpele consument (zo noem ik het voor het gemak even) staan erbij en kijken ernaar. Het is niet onrealistisch om te denken dat PC gaming weer een elitaire hobby gaat worden voor de gefortuneerden onder ons. Voor een medium kaartje koop je tegenwoorden 2x een XBSX ... vraag me oprecht af hoe lang we hier nog mee doorgaan. 600 EUR voor een budget kaart ala een 3600 die on par presteert met een 2070S van jaren geleden. Het word tijd voor een nieuwe speler op de markt die de verhoudingen eens echt op scherp gaat zetten.
RTX 3090 Ti;1;0.7172614932060242;Exact dat dus ja. Heb zelf ook meegedaan aan de hype kwam er alleen al snel achter dat het allemaal maar een hype was en nog steeds is. In dat hele crypto gedoe heb ik ook totaal geen vertrouwen meer in.
RTX 3090 Ti;1;0.5442658066749573;'Met een prestatiewinst van nog geen 10 procent ten opzichte van de RTX 3090 in het achterhoofd, is het 35 procent hogere energiegebruik van de RTX 3090 Ti bijzonder lastig goed te praten.' Toch doen jullie heel erg je best: 'De verbetering ten opzichte van de RTX 3090 is niet extreem, maar met gemiddeld 8 procent nog wel significant te noemen.' Een van de meest waardeloze idiote producten ooit en Tweakers noemt de verbetering niet extreem, maar wel significant.
RTX 3090 Ti;1;0.5231335163116455;8% prestatiewinst is gewoon objectief significant. Dat is geen waardeoordeel.
RTX 3090 Ti;1;0.7999365925788879;8% is niks en al helemaal niet objectief significant en zeker wel een waardeoordeel. 8% haal je nog door je nog door je gpu te overklokken. En die 8% staat ook nog eens niet op zichzelf, maar er staat een enorme idiote hoeveelheid stroomverbruik tegenover en een nog krankzinnigere prijs. Maar goed, als ik dat moet gaan uitleggen..
RTX 3090 Ti;2;0.41692784428596497;Dus als je volgende maand 8% meer aan huur/hypotheek/... moet betalen vind je dat ook niks en niet significant? Er is een duidelijk verschil, in dat opzicht is het significant. Of het dat waard is gezien wat je ervoor in moet leveren aan aanschafprijs en wat die slurpt is daarin niet relevant.
RTX 3090 Ti;2;0.4212319552898407;1. een significant verschil [groot genoeg om belangrijk te zijn] Nee in dit geval is het dus absoluut niet significant als je het aan mij vraagt, Als je hypotheek 100 euro per maand is, dan is 8% niet significant nee
RTX 3090 Ti;1;0.5635979175567627;Als je hypotheek 100 euro per maand? WAAR en wie heeft dat? is dat voor een konijnenhok?
RTX 3090 Ti;3;0.6124053597450256;Je snap hopelijk wel wat ik bedoel.. Het hangt er vanaf hoe hoog je hypotheek is om met 8% een groot verschil te maken, voor mij persoonlijk zal het hooguit een paar tientjes extra zijn
RTX 3090 Ti;1;0.817240297794342;Dat is toch eigenlijk ook precies wat deze kaart is? Een overgeklokte versie van iets wat er al was... Ja en iets meer cores maar dat is verwaarloosbaar... Dat is gewoon absoluut de zwakte van snelste versies van deze generatie NVidia kaarten. Het kost zo idioot veel power om de laatste procentjes eruit te persen. Zelfs bij de 3080 kaarten al, die komen allemaal af fabriek met een achterlijk power profile waarmee de kaart loeiheet wordt. Undervolt de kaart een klein beetje en het verbruik gaat zo 10-tallen Watts omlaag zonder noemenswaardig verlies. Ik heb mijn 3080 undervolted zodat hij tegen de 50W minder verbruikt, en ik zie in benchmarks echt amper 5% prestatie verlies. Hoe dan ook is deze kaart gewoon een belachelijk product. Ik vind het prima dat ze het maken en als mensen het willen kopen moeten ze dat lekker zelf weten, maar ik vind het geen goede reclame voor NVidia dat ze dit soort producten op de markt brengen eerlijk gezegd. Het gaat volledig tegen de trend in dat we efficientere producten willen hebben, niet producten die x% 'beter zijn' voor y% meer energieverbruik, waarbij y signficant groter dan x.
RTX 3090 Ti;3;0.4134896993637085;Is dat stroomverbruik niet flink terug te schroeven met dezelfde prestaties? M'n 2080 ti is ook iets overklokt terwijl ik de voltage juist lager heb gezet.
RTX 3090 Ti;3;0.421480268239975;Vaak wel, en dat zou eigenlijk ook een onderdeel moeten zijn van de review. Hoe doet de kaart het op stock, en hoe doet de kaart het getuned. Met zowel AMD als Nvidia kaarten kun je tegenwoordig het verbruik vaak flink tunen, terwijl de prestaties vrijwel niet afnemen, en soms zelfs toenemen.
RTX 3090 Ti;3;0.45969322323799133;Alleen is een undervolt / overclock toch vaak ook exemplarisch. Desondanks, als meerdere sites dit doen valt er toch wel vaak een patroon te herkennen waar men een aankoop op kan baseren. AMD heeft immers ook een aantal kaarten gehad waar men eigenlijk de chips te ver heeft gepushed om maar mee te kunnen met de concurrentie wat prijs/prestatie betreft, terwijl verbruik/temperatuur/geluid uit het oog werden verloren. De kaarten waren met een kleine undervolt/underclock eigenlijk beter terwijl je er eigenlijk met gaming amper op achteruit gaat. Een bedrijf heeft nu eenmaal andere belangen dan de klant.
RTX 3090 Ti;2;0.4128362536430359;Ik mis Pascal. Hopelijk gaan we zsm terug naar dat soort efficiëntie.
RTX 3090 Ti;3;0.44021525979042053;Dit is wel een interessant punt. Ik heb namelijk een RTX 3070 die out of the box 220W verbruikt. Maar met een undervolt naar 875 mV loopt het verbruik terug naar 160W (zonder noemenswaardig prestatieverlies - vergelijkbaar met 2080 Ti). Daarmee zit je in de buurt van het praktijkverbruik van de GTX 1070, dus een enorme verbetering van de efficiëntie (fps per watt). Pascal liet zich trouwens ook vrij goed undervolten, dus daar was ook nog wel wat efficiëntieverbetering mogelijk. Maar goed, wat ik probeer te zeggen is: ja, de efficiëntie lijkt tegenwoordig om te huilen, maar dat is vooral bovenaan de range.
RTX 3090 Ti;1;0.6214352250099182;65% hoger verbruik en 100% duurder dan een 6800XT en dat voor 16% hogere prestaties. Ook tov de 3080 Ti en 3090 is dit echt een waardeloos inefficiënt product Alleen nuttig voor overclockers en Tweakers. En voor he t behoud van de prestatie kroon tov de 6950XT. Maar qua prijs prestatie en qua performance per watt gaat hij het niet winnen. Voor gamers is deze kaart ook af te raden dan zijn de 3080 en 3080 Ti veel betere keuzes. Zeker met het oog op een groenere wereld moeten we kaartje van 450-600 watt die maar 10-15% sneller zijn voor 50% hoger verbruik niet willen. Maar van wat ik tot nu toe aan leaks lees gaat deze trend zich volgend jaar helaas door zetten
RTX 3090 Ti;5;0.3225967288017273;AMD wint gewoon echt vet hard deze generatie. Je bent wel gek als je nog NVIDIA koopt.
RTX 3090 Ti;2;0.44858765602111816;Ligt aan de modellen maar de 3090 Ti hebben ze duidelijk te ver gepushed. De 3080 Ti was volgens mij qua prestaties per watt ook niet de meest ideale kaart de 3060, 3070 en 3080 doen dat prima. AMD heeft qua efficiëntie vooral bij de 6800(XT) en 6900XT wel een streepje voor. De lagere modellen zijn ze ook wat verder gaan pushen omdat nVidia dat ook deed met de Ti modellen. Het lijkt er op dit moment wel op dat AMD de efficiëntie kroon gaat houden bij de volgende generatie kaarten en het gat groter gaat maken. Volgens geruchten gaat nVidia Lovelace naar 600 watt pushen om de prestatie kroon tehouden vs de Chiplett based Navi 31 chips die volgens de huidige geruchten 450 watt gaan gebruiken om prestaties te verdubbelen. Tijd zal leren of dit uit komt. Ik kan het me van nVidia wel voorstellen dat ze dit voor een eventuele 4090 gaan proberen. Maar ik hoop dat niet de hele lineup ver voorbij zijn sweetspot gepushed wordt.
RTX 3090 Ti;2;0.379755437374115;Ik game vrij veel uurtjes per week en heb al besloten dat de kaart die ik ga aanschaffen er eentje is die mij de meeste frames geeft bij max 250 watt. Undervolting inbegrepen.
RTX 3090 Ti;3;0.4504857063293457;Ik heb nog altijd die 6800 die ik niet eens meer gebruik, want de Intel-based MacBook is eindelijk vervangen met een M1 Max die prima 4x 4K 120hz kan aansturen (en dan slaan de fans van de laptop niet eens aan...), maar de 3080 van september 2020 blijft denk ik ook nog wel een tijdje zitten. Door hem net iets slimmer af te stellen blijft ie onder de 260W piek en is daarmee erg efficient, terwijl ie alsnog de 1950MHz aantikt. Al die geruchten over de toekomstige Nvidia GPUs geven mij weinig hoop voor een degelijk vervangend exemplaar. Over AMD kaarten heb ik nog niet veel opgevangen, dus ik weet niet zo goed hoe het daarmee voorstaat. Overigens moet ik bekennen dat dit voor een groot deel een gevolg is van het gegeven dat ik amper game. Elden Ring heeft mij nog een paar dagen weten te vermaken, maar meestal staat mijn pc weken of zelfs maanden ongebruikt stof te happen.
RTX 3090 Ti;3;0.24650512635707855;Nvidia loopt nog altijd voor qua features. Games die raytracing ondersteunen op AMD kaarten zijn nog steeds een zeldzaamheid, en daar lopen ze qua prestaties ook flink achter. Om nog maar te zwijgen over de professionele features zoals CUDA en Optix, waar AMD eigenlijk niet eens meedoet. Dat neemt niet weg dat het stroomverbruik bij Nvidia wel echt belachelijk begint te worden.
RTX 3090 Ti;1;0.6451356410980225;Voor het geld doet het er gewoon niet toe, je kan voor de prijs van 1 nvidia 2 AMD's kopen die dan 90% sneller zijn..
RTX 3090 Ti;2;0.5373049974441528;Over het algemeen is de prijs/prestatie verhouding nagenoeg hetzelfde, je krijgt alleen geen RTX of DLSS. Ik zie dan ook geen enkele reden om een AMD kaart te kopen. De huidige generatie is met z'n zwakke raytracing mogelijkheden niet bepaald future proof. Voor current-gen games zal de Port Royal benchmark de meest accurate zijn en daar legt AMD het gewoon af, met 10% verschil tussen de top-end 6900XT en Nvidia's RTX 3080.
RTX 3090 Ti;1;0.32605844736099243;Een 3080 kost in de pricewatch hetzelfde als een 6800xt.
RTX 3090 Ti;1;0.5416964292526245;Sorry maar je bent letterlijk aan het trollen als je zegt dat een RT6600 exact hetzelfde presteert als een 3080. Laatst genoemde is ruim sneller in elk aspect. Google zelf maar de benchmarks.
RTX 3090 Ti;1;0.44849085807800293;Nog eens, voor het geld is 10% extra performance het niet waard...
RTX 3090 Ti;2;0.22993627190589905;Call me crazy! :-p Ik heb zeker veel meer interesse in Raytracing en DLSS dan wat AMD momenteel bied op dat vlak. AMD is sterk op andere vlakken en andere doelen dan gaming met dergelijke features. Met integrated graphics, de 6000U serie en RDNA2 is AMD imho heer en meester op het laag wattage gebruik machines ivm. Intel, Nvidia is imho helemaal geen speler op die markt. Maar als ik een highend gaming rig wil en prijs is niet echt een probleem, dan ga ik voor Intel + Nvidia. Zelfs een highend model 3090 Ti van €2500+ zou ik nog overwegen, alleen trekt die natuurlijk behoorlijk wat stroom, op 4k is 750W+ uit het stopcontact (met mogelijk hogere pieken) geen verrassing. Dat is geen issue voor een groot deel van het jaar in Nederland, maar we gaan juist weer de warme periode van het jaar tegemoet, dan zit ik niet te wachten op een straalkacheltje, terwijl na de zomer we juist weer een nieuwe generatie Intel CPUs en Nvidia GPUs verwachten... Ik zou dan eerder voor een streaming abo gaan voor de zomer, dat kan ik zelfs spelen op mijn AMD 4800U machines... En dan zie ik in het najaar wel wat de nieuwe generatie brengt, wellicht dat de rest van de GPU lineup dan ook naar de reguliere niveaus is gezakt...
RTX 3090 Ti;1;0.47291436791419983;Raytracing kan AMD ook, en voor het geld is het gewoon niet waard.
RTX 3090 Ti;5;0.4388534426689148;Je moet dan je game kamer als server ruimte in richten met airco met voldoende capaciteit om full rack aan apparatuur te koelen op tropische NL dag.
RTX 3090 Ti;2;0.37186941504478455;Nauwelijks ervaringen met high end NVIDIA kaarten, maar wat ik zo veel beter vind bij AMD is toch echt wel de drivers en de ondersteuning. Vaak zijn ze ook nog een stuk efficiënter en brengen ze meer nieuwe features als eerste op de markt. Dit was bij de HD 5xxx series althans zo en de HD 7xxx series. Geheugen busbreedte, nieuwer geheugen type, hogere DX versies en dergelijke. Maar of dit nu nog zo is moet je me niet vragen. Totaal geen idee van hoe dat tegenwoordig is.
RTX 3090 Ti;5;0.5079454183578491;Toekomstbestendige kaart als je een dure elektrische verwarming wilt gebruiken.
RTX 3090 Ti;3;0.266247034072876;Goedkoper dan gas...
RTX 3090 Ti;2;0.40231361985206604;Een GPU zal een COP waarde van maximaal +-1 hebben verwacht ik, bij COP 1 is voor zover ik weet een m3 gas ongeveer vergelijkbaar met 8 kWh, nu ken ik natuurlijk jouw prijzen niet, maar een m3 gas is hier nog altijd goedkoper dan 8 kWh, waardoor verwarmen met een GPU me niet goedkoper lijkt dan met gas, tenzij je zonnepanelen hebt met een ruim overschot dat je nog moet weg salderen.
RTX 3090 Ti;1;0.35489532351493835;Tenzij je tegelijkertijd crypto's minet? Dan verwarm je je huis gratis, zeg maar. (full disclosure: ik heb hier geen berekeningen op losgelaten)
RTX 3090 Ti;3;0.44526398181915283;Dan moet je wel een heel klein huis hebben, of heel veel kaarten Ik heb een 6900XT, die getuned 150W verbruikt tijdens het minen (bij gamen tot 300W), je hebt er uiteraard wel iets profijt van, maar een GPU die 24/7 150W verbruikt verwarm je normaliter geen huis mee Zelfs een klein kamertje zal je in de winter al snel moeten bij verwarmen, daar 150W aan warmte afgifte niet heel veel is.
RTX 3090 Ti;3;0.37150973081588745;Maar kun jij gamen met je verwarming?
RTX 3090 Ti;3;0.3699931800365448;Maar dan in de zomer een airco naast je pc.
RTX 3090 Ti;3;0.3286905586719513;Maar wanneer je zelfs geen verwarming in huis hebt, maar alleen airco, is dit ding wel een uitdaging... Misschien monteren in de keuken, als kookplaat? En met een anti-aanbaklaag kun je er direct een ei op bakken.
RTX 3090 Ti;1;0.8674789071083069;Met een auto OC die zelf via de nvidia software kan haal ik deze score ook op mijn 3090, echt nul meerwaarde deze kaart.
RTX 3090 Ti;2;0.47755488753318787;Nondeju. Ik begin een klein beetje een Intel vibe te krijgen. TDP flink opschroeven en dan pronken met meer performance. Wat een verbruik, dat gaat toch nergens meer over. Wat is het 35% gestegen? Als de performance nou eenzelfde sprong zou maken, maar helaas. What's next een 6-slots koeler en een TDP van 1kWh?
RTX 3090 Ti;3;0.4461487829685211;Ik zie maximaal 35% meer verbruik t.o.v. het vorige top model, de RTX 3090 met maximaal 8% betere prestaties. Alsnog niet goed, maar wel beter dan de genoemde 65%. Edit: Ik zie dat de genoemde 65% t.o.v. de 6900XT is, voor de pure gamers natuurlijk ook een concurrerende kaart, voor zakelijk gebruik echter een minder gezien het feit dat err geen feature partity is tussen AMD en Nvidia bij zakelijk gebruik en het AMD eco systeem helaas nog altijd stukken kleiner is.
RTX 3090 Ti;1;0.9103447198867798;Wat een belachelijk product qua stroomverbruik. Moeten we dit willen? En waarom zo weinig kritiek op dit punt?
RTX 3090 Ti;5;0.5778646469116211;De prescott van de gpus. Ben benieuwd hoe lang die kaarten het volhouden bij game gebruik (sterk wisselende belasting met zo'n hoog energieverbruik...). Gelukkig maken ze ook efficiente GPUs. Ik denk niet dat dit de richting is waar mid-high end GPUs op gaan.
RTX 3090 Ti;3;0.39354628324508667;Klinkt wel logisch die subtitel: meer ampere is ook meer watt
RTX 3090 Ti;5;0.2565622925758362;2250 daar bouwde je eerst 2 game pc’s voor😂
RTX 3090 Ti;1;0.5660271048545837;Alsof iemand die 2250 euro voor een videokaart aftikt zich zorgen maakt om het stroomverbruik Dat is hetzelfde als een Ferrari aanschaffen en klagen dat het verbruik zo hoog is.
RTX 3090 Ti;2;0.37530195713043213;Maarja je gaat toch ook niet de hele dag gamen. Tenminste ik niet. Is een 850W Voeding nog genoeg? Gebruik verder een Ryzen 3900x, 3 ssd's en 3 hdd's
RTX 3090 Ti;2;0.5258488059043884;Ik zou zelf graag naar een 4K scherm gaan maar wil er dan wel een systeem bij dat games ook netjes op 4K kan draaien. Het valt me op dat deze kaart wat dat betreft nooit echt het verschil maakt in de geteste games: of 4K ultra gaat ook wel met 60 fps op een kaart die minder verbruikt en minder kost in aanschaf of het gaat ook niet met 60 fps op deze prijzige stroomslurper. Als deze kaart daar wel het verschil had kunnen maken had ik het mogelijk nog wel interessant gevonden, maar nu is het voor mijn use-case geen goede oplossing. Hopelijk zijn de rtx 4000 /rx7000 generaties wel zover dat ik over kan stappen (ja ik ben bekend met dlss en fsr en details lager instellen, maar ik wil gewoon weer een hele tijd klaar zijn als ik een nieuw systeem koop).
RTX 3090 Ti;5;0.6000515818595886;Dit is het meest toepasselijke filmpje voor deze relase:
RTX 3090 Ti;5;0.5769140124320984;Prachtig, jammer dat er altijd mensen zonder humor je gaan downvoten.
RTX 3090 Ti;3;0.3479782044887543;Kan je de verwarming dus uitschakelen komende winter
RTX 3090 Ti;1;0.46232062578201294;Is het misschien een idee (en een keer tijd) om 1 vr game aan de testbank toe te voegen? 🤔
RTX 3090 Ti;1;0.6688442826271057;En wij de 100watt peertjes allemaal vervangen voor leds om maar wat te besparen, gaat nvidea helemaal los en trekt zich niets van het milieu aan. What's next? Over 5 jaar 3500 watt om een beetje 8k te kunnen gamen?
RTX 3090 Ti;5;0.5130003690719604;Dat is ook meteen een heel zorgelijke opmerking. Energiebesparing zou in deze tijd beoordelingspunt #1 moeten zijn.
RTX 3090 Ti;4;0.40045103430747986;Die energie besparing nu hot item is omdat deel van bevolking op rand zitten van nog net rond te komen. Dan is gaan voor besparing belangrijk. Omdat je er ook last van in de beurs krijgt. Voor die gamer met wat meer cash reserves kan nog altijd gaan voor ultieme performance.
RTX 3090 Ti;1;0.6821864247322083;"Alsof mensen die op de rand van nog net rond komen een videokaart van €2250,- gaan kopen?!? De mensen die zo'n RTX 3090Ti kopen boeit het echt weinig tot niks als elektriciteit 2-3x duurder is dan ""normaal"" (wat is nog normaal tegenwoordig?!?).."
RTX 3090 Ti;2;0.43789735436439514;Geld is een argument maar de elektriciteit moet ergens vandaan komen. Aangezien elektriciteit voornamelijk opgewekt wordt met kolen/gas is een toename van het wattage van videokaarten echt een probleem voor het milieu.
RTX 3090 Ti;1;0.728276789188385;Beter tijd steken in het vergelijk van realistische kaarten. Wat een vreselijk slecht concept is deze kaart toch. Semi professioneel allicht nog een beetje te verantwoorden maar … voor spelletjes: neen!
RTX 3090 Ti;5;0.2839389145374298;Ben bang dat de RTX 4Xxx serie nog wel even op zich laat wachten
RTX 3090 Ti;1;0.2494339942932129;Kinders, kom je maar warmen aan de pc want papa gaat gamen Of we besteden het geld toch maar om de energierekening te betalen? Voor de mensen die deze kachel kopen, veel spelplezier gewenst.
RTX 3090 Ti;2;0.2992406189441681;Dubbele reactie.
RTX 3090 Ti;1;0.45659250020980835;Binnenkort maar 2 voedingen waarvan 1 geheel voor Gpu?🤷🏻‍♂️ @rsnubje
RTX 3090 Ti;1;0.25953981280326843;Als het zo door gaat
RTX 3090 Ti;2;0.5662385821342468;Nou, ik vind de prestaties maar tegenvallen eerlijk gezegd. Zoveel power, en dan nog trekt ie de raytracing games maar met krap aan net speelbare framerates van rond de 30 op 4K. Dat is het geld en de hoeveelheid stroom die hij slurpt echt niet waar. Hopelijk wordt de opvolger een verfijning waarbij efficiëntie weer een rol gaat spelen, want dit is niks.
RTX 3090 Ti;1;0.8928694725036621;Ik lach me kapot met mijn RTX 3080 die ik eind 2020 gekocht heb voor +/- €800,-.. Deze RTX 3090Ti is gemiddeld slechts ongeveer 20% sneller maar bijna 3x zo duur.. Totaal absurd.. Wie dit koopt is echt niet goed bij z'n hoofd en/of heeft veel teveel geld over om te besteden aan dit soort enorm overpriced producten..
RTX 3090 Ti;3;0.2440936118364334;Ik heb zelfs de A5000 hoe presteert deze tegenover de de 3090TI ?
RTX 3090 Ti;1;0.7317913770675659;Dus: weer zo'n 2200 euro voor een RTX 3090 Ti, voor een prestatie winst van ca. 21 vs. 19 tov van mijn RTX 3080 Ti. Dat is bar weinig, voor heel veel meer watt en geld. Heeft dus totaal geen zijn als update kaart (tenzij je, zeg, van een 1080 Ti komt). Gewoon maar erven wachten op de 40xx kaarten.
RTX 3090 Ti;2;0.4400751292705536;WTF 500 watt. Doet me een beetje denken aan de r9 295x2. Echt bizar dat in een tijdperk van stijgende energieprijzen en een wereldwijd toenemende focus op broeikasgassen nvidia met zoiets komt. Is het zo lastig om performance per watt op te schroeven of zo?
RTX 3090 Ti;3;0.4078076481819153;Ja uiteraard nu moeten betere materialen gebruiken, meer koper, meer koeling, meer oppervlakte, en meer gezeur van klanten aanhoren over/door het hoge verbruik. Dus ja het is voor hun ook het doel om steeds efficiënter te worden... Anderzijds moet je het zien als een tube tandpasta, tandpasta er uit knijpen begint super makkelijk, maar de laatste beetjes kosten veel meer moeite. Dit soort kaarten gebruiken net die laatste beetjes die dus relatief veel moeite kosten (en dus energie).
RTX 3090 Ti;5;0.49200496077537537;Bij de game tests, iemand ook op gevallen dat ID tech 7 engine, daadwerkelijk van een ander niveau is? Heerlijk goed geoptimaliseerd lekker brute performance zelfs in 4K ultra
RTX 3090 Ti;5;0.46674826741218567;"Geweldig! Ik begon al bang te worden dat er niet nog meer onbetaalbare videokaarten op de markt zouden komen. /s Vraag me wel af waarom deze kaart niet onder de benoeming ""Titan"" op de markt is gekomen. Al zit er wel een groot verschil in prijs/prestatie verhouding van de 3090 vs 3090 ti en de 2080 ti vs Titan"
RTX 3090 Ti;1;0.4494223892688751;Ze zijn echt knettergek geworden bij Nvidia. 530 watt voor alleen je GPU? Koekkoek
RTX 3090 Ti;1;0.4653419554233551;"""Mam, kunnen we krachtstroom krijgen? Ik wil een nieuwe computer"" Van de zotte, en dan te bedenken dat het alleen maar meer wordt."
RTX 3090 Ti;2;0.3939546048641205;Ik ben benieuwd wat de temperatuur van het gddrx6 geheugen gaat zijn. Dat was bij de rtx3080 ook al 92 graden zonder aangepaste fancurve. Helaas zie ik nog geen temperaturen staan in de review.
RTX 3090 Ti;2;0.4944833815097809;Het ligt aan mij ik weet het, maar ik blijf me enorm irriteren aan die verkoop adviesprijzen van grafische kaarten. Die zijn echt utopisch wanneer het de consument aangaat voor wat betreft een vergelijk.
RTX 3090 Ti;2;0.25879350304603577;Van alle nieuwe kaarten afgelopen tijd die k gebruikt voor mijn werk als pc bouwer (3080 10gb, 12gb / 3080 ti / 3090 en ti) ga ik voor de normale 3080 10gb versie. Deze is verhoudingsgewijs het beste. Het minste problemen met geheugen/bugs en vooral qua temperatuur en verbruik kies ik voor deze. Met name de 3080 Gigabyte Gaming Oc en daarnaast de Asus Tuf (oc). Vele andere kaarten stellen me teleur qua verbruik en temperaturen. Staat niet in verhouding met het aantal fps wat je meer krijgt.
RTX 3090 Ti;3;0.3863822817802429;Vraag mij of wat de maximale prijspunt van high end gamer wordt. Blijkbaar vinden genoeg gamers het ok om tussen de 1000- 1600 voor een high end kaart uit te geven. (toegegeven, ook ik had bijna een 6900 gehaald voor 1300, het is maar dat het net verkocht is)
RTX 3090 Ti;2;0.2841584384441376;Ik blijf gewoon netjes wachten op de komst van de RTX 4080 tegen het einde van 2022. Verder wat mij opvalt, bijna 500 watt in gamen dat is extreem en ik hoor in het filmpje dat ze van plan zijn om naar 600 watt te gaan. De prijs vind ik extreem hoog. Ik ben benieuwd naar de RTX 4080/4090. De vraag is of mijn voeding het allemaal aan kan, ik heb een Ryzen 7 5800X computer met een Seasonic Platinum Series 860W v2 voeding
RTX 3090 Ti;3;0.2572439908981323;Ben dit allemaal zo aan t lezen. Moet ik denken aan kaarten die draaien voor cryptoxurrency en wat dat voor t klimaat betekend. Dat draait geloof ik wel 24/7 ook. Dan kan je in zekere zin echt afvragen waarvoor doen we t? (...Ja ik weet waarvoor)
RTX 3090 Ti;5;0.33945584297180176;Gezanik over energie gebruik , who cares
RTX 3090 Ti;4;0.4335853159427643;Is het mogelijk om ervoor te zorgen dat je de prestatiescore in pdf kan downloaden? Dit was ook zo bij HWI en vind ik persoonlijk erg handig!
RTX 3090 Ti;5;0.23552557826042175;2 redenen RTX, DLSS
RTX 3090 Ti;2;0.40548861026763916;Onzin voor de prijs.
RTX 3090 Ti;2;0.2857556939125061;Nu kopen AMD 6 serie of nv 30 serie, met nextgen aan de horizon. Is zonde. En dan is maar de vraag of DLSS en RTX dan nog nv sterkste kant zal zijn. DLSS 1.0 2.0 3.0 4.0 als je native reso heel dicht wilt benaderen. Waar reviewers beelden aan analyseren zijn. Op gegeven moment voldoet het en zal met gen 3 van XeSS FSR DLSS het klaar zijn. Belangrijkste is of er veel games zijn met artifacts.
RTX 3090 Ti;2;0.46025481820106506;De meeste gamers kopen niet halo product en ook geen high-end. De meeste gaan voor lower tot upper midrange en kakt in bij sub high-end. Nu met 300++ watts en next gen 400+++ watts gaat dat nog extra barrierre zijn om niet voor high-end en al helemaal niet de nextgen halo sku. Omdat dat juist extra duur wordt door ook PSU u[grade maar 1Kwatt of meer. Dit kan je vergelijken met die paar gasten met Hummer of 12cylinder De meeste rijden in middeklasser
RTX 3090 Ti;3;0.6312320232391357;Ik denk dat het niet zoveel zin heeft om het topmodel te vergelijken met het instapmodel. Mensen die in de markt zijn voor deze gpu, zullen niet een 3060 overwegen. Maar ik ben het wel met je eens, ik vind het ook wel interessant om de prestaties van het topmodel naast die van het instapmodel te zetten. Al was het alleen maar om even duidelijk te zien hoeveel extra prestatie je krijgt voor de hogere prijs en het hogere verbruik...
RTX 3090;2;0.331705778837204;Ik mis nog de compute tests zoals premiere pro en een render sofware zoals octane. Nvidia zet deze kaart toch op de markt als vervanger van de Titan? Dan verwacht ik ook dergelijke tests.
RTX 3090;4;0.3683133125305176;Nvidia heeft het over Titan-class performance voor deze GeForce-kaart, en we hadden ook graag met dergelijke GPU's vergeleken. Voor nu zou je wel even bij ComputerBase naar hun vergelijking kunnen kijken.
RTX 3090;3;0.3446609079837799;Zou eerlijk gezegd ook meer compute & render performance standaard willen terug zien op tweakers.net “We hadden ook graag met dergelijke GPU’s vergeleken” Worden jullie gebonden aan het type tests wat Nvidia wilt uitdragen?
RTX 3090;2;0.42525625228881836;"Jullie ""willen"" wel heel veel... Is denk ik niet zo beleefd om het zo te formuleren. T.net heeft zijn testen met deze kaart afgerond, als je andere soort testen""wilt"" zien, ""moet"" je maar supplementair andere websites raadplegen"
RTX 3090;2;0.5141273736953735;Ik snap je reactie niet... Waarom vindt je het teveel gevraagd? Tweakers.net zou zich toch meer op tech focussen ipv HTK gebruiker? De RTX3090 is een Titan class card en soort van “cherry on top” net als een Ryzen 3950x, deze reviewen zonder transcoding, rendering en compute zie ik als een gemiste kans, en moet deze info wederom opgezocht worden bij concurrenten/collega’s. Gaming performance is niet het allerbelangrijkste. Dit lijkt een zeer goede kaart voor o.a. amateur/starting content creators zonder de overstap te moeten maken naar Quadro’s.
RTX 3090;2;0.4703519344329834;Nuance: dat deze kaart voor gamen tegen valt lijkt mij daar niet onder vallen. Vóór de reviews wisten we niet wat deze kaart zou gaan doen. Voor het zelfde geld waren de verschillen vergelijkbaar met die van de 2080 Ti vs 2080 destijds. Hetgeen jij beschrijft kan interessant zijn maar dan moet je met die insteek gaan testen en dus ook diverse quadro opties mee pakken. Niet dat je tweakers van mij niks mag verwijten. Deze keuze voor de 4.2 Ghz bottleneck op de RTX 3080 was erg slordig en ook deze keer is er weer een fout ingeslopen (resultaten control 8K zijn 4K resultaten). Het lijkt erop dat de HWI crew nog even moet wennen binnen het tweakers format dan wel dat de verkeerde persoon nu de goedkeuring geeft over de opzet en uitvoering van de reviews.
RTX 3090;3;0.38055896759033203;Ik ben ook wel een fan van als Tweakers de temperaturen van de videokaarten naast elkaar zet wanneer ze idle zijn of onder load. Als ik een videokaart koop dan zijn prestaties en stroomverbruik niet het enige waar ik naar kijk. De temperatuur is ook wel belangrijk voor mij, zeker in de hete zomers is het mooi om een videokaart te hebben die niet extreem warm wordt
RTX 3090;2;0.4653363823890686;Temperatuur is inderdaad een belangrijke factor, maar bij het vergelijken van 3D-chips zegt het vermogen meer dan de temperatuur van de chip. De temperatuur is namelijk afhankelijk van de koeler, en die is voor elke videokaart met de 3090 chip weer anders. De temperatuur van die chip zal dus ook op alle videokaarten weer anders zijn. Als we videokaarten met elkaar gaan vergelijken focussen we natuurlijk wel op de temperaturen en de koelers.
RTX 3090;2;0.4244469404220581;Als je wil dat je kamer niet te warm wordt moet je alleen naar het verbruik kijken. Temperatuur zegt dan niks: een kaart die je kamer opwarmt met 300 W maar goede koeling heeft kan een lager temperatuur hebben dan eentje die 100 W verbruikt met slechte koeling. Maar hij warmt je kamer wel drie keer zo snel op.
RTX 3090;2;0.4550211429595947;Dit klopt niet helemaal... De warmte energieflux, is bij beide hetzelfde namelijk 300W, en zal dus jouw kamer precies hetzelfde opwarmen. Naast de temperatuur zijn er nog meer grootheden die een rol spelen, zoals het totale oppervlakte waar de warmte energie wordt afgegeven en de snelheid van de luchtstroming langs het oppervlak. Als de beide groter dan, dan de temperatuur lager zijn. Lagere temperatuur is beter voor de levensduur van de chip en overclock potentieel, maar beide warmen de kamer gelijkwaardig op...
RTX 3090;3;0.32316675782203674;Hij vergelijkt met een 100W chip die op een hogere temperatuur loopt. Misschien had je daar overheen gelezen?
RTX 3090;1;0.4702073335647583;Dat klopt helemaal! Sorry, was nog vroeg . Bedankt voor de aardige correctie
RTX 3090;2;0.36557164788246155;Edit: ik las het volgens mij niet helemaal goed. Maar eens: Als je een grote straalkachel van 100 watt aanzet die een groot oppervlak heeft maar minder warme onderdelen, of een kleine van 100 watt die heel geconcentreerd warm is, dat maakt niet uit, het gaat om het wattage: Energie gaat niet verloren. Misschien voelt de intiële piek 'warmer', maar over enige tijd is wattage in temperatuur erbij. (Zelfs (led) lampen produceren vooral warmte overigens). Nu is je huis geen geheel gesloten systeem, maar de energie moet ergens blijven dus het lijkt me zeer sterk dat een kaart die even veel wattage gebruikt magischerwijs je huis minder opwarmt, want per definitie produceren ze evenveel warmte.
RTX 3090;2;0.4724586308002472;Met welke reden wil je dat hij niet warm wordt? Voor de performance, of omdat hij dan meer warmte afstoot? Temperatuur is niet gelijk aan warmte. Een 3090 slingert meer hitte je kamer in, ongeacht of hij warmer/koeler draait dan een GTX 1060 oid.
RTX 3090;3;0.43098798394203186;Stroomverbruik is al een redelijke indicatie voor de temperatuur.
RTX 3090;3;0.375048965215683;Voor het effect op de temperatuur in je kamer is het dé enige indicatie die ertoe doet. 300W in -> 300W hitte uit.
RTX 3090;1;0.7129273414611816;Nee, deze kaart is door Nvidia in de markt gezet als 8K Gaming kaart. Geen grap. Aangezien dit volledig belachelijk is omdat het overduidelijk de opvolger van de Titan is, heeft GamerNexus hier vervolgens gekscherend een 'Gaming Review' van gemaakt waar ze gelijk even de grond gelijk maken met de marketing afdeling van NVIDIA.
RTX 3090;1;0.2810087203979492;Hij draait toch control bijv op 8k rond de 60 fps? Dan is dat toch geen misleidend statement? Edit: bron: deze review! reviews: Nvidia GeForce RTX 3090 - De overtreffende trap van Ampere
RTX 3090;1;0.41976603865623474;Het is geen 8K, het is upscaling naar 8K en dat is niet hetzelfde.
RTX 3090;3;0.374269038438797;Onze benchmarks van Control op de 8K gaming pagina maken geen gebruik van DLSS / upscaling. Dit is op een 8K monitor getest. Wel hebben we voor de 8K resolutie de anti-aliasing opties uitgezet, gezien dat op een 8k resolutie niet heel veel nut meer heeft.
RTX 3090;1;0.5088035464286804;Volgens mij bedoel ze Doom Eternal. Een 3080 haalt op 4k geeneens 50fps dus een 3090 haalt echt geen 60fps bij 4x zo hoge resolutie
RTX 3090;2;0.3962000906467438;Wij bedoelen zeker geen Doom Eternal. Je link naar TPU zegt ook niet zoveel, gezien we de instellingen niet weten die zij gebruiken.
RTX 3090;1;0.37989866733551025;Kijk dan naar de benchmark van Tweakers zelf.. pagina 6. Control 4k 67fps en ... 69fps in 8k
RTX 3090;1;0.5294018983840942;Kon het toch niet hebben en heb Control zojuist opnieuw gedraaid op onze 3090. De render resolution bij de vorige test bleek nog op 4K gestaan te hebben, dat was het euvel.. Nu uiteraard veel lagere framerates. Mea culpa! En dank voor jouw oplettendheid Ik heb de alinea over Control op 8K for the time being even weggehaald, morgenochtend mag de auteur kijken wat hij er mee wilt.
RTX 3090;5;0.3398284316062927;Dat zal het zijn inderdaad! 👍
RTX 3090;3;0.4386143982410431;Staat er gewoon bij. Maar hoe kan het dat tweakers bij control op 8K met DLSS en Ray tracing op uit 69FPS haalt terwijl TDU 60FPS haalt op 4K ook met DLSS en Ray tracing op uit, 8K heeft 4x meer pixels dan 4k dus is het veel zwaarder voor de GPU. Dan kan het niet zo zijn dat de kaart dan meer FPS haalt. Naar mijn idee kan het geen kwaad om het nog eens te controleren.
RTX 3090;3;0.3735131323337555;TPU gebruikt RTX on in Control, wij in de normale tests niet, tenzij het er specifiek bij staat.
RTX 3090;3;0.3275271952152252;Nee dat stond gewoon uit... Dus toch wel zeer opvallend dat Tweakers 69FPS haalt op 8K!
RTX 3090;1;0.45144790410995483;Leesfoutje dan, mijn fout. Ik zal er morgen even naar kijken.
RTX 3090;5;0.4538736939430237;mijn gtx 1080ti haalt 70fps in doom eternal op 4k nightmare setting
RTX 3090;2;0.4142202138900757;Reconstructie is niet hetzelfde als upscaling. DLSS weet er met zijn algoritmes toch echt details er weer uit te halen die niet bestaan op lagere resoluties en ook niet tevoorschijn komen met upscaling. In dat opzicht is DLSS een gamechanger, namelijk graphics op het beeld toveren door slimmer met de mogelijkheden om te gaan in plaats van puur raw performance. Of bekijk jij ook liever films zonder hardware versnelling zodat je cpu met volle belasting een 4K film probeert te decoderen terwijl je gpu dit kan zonder enige moeite?
RTX 3090;3;0.5102774500846863;DLSS2.0 ziet er juist beter uit dan native op veel punten, op sommigen minder. Maar het geeft je wel fors meer performance. DLSS zorgde ervoor dat je control op 1440p met alle raytracing effecten kon spelen dus mooier, death stranding was scherper, F1 2020 oogt ook beter en zo kan ik meer voorbeelden opnoemen. Je ziet alleen relatief kleine artifacts. DLSS1.0 was wel echt waziger.
RTX 3090;5;0.3235796391963959;Volgens de Nvidia marketing.... bedrijven laten het altijd er mooier en of beter uitzien dan de werkelijkheid, zie bijvoorbeeld ook de game trailers van UBIsoft. Ik heb hier zelf een 2070 super en het ziet er niet beter uit dan native resolutie, zonder het te weten zou ik ook zo kunnen zeggen of DLSS2.0 aan of uit staat.
RTX 3090;3;0.6760545969009399;Ik vind het er zelf wel beter uit zien in de latere lichting games. Vooral als het games zijn die gebruik makem van TAA.
RTX 3090;1;0.529539942741394;Mijn punt was dat realistisch gezien gamers zijn voor deze kaart simpelweg niet de doelgroep zijn, net zo min als ze dat waren voor de Titan. Daarom is het belachelijk dat NVIDIA hem wel zo market. De doelgroep voor deze kaart zijn creatievelingen en professionals die in hun (3D/CAD) workflow een enorme berg geheugen en rekenkracht nodig hebben. Hoewel NVIDIA zelf de positie van hun 3090 heeft ondermijnt doordat er aanwijzingen zijn dat er een 3080 met 20GB werkgeheugen gaat komen die waarschijnlijk nauwelijks gaat onderdoen voor deze 3090, maar wel gunstiger geprijsd gaat worden. Al is dat laatste natuurlijk speculatie en nog afwachten. De claim die NVIDIA doet met '8K gaming' is daarnaast simpelweg onjuist. De kaart produceert op 8K nauwelijks 30fps met verschrikkelijke frametimes in de meest gunstige gevallen, zie daarvoor vooral ook de review op Youtube die ik hierboven linkte Edit: ik zie dat Tweakers inderdaad indrukwekkende cijfers van 60+fps haalt op native 8k zonder AA. Opmerkelijk, want GamerNexus kreeg dat niet voor elkaar en die zijn doorgaans erg thorough met hun tests. Het 8k-hoofdstukje hier in de Tweakers review had ik trouwens totaal over het hoofd gezien, ik keek naar alle testresultaten daarvoor...
RTX 3090;2;0.3826693892478943;Het artikel spreekt dat tegen:
RTX 3090;1;0.41678184270858765;Dat zijn fps behaald op 4k. Edit: ik had ongelijk, zie ook m'n aangepaste bericht hierboven.
RTX 3090;3;0.3813842236995697;Je hebt juist wel gelijk
RTX 3090;1;0.436381995677948;Volgens mij is 4320p juist de benaming van 8K, dus... gehaald op 8K
RTX 3090;1;0.6112262606620789;Nou echt niet. Genoeg positieve videos gezien met 60 fps. Waarom zou een ontwerper een rtx 3090 kopen? Daar zijn de quadros voor die beter met 64 bit floating points overweg kunnen. Je verhaal klinkt meer als graag willen zien dat Nvidia de boel voor de gek houdt.
RTX 3090;3;0.4298079013824463;60fps @ 8k? Kom maar door met die video's. Verder: prima, wat jij wil Edit: ik heb ondertussen het 8k-hoofdstukje hier op Tweakers ook gezien, daar hangt 8k inderdaad rond de 60fps (met AA uit). Indien dat klopt is dat zeker indrukwekkend. Ik zie dat soort resultaten echter bij vrijwel geen enkele andere reviewer terug.
RTX 3090;4;0.2846148908138275;
RTX 3090;2;0.376336932182312;"Maar gamers zijn met de 3090 juist wel de doelgroep, dat is overduidelijk aan de marketing te zien, er staat met koeienletters op de site van nvidia zelf ""BUILT FOR 8K GAMING"", dat ze er niet bepaald in uitblinken en de 3090 beter ingezet kan worden als workstation kaart is een ander verhaal."
RTX 3090;1;0.6436331272125244;"Marketing =/= waarheid. Nvidia hoopt er een lading te kunnen verkopen aan gamers met een hoop geld; voor een hoop geld. Dat hij voor workstations is maakt niet uit. De marketing voor die tak is compleet anders, direct gericht op bedrijven. En die zie je niet voorbij komen op youtube of tech-sites. Zelfde idee dat er toch mensen waren met meer geld dan verstand die Titans kochten om op te gamen. De naam Titan veranderen naar xx90, dan is het stigma weg en hopen dat je een lading kan verkopen."
RTX 3090;2;0.40363427996635437;Klinkt alsof hij voor gamers is. Zijn zat gamers of lui die gewoon t dubbele willen betalen voor t hipste. Voor workstations zal hij vast ook wel verkocht worden, op termijn zullen ze daar uiteindelijk ook wel aparte lijn voor gebruiken met bijpassende branding (quattro ofzo) of het zelfde product met andere marketing slide. Meeste spul voor die workstation markt is toch kant en klaar gebouwd. producten kunnen ook geschikt zijn voor meerdere doelgroepen / use cases.
RTX 3090;3;0.421575129032135;Maar dat gaat ze ook wel lukken, streamers willen de top of the art omdat snelheid toch echt werkt in sommige gevallen ook al is het maar 1 frame winst op de tegenstander, en krijgen dat wel gesponserd, rijke kids willen de zelfde performance en zullen de streamers na doen en die dure kaart gaan kopen. Als je nu al ziet hoe games op eens goed verkopen op steam als een paar streamers ze spelen, zal dit ook zijn gevolg hebben in de hardware die gebruikt word, je ziet ook zo vaak de vraag langs komen op wat voor pc de streamen aan het spelen is dat ze dat al ergens hebben neergezet zodat ze het niet iedere keer hoeven te herhalen.
RTX 3090;1;0.3962739408016205;"Zorgde de Titan niet voor een grijs gebied tussen de mining/gaming kaart en de Tesla? Door de Titan naam weg te halen en deze in de ""normale thuisgroep"" te plaatsen hebben ze nu dus alleen de RTX en de Tesla lijn?"
RTX 3090;2;0.27623748779296875;Je hebt helemaal gelijk, dat is ook wat ik wilde overbrengen (zie m'n eerste reactie) maar heb ik in m'n haast onzorgvuldig geformuleerd. Nu aangepast, dank
RTX 3090;3;0.4014994204044342;Nou ja, het zal je verbazen hoeveel gamers een titan RTX, of dan nu een 3090 kopen om te gamen. Ik zelf kocht destijds een maxwell Titan X om op 4K te kunnen gamen. Achteraf gezien had ik beter een generatie kunnen wachten, aangezien mijn 4K performance niet ideaal was, maar goed, ik had hem wel
RTX 3090;3;0.3498871624469757;Wat is dan de doelgroep van een Titan? Mijn oude sli opstelling was vooral goed om de kamer te verwarmen bij het spelen van titanfall.
RTX 3090;2;0.4358803629875183;En de 3080 zit net onder de 60(ca.58 afhankelijk van de processor). Daar word ik niet zo blij van eerlijk gezegd. Díe slechts 14% winst is eigenlijk best welkom. Wel voor een kaart die een behoorlijk aantal jaren mee moet in elk geval. Heel misschien dat de 20GB versie dit nog kan verhelpen, maar denk het eerlijk gezegd niet.
RTX 3090;2;0.5400025844573975;Op hogere resoluties zal je waarschijnlijk wel wat winst zien. Of de 3080(20gb) ook 8k (DLSS) aan kan... Als het vergelijkbare resultaten laat zien, dan is de 3090 een hele slechte keuze voor gamen... (Zal natuurlijk aan de prijs liggen...)
RTX 3090;1;0.7286224365234375;Dit klopt niet. Control draait geeneens 60fps op 4k
RTX 3090;3;0.3070344626903534;Volgens dit tweakers review toch echt wel: reviews: Nvidia GeForce RTX 3090 - De overtreffende trap van Ampere
RTX 3090;3;0.40380847454071045;Ik weet niet hoor maar de benchmarks die tweakers heeft gedraaid zitten op 60fps of vlak erbij. Dat is ongelooflijk indrukwekkend. Dus ik vind het geen misleidende marketing om te zeggen dat dit een 8k gaming kaart is.
RTX 3090;1;0.3863350450992584;Klopt, ik had het 8k hoofdstuk hier op Tweakers over het hoofd gezien, excuus Zie m'n aangepaste reactie hierboven.
RTX 3090;4;0.3828137516975403;Netjes dat je het toegeeft 😉
RTX 3090;1;0.7481374144554138;Die gast heeft wel goede filmpjes. Maar holy crap, waarom heeft hij altijd zo'n zeiktoontje, ik kan het niet aanhoren.
RTX 3090;3;0.47320616245269775;Hij zou het allemaal wel wat minder cynisch en zeikerig mogen brengen idd. Ik skip zijn filmpjes iig.
RTX 3090;2;0.5085569620132446;Ik dacht dat het aan mij lag......ik kan zijn stem ook niet aanhoren. Helaas, want inhoudelijk is hij wel goed op de hoogte.
RTX 3090;2;0.2864671051502228;"Dat is niet belachelijk als je er even over nadenkt. 8k gaming is voor bijna niemand belangrijk, dat is alleen leuk als gimmick bij sommige professionele presentaties/shows, of voor mensen met enorm veel geld. En natuurlijk is buiten 8k gaming de 3090 relevant bij wat productivity workloads die veel VRAM gebruiken. En dat is precies waar deze kaart voor bedoeld is, de 3090 is niet bedoeld als mainstream kaart. Als je vind dat 1500 euro te duur is (en dat vind ik ook) en je de prijs/prestatie verhouding voor gaming niet goed genoeg vind (ben ik het ook mee eens), is deze kaart gewoon niet voor je bedoeld. En het fijne van de huidige situatie is dat de normale gamer bijna niets verliest met een 3080 ten opzichte van de 3090. Dus zolang je de VRAM van een 3090 niet nodig hebt kun je gewoon heel tevreden zijn met een 3080 (of misschien later een AMD kaart) zonder dat je iets significants mist. En Nvidia geeft dit ook letterlijk aan in hun ""review guide"", zie"
RTX 3090;3;0.4920535087585449;"Opvolger van de Titan misschien wel ja, maar niet ècht een Titan; de Titans waren altijd efficiënter dan de Ti kaarten, en dat is de 3090 niet. Daarnaast verstookten de Titans doorgaans ook 'maar' 250 watt, behalve de laatste, die was 280, maar dat is nog steeds bijna 100 watt minder dan deze 3090. Vandaar dat Nvidia het heeft over 'Titan class performance' maar niet over een Titan; hij past niet echt in het totaalplaatje van een het Titan merk. De prestaties zijn bruut maar de gebruikte Samsung node is niet erg zuinig, dus ik verwacht eigenlijk geen Titan-achtige kaart deze keer."
RTX 3090;1;0.6242287158966064;Ik had interesse voor computer vanwege de omvang van het vram, maar na de gigabyte lek wacht ik even wat een 20GB 3080 gaat kosten voor ik zoveel geld uitgeef.
RTX 3090;5;0.4198837876319885;"Of een 16GB 6000 kaart. ;-)"
RTX 3090;1;0.4084767997264862;Helaas geen optie, dan moet ik al mijn bestaande CUDA code omzetten
RTX 3090;3;0.5189810991287231;Eens gekozen blijft gekozen, met CUDA spul, ja. Maar het codeert wel zo veel makkelijker dan openCL.
RTX 3090;2;0.42510172724723816;Het trieste is dat toen ik begon met omzetten van software naar GPU ik heb getest met OpenCL 2.0 op een AMD gpu en met CUDA. OpenCL 2.0 met C++ wrapper was iets meer boiler plate als CUDA, maar goed te doen en price performance was AMD beter. En toen besloot AMD voor wij beslist hadden om zijn hele driver en compute omgeving om te gooien. Het was een drama om aan de praat te krijgen en had ook nog eens geen geen function parity met de oude stack. Conclusie: als we OpenCL wilden hanteren moesten veel opnieuw schrijven en eigenlijk bij OpenCL 1.2 blijven. Ik krab nog liever mijn ogen eruit dan dat ik naar OpenCL 1.2 code kijk...
RTX 3090;3;0.48260360956192017;Misschien is het interessant voor je om naar openACC te kijken, mocht je het niet al kennen. Het laadt parallel rekenwerk naar de GPU, op basis van precompiler tags, zonder dat je CUDA kernels of wat dan ook hoeft te schrijven die dat expliciet doen. Ik heb er wat voorbeeldcode van, als je er eens mee wilt spelen. Natuurlijk lever je wel wat aan runtime in, maar je krijgt er veel programmeertijd voor tijd terug.
RTX 3090;3;0.3861773610115051;Deze site heeft octane benchmarks: Het lijkt dat de 3090 ongeveer 19% sneller is dan de 3080. Ongeveer het verschil in aantal cores.
RTX 3090;2;0.5388901829719543;Nog altijd niet echt interessant. Bv de PCIE gen 3 vs 4 vergelijking - waarom zit daar geen enkele creator benchmark in? OctaneBench? Premiere Render time? Zeker bij een kaart die expliciet gemarket wordt bij creators vind ik dit een groot tekort in deze review.
RTX 3090;2;0.45839568972587585;Ik had graag een vergelijking gezien met de nieuwste titan kaart maarja wie ben ik om,zoiets te zeggen. de intel en amd processoren lopen bijna gelijk wat deze benchemark laat zien. Je ziet zelfs dat die amd 3700x processor best goed is en kan het intel geweld goed bijbenen. Tevens zitten deze nieuwere kaarten die compatibel zijn aan de pciexpress 4.0.Wat ook weer minstmarge te zien is op deze synthetische benchemarks. Het ziet er allamaal heel erg leuk uit maar ik ga geen kaart kopen over de 1000,00 euro laat staan 1500 euries.Deze kaart is nog duurder dan mijn geheel systeem.En teveel overkil wat ik met mijn compuiter doe.Als fervent gamer zou men deze kaart dus wel weer kopen.
RTX 3090;4;0.23153817653656006;Puger Systems - een Amerikaanse specialist ij het bouwen van workstation heeft de RTX 3090 gereviewed met alle denkbare rendermogelijkheden. De PugetBench die Computerbase gebruikt komt uit hun koker, zie hier de review op Puget Systems.
RTX 3090;1;0.7944008708000183;Nee, Nvidia zet deze GPU neer als pure gaming kaart.
RTX 3090;2;0.33606255054473877;Maar dit gaat over de Nvidia 3090 en niet over de AMD Radeon 7.
RTX 3090;1;0.7331730127334595;Klopt, maar om diezelfde comment meerdere malen te spammen terwijl het niks toevoegd aan de discussie toont alleen maar fanboy gedrag. Helemaal als je selectief gaat shoppen uit de marketing materialen.
RTX 3090;3;0.29957160353660583;"Wat je doet is selectief shoppen uit marketing materialen. ""The GeForce RTX™ 3090 is a big ferocious GPU (BFGPU) with TITAN class performance. It’s powered by Ampere—NVIDIA’s 2nd gen RTX architecture—doubling down on ray tracing and AI performance with enhanced Ray Tracing (RT) Cores, Tensor Cores, and new streaming multiprocessors. Plus, it features a staggering 24 GB of G6X memory, all to deliver the ultimate gaming experience."" Het eerste paragraaf onder de slider praat over zaken als raytracing en AInperformance met dan pas aan het eind gaming. Meerdere page secties zijn gericht op content creation. Maar jij claimed dat zijn het puur marketen als gaming GPU wat gewoon niet waar is."
RTX 3090;2;0.35072898864746094;Dan negeer je dus de feiten, dat mag uiteraard maar daar ga ik niet in mee. Feit is en blijft dat het een pure gaming GPU is zoals Nvidia het zelf aangeeft. Allemaal (dus de RT cores, Tensor Cores, Streaming multiprocessoren en de 24GB aan GDDR6X geheugen) voor de ultieme GAMINGERVARING! Ray Tracing en AI-presentaties zijn ook bedoeld voor Gaming. Je mag best toegeven dat je ernaast zit. I rest my case.
RTX 3090;1;0.3533603549003601;Je zit er echt naast, waarom dan toch zo doordrammen? Alles ademt gaming op die pagina. Er is een klein stukje wat over “creativiteit” gaat en ook daar hebben ze het niet specifiek over de 3090, daar hebben ze het over de 3000 serie dus alle kaarten wat ik zelf dus al aangaf. Zoals gezegd mag je best toegeven dat je het verkeerd gelezen hebt, het is niet erg dat je een foutje maakt maar ga dan niet doordrammen om maar je gelijk te halen, want dat heb je niet gezien de pagina van Nvidia.
RTX 3090;2;0.39636436104774475;Bij RDR2 op 4K en 1440p zien we dat het bij de 3090 voor de framerate niet uitmaakt of de AMD of de Intel CPU gebruikt wordt. Met de 3080 is het Intel platform een stuk sneller dan dat van AMD. Is dat verklaarbaar?
RTX 3090;5;0.5672640204429626;Goed gespot! We hebben deze bench even opnieuw gedraaid en de resultaten bijgewerkt.
RTX 3090;3;0.3956453800201416;Mogelijk door de extra VRAM op de 3090. Dan vormt het aanspreken van het systeem RAM misschien minder een bottleneck. Geen idee of dat de reden is, maar zou een verklaring kunnen zijn.
RTX 3090;2;0.44629034399986267;Geen temperatuur test jammer tweakers. Volgens guru3d en tech wordt de kaart ongeveer 75 graden onder load wat aardig laag is voor zon stroomvreter.
RTX 3090;4;0.35491350293159485;Omdat we geen RTX 3090 Founders Edition hebben maar een custom model, hebben we er voor gekozen deze vergelijking weg te laten. Bij een roundup van custom modellen onderling is temperatuur een van de belangrijkere benchmarks, die roundups gaan er zeker komen
RTX 3090;2;0.3250616490840912;Zijn de 8K benchmarks met of zonder DLSS uitgevoerd? Als DLSS aan stond, was het geen native 8K. Dus als het native 8K resultaten zijn, wat zijn de scores met DLSS aan? Of Als DLSS wel aanstond, wat zijn dan de scores zónder DLSS? Enige duidelijkheid hierover zou welkom zijn.
RTX 3090;1;0.44719046354293823;"Kijk eens bij Gamers Nexus hoe het de 3090 vergaat in ""8K"" : ( spoiler : niet goed )"
RTX 3090;2;0.35358795523643494;De benchmarks op 8K zijn uitgevoerd zonder DLSS, anders hadden ze uiteraard onder het DLSS kopje gestaan.
RTX 3090;1;0.489918053150177;Control 4k: 67 fps Control 8k: 69 fps Wat klopt hier niet ?
RTX 3090;1;0.6037861704826355;Zoals in de comments ook al gezegd, MSAA staat uit op 8K. Dat scheelt een slok op een borrel
RTX 3090;1;0.6956243515014648;Hier klopt toch niks van. GamersNexus komt niet verder dan dan 30FPS bij de meeste games zonder DLSS...
RTX 3090;1;0.3616774380207062;Je hebt helemaal gelijk. We hebben zojuist de 3090 nog opnieuw getest op 8K, de render resolution lijkt verkeerd te hebben gestaan. De FPS waardes kwamen nu op ~25 fps uit voor 8K Medium. Voor nu is het stuk over 8K even uit de review gehaald in ieder geval. Dank voor de oplettendheid in ieder geval!
RTX 3090;5;0.4176427721977234;Graag gedaan en bedankt voor de update!
RTX 3090;1;0.6148331165313721;Het is geen grap: 3090 is minstens 100% duurder en niet meer dan 20% sneller dan de 3080. De 3080 zou dus de beste keus zijn. De 3090 kost vanaf €1600 tot €2049 bij Alternate.
RTX 3090;4;0.2998795807361603;Ik verwacht de RTX 3080 de meeste gekozen kaart zal gaan worden. Ik zou zelf nooit zo'n bedrag neertellen voor een videokaart als de RTX 3090 en nog eens niet 20% sneller. De RTX 3080 doet het net zo goed. Mooi dat AMD cpu apart is getest is.
RTX 3090;5;0.3275686502456665;Moeilijk te zeggen nu. Ik verwacht dat er straks meer 3060/3070 modellen verkocht gaan worden omdat die in een price range zitten die meer mensen aanspreekt. En laten we vooral AMD niet vergeten, ben heel benieuwd waar zij mee gaan komen en wat het moet gaan kosten.
RTX 3090;1;0.6419153809547424;Klopt, wat ik zelf denk is dat de 3060 als eerste de beste verkocht gaat worden, daarna de 3070 en dan de 3080. De 3090 heeft echt de slechtste prijs/prestatie verhouding.
RTX 3090;1;0.8029175400733948;Mee eens. De 3090 zal echt niet mijn keus en voor zulke bedragen ga ik absoluut nooit aan uitgeven! Ik weet niet hoe ze het in hun kop halen om zulke absurde bedragen te vragen.
RTX 3090;2;0.39898964762687683;Bij SIComputers staat de Asus 3090 OC (waar ik me gisteren eentje van heb besteld lol) voor 1682 (exBTW zoals in mijn geval, komt dat aan net geen 1400 ) Aan die prijs gaat Alternate der niet veel verkopen. ik heb in mijn 20j pc gebruik nog nooit de top van GPU gehad. Dit keer ga ik em hebben.
RTX 3090;3;0.2932520806789398;Volgens HWI is de 3090 veel beter in staat om te minen. Van mij mogen de miners de 3090 opkopen lol
RTX 3090;5;0.4800329804420471;Miners mogen van mij doen wat ze willen, kan me geen barst schelen. Zolang ik de mijne maar krijg
RTX 3090;2;0.3804240822792053;3090 is 10% a 15% sneller maar 2x zo duur. Je moet wel een echte fan zijn met een dikke portemonnee om als gamer de te kiezen voor een 3090 ipv een 3080.
RTX 3090;5;0.39445212483406067;Klopt, ik heb er zojuist één gehaald. Ben het met je eens dat de 3080 een betere prijs/kwaliteits verhouding heeft. Mja ergens wil je het beste van het beste, het is een dure hobby. Maar wat een monster kaart, kan niet wachten!
RTX 3090;3;0.292133629322052;Of je wil spellen spelen die per se die 24 GB vol gaan pompen. De 3090 is niet bedoeld als een kaart die veel sneller is dan de 3080, maar als een kaart die veel meer geheugen heeft dan de 3080.
RTX 3090;1;0.4634881019592285;Er komen ook 3080 versies me 20GB, hoewel daar geen prijzen van bekend zijn. nieuws: Gigabyte-website noemt RTX 3080- en 3070-videokaarten met 20GB en 16G... Wat is het verschil in games tussen 24GB en 10GB?
RTX 3090;2;0.34894999861717224;14GB? Geintje, ik denk dat je eerder moet kijken naar Creator Applications waar dit een verschil kan maken. Ik ken geen games waar dit een verschil zal maken.
RTX 3090;2;0.375052273273468;Flight Simulator en rFactor 2 bijvoorbeeld. Stop meer scenery / voertuigen etc in de sim en het geheugen loopt op.
RTX 3090;2;0.4012224078178406;Geen idee hoe hard dat voor spellen uitmaakt, maar gezien de ruimte er is, zullen de programmeurs ongetwijfeld gedetailleerdere textures er in gaan stoppen.
RTX 3090;3;0.37508004903793335;Of als je Skyrim heel goed mod, dan kan het vram gebruik ook flink oplopen.
RTX 3090;1;0.3188381791114807;misschien gddr6 en gdd6x??
RTX 3090;2;0.451238214969635;Vol pompen kunnen heel veel games. Het ook gebruiken is een tweede. Veel games geven aan dat ze de max GB in gebruik hebben maar dat is meestal niet het daadwerkelijke.
RTX 3090;5;0.258319616317749;Men koopt ook telefoons van 1200 euro terwijl je met eentje van 300 hetzelfde kan. In dat opzicht vind ik 1500 nog wel meevallen. Sommige mensen lopen met 2000 euro aan Apple spul rond, dus dan kan dit ook wel. Als je echt het beste van het beste wilt hebben.
RTX 3090;2;0.3125607371330261;NVidia zegt dat hun RTX 3090 kaart eerder vergeleken moet worden met de Titan varianten. Is er een reden dat deze niet in de overzichten staan?
RTX 3090;2;0.3399556577205658;We hebben helaas geen Titans liggen om mee te kunnen vergelijken.
RTX 3090;3;0.4392798840999603;Maar wel een 8k monitor?
RTX 3090;3;0.41739368438720703;Die kwam toevallig wél net op tijd binnen al had het anders met een virtuele resolutie opgelost kunnen worden.
RTX 3090;1;0.7026710510253906;Hebben jullie Koekie1 een berichtje gestuurd? Niet geschoten is immers altijd mis!
RTX 3090;5;0.4573129117488861;JayzTwoCents heeft de 3090 vergeleken met de Titan RTX.
RTX 3090;3;0.48794999718666077;Ja en die heeft ook met en zonder RTX gebenched, maar zonder DLSS jammer genoeg. Verders een leuke review.
RTX 3090;5;0.5912169218063354;Volgens Nvidia is het een pure gaming kaart, de 3090 is dus een 100% gaming kaart. Aldus Nvidia.
RTX 3090;1;0.47970473766326904;Onzin, is gewoon voor alles. Of toch enkel voor broadcasting?
RTX 3090;1;0.2798544466495514;Bijvoorbeeld een 3060, 3070, 3080 kan ik ook voor alles gebruiken, punt is dat onder andere de 3090 puur gemaakt is voor gaming volgens nota bene Nvidia zelf, de hele marketing is erop gebouwd voor gaming.
RTX 3090;2;0.5028281807899475;Ik haal ergens volgend jaar wel een RTX3080, ik moet het nu maar even met m'n 2070 doen. De RTX3090 is mij veel te duur voor wat je er voor terug krijgt. Prijs/kwaliteit verhouding is bar slecht vergeleken met een 3080.
RTX 3090;2;0.38410553336143494;Moet het maar even doen met een 2070, alsof dat een straf is Ik moet zeggen dat de stap op 1440p me verbaast t.o.v. mijn 1080, wat ook al geen hele oude/makke kaart is. Had de 3080 gewoon normaal leverbaar geweest had ik er waarschijnlijk al een gehad. Nouja.
RTX 3090;3;0.375444620847702;Verbaasd als in 'toch meer power nodig om XX/XXX frames te halen op 1440p'?
RTX 3090;2;0.5161405205726624;Dat het gat al zo groot is, waarbij dat tussen de 1080 en 2080 toch relatief meeviel. Ja het is een extra generatie, maar meer dan een verdubbeling had ik niet aan zien komen.
RTX 3090;1;0.3545272946357727;Lol..... 2070.... Ik zit hier nog met een 1060 (6GB)
RTX 3090;3;0.2834393382072449;Wees blij met een 1060, je slaat waarschijnlijk de 2000 serie over, dan heb je veel meer aan een 3070/3080 . Goed dat je hebt gewacht hahaha
RTX 3090;3;0.5612548589706421;Voor 1080p 60fps is de 1060 goed genoeg (wellicht niet op ultra of high voor toekomstige games), maar ik hoef voorlopig nog niet te upgraden.
RTX 3090;5;0.4700247049331665;Een 1080Ti kan nog een optie zijn, omdat deze 11Gb heeft. Dat is ook tegenover een 1070Ti een flinke upgrade qua performance. Hetzelfde als de 2070 Super.
RTX 3090;2;0.345639705657959;Waarom alleen game benchmarks voor en kaart die voornamelijk gebruikt zal worden in workstations? Had leuk geweest om wat dingen te zien als octanebench of wat opencl en cuda benchmarks. Al is hij inderdaad wel mismarketed door nvidia als gamekaart
RTX 3090;2;0.4150267243385315;"nvidia wil deze kaart gewoon meer pushen op gamers, dat deden ze met de titans ook al langzaam (alleen veel minder direct). Gewoon een kwestie van ""ja die markt kunnen we net zo goed er ook wel bij pakken"" Professionals kopen die kaarten toch wel omdat: 1. Ze zijn goedkoper dan quadro's 2. Veel software heeft een hardware ""lock"" voor nvidia door alleen cuda te ondersteunen en niet openCL* 2.2 AMD helaas niet echt concurrentie hier tegen kan bieden *en ik bedoel de sofware zelf, nvidia zelf ondersteunt wel openCL"
RTX 3090;3;0.328914076089859;De software die ik gebruik werkt prima met openCL op nvidia (Houdini) De renderengines die ik gebruik zijn inederdaad cuda only. Maar de 3090 is juist een kaart die je als 3Der wil hebben, nvidia heeft notabene op hun eigen youtube kanaal hier promo filmpjes voor geupload over waarom het een goede kaart is voor mensen in de visual effects industrie. Beetje gemixte berichtgeving vanuit nvidia inderdaad.
RTX 3090;1;0.2798590362071991;Nvidia heefr net twee van de grootste tech YouTubers ee gigantische zak geld gegeven om een promo filmpje te maken (ja, een promo, want ze mochten geen benchmark tonen) over hoe dit ding op 8k kan gamen. Daarnaast zijn de driver optimalisaties die je in de Titan kaarten zag uitgeschakeld waardoor de kaart trager is in pro toepassingen dan de Titan rtx., dit is geen titan. Het is dus 110% correct om dit ding als een gaming gpu te reviewen, want zo zet nvidia hem daadwerkelijk in de markt.
RTX 3090;1;0.4177291691303253;Dat is hoe nvidia hem in de markt zet, ja. Maar de enige mensen die ik die een dergelijke kaart kopen (inclusief ikzelf) kopen hem voor gpu rendering en dergelijke en daar is hij prima geschikt voor, dus dergelijke benchmarks hadden niet misstaan in de review. Is het enige waar ik en vele anderen in geintresseerd zijn, die gaming performance zal me een worst wezen. Had de review net wat completer gemaakt.
RTX 3090;3;0.49723634123802185;Geen informatie over de geluidsproductie? Dit vind ik toch wel redelijk essentiële informatie. Mocht de 3090 een pak koeler en stiller zijn dan de 3080, zou dat toch een extra argument kunnen zijn om de meerprijs te betalen.
RTX 3090;2;0.47159343957901;Het heeft weinig zin om een 3080 founders edition te vergelijken met een 3090 AIB op dat vlak. Dan zou je een Gigabyte 3080 en 30980 willen. Ik denk niet dat de 3090 koeler gaat zijn, want meer verbruik, waardoor die waarschijnlijk ook niet stiller zal zijn. Dan kan je beter een custom waterkoeling set kopen met een 3080, stiller, koeler en goedkoper.
RTX 3090;2;0.40075981616973877;Maakt niet uit, zolang ze maar correct weergeven welk model er precies getest wordt geeft het toch een degelijke indicatie van wat je mag verwachten. Nu ontbreekt alle geluidsinformatie gewoon. Gelukkig zijn er reviews van Guru3D etc die dit wel vermelden, maar normaal geef ik de voorkeur aan Tweakers reviews.
RTX 3090;2;0.3475833833217621;Las in een eerdere reactie van 1 van de testers dat het een aftermarket model is en niet de FE dus daarom hebben ze het weggelaten omdat het geen eerlijke vergelijking is. Van mij had het wel meegenomen mogen worden alleen dan wel expliciet vermeld dat het geen FE is, dan is het aan de lezer om te bepalen of het nuttig is of niet...
RTX 3090;2;0.4031074643135071;Het ziet er naar uit dat Nvidia de 3080 enorm heeft gepushed qua performance om AMD voor te kunnen blijven. Daardoor is er voor een versie met verschil in performance tussen de 3080 en 3090 geen ruimte meer (TI of Super), wel voor de 3080 met meer memory. En de 3090 is gewoon de titan want boven de 3090 kan ook niets meer geplaatst worden met meer performance want de full die geeft maar 2% meer cores. Vreemde strategie. Maar bij Nvidia hebben ze er voor door geleerd denk ik ?
RTX 3090;1;0.43869486451148987;Ik had 4 preorders lopen voor de RTX3080. Inmiddels vandaag de laatste gecanceled en ik hebweer meer dan 3000 euro terug want ze waren alle 4 betaald lol, gekken werk. Ik ben er eigenlijk over heen, voor het eerst in mijn leven afgekickt van de upgrade gekte en dat cold turkey. Nu wacht ik gewoon op AMD hun announcement en met een beetje geluk zakt de prijs van de 3080 dan ook gelijk weer.
RTX 3090;2;0.3800228238105774;Jammer dat jullie onder de spellen geen Flight simulator hebben gebruikt. Wellicht voor in de toekomst?
RTX 3090;2;0.31437382102012634;Jullie oordelen allemaal de 3090 op 1 enkele manier maar bekijk het eens anders. Als ik nu een 3080 koop en de 3080 super komt uit om daarna een 3080 ti te zien verschijnen dan is men 800€ ook zuur qua smaak. Ik wil op mijn gemak zijn voor de komende 4 jaar net zoals de mensen met een 1080ti waren. Heb ik een kaart nodig die 8K aankan? Nee, absoluut niet. Wil ik iets “futureproof”? Ja. Als de 3090 met 24gb slechts 15% beter is dan een 3080 10gb dan moet je je de vraag stellen wat het verschil zal zijn tussen een 3080 20gb en de 3090... EN toch zitten er mensen te wachten om de 3080 20gb te kopen. Zo zie je dat alles relatief is.
RTX 3090;1;0.5837181210517883;Wat ik mis is op welke klokfrequenties de geteste 3090 van Gigabyte loopt. Zijn deze exact hetzelfde als de 3090 FE? Zo niet, hebben jullie de klokfrequenties dan gelijk getrokken?
RTX 3090;3;0.4348067343235016;Goed om te zien dat deze keer naast de 3900 XT op 4.2 Ghz nu ook de 10900K op 5 Ghz is getest. Zoals verwacht wint cpu snelheid het het van PCI 4.0, ook op 4K. Op 1440p zag ik zelfs 22% voorbij komen. Wel jammer dat jullie niet terug komen om de eerdere discussie waar de cpu aanpassing ter sprake kwam en waarop ontkennend werd gereageerd: Ik zie nu 5.4% cq 9.5 fps verschil bij F1 2020 op 4K met de RTX 3080 met de 3900XT vs 10900K. Maar goed, het is aangepast dus blijkbaar heeft de post alsnog effect gesorteerd. Gebrek aan waardering zien we dan maar even door de vingers . Wellicht een idee om de grafieken in de originele review even bij te werken? Gezien de populariteit van de RTX 3080 zal hij voorlopig nog wel veel bezocht worden.
RTX 3090;2;0.5071358680725098;"Ik heb er dubbele gevoelens bij, enerzijds zie ik, als ik 'm vergelijk met de 2080ti toch wel substantiële vooruitgang; puur qua specs lijkt het op papier een goeie deal. Totdat je de prijzen ziet icm de prestaties van de 3080. Dan is de 3090 ineens een matige stap voorwaarts. De 3080 is een prachtig stukje taart, de 3090 een toefje slagroom dat net zo duur is als de hele taart. maar slagroom is wel lekker.. Nvidia heeft de knuppel in het hoenderhok geslingerd en een redelijk chaotische propositie gecreëerd. Is dit de vervanger van de titan of van de 2080ti? Dat hangt bijna af van welke kaart je 'm qua prijskaartje mee vergelijkt; vergeleken met de titan rtx is het een goeie deal, vergeleken met de 2080ti is 't een redelijke deal (zolang retailers de 20xx prijzen zo hoog houden that is), en vergeleken met de 3080 is 't een achterlijk slechte deal (mits ze verkrijgbaar zijn) Ik heb (en ik ga hiervoor naar de hel, ik weet het 😅) mijn 3080 doorverkocht, en de winst daarvan in een 3090 op de zaak geïnvesteerd, en zelfs met dat bedrag en de btw er af voelt het alsof de 3090 nog te duur is. De enige reden dat ik uiteindelijk wel heb gekozen voor een 3090 is de absurde prijssituatie omtrent de 3080. Een 3090 msrp wordt een stuk interessanter als de 3080's ook richting de 1000€ beginnen te klimmen. Eigenlijk is het dus allemaal de schuld van de 3080. Te goed werk geleverd, Nvidia"
RTX 3090;1;0.3935830295085907;Zou het voor personen die überhaupt overwegen om 1500 Euro aan een grafische kaart te spenderen niet nuttig zijn om twee 3080 kaarten naast de 3090 te zetten? Als twee 3080 kaarten beter presteren dan één 3090, wat is dan de business case voor de 3090? En als deze niet beter presteert, wat is dan het nut NVLink?
RTX 3090;1;0.34262287616729736;De 3080 heeft geen SLI connector, enkel de 3090 heeft die.
RTX 3090;5;0.2643357515335083;"Mooi hoe Nvidia gamers bespeelt. De kaarten die kwa prijs-performance interessant zijn, maken ze kreupel door geen SLI te faciliteren. Precies zoals zij met de RTX 2060 hebben gedaan. Nu een meer dan dubbele prijs vragen voor een ""vlaggeschip"" dat max 20% aan frames tevoorschijn tovert en alleen díe van SLI voorzien. Heel consumentvriendelijk. Hiewel SLI niet zo denderend presteert tegenwoordig. Aan de andere kant... ze hebben nu een RTX 3080 gebracht dat veel beter presteert dan de RTX 2080 ti voor een fractie van de prijs daarvan. De aandeelhouders hebben ingeleverd. Ze houden toch wel van de consument. Sinds mijn GTX 1070 geen Nvidia GPU meer voor mij. Principieel niet."
RTX 3090;3;0.3377103805541992;Niet alle games werken toch even goed met dubbele video kaarten ?
RTX 3090;3;0.45182570815086365;De 3080 beschikt naar mijn weten niet over sli, dus dan zou je al games moeten hebben die dit specifiek ondersteunen. Een ander punt is de hoeveelheid psu power die je nodig hebt voor twee 3080's en vooral de warmteontwikkeling die daarbij gecreëerd wordt. (dan zou je al moeten wachten op kaarten van andere fabrikanten, want het nvidia fan design lijkt mij niet optimaal om twee kaarten onder elkaar te monteren)
RTX 3090;3;0.3825519382953644;Misschien nuttig om even hier naar te kijken:
RTX 3090;2;0.5326284170150757;Helaas, nog steeds geen waardige opvolger van mijn 1080 GTX Ti. Weer een generatie wachten.
RTX 3090;1;0.4160391688346863;is die echt nog steeds zo snel dat een 3080 niet interessant is ?
RTX 3090;1;0.3342917561531067;Ja een 1080 TI is nog steeds snel, maar een 3080 haalt ongeveer 2x zoveel fps. Ik kan niet ontkennen dat ik voor het eerst sinds 2017 toen ik mijn 1080 TI kocht weer de upgradekriebel voel!
RTX 3090;3;0.5106981992721558;Jawel, heb er wel een beetje last van natuurlijk. Volgende gen ben ik wel de klos vrees ik. Maar trek nu op 1080p nog makkelijk els spel open op ultra. 4k vind ik de hitte en herrie van mijn computer niet waard.
RTX 3090;3;0.448543906211853;Ahh, ja op 1080p hoef je nog niet te upgraden. Vanaf 2560x1440 wordt het wel zinvoller!
RTX 3090;3;0.41192179918289185;Yep, op 4k gaat de kaart zeker kraken. Maar ik red het zo nog wel even.
RTX 3090;2;0.41853848099708557;Gemiddeld 3x zo snel is niet voldoende ?
RTX 3090;2;0.35182252526283264;3x sneller nee.. eerder 75% bijvoorbeeld.
RTX 3090;2;0.4146125614643097;Welnee, zeker niet voor die prijs. Prijs/kwaliteit is bij mij zeker een dingetje.
RTX 3090;3;0.490892231464386;Helaas? Ben wel benieuwd wat, maar vooral waarvoor, een waardige opvolger zou zijn.
RTX 3090;4;0.4447714388370514;Wat mij opvalt is dat de RT 3090 ook vooral op full hd echt nog wel een stukje meerwaarde kan bieden. Wanneer je 144hz of hoger speelt en je kan deze hoge prijs veroorloven, dan is deze kaart ook nog een goede keus.
RTX 3090;1;0.33934661746025085;Want?
RTX 3090;2;0.29488417506217957;Veel/sommige games halen geen 144 fps op 1080p, zie ook deze review. Wil je optimaal gebruik maken van 144hz, laat staat 240hz of meer dan wil je ook de bijhorende fps halen. En veel mensen willen uiteraard op hoge instellingen spelen.
RTX 3090;3;0.42898404598236084;Games waar 3080 geen 144 haalt haalt de 3090 het meestal ok niet, verschil is immers maar een 10-15%
RTX 3090;1;0.3759138286113739;Welke game haal je geen 144hz op full hd met een rtx 3080 dan?.... Edit ik zie het staan
RTX 3090;3;0.3125324845314026;Flight Simulator?
RTX 3090;2;0.3631451725959778;Ik denk juist dat ze het met gemak halen want als die 144FPS zo belangrijk is dan set je de setting terug of tweak je die dat 144fps of 240 gehaald wordt. Dat is als je CS:go esport om hoofprijs gamed dan set je de setting terug. voor de meeste non esport gamer en slow pace games heb je geen ruk aan 240fps of zeer stabile 144. Die nPC in adventure doen niet aan bunny hoppen om je heen met SMG in full burst. Meeste esport game zijn ook van die ouwe games die vaak 144fps wel halen.
RTX 3090;1;0.3849259316921234;Ik weet nog wel dat ik een GTX 480 kocht voor 380 euro.. Waar gaat dit heen mensen. AMD DOE OOK EENS IETS
RTX 3090;3;0.3858233392238617;Sommigen onder ons vinden een rtx 3080 goedkoop omdat de 2080ti en de 3090 ver boven de 1000 zijn. Geeft wel het niveau aan hoe men tegenwoordig rekent haha.
RTX 3090;2;0.32514768838882446;Mensen denken steeds minder na over hun uitgaven blijkbaar.
RTX 3090;1;0.4495747983455658;Wat is dat gedoe met die 99p en 99.99p?
RTX 3090;3;0.296922504901886;Dat kan je vinden in de rest verantwoording. Daar staat ook deze link, waar het in detail uitgelegd is:
RTX 3090;2;0.30910491943359375;Ben ik de enige die prijs/prestatie verhouding zegt : Meh niet onder de indruk ? Ik wordt warmer van de 3080 en ik denk prijs/prestatie warmer van de 3070.
RTX 3090;3;0.3706333041191101;In het hoge segment is er in geen enkele markt een lineair verband.
RTX 3090;1;0.536123514175415;een 3070 3080 is eigenlijk niet eens extreem hoog met 4K in het achterhoofd... uhm 80 fps op 4k is echt niet denderend als je een 144hz 4k scherm hebt staan
RTX 3090;1;0.28697484731674194;Wanneer is het vlaggenschip van welke fabrikant ooit goed geweest op prijs/prestatie? Als je de beste daarvan wil moet je waarschijnlijk wachten op de 3070 zoals iedere x70 tot nu toe.
RTX 3090;2;0.45366525650024414;Ik denk dat je kamer het warmst wordt van een 3090 Qua prijs/prestaties is een 3090 onnozel: 10-20% performance is leuk, maar niet game-breaking. Er zijn geen games die je kunt spelen met een 3090 en niet met een 3080 (tenzij je natuurlijk op 8k gaat gamen). Je loopt dus ook niets mis door een 3080 te kopen. Als je op 8k goed wil gamen tegen een eerlijke prijs/prestatie verhouding, is het beter om 2-3 jaar te wachten op een RTX 4070. Die 800 euro extra geef je puur uit om die prestaties alvast 2 jaar eerder te krijgen.
RTX 3090;2;0.3260473310947418;Ik kon deze review niet meer serieus nemen toen Project Cars 3 als benchmark game werd meegenomen, dit spel heeft graphics die 10 jaar oud zijn.
RTX 3090;4;0.2791575491428375;Het zijn games die aangeleverd worden als test door nvidia zelf. Zie de yt video's van linus of mk
RTX 3090;2;0.3305947482585907;Slavy heeft opzich wel een valide punt. Niet dat de hele review onzin is, maar de game zelf ...kom op, hij heeft gewoon een punt wat betreft PC3. overingens hebben ze blijkbaar zelf de games uitgezocht hiervoor: Gameselectie Zoals je van ons gewend bent, testen we gpu’s met een selectie van tien games, die zorgvuldig is samengesteld om een zo representatief mogelijk beeld te geven. Hierbij hebben we onder meer rekening gehouden met de api, de engine, het genre, de AMD/Nvidia-verhouding, de leeftijd en de technische benchmarkdetails van elke game. Voorafgaand aan elke test wordt de videokaart opgewarmd, bijvoorbeeld door 3DMark te draaien of in het menu van de game als dat in real time gerenderde graphics bevat.
RTX 3090;1;0.34098461270332336;Ik kan hierbij bevestigen dat wij als redactie absoluut zelf de keuze maken over welke games we testen. Fabrikanten doen zo nu en dan suggesties in bijvoorbeeld reviewers guides, maar het vereisen van specifieke benchmarks zou compleet onacceptabel zijn.
RTX 3090;2;0.35546091198921204;Bijzonder dan dat er door bijna iedere reviewer wordt gekozen voor Control en Doom als 8K game. RDR2 zou een te negatief plaatje schetsen...
RTX 3090;1;0.4687637388706207;Red Dead Redemption 2 is toch ook gewoon getest? Zie pagina 13. Edit: ah, als 8K game. Dat had ook RDR2 kunnen zijn, hebben we niet specifiek de keuze voor gemaakt om weg te laten.
RTX 3090;3;0.3164215087890625;Bijzonder om te zien dat de combi met een AMD of Intel CPU redelijk inconsistent is, als je kijkt naar de 3080 en 3090 4K resultaten.
RTX 3090;1;0.5619317889213562;Heb je een voorbeeld van deze inconsistentie? Als ik naar de prestatiescore kijk zijn ze nagenoeg identiek namelijk op 4K.
RTX 3090;1;0.25121113657951355;Tomb raider. 3080 + i9 > 3090 ryzen 9 op 1080p. Op 4k wint ryzen
RTX 3090;2;0.40677016973495483;Maar dat is een verschil tussen 1080p en 4K, te verklaren door de cpu bottleneck bij de een (1080p) en de gpu bottleneck bij de ander (4K). Het leek erop alsof er werd gesuggereerd dat er inconsistentie was tussen de 4K resultaten onderling.
RTX 3090;3;0.35987183451652527;Zal zeker het geval zijn. De ene game is beter geoptimaliseerd voor Nvidia dan AMD/Intel dan AMD en vice versa. Kijk naar Forza. Zover ik weet was dat bijna altijd zeer close met Nvidia vs AMD. .
RTX 3090;2;0.5425832867622375;Je hebt inderdaad gelijk, soms is op 1080p intel beter en op 4k vervolgens amd. Andersom kan dus ook. Ik dacht eerst dat het zou gaan om cpu bottleneck, maar het is telkens stuivertje wisselen en inderdaad inconsistent.
RTX 3090;3;0.44457724690437317;Bij lagere resoluties is single core performance nog altijd king, hoe hoger de resolutie hoe minder belangrijk de cpu is, ook gaat via pcie 4 binnenkort ook de ssd rechtstreeks door de gpu gelezen worden en niet meer door de cpu en ram, ik vraag me af wat hier de impact van zal zijn. Het is raar op te zeggen maar heb je problemen dat de cpu niet meekan is het soms een goed idee om een scherm met een hogere resolutie te kopen en een nieuwe gpu om de cpu wat te ontlasten.
RTX 3090;3;0.36557915806770325;Zou dat kunnen te maken hebben met PCIe 4 vs PCIe 3 waarbij het verschil wel merkbaar wordt tijdens 4K? Heb de volledige review nog niet gelezen, maar dat is het eerste wat in me opkomt.
RTX 3090;3;0.4396130442619324;Igv 4K zullen er mipmap met 4K versie zijn en als memory niet ruim voldoende is zullen er mipmap geflushed en andere geload worden en dus wordt bij 4K de PCIebus wel wat intensiever gebruikt tov 1080p. Die extra mipmap level komt boven op de kleinere mipmap en gactor 4 groter. Dus 24GB vs 10GB compenseerd niet voledig. Dus afhankelijk game hoe intensiever de texture manager bezig is over de PCI-e bus. En dan heeft AMD CPU met PCi-e 4.0 een subtiel voordeel.
RTX 3090;1;0.38077792525291443;Ik ga een 3070 kopen en dan als de 3080 versies met meer RAM komen daar eventueel op overstappen tzt.
RTX 3090;2;0.42683568596839905;Dan zou ik gewoon een maanden of twee wachten op die 3080 met meer RAM. Kan zo lang niet meer duren als het vermoeden dat NVidia het spectrum zo veel mogelijk af wil dekken om AMD de wind uit te zeilen te nemen correct is.
RTX 3090;2;0.3952528238296509;Denk dat er toch een beste meerprijs gaat zitten op die 20gb versie van de 3080, als die komt.Ik geloof dat een jaar geleden GDDR6 op hoge snelheid voor 12$ / GB over de toonbank ging. Even de schaalfactor niet meegerekend. Als je dan bedenkt dat gddr6x waarschijnlijk duurder is, zal er minimaal 100 euro bovenop komen. Dat zal de AIB kaarten snel richting de 850 duwen voor de lower end en gezien huidige prijzen al snel minstens 900.
RTX 3090;3;0.5705546140670776;GDDRx is ECC geheugen, dus kan nog wel wat duurder zijn. Ik denk dat een 20GB 3080 eerder naar minimaal 1000 euro gaat. Al was het maar om de 3090 niet volledig uit de markt te prijzen. Maar dat is natuurlijk wel afhankelijk van waar AMD mee komt.
RTX 3090;4;0.5007677674293518;Mooie review! Wat ik graag zou willen weten is wat ik mag verwachten aan performance in Premiere Pro ten opzichte van de 3080?
RTX 3090;2;0.4114360809326172;Volgens mij zie ik bij 8K een verschil van 14-19%. En niet 10-15%. Dus dan is de conclusie dat het verschil bij hogere resoluties hetzelfde is toch niet goed?
RTX 3090;5;0.545646607875824;1699,- en 350+ watt aan power. Deze skip ik.
RTX 3090;5;0.42918938398361206;Ja, iedereen aan de ledlampen en een pc van 600 watt.
RTX 3090;1;0.5526275634765625;Kanttekening een 3080 voor 719,- ga je nergens vinden nahjah een 3090 voor 1499,- waarschijnlijk ook niet
RTX 3090;3;0.34453538060188293;De kaart ga je nergens op voorraad vinden, maar er zijn nog genoeg pre-orders voor 719. Bijvoorbeeld bij Alternate. Even wachten, maar uiteindelijk krijg je hem voor dat geld.
RTX 3090;2;0.39245283603668213;Als de Chip van de 3080 en 3090 gelijk zijn met maar alleen meer ingeschakelde transistors. Dan zou je imho het yield grof kunnen berekenen.
RTX 3090;3;0.304190456867218;Zo te zien is de RTX 3090 vooral interessant als je echt alleen maar in 4K Ultra wilt gamen met een beetje FPS, zoals verwacht. De RTX 3080 is inderdaad de 'best bang for the bugs GPU van nVidia' deze generatie tot nu toe met € 719,00. Echter voor deze prijs niet meer te vinden. Ook leuk om te zien dat je met een tweedehandse GPU van iets meer dan € 200,00 eigenlijk heel erg goed op 1080p Ultra kan gamen en dat zal ook nog wel even zo blijven. Ook eigenlijk wel grappig we zaten als enthousiasts zo'n 10 jaar op 1080p (zeg 2008 -2018), voordat complexe games echt goed vloeiend en snel (1080p 120fps Ultra) waren op deze resolutie. Ik ben benieuwd hoe lang we er over doen voor de stap naar 4K 120fps Ultra.
RTX 3090;5;0.7033088207244873;Best bang for the bugs, I see what you did there
RTX 3090;2;0.5232245326042175;Veel ultra settings doen niet veel en zijn amper te onderscheiden van high settings, dus 4K 120fps is nu te doen met 98% vd games met een 3080..hoef je niet te wachten.
RTX 3090;2;0.3257940709590912;Klopt het dat er ook een upgrade benodigd is voor een moederboard van een/twee jaar oud. mn boardje heeft pci 3.0 en 3080 heeft pci 4.0.... dus prestaties zullen niet maximaal zijn vermoed ik dan?!
RTX 3090;3;0.42224758863449097;Zowel je processor als moederbord hebben een upgrade voor PCI-E 4.0 nodig. Deze dienen beide PCI-E 4.0 compliant te zijn. Dit hoeft echter niiet perse hoeft met deze kaarten, gezien het performance verschil meestal tussen de 1 en 2 procent is.
RTX 3090;2;0.3670150637626648;Als je 146 fps tegen 148 fps niet maximaal vindt, dan ja.
RTX 3090;2;0.47726863622665405;thnx daar was ik naar opzoek. die 2 frames zullen me niet helpen eerder gekilled te worden in een FPS niet te verwarren met fps
RTX 3090;2;0.3956906199455261;"Persoonlijk twijfel ik of de keuze voor de 8K-titels in dit geval helemaal terecht zijn. Op basis van de cijfers in deze review, zou je kunnen aannemen dat 8K zeker een 'haalbare' optie is (ondanks dat je niet ten allen tijden aan de minimale 60FPS zit). De reden van twijfel die ik nu heb komt doordat ik de video review van Steve van 'Gamers Nexus' heb gezien voordat ik aan de Tweakers review ben begonnen. Hij schetst nogal (understatement) een ander beeld van het ""gamen"" op 8K resolutie en laat daar geen gras over groeien. Op dit moment staan beide reviews lijnrecht tegenover elkaar. Gezien het feit dat 4k gamen pas net 'ingeburgerd' is dus ik vind de statement van NVidia om de 3090 al zo in de spotlight te zetten als 'de' 8K niet heel slim. Naar mijn inziens had het 8K dan ook kritischer in de spotlight gezet mogen worden, rekenhoudend dat 8K 4*4K is."
RTX 3090;1;0.4280274510383606;Ik zou zelf gewoon voor de RTX 3080 kiezen. De RTX 3090 is gewoon overdreven geprijsd.
RTX 3090;2;0.3612872064113617;Als er geen gebruik wordt gemaakt van de extra bandbreedte op de pci Express. Zou dat straks nog uit kunnen maken als gegevens straks direct naar de gpu kunnen worden gestuurd?
RTX 3090;1;0.6699468493461609;Gaaaaap! Geen overclock! Weer niet. Wat is dat toch.....
RTX 3090;2;0.4694225788116455;Mooie test, ik mis echter de 8K benchmarks! Er staan er 3 gedaan, terwijl dit een 8K videokaart zou zijn, en meer van belang, waarom staan er op 8K niet beide cpu's op? Daar ben ik erg naar benieuwd, of AMD dan verder uitloopt, waar ze in 4K al een kleine voorsprong hebben! En verder is de kaart uiteraard veel te duur in verhouding. Ook ben ik blij dat ik mijn 2080ti niet heb verkocht, want dan zou ik nu iets anders moeten kopen, omdat Nvidia gewoon een paper-lauch heeft gehad...want er is gewoon geen voorraad! De RTX3080 is een hele mooie kaart, vrees echter dat de 20GB en de super of ti modellen ook weer extra duur zullen worden. Verder is ray-tracing wel tegenvallend, ze zijn niet echt sneller geworden dan de 1e gen. Hopelijk kan AMD straks hierdoor heel fijn aansluiten! En dat zie ik wel gebeuren, verwacht dat de 6900 sneller dan de 3080 en wellicht zelfs de 3090 zijn... Mooie tijden komen eraan. Deze kaart is voor mij iig niet gemaakt..ruim 2x de prijs en 14% meer? Ik heb de 2080ti overigens niet nieuw gekocht...dat is mij te duur...
RTX 3090;3;0.3754146695137024;But can it run Crysis?
RTX 3090;4;0.3722054958343506;Flight Simulator
RTX 3090;3;0.41526052355766296;Zeg ik iets doms als ik denk dat 2x 3080 goedkoper en sneller is dan de 3090?
RTX 3090;5;0.3821621835231781;De 3080 ondersteund geen SLI.
RTX 3090;3;0.2922286093235016;Enkel 3090 is sli
RTX 3090;2;0.41722485423088074;dat is dan wel weer vervelend slim van Nvidia lol
RTX 3090;3;0.2616589367389679;ik wacht op de 3080 (ti) ofterwel de 20gb versie..
RTX 3090;2;0.4277192950248718;Disclaimer: I probeer niet te zeggen dat een 3090 een goede koop is voor een gemiddelde gamer. Maar deze stukken van de conclusie zijn imo wat raar: Wat maakt dat uit? Een conclusie baseren op een vergelijking met kaart waarvan je de prijs en releasedate nog niet eens weet is nogal vreemd, op zijn zachts gezegd.
RTX 3090;1;0.5320867896080017;Dit is hoe het eigenlijk zou moeten zijn anno nu. 3090 600€ Radeon 5800 XT 440€ 3080 480€ Radeon 5700 XT 220€ 3070 320€ Maar nee, de 3080 wordt nog gekocht voor meer dan 2k..
RTX 3090;2;0.4572494924068451;Nu blijkt de 3090 dus minimaal meer te bieden voor een veel hogere prijs. Zie ik hier meer kans voor AMD ? Laten we zeggen een kaart van 600.- die bijna vergelijkbaar is met een 3080 van ruim 700.- En vervolgens nog een ''budget'' kaart die heel veel waar voor het geld biedt. Ik zie het best rooskleurig in voor AMD. Als ze het probleem met de drivers daadwerkelijk hebben opgelost natuurlijk.
RTX 3090;2;0.4554143249988556;Jammer dat er bij dit soort grafische nooit gekeken wordt naar naar de prestaties in 3d ontwerp programma's zoals 3ds Max. Dat lijkt mij juist een markt segment waar 15% meer produktiviteit doorslaggevend kan zijn ondanks de prijs.
RTX 3090;1;0.4540347456932068;Welk 8k scherm hebben jullie staan?
RTX 3090;5;0.6143070459365845;"Mooie kaart voor mijn ets2 pc met 4 43"" monitoren :-)"
RTX 3090;1;0.4417462646961212;"Als we de prestatiescore van twee pagina's terug afzetten tegen het gemeten stroomverbruik onder belasting, komen we tot het aantal frames per seconde per watt. Leuk he statistiek Zou je zomaar aan jezelf en anderen kunnen verantwoorden waarom je belachelijk veel energie slurpt zonder je schuldig te voelen. En de marketing machine maakt maar al te graag gebruik van dit soort tabelletjes. Maar een vooruitgang is het natuurlijk niet. Dat zou het pas voor mij pas zijn als je met hetzelfde of minder energie verbruik dit soort prestaties neer zou zetten. Dit is een Jeremy Clarkson kaart: ""POOWEEEER!"""
RTX 3090;1;0.5193155407905579;Founders Edition: not found / tweakers is gewoon te klein, want youtube staat vol met grote spelers die een nvidia rtx3090 hebben gekregen.LTT
RTX 3090;3;0.2867306172847748;Zouden jullie in vervolg ook 3440x1440 kunnen meenemen? Ik zie steeds meer ultrawide schermen langs komen van diverse fabrikanten. En heb zo het vermoeden dat deze schermverhouding toch echt populariteit aan het winnen is. Dit is bij jullie of de buren wel eens getest maar misschien 1x extra testen voor het spel wat het meest de gemiddelde waarde benaderd? Om een beetje meer gevoel er bij te krijgen.
RTX 3090;4;0.28524285554885864;Maar ik ga toch voor deze kaart..... nieuws: Diamond Viper V550 review Very, VERY impressive! volgens Beyond Computing.
RTX 3090;4;0.3514341413974762;LOL, goud But being able to play HalfLife (and I really mean playable, not slow motion crap) in 1280x960 for a card of this price just cannot be beat!
RTX 3090;2;0.43139466643333435;Prijzen vanaf ... klopt niet echt. In bij SI Computers woensdag mijn Asus ROG STRIX RTX3090 OC Gaming besteld. Staat die aan net iets boven de 1600
RTX 3090;1;0.6839365363121033;NIEUWS! Nvidia haalt introductie videos van youtube van de 3000 series Een dikke flop dus die 3000 problemen
RTX 3090;2;0.42711663246154785;Er zijn alweer nieuwere versies aangekondigd die Ti erachter krijgen. Net als 2080. Uit youtube heb ik nog wel groot kritiek binnen over 3090. De marketing was namelijk nogal overdreven door te zeggen dat RTX3090 presentatie verdubbeld is tegenover RTX3080. Is dat reden waarom ze de prijs verdubbelen? Uit de game tests blijkt dat het uiteindelijk 10-20% sneller doet in veel games. Dus je betaalt dubbel zoveel voor 20% meer FPS? Dan is het eerder oplichterij: dubbel betalen betekent voor velen dat het 2 keer beter moet zijn. Dat is helaas niet zo. Linus merkte op dat het géén Titan is. Want speciale features voor Titan zijn niet ingeschakeld, of de drivers hebben dat niet aangezet. Linus zag dat andere kaart Titan RTX betere resultaten hebben bij sommige programma's. Waarom heeft twee keer zo dure kaart zelf niks in huis? Om deze reden is RTX3090 door dubbele prijs veel te duur geworden in vergelijking met RTX3080. En hoeveel mensen gaan gelijk 8K gamen? Dat zullen alleen mensen zijn met extreen dikke geldbuidel en mensen die verslaafd zijn om zo snel mogelijk beste in huis te halen, geld doet hen helemaal niks. De grootste deel zal niet voor RTX3090 gaan omdat het slechte prijs/performance verhouding kent. Nog meer mensen zijn wel tevreden met RTX3070. En we hebben ook straks Ti uitvoeringen, dus daar wachten velen nog even mee welke het wordt met scherpe prijs/performance verhouding. En heel belangrijk: je kan geen 120 Hz 8K mee halen. Dat kan RTX3090 niet eens in veel games. Bij 4K is dat mogelijk. Dan komt 360Hz gaming wel beetje in de buurt. Maar de winst voor 360Hz is zo klein tegenover 240Hz. En daar word je niet echt goed van door veel te veel geld neer te leggen voor paar procenten winstkans. Nvidia mag prima lekker stoer zijn met RTX3090 en geeft dat aan grote computer reviewers on Youtube, het zal de verkoop niet sneller maken aangezien velen toch voor beter geprijsde RTX3080 gaan. Niet iedereen zit aan 4K gaming.
RTX 3090;3;0.35592982172966003;"Vandaag de dag zouden er mijn inziens wel vaker VR games meegenomen mogen worden in dit soort reviews. VR is zeer GPU afhankelijk en draagt erg bij aan het comfort en beleving van een VR game. Dus vooral met deze nieuwe generatie videokaarten is juist VR een erg interessant verhaal. Bij ""gewone"" monitor-games gaat het voornamelijk over op hoeveel FPS een game draait en of het dan op 140 of 150 fps draait..."
RTX 3090;5;0.5993765592575073;Dank voor de review! Resume: weer een hoop gebakken lucht. Kan niet wachten op de AMD kaarten. Die gaan lekker tegen de prestaties van 3080 zitten tegen een lagere prijs.
RTX 3090;3;0.4482479393482208;"Nou ja, de top van de top heeft zich natuurlijk altijd wel gekenmerkt door een volledig scheve prijs/kwaliteitsverhouding, alleen was daar het verschil in performance vaak wel iets groter (bijvoorbeeld 30%). Maar uiteindelijk zullen waarschijnlijk een 3060 en 3070 de meest verkochte kaarten gaan worden en die zullen wel weer een betere prijs/prestatieverhouding hebben dan de 3080. Wat de precieze redenatie is weten we natuurlijk niet, maar feit is wel dat de kans dat een chip volledig functioneel en zouder fout is relatief laag is. Is er een beetje kapot, dan kan het voor een 3080 door gaan, is er nog meer kapot, dan kan het voor een 3070 door gaat. Dat maakt het volgens mij sowieso wel dat de snellere kaarten duurder zijn. En belangrijker; ze kunnen het blijkbaar maken om een kaart zo duur te maken voor relatief weinig performance winst voor net die mensen die het er voor over hebben. Maar gebakken lucht? Nou dat zou ik deze monsterkaart echt niet noemen. Te duur voor mij? Ja zeker. Gaat AMD daar iets tegenover zetten? Ik betwijfel het ten zeerste."
RTX 3090;3;0.31250661611557007;Gebakken lucht niet , het zijn mooie step up tov vorige gen Turing. Monster kaart nou nee het is de 2de chip en kleiner kwa die size dan voorganger. De monster chip is GA100 Titan waren de mindere gekortwiekte ??100 variant van de grootste chips. Nv en AMD zullen voor de pro markt HBM gebruiken omdat deze pro lasten beter ondersteund. Voor gamer markt met lagere ASP is HBM een te grote gok. Mogelijk de reden om voor GDDR6x semi pro/top high-end gamer kaart te gaan. Zodat €800 en €1500 mogelijk is met de 2de chip ..102 ipv Titan van €2500,- met ..100 Denk ook dat GA100 te groot is om rendable te zijn voor gamers markt plus TDP is insane 400Watts
RTX 3090;5;0.39375829696655273;Vertel DDR, blijkbaar weet jij iets wat niemand anders weet!
RTX 3090;5;0.2887391746044159;Terugkerend fenomeen. Cijfers Nvidia = berg zout Cijfers AMD = 100% waar. Zelfde zie je in kringen met veel Xbox fans. Nu Microsoft deze aankopen heeft gedaan kan Sony wel opdoeken. Je ziet het ook terug met drivers, deze cijfers kunnen nog veranderen maar dit lijkt bij velen alleen bij merk X te kunnen en niet merk Y. Ik vraag me echt af waarom mensen zo hard iets roepen voor dat het onafhankelijk getest kan worden
RTX 3090;2;0.3847437798976898;"We weten pas of nV sterk staat als de big navi er is. Nu weet je de performce van nV maar dat zegt meer iet over tov van vorige gen dus Turing. T.o.v. AMD is er nog niks bekend. We weten pas hoe goed of slecht nV het gedaan heeft als er AMD big navi gebenched is. Ik zie meer nV aanhangers hyped. Er is gezegde. ""De huid van de beer verkopen voordat het afgeschoten is."" iig is nVidia nextgen een degelijke vooruitgang zonder wat AMD brengt weten we niet of het genoeg is of dat de concurrent iets minder bied voor minder bucks maar ook TDP. Ik heb Vega 56 en geen haast wacht fijn tot volgend jaar."
RTX 3090;4;0.35751259326934814;Ik betaal graag voor een kaart die leverbaar is
RTX 3090;1;0.29302066564559937;Want dat is bij AMD de laatste paar launches het geval? Vega was ook nog niet zo heel lang geleden.
RTX 3090;2;0.2925848066806793;AMD is door de betere omzet en winsten nu op alle afdelingen aan opschalen en Radeon divisie is op punt beland voor high-end GPU release na de krimp van de AMD CPU dark ages. Dus als er beperkingen waren in driver divisie zal die ondertussen ook tot oude ATI opgeschaald weer zijn. Denk ook dat ivm nieuwe architectuur de driver team flink aan de slag is gegaan omdat het anders is dan GCN en iig RDNA2 ook voor consoles.
RTX 3080 Ti;1;0.6176289319992065;"Ik zie hem nog niet te koop op nvidia.nl. Je kunt wel je e-mailadres invullen om een mailtje te krijgen wanneer hij te koop is. Bij de introductie van de 3070 heb ik dit gedaan maar toen ik een mail kreeg dat hij beschikbaar was kon ik hem alsnog niet bestellen vanwege 0 voorraad. Uiteindelijk heb ik er maar een op eBay gekocht uit Engeland omdat de founders edition kaarten nooit lijken te zijn uitgebracht in Nederland. Nog steeds niet trouwens, ook niet voor de andere modellen. Het zou beter zijn wanneer Nvidia de founders edition kaarten via haar board partners zou verkopen, net als bij de GTX 1080 serie. (Ik heb lang met tevredenheid een ""MSI Geforce GTX 1080 founders edition"" gehad, wat gewoon een referentiekaart is uit de nvidia fabriek."
RTX 3080 Ti;1;0.5260547399520874;Een 3080 op voorraad wordt nu verkocht voor 2000 euro. Ik wilde morgen nog wel eens een poging gaan wagen voor wat vrienden, maar geen enkele webshop gaat een aftermarket versie voor minder verkopen dan de prijs waarvoor ze nu een 3080 verkopen. In die zin gaat het eerste getal van de prijs gewoon een 2 zijn en het totale bedrag (voor de komma) uit 4 cijfers bestaan. Daar is gewoon geen enkele twijfel over. NVIDIA GeForce RTX 3080 Ti Founders Edition Review Palit GeForce RTX 3080 Ti GamingPro Review EVGA GeForce RTX 3080 Ti FTW3 Ultra Review Zotac GeForce RTX 3080 Ti AMP HoloBlack Review ASUS GeForce RTX 3080 Ti STRIX LC Liquid Cooled Review MSI GeForce RTX 3080 Ti Suprim X Review
RTX 3080 Ti;1;0.7365167737007141;Check de evga webshop en schrijf je daar in. Heb daar nu al meerdere kaarten voor anderen vandaan geplukt en duurde gemiddeld zo'n 4 weken.
RTX 3080 Ti;3;0.3784542381763458;Voor wat voor prijzen dan in vergelijking met de adviesprijs?
RTX 3080 Ti;3;0.3858630061149597;Best normale prijzen naar mijn mening:
RTX 3080 Ti;1;0.531687319278717;Geen idee hoe je dat hebt gedaan? Sta bij evga ingeschreven voor de 3060TI, 3070 en 3080. Nu al 7 Maanden, op fora precies het zelfde verhaal.
RTX 3080 Ti;3;0.344919890165329;Welke varianten. De 3060TI schint het lastigste verkrijgtbaar te zijn. Maar heb er een 3070 XC3 en een 3080FTW weten te scoren. Eind 2020/begin 2021.
RTX 3080 Ti;1;0.3288072943687439;Deze heb ik mijn lijst staan voor ca 6 maanden. EVGA GeForce RTX 3080 XC3 BLACK GAMING, 10G-P5-3881-KR EVGA GeForce RTX 3080 XC3 GAMING, 10G-P5-3883-KR, 10GB GDDR6X, iCX3 Cooling, ARGB LED, Metal Backplate EVGA GeForce RTX 3070 XC3 BLACK GAMING, 08G-P5-3751-KR, 8GB GDDR6, iCX3 Cooling, ARGB LED EVGA GeForce RTX 3060 Ti FTW3 ULTRA GAMING, 08G-P5-3667-KR, 8GB GDDR6, iCX3 Cooling, ARGB LED, Metal Backplate EVGA GeForce RTX 3060 Ti XC GAMING, 08G-P5-3663-KR, 8GB GDDR6, Dual-Fan, Metal Backplate
RTX 3080 Ti;1;0.626915454864502;Raar, want die 3070rtx xc3 heb ik begin december bij evga weten te scoren. 2 maanden later een 3080.
RTX 3080 Ti;1;0.706207275390625;Ik hetzelfde als jknl op de 3080 alle varianten. Geen mail ervan ontvangen. Eind november geregistreerd ervoor.
RTX 3080 Ti;1;0.4532245993614197;Ik heb hetzelfde voor een 3080xc3 sinds 20dec ingeschreven nog nooit wat gehoord..
RTX 3080 Ti;1;0.6101921200752258;ik ben er sinds 22 september ingeschreven voor alle 3080 kaarten. Geen notifications nog...
RTX 3080 Ti;1;0.4312674403190613;Komt nog 21% btw overheen he
RTX 3080 Ti;3;0.42497923970222473;Correct, Dit zal in het winkelmandje worden verrekend en word bepaald op basis van waar na welk land het gestuurd word. Evga heeft trouwens geen voorraad in de eu alle bestellingen worden verzonden uit Taiwan.
RTX 3080 Ti;1;0.38104745745658875;worden er dan niet ook nog allerlei importkosten bijgedaan? Of valt dat mee?
RTX 3080 Ti;3;0.44777920842170715;Bedankt voor de tip. De 'notificeer me' wanneer ik in de wachtrij aan de beurt bent is het meest eerlijke. Alleen vind ik de tijdvak van 8 uur om de bestelling te plaatsen aan de krappe kant. Stel je gaat slapen en wordt de volgende ochtend wakker, dan heb je dikke pech als je tijdens je slaap de bestelling had 'moeten' plaatsen.
RTX 3080 Ti;1;0.5290889143943787;Evga heeft aan gegeven dat ze alleen tijdens lokale kantooruren uitnodigingen sturen. Dus je mag er van uitgaan dat je na 7 uur avonds geen bericht krijgt. Volgens mij gaat het hier om Evga Duitsland van wie de kantooruren word aangehouden
RTX 3080 Ti;4;0.4527176320552826;Dat wist ik nog niet, maar fijn om te lezen! Dan is dat een fijne service van EVGA. Bedankt voor de opheldering Nu nog wachten (waarschijnlijk heeeeel lang)
RTX 3080 Ti;1;0.3688580393791199;Ik ben EVGA Elite member (net mijn 1080TI geregistreerd) maar is het de bedoeling om tot de exacte release tijd `June 3 at 6AM PDT / 3PM CEST` woensdag 15:00 te wachten en dan naar de product pagina gaan, product kiezen en daar auto-notify te kiezen?
RTX 3080 Ti;1;0.34504830837249756;Enig idee wanneer ze daar online komen? Is dat ook altijd op de officiele release datum?
RTX 3080 Ti;2;0.37352612614631653;Geen vast schema bij evga helaas
RTX 3080 Ti;4;0.2749732434749603;Deze site laat zien waar ze zijn met leveren. Bijvoorbeeld de 3080 xc3 ultra gaming is nogsteeds op launchdate om 11:04 met leveren..
RTX 3080 Ti;1;0.4029633402824402;Daar klopt dan weinig van, aangezien er een 3080 XC3 in de tuissentijd bij mij op de deurmat is beland .
RTX 3080 Ti;5;0.5427516102790833;Kun je gelijk een nieuwe voeding kopen, de FTW3 is een grootverbruiker. ook 3080 en 3090. Ik ga voor 10fps geen Ti kopen, maar hou lekker mijn RTX3080
RTX 3080 Ti;5;0.5370972752571106;Mooi undervoltje doen en je zit rond de 240watt verbruik naar mijn ervaring.
RTX 3080 Ti;1;0.5301468968391418;Doen trouwens meer fabrikanten dit. Verkopen op hun eigen site voor redelijke normale prijzen? Waarom kun je niet bij elke kaart een notificatie doen?
RTX 3080 Ti;1;0.41772323846817017;Voor zover ik weet niet, in ieder geval niet met een directe waiting list.
RTX 3080 Ti;1;0.6747859716415405;tja daar gaan we dan, marge is niet vies... en kom idd niet meer op evga site :-D Amazon 1 op voorraad voor 2k...
RTX 3080 Ti;5;0.7047581672668457;"Dit was dan ook wel een van de meest veilige ""bets"" die ik ooit heb gemaakt ."
RTX 3080 Ti;1;0.4504678249359131;Heb je mijn V&A historie wel gezien? Je zegt dit denk ik tegen de foute persoon.
RTX 3080 Ti;1;0.6596512794494629;Dat hoeft toch helemaal niet? Ik sta ook nog op lijsten voor een PS5s, terwijl ik er al 1 heb. Die gaan gewoon fee-loos naar vrienden. Volgens mij geen uitzonderlijke situatie.
RTX 3080 Ti;3;0.32267889380455017;Is er ook een test op miningprestaties (om bijvoorbeeld na te gaan of er daadwerkelijk een cap is)? Ik zou als Tweakers toch wel druk voelen om na te gaan of de prestaties (hash rates) tov een eerder uitgekomen RTX3080 minder zijn. EDIT: Dank Trygve!
RTX 3080 Ti;1;0.35295307636260986;Op pagina 1 staat het genoemd:
RTX 3080 Ti;1;0.2758069336414337;Goed artikel, wat ik nu nog niet begrijp is waarom de Nvidia kaarten niet voor MSRP verkocht moeten worden? De nieuwe XBOX en PS5 zijn ook niet te krijgen, maar de beperkte exemplaren die soms toch binnenkomen bij normale retailers worden gewoon voor MSRP verkocht, geen euro erboven. Waarom kan Microsoft en Sony wel een max prijs afdwingen, en Nvidia niet? De volledige keten maakt nu exorbitante winst op de verkoop van deze kaarten. Vraag en aanbod is natuurlijk prima, maar niet als het door bulk opkopers (Miners) en scalpers gebruikt wordt om enorm aan te verdienen.
RTX 3080 Ti;1;0.2947467863559723;Heel simpel, Nvidia heeft zijn eigen kaarten (FE), andere fabrikanten hebben hun eigen kaarten met reskins en mogen daarom ook zelf bepalen wat zij ervoor vragen. Bij sony of microsoft heb je een standaard Xbox en PS5 en geen andere fabrikanten die eigen Ps5/Xbox maken met reskins.
RTX 3080 Ti;1;0.3637164533138275;Toch raar dat je bij Nvidia zelf überhaupt geen kaarten kunt bestellen. Dat kan bij AMD wel (als je geluk hebt dan).
RTX 3080 Ti;1;0.36686789989471436;Dat kan wel, maar NL heeft geen eigen distributiepunt. Voor Nvidia valt Nederland onder Duitsland, en Belgie onder Frankrijk. Dat betekent op het goede moment bestellen bij respectievelijk Notebooksbilliger.de en LDLC.com, met een zelfde opzet als AMD.com, maar dan zonder strak schema (dus : zonder reservering of voorverkoop). Dat gezegd hebbende : er is goede hoop dat morgenmiddag via die kanalen de eerste Ti's aangeboden worden - dus kwestie van de bekende Discords in de gaten houden.
RTX 3080 Ti;1;0.47967851161956787;MSRP is Manufacturer's Suggested Retail Price. Die suggested staat er niet zomaar in en heeft veel legal complicaties. Veelal draait dit om het feit dat prijsafspraken maken verboden is. Jij mag als manufactorer uiteraard je verkoopsprijs bepalen maar wat iedereen anders er mee doet is enkel en alleen aan de markt. Ik denk dat de vraag eerder omgekeerd moet zijn. Hoe komt het dat MS en Sony er wel in slagen om wel hun msrp aan te houden? EDIT: misschien wel even erbij zetten dat de MSRP meestal gebruikt wordt om de prijs te verhogen. Ik verkoop iets aan de distributeur voor €100, de msrp is €150 om je wat marge te geven. Ik mag als manufactorer niet zeggen dat de minimumprijs 150 is want dat zijn illegale prijsafspraken. De distributeur is dus vrij om je dan toch korting te geven op de msrp. In dit geval is het omgekeerde van toepassing en dat is eerder uitzonderlijk.
RTX 3080 Ti;1;0.4189642667770386;Omdat er maar 1 Microsoft is and maar 1 Sony. Met videokaarten heb je al gauw een paar dozijn verschillende fabrikanten die hun eigen versie van de FE maken.
RTX 3080 Ti;2;0.4501704275608063;"Ik ken 't als Maximum Suggested Retail Price; het was (ooit) wel degelijk de bedoeling dat 't daar ophield, dacht ik.. (Blijft natuurlijk suggested.) Ten aanzien van de vraag van @michielonline: vooral de strategie van de fabrikant denk ik, gecombineerd met eigen consumentenverkoop. Nvidia lijkt het prima te vinden en z'n eigen prijzen naar de keten toe ook fors verhoogd het hebben. Microsoft en Sony doen dat niet én verkopen zelf voor de adviesprijs aan consumenten. Dan sta je er als winkel gelijk raar op als je hoger gaat zitten. Dat verschil in strategie komt, denk ik, omdat die laatste twee toch al nooit de bedoeling hadden aan die hardware te verdienen, maar aan de games / het ecosysteem. Nvidia moet 't daarentegen gewoon van de verkoop van de hardware zelf hebben."
RTX 3080 Ti;3;0.43109509348869324;Tegenwoordig is het eerder minimum suggested retail price
RTX 3080 Ti;1;0.4548601806163788;Ik zag vorige week een video over mining en als je het zo stelt als in dit filmpje dan zou mining verboden moeten worden. Wereldwijd verbruikt men 129 terawatt energie per uur. Google verbruikt wereldwijd 12,4 terawatt per uur met zijn datacenters. De carbon emissions zijn door mining qua vervuiling even veel als geheel Argentinië. (Vervuilende kolencentrales).
RTX 3080 Ti;1;0.6088685393333435;Daarom is Tesla gestopt met het accepteren van crypto's. Al zou je eventueel wel je zonnepanelen kunnen gebruiken om het stroomverbruik te minimaliseren. Dus overdag minen en na zonsondergang weer stoppen.
RTX 3080 Ti;2;0.4248155355453491;Maar een maand ervoor was de situatie niet anders en toen begonnen ze met crypto. Meer een pump-and-dump verhaal.
RTX 3080 Ti;2;0.5240067839622498;Idd crypto is enorm millieuvervuilend , dat is ook zo ontworpen in veel gevallen (zie bitcoin bijvoorbeeld) en dus niet iets eenvoudig aan te passen.
RTX 3080 Ti;1;0.8145119547843933;Geen.Terawatt.per.uur! Doet zó veel pijn aan mijn ogen!
RTX 3080 Ti;1;0.5381525754928589;"We need Coal, unfortunately all industries use Coal and if you think we go green and that is so cool, and we rescue the planet, all this green ""invention"" need a coal unfortunately .. Tesla for production cars and charging vehicle, wind farms short lasting also using Coal (20/30y everything to maintain and fix after small timelap) , solar panels made from big amount of Coal & Quartz ! Burn in 1800C, so industrialization try to keep what they do with founding ""eco fellas"" so they can think planet will be change, and leave big weals in peace. Bio bricket do deforestation because they have not enough of resource ... Pity when everyone trying to do the cash on anything they can.. and selling empty wishes.. something to consider"
RTX 3080 Ti;4;0.2583271563053131;Wat jij zegt kan voor die landen kloppen. Dat zijn echter wel landen waar de lokale munt het niet heel goed doet en waar de lokale bevolking alternatieven zoekt. Dat ze daar geen groene stroom hebben is hel jammer, zeker gezien de zonuren daar. Toch wil ik je op deze link wijzen: En op deze link: Maak daarvan wat je wilt. Ik heb zelf Bitcoin als afdekkingsinstrument tegen de inflatie door het eindeloze printen van de FED en de ECB en geloof heilig in cryptocurrencies en niet in alle waardemiddelen die uit de lucht geprint kunnen worden.
RTX 3080 Ti;1;0.5525614023208618;"het misleidende verhaal vdat het bitcoin energie verbruik best meevalt is mij wel bekend, let er op dat ze je bewust proberen te misleiden door er niet bij te vertellen dat : a ) de huidige financiele sector heel wat meer doet dan betalingen, denk ook aan je hypotheek, verzekeringen, pensioen,... e. d. b ) dat het stroomverbruik van crypto exponentieel stijgt. ik zou zeggen verdiep je in deze materie en help vervolgens mee de leugens hierover te bestrijden. "" ik heb zelf bitcoin als afdekkingsinstrument tegen de inflatie door het eindeloze printen van de fed en de ecb en geloof heilig in cryptocurrencies en niet in alle waardemiddelen die uit de lucht geprint kunnen worden. "" grappig : juist crypto ' s kunnen eindeloos gemaakt worden. 21 miljoen bitcoin zul je zeggen, jazeker, maar er zijn ook nog andere coins en als de plannen doorgaan komt er volgend jaar bitcoin 2. let op : je moet het natuurlijk niet in een matras stoppen, dan is het nog minder waardevast dan schelpen. geld moet je voor je laten werken! superieur : aandelen : op lange termijn het meest rendabel, inflatie + gemiddelde winstmarge van een bedrijf. als je een vriendelijke overgrootouder had die voor jou in 1950 $ 1. 000 in goud en $ 1. 000 in aandelen had geinvesteerd, dan was de waarde daarvan begin dit jaar : goud : 46. 708 aandelen : 2. 272. 561 mocht je je kennis willen verdiepen dan kan ik je de volgende website van harte aanraden : ( al was het maar om beter te begrijpen wat je pensioenfonds aan het doen / laten is )"
RTX 3080 Ti;5;0.2450265735387802;Optie 4 en 5: huizen resp land
RTX 3080 Ti;3;0.2851400673389435;Hoe Zit het dan met het stroomverbruik, is die ook gehalveerd of is dat onder full load?
RTX 3080 Ti;2;0.43244946002960205;Op zich is het natuurlijk wel mooi dat deze kaart op 4K Ultra zo'n 11 a 12% sneller is als de gewone 3080, en op zich zou dat ergens nog wel prijstechnisch uit te leggen gezien de huidige markt zijn als deze kaart inderdaad voor 1200 euro te koop is. Ik heb echter al prijzen van 2499 of 2599 voorbij zien komen bij Nederlandse webshops, dus dan gaat dat al helemaal niet meer op. Als je dit vergelijkt met de 2000 euro die je nu neerlegt voor een 3080 is het procentueel gezien allemaal nog niet zo heel gek. Ik zou me ergens als potentiele koper toch ook wel zorgen maken over het stroomverbruik, de hitte en de koeler op de 3080Ti. Je hebt namelijk een 3090 met de helft aan ram in het jasje van een 3080. Ik zie reviews voorbijkomen waar de kaart rustig op 77C voor core en 89C voor geheugen komt, en de junction temperatuur op 105C ligt.
RTX 3080 Ti;1;0.355231374502182;De msrp van een founders edition 3080 zou 719 euro zijn. Hoe zie jij die rechtvaardiging voor de 10% verschil in performance, prijstechnisch?
RTX 3080 Ti;1;0.4338432252407074;Dat kan ook komen doordat de Founders Edition (elke $1200) zou moeten kosten in Nederland niet wordt verkocht. Opmerking was op basis van: Het lijkt erop alsof nvidia Nederlandse kopers wel wil informeren wanneer de kaart te bestellen gaat zijn. Ik ben benieuwd of er ook daadwerkelijk kaarten leverbaar gaan zijn. Handelaren en andere fabrikanten kunnen uiteraard ook andere prijzen vragen.
RTX 3080 Ti;2;0.501027524471283;Nu niet idd. Ik wacht geduldig want deze kaart is ongeveer mijn rtx2080 * 2 qua prestatie. Dat zou een flinke upgrade zijn en ik wacht geduldig totdat mijn systeem het begeeft of te oud wordt. Ik kan nog prima gamen met mijn oude rtx2080. Godzijdank heb ik een 1440p G-Sync scherm en dan is het niet zo'n straf als je in CP77 maar 40-50 fps doet (high settings met RT en DLSS quality). Tzt koop ik een geheel nieuw systeem want dan zijn ze goedkoper als los kopen. Bij zo'n kaart wil je ook meteen een goede CPU en snel RAM geheugen. Ook zul je een goede voeding willen want die kaart zal niet bepaald zuinig zijn. In hoogzomer geen nadeel want de PC staat recht onder de airconditioning. Warmer dan 22 graden wordt het binnen niet. Doet dat er wat toe om dit te lezen? Zeker. Ik benadruk dus dat het niet stil ligt in GPU land. Toen ik mijn rtx2080 kocht was dat een grote wauw. Nu krap 2 jaar verder zijn de prestaties verdubbeld! Normaal als ik een upgrade deed ging het om 30-40% boost.
RTX 3080 Ti;2;0.48199447989463806;Voor mensen die op dit moment een gpu zoeken mis ik in de prestatie vergelijking de RX6800. Deze lijkt vanuit de AMD kant de beste prijs/prestatie te hebben en waar ook regelmatig voorraad van is. Ik heb het idee dat de RX6800XT helemaal niet te krijgen is en de RX6900XT & RX6700XT een slechte prijs/prestatie hebben. Ik kan me voorstellen dat deze niet meegenomen is omdat er eerder over geschreven is, maar ik mis 'm toch.
RTX 3080 Ti;3;0.35241061449050903;De 6800 XT was afgelopen donderdag wel even beperkt op voorraad, maar dat was voor het eerst sinds vijf weken ongeveer. De prijs-prestatieverhouding van de 6800 XT is vrijwel gelijk aan die van de gewone 6800:
RTX 3080 Ti;3;0.3227337598800659;Precies, dat is inderdaad het punt wat ik probeer te maken. Het is logisch dat iemand die een gaming pc in elkaar wil gaan zetten de laatste benchmarks en artikelen erbij pakt en hier zijn spel in opzoekt. Nu lijkt of vanuit de AMD kant alleen de RX6700XT of de nog zeldzamere RX6800XT competative zijn. Terwijl de RX6800 voor velen een betere keus is dan de RX6700XT en minder zeldzaam dan de RX6800XT.
RTX 3080 Ti;1;0.3469119071960449;MSRP 3080: 699 MSRP 3080 Ti : 1199 +71% euro voor +11% prestaties = Voor consumenten komt daar bovenop de torenhoge marge van de verkoper a +50%. Prijsprestatie techinisch lijkt het alsof NVIDIA 200e van de marge die de verkoper zou gaan pakken in zijn eigen MSRP heeft gestopt. 55MH/s is echt een grap een 1080 Ti haalt makkelijk 45MH/s. Je peperdure aankoop terugverdienen met ETH minen (wat btw op zijn einde loopt) zit er dus ook niet in. Prettige wedstrijd iedereen, nieuw dieptepunt in het chipstekort
RTX 3080 Ti;1;0.29613393545150757;Dit is maar goed ook, mag het minen in een snelle pijnlijke dood sterven.
RTX 3080 Ti;1;0.509594202041626;Het is juist de bedoeling dat deze kaart niet voor miningactiviteiten gekocht gaat worden...
RTX 3080 Ti;1;0.6785169243812561;Maar als ie 2400 kost gaat ook niemand hem kopen......
RTX 3080 Ti;3;0.367593914270401;Als niemand hem voor die prijs koopt, dan daalt de prijs vanzelf. Top dus. Hopelijk komt er niet weer een makkelijke manier om hem alsnog volledig te unlocken voor mining.
RTX 3080 Ti;3;0.32862043380737305;Ja dat is afwachten, ik hoop echt dat ze het nu wel goed dichtgetimmerd hebben, maar ja alles is te kraken of te omzeilen op een of andere manier, dus ben benieuwd
RTX 3080 Ti;3;0.4130505919456482;tja wel dus blijkbaar?
RTX 3080 Ti;2;0.4591417610645294;In het overzicht voor het opgenomen vermogen ontbreekt de reguliere 3080 (non-ti). Aan de testverantwoordingen te zien kunnen de resultaten helaas niet tussen de reviews vergeleken worden. Ligt deze iets lager als de 3080ti of liggen deze dermate kort bij elkaar dat het verschil binnen de marge ligt?
RTX 3080 Ti;3;0.3619827330112457;Opgenomen vermogen van de normale RTX 3080 zou inmiddels er wel bij moeten staan, wellicht helpt een refresh van de pagina.
RTX 3080 Ti;5;0.4093281328678131;Bedankt voor het toevoegen!
RTX 3080 Ti;3;0.32638323307037354;Mis ik nu de geluidsproductie tabellen?
RTX 3080 Ti;3;0.4376990795135498;Dat klopt, we zijn momenteel bezig met een nieuwe geluidstest. Die is bijna klaar, maar voor deze review mist die data nog wel.
RTX 3080 Ti;1;0.3664008677005768;Toch blij dat de 6900XT via amd.com redelijk verkrijgbaar is voor €980,-. Vorige week donderdag gewoon 1 gekocht zonder script ofzo. Gek genoeg lijkt dat in deze tijden opeens een koopje voor zo'n kaart.
RTX 3080 Ti;1;0.3264558017253876;Rond welke tijd had je die kaart besteld? Of weet je hoelaat AMD ongeveer hun website bevoorraad aan producten? Ben zelf ook van plan een 6900XT te bestellen namelijk.
RTX 3080 Ti;4;0.41738399863243103;Je moet er precies om 17:30 bij zijn, de drop is meestal om 17:33. Maar op de tweakers discord helpen ze je graag verder. Zie ook het topic op GoT. [AMD Radeon RX6000 series] Levertijden & Prijzen
RTX 3080 Ti;5;0.49327147006988525;Thanks! zal daar morgen even naar koekeloeren
RTX 3080 Ti;2;0.4916796088218689;Ik denk eerlijk gezegd dat je beter de 3000-serie links kan laten liggen en dat je beter kan wachten tot de volgende generatie uitgebracht wordt. Ik ben bang dat met alle tekorten vanuit de chip-fabrikanten dat het gewoon niet rendabel is om nu zoveel geld te investeren in een videokaart. Denk overigens ook dat met de prijzen die dik boven MSRP liggen dat het in de toekomst gewoon niet meer te doen is om een GPU te kopen. Wel dik balen, helemaal omdat er waarschijnlijk met E3 weer leuke dingen aankomen.
RTX 3080 Ti;2;0.3132418692111969;De 4000 serie word door dezelfde fabrieken gebakken waar nu een tekort is (Samsung of TSMC), dat tekort zou zo maar eens lang kunnen aanhouden
RTX 3080 Ti;1;0.5039200186729431;Ik had begrepen dat de tekorten hoogstwaarschijnlijk nog een jaar of twee aanhouden, dacht dat ik dat ergens gelezen had, maar kan het ook mis hebben! Zover ik weet komen er elke twee- tot drie jaar nieuwe series aan grafische kaarten vanuit Nvidia. Dus dat zou betekenen dat ergens in 2023 een nieuwe serie uit zou moeten komen, houd m'n vingers gekruist!
RTX 3080 Ti;3;0.44942089915275574;Dank voor de review. De 6800XT doet het niet eens zo gek, gem nog geen 10% lagere score dan 3080Ti. Als we de performance per EUR bekijken dan doen de kaarten het 2019 heel goed
RTX 3080 Ti;4;0.3546430766582489;De RX 6900xt is 2% minder snel voor 200 euro minder, dus wat dat betreft is het ook een hele makkelijke keuze. Valt me op dat de RX 6800xt net een tikje sneller is geworden, en als je naar 1440p kijkt is de conclusie gewoon AMD = sneller
RTX 3080 Ti;4;0.4604724943637848;Ik ben heel blij met de nuancering in het artikel over de verkrijgbaarheid. Dat mistte ik wel bij vorige artikelen/reviews maar is nu helemaal goed. Klagen is één ding maar een complimentje geven hoort er dan ook bij
RTX 3080 Ti;1;0.3946569859981537;Hoe lang kabbelt dit nog verder voor de gaming, auto, en elektronica industrie zich hier actief in gaan mengen. Deze 'nieuwe' industrie verstoord deze industrieën inmiddels flink. Het is een kwestie van tijd voor dit terug is te zien op hun balans, en ze zorgen dat dit stopt. Crypto moest de wereld verbeteren. Het vervuild, verstoord andere industrieën. Heeft een flinke impact op het besteedbare inkomen door stijgende prijzen van elektronica. Begrijp me niet verkeerd ik verdien er goed aan, maar ook ik snap dat dit geen lang leven meer beschoren is.
RTX 3080 Ti;1;0.42526084184646606;Who needs scalpers, when NVIDIA releases a card at scalping prices +60% prijsverhoging voor tot 10% betere fps?
RTX 3080 Ti;5;0.5059754252433777;Ben ik even blij met de overwaarde op mijn huis, kan ik mooi de hypotheek ophogen en een nieuwe videokaart kopen
RTX 3080 Ti;1;0.565935492515564;Zolang de bitcoin miners alle kaarten opkopen voor belachelijke bedragen, zijn dit soort reviews (hoe goed bedoel dan ook) voor de meeste 'normale' gamers totaal overbodig.
RTX 3080 Ti;3;0.3539952337741852;Wel goede zet van Nvidia wat betreft gratis exposure. Allemaal producten die we waarschijnlijk nooit te zien krijgen worden op alle techsites beschreven en besproken. Is het misschien geen betere optie om spul wat we niet kunnen krijgen niet zo uitgebreid in het zoeklicht te plaatsen? Het enige wat het doet is frustreren.
RTX 3080 Ti;1;0.5496015548706055;Hij is niet onverkrijgbaar op dit moment, hij is volgens alternate op voorraad, wel ff 2500+ euro aftikken
RTX 3080 Ti;5;0.5097622275352478;Ik blijf nog lekker mijn6800xt gebruiken. Zit gemiddeld over alle games boven de 3080 en tegen de 3080ti aan. En dat voor 670 euro.
RTX 3080 Ti;1;0.7018627524375916;Videokaarten... Het is op het punt gekomen dat zelfs een revieuw echt lezen geen zin meer heeft. Kaarten zijn immers niet verkrijgbaar, of bijna 3 keer zo duur als ze ooit bedoeld waren. Ik ben blij dat ik 2 jaar geleden een tweedehands GTX 2070 super wist te kopen voor een goede deal. Kijken naar een upgrade zit er voorlopig niet in voor mij.
RTX 3080 Ti;1;0.3716120421886444;*RTX?
RTX 3080 Ti;1;0.6344907283782959;De retailers (Megekko, Alternate,... ) zijn zelf scalpers geworden
RTX 3080 Ti;2;0.45884108543395996;"Ik zal je een echt goed youtube kanaal geven met het tijdstip waar Steve meteen dat VRAM ""probleem"" bespreekt. Gamers Nexus Edit Dat gezegd hebbende zal je misschien altijd wel een paar games houden die VRAM ""tekort"" komen door het slecht programmeren en\of uitzonderlijk hoge eisen door een nieuw triple A game. Ook zitten daar vaak instellingen bij die ontzettend veel VRAM eisen maar nauwelijks tot niets af doen aan het grafisch plaatje. De games die ""getest"" zijn door die youtuber die je linkte moet dan ook zijn instellingen delen die hij in de games heeft toegepast. Hier kan je dan voor honderden extra euro's aan hardware tegenaan gooien maar je zal in verhouding nooit de extra FPS of soepelere gameplay voor terug krijgen. GTA 5 toen die net uitkwam is een goed bijvoorbeeld en Cyberpunk bij launch meen ik ook van wat ik gelezen heb (heb ik niet gespeeld). Veel triple A games zijn bij launch ruk wat dat betreft. Maar waar ligt het dan aan? VRAM tekort of desbetreffende game die, voor dat moment, teveel vraagt van de hardware die beschikbaar is en \of slecht is geprogrammeerd. Edit 2 Quote uit zijn beschrijving onderaan de video die je linkte. In die laatste remark van hem spreekt hij zichzelf wel wat tegen in mijn ogen in wat ik eerder aanmerk in het dikgedrukte. Eerst is het allemaal maar twijfelachtig en dekt hij zich in om daarna ""bewijs"" te geven... jaja! Ander goed review kanaal voor pc hardware is hardware unboxed. Dan heb je alvast twee goede."
RTX 3080 Ti;2;0.34581685066223145;"''Dat gezegd hebbende zal je misschien altijd wel een paar games houden die VRAM ""tekort"" komen door het slecht programmeren en\of uitzonderlijk hoge eisen door een nieuw triple A game'' Precies. Onder andere daarom zou ik gewoon even wachten tot de volgende generatie Nvidia of de 16gb AMD kaarten kopen. Aardig van je dat je 2 kanalen geeft ..... maar deze ken ik al. Ze staan zelfs als snelkoppeling op mijn tv. Dan zal ik je gelijk maar even de rest van mijn snelkoppelingen geven met Red Gaming Tech als favoriet. Niet zo'n groot kanaal misschien, maar juist daarom! Het gaat wat dieper allemaal daar, mocht daar je interesse liggen. Red Gaming Tech The Good old Gamer Moore's Law is dead Notanapplefan Hardware Canucks Hot News Tech Yes City Digital Foundry DerBauer Gamer Melt (eigenlijk een schandelijke jatter van de andere kanalen, maar handig voor de samenvatting!) En JayTwoCents en Linus hoef ik je denk ik niet te vertellen."
RTX 3080 Ti;1;0.48233065009117126;Ik zou nog een jaar of 5 wachten, stel je voor dat je die games die over 4 jaar uitkomen niet kan spelen.
RTX 3080 Ti;2;0.3730360269546509;Op Cyberpunk na draaien alle games in deze review op 4K ultra 60+ fps. Misschien helpt het als je uitlegt waar je naar op zoekt bent als je zegt “Nu al te weinig voor max/4K”.
RTX 3080 Ti;2;0.519306480884552;En toch noem je nergens in bron in je statement. Hoeveel moet je dan hebben om 4K goed te kunnen spelen? Ik ga je alvast helpen, dat antwoord heeft niemand concreet met goede onderbouwing en zal pas later duidelijk gaan worden als er meer games komen met hogere eisen. Er zijn nu volgens mij 2 games die meer geheugen kunnen gebruiken (los van mods) en waar je een bottleneck kunt creëren opo geheugen. Dit is echter niet representatief voor de werkelijkheid en over alles games. De verschillen tussen een stap lager zie je niet. 4K 120FPS in een nieuw spel op max settings kun je voorlopig vergeten. Er is pas net hardware in staat op de markt om 120FPS weer te geven op 4K en jij zeurt gelijk al dat deze niet gelijk in volle glorie benut kan worden. Dan zet je de games op High ipv uber insane en dan kom je er wel
RTX 3080 Ti;5;0.8207422494888306;"The ASUS RoG STRIX XG438Q a MASSIVE 43"", 4K, 120Hz Freesync 2 HDR screen the best gaming monitor released in 2019. Ik heb deze in combinatie met een Asus Tuf 3090 OC en dus 120FPS op 4K al 2 jaar dus en niet PAS NET"
RTX 3080 Ti;1;0.7008426189422607;4k 120fps heeft echt niks te maken met de vram, als dat de bottleneck zou zijn dan zou je zien dat de 3090 FLINK sneller is(als je vram op is gaat het heel hard omlaag) dan de 3080ti, waar op benchmarks geen 1 voorbeeld van is. Als ik jou was zou ik die youtube kanalen mijden.
RTX 3080 Ti;2;0.3659226894378662;Het is toch wat dat Cyberpunk 2077 niet op Ultra 4k goed kan spelen, zelfs niet met een GeForce RTX 3080 Ti, en dat je onder de 44fps zit, en je pas op Medium 64fps haalt op 4k, met zelfs Metro Exodus krijg je 77fps op Ultra 4K, of Red Dead Redemption 2 met 81fps op Ultra 4k.
RTX 3080 Ti;3;0.3749364912509918;Alleen jammer dat uw uitspraak niet klopt. De meeste games draaien zelfs met 8GB op 4K/max. Geheugen allocatie != nodig
RTX 3080 Ti;1;0.4531465768814087;En waaraan stel je dat geheugen tekort vast? Wat is de framedrop?
RTX 3080 Ti;1;0.2881188690662384;Houd je ook rekening met het feit dat de vram op de 3080ti een flink stuk sneller is dan een generatie of 2 terug? Hoe sneller de vram hoe minder er eigenlijk in de buffer hoeft te staan, dus meer vrij ruimte. Games roepen over het algemeen maar wat, en ben met me 2080ti, die ook 12gb vram heeft, nog nooit vram te kort gekomen. En ik speel werkelijk alles wat los en vast zit. (dit is inclusief games die zeggen dat ik eigenlijk meer vram nodig heb) Graag een lijst met games, ik wil het graag zelf ervaren
RTX 3080 Ti;2;0.533666729927063;"Uit de inleiding: ""Als we kijken naar de huidige marktprijzen voor RTX 3080's en RTX 3090's die op voorraad zijn, zou het ons niet verbazen als de RTX 3080 Ti in de praktijk al gauw rond de 2400 euro gaat kosten."" Aangezien de 3080 momenteel al rond de €2300 kost lijkt mij dat sterk. De kans is groot dat de Ti 2800 tot 3000 gaat kosten, waar ik de 3900 voor €3400 heb gezien. Hoe dan ook, erg jammer. Het lijkt overigens ook alsnog de 3080Ti niet de zelfde prestatiewinst levert als de 1080Ti tegenover de 1080 destijds. Kan aan mij liggen. (Edit: iemand mag mij even uitleggen wat hier off-topic aan is...)"
RTX 3080 Ti;1;0.6128430962562561;Het is erg dat zelfs de basisprijzen zo absurd veel gestegen zijn. Bij de laatste kaart die ik gekocht heb (amd rx480) kreeg ik voor 300 euro een stevige midrange kaart die *checkt* 5 jaar later nog altijd zijn mannetje staat in 1080p in de meeste nieuwe games (en blijkbaar kan ik daar nu 400 euro voor krijgen als ik hem zou verkopen, te absurd voor woorden). Nu zijn zelfs de goedkoopste instapkaarten van de nieuwste generaties bijna drie keer zo duur als die midrange van toen. Ik was vroeger een pure pc gamer. Daar is de laatste jaren ook een xbox bij gekomen om sporadisch gamepass games te spelen. Los van de scalperprijzen: Als de adviesprijzen niet drastisch zakken de volgende jaren zal dat 100% consolegaming worden, want het is financieel anders echt niet goed te praten.
RTX 3080 Ti;2;0.49188247323036194;De prijzen gaan een crash veroorzaken in de PC game markt waar je u tegen zegt. Het overgrootte deel van de gamers zit al jaren op GPU's van 300 euro of minder. Zelf heb ik ~3 jaar geleden een RX580 gehaald en normaal zou de planning zijn om dit jaar of volgend jaar (wanneer ik op 1080p ook tegen te veel beperkingen ga aanlopen) te gaan vervangen. Laat maar zitten, voor de prijs van een beetje deftige GPU waar je deze console generatie mee kan doorzitten kan je zowel de Xbox als Playstation kopen. Dan ga ik daar wel de AAA titels op spelen.
RTX 3080 Ti;2;0.4356123208999634;Inderdaad. Je bent toch knettergek als je een videokaart van >1000 of zelfs 2000 euro gaat kopen. Tenzij je bijna verdrinkt in het geld natuurlijk. Of je er één nodig hebt voor professionele doeleinden.
RTX 3080 Ti;3;0.44210290908813477;Voor mij is deze review weer een verduidelijking waarom je beter kan gaan voor AMD 6900XT voor 1000 euro. De enige echte grote waarneembare verschillen zie je op moment dat je Raytracing aanzet. Ik wil minimaal 144 fps dus zal mijn raytracing altijd standaard uitzetten met wat andere settings om dit te behalen.
RTX 3080 Ti;2;0.37856799364089966;En DLSS? Het AMD alternatief is er nog niet. En wat we van de keynote zagen is het maar een post processing filter ipv AI processing zoals bij DLSS2. Kan natuurlijk nog verbeteren met de tijd. Maar bij Nvidia is her er NU. Kaarten kopen voor wat ooit zou kunnen komen is nooit ene goed idee. Radeon is zeker niet verkeerd gezien de prijsverschillen als je eentje rechtstreeks bij AMD kan scoren. Maar voor dezelfde prijs is Nvidia een nobrainner.
RTX 3080 Ti;3;0.5649149417877197;Tja, voor mij blijft ook DLSS 2.0 een upscaling techniek dat zeker een mooi alternatief kan zijn voor minder goede hardware maar persoonlijk blijf ik liefst gewoon native draaien zonder vorm van upscaling. Voor mij persoonlijk liever iets minder mooie graphics en geen upscaling dan upscaling waarbij je mooiere graphics krijgt maar zo nu en dan toch een glitch.
RTX 3080 Ti;2;0.39506492018699646;Dus als uw kaart de game maar op 1080p kan draaien verkies je dat boven bv 1440p met goeie dlss? Bovendien zal de 1080p gameplay er minder goed uit zien dan de 1440p met dlss? Natuurlijk mag je bv native 4K niet vergelijken met 4K+DLSS(al is het visueel verschil heel klein) Maar je moet beseffen dat je dankzij DLSS naar een hogere resolutie kan gaan zonder verlies in fps. Daar draait het hem om. En het AMD alternatief zou eerder op niveau van DLSS 1.0 zitten. Op zich niet een drama en ook logisch. En ik vermoed dat dit ook zal verbeteren met de tijd. Maar zoals gezegd koop je geen gpu voor wat ooit zou kunnen komen. DLSS is wat me bij Nvidia gehouden heeft bij mijn laatste aankoop van de 3070. Heb op het punt gestaan een 6800 te bestellen, maar die haalt het niet van een 3070 met DLSS. En dat de 3070 beter is in minen waardoor ik de overprijsde kaart kan terug verdienen is mooi mee genomen. Sinds maart al zo'n 300€ netto terug verdiend terwijl ik niet gamede. Ben ik al onder de msrp
RTX 3080 Ti;3;0.5980306267738342;Niet helemaal waar. Ik zou de DLSS deep dives van Gamers Nexus eens kijken. DLSS is goed maar geen wondermiddel. Het AMD alternatief is welliswaar niet beter maar het werkt wel op alle video kaarten. Of deze nu AMD zijn of Nvidia. Zolang ze maar nieuwer of vergelijkbaar zijn aan de GTX 10xx series. Ja DLSS is beter maar het is niet iets waarvoor je specifiek je kaart moet kopen, dat doe je op basis van performance. De DLSS implementatie is ook sterk beïnvloed door de implementatie van de ontwikkelaar en aan hoe de game zich hier veel baat bij heeft. Cyberpunk had een aantal dingen aanzienlijk beter met DLSS dan zonder, maar de game werd ook zachter qua uitstraling.
RTX 3080 Ti;1;0.612158477306366;een 6800XT veegt de vloer met een 3070 het prijsverschil is maar 75eur tussen 6800 en 6800XT tijdens de AMD drop , bereken eens mining met een 6800-6800XT tegen drop prijzen dan was je kaart al halfweg gratis....en zeg niet dat het loterij is of onmogelijk want ik heb er ondertussen al 2 in mijn gaming pc's zitten. trouwens dlss van 1440 naar 1080 is niet aan te raden want dat is geen 1op1 schaling, 4k naar 1080 wel. en waarom zou je dlss verkiezen boven native? snap het plaatje helemaal niet. Met een XT kan je 1440 med of ultra settings op de meeste games spelen tegen 144fps.. dat is in lijn met je 144hz monitor. DLSS is gewoon marketing hype.
RTX 3080 Ti;1;0.3917582929134369;Heb dat idd vanaf half januari tot half maart geprobeerd. Maar telkens bot gevangen, net zoals zovelen. Onmogelijk is het nooit. Het is ook niet onmogelijk om in een baan om een voet op de maan te zetten, maar niet voor iedereen weg gelegd. Het blijft loterij, of je dat nu leuk klinken vindt of niet. De site api was meestal onbereikbaar en tegen dat ik er op raakte was alles weg. Heb dan ook niets tegen AMD zoals je leest heb ik ook geprobeerd. Maar ben nu blij dat ik uiteindelijk de 3070 heb. 6800xt is zo'n 10% sneller in klassieke rendering terwijl het idd ook duurder is dan de 3070 die ik betaalde. Maar moet de duimen leggen zodra je de tensor cores gaat benutten zowel voor raytracing of voor DLSS die zowat de standaard wordt in iedere nieuwe AAA game. En zoals je zelf zei. Fidellity FX super resolution(pfff, lange tekst XD ) zal voor alle hardware werken. Dus mocht het ooit beter worden gebruik ik gewoon dat:) En mocht AMD uiteindelijk wel zijn kaarten op punt hebben bij de volgende generatie koop ik gewoon die. Ben dan ook geen fanboy. Dat is nooit slim. ik koop wat ik op dat moment de beste prijs/kwaliteit vind en ook daadwerkelijk kan kopen
RTX 3080 Ti;2;0.3707103133201599;Dit maakt voor DLSS helemaal niets uit. Naast DLSS is een prima oplossing. Dankzij DLSS kan ik bijvoorbeeld Cyberpunk op near native 4K visual met raytracing aan op 60+ FPS spelen. Het gaat om het eindresultaat en hoe de GPU daar komt maakt mij niks uit. Het biedt meer performance met mooiere visuals dan andere GPU's in deze price range. Zo heb ik nog meer voorbeelden.
RTX 3080 Ti;2;0.46268510818481445;hoe kan je nou betere visuals hebben op 4k native.. wat je eigenlijk doet is pixels weglaten en een AI er wat pixels tussenplakken... op 30+ inch kwaliteits monitoren zie je dan ook gewoon het verschil, kan je evengoed je kwaliteit wat lager zetten. Zeg dan gewoon tegen de sw ontwikkeling dat ze hun beeld anders moeten opbouwen... je geeft het ook zelf aan, je zet RT en dan heb je DLSS nodig om geen slideshow te hebben. eerst wil je enhanced beeld om daarna gewoon terug pixels weg te laten en de kwaliteit van het beeld te verlagen.mooi voorbeeld hier Discussie ook al gehad over RT, net hetzelfde, beter beeld tot op een moment dat je een slide show aan het kijken bent.... dus het nut hangt af van het doel. Als je GU niet sterk genoeg is kan je het beter weglaten, nu ga jij de beeld kwaliteit die je wint gewoon teniet doen om toch maar FPS te hebben. Ik zie hier wel nut voor budget oplossingen waar zowel GPU en monitor binnen beperkt budget valt, 9/10 hangt daar dan toch weer een budget monitor op en zien ze amper het verschil.
RTX 3080 Ti;2;0.3398666977882385;Ik heb negens betere visuals gezegd, ik heb near native visual gezegd. Wat het objectief ook doet gebasseerd op wat ik zelf ervaar maar ook vele analyses welke je terug kan vinden bij verschillende tech kanalen op youtube. DLSS doet wel een paar dingen beter, zaken als Anti aliasing. Met de juiste settings kan je ook texture scherpte boosten en heb je per saldo gewoon een hele goede beeldkwaliteit zonder settings terug te schroeven op een hoge framerate. RT is op vele resoluties met de nieuwste kaarten vlot genoeg, helemaal in de 2de gen games zoals Control, RE8, Metro Exodus remaster etc. Maar in Cyberpunk wordt het wel krap als je op een resolutie van 5120x1440P gamed en dus dubbel de FOV hebt. Dat is een worst case scenario voor RT en dan is DLSS een redding. Heb je het wel zelf getest vraag ik mij af?
RTX 3080 Ti;1;0.43277016282081604;ik heb een 6800XT voor 640eur en ik gebruik 1440p 144hz native all max... geef me een reden waarom ik mee zou doen met die groene marketing hype tegen die prijzen om dan nog eens meer te betalen voor een fake G-sync complaint monitor?? tegen dat 4K de markt standaard is kwa perfomance zonder trukendoos zijn we weeral enkele jaren verder. Koop ik gewoon een nieuwe kaart. Zoals zoveel is de N-1 of optioneel N-2 steeds de beste bang for buck en marktconform...
RTX 3080 Ti;2;0.43253445625305176;Pas op je fanboyism druipt er uit. Waar hebben wij het over Gsync gehad? Wij praten over nu, en waarom zou je niet het uiterste er uit willen halen op slimmere manieren dan alles te brute forcen? DLSS 2 is bewezen een goede technologie om je framerate te boosten met minimaal verlies van grafische pracht en praal. Wat is daar mis mee? Omdat het Nvidia is, is het opeens marketing hype? Ik kan prima op 4K gamen met een 3080 op hogere framerates, nog mooier dat ik dit kan boosten ook in zware open world titels dankzij DLSS. Waarom progressie teniet willen doen? Alle render technieken in games voor real time rendering zijn tools uit een truken doos. Heel de screen space reflection methode is zoiets, zet je dat dan ook uit omdat het geen echte reflecties zijn?
RTX 3080 Ti;2;0.517764687538147;Als ik de benches bekijk en zijn duidelijke verhaal dat RT/DLSS voor hem de performancehit niet waard is is het voor hem dus duidelijk dat ie een prima kaart heeft en de switch helemaal geen nobrainer zou zijn. Dat jij daar wel waarde aan hecht is je goed recht maar dat maakt voor hem toch geen verschil? Zelf zou ik de 3080ti wel willen hoor, ff testen naast de 6900xt, de kaart met meeste FPS in warzone hou ik dan. RT doet mij ook bijzonder weinig (had hiervoor een van de dikkere 3080's) en trekt me niet over de brug. Ik had altijd gelezen dat die (nvidia) encoder voor het opnemen tijdens het gamen superieur zou zijn maar daar zie ik helemaal geen verschil in dus ook dat trekt me niet naar kamp a of b.
RTX 3080 Ti;4;0.33670729398727417;Ik zit zelf nog met een igpu. Maar dlss en RTX voice klinken mij erg interessant in de oren.
RTX 3080 Ti;4;0.39571183919906616;Nou als ik op de benchmarks van Battlefield en Assassins Creed kijk dan doet de 6900XT toch echt beter. Het enige nogmaals is raytracing en indien je behoefte hebt de upscaling techniek (DLSS). Ga je puur voor native en boeit raytracing je niet dan biedt de 6900XT gewoon veel meer performance voor een zachter prijs.
RTX 3080 Ti;3;0.3984912037849426;Nou... Er zijn genoeg spellen waar amd gewoon sneller in is. Voor minder geld, en minder stroomkosten. Het ligt er maar net aan wat je ermee wil doen. Ik zou mijn 6800xt voor geen goud willen ruilen met een 3080 iig.
RTX 3080 Ti;1;0.37571558356285095;"Ik snap al dat gezeur niet, feit is is dat het voor MSRP een verdomd goede kaart zou zijn. Eigenlijk is dit meer een ""3090 Gamer LHR edition""."
RTX 3080 Ti;1;0.6683644652366638;Hoe dan? Want een 70% hogere prijs voor ~10% betere prestaties is gewoon lachwekkend slecht.
RTX 3080 Ti;2;0.35672083497047424;Als ik de testresultaten bekijk ben ik best wel blij dat ik de 3080 verkocht heb en een 6900XT terug gekocht heb (in andere volgorde want ik ga niet zonder kaart zitten), game tussen 1440p en 4k in. Als ik een 3080TI kan krijgen voor de FE-prijs dan zou ik dat doen en doe ik de 6900XT weer van de hand, de 3090 heb ik (de FE) altijd al te duur gevonden (als was dat achteraf helemaal geen slechte keus).
RTX 3080 Ti;5;0.23792876303195953;Ik heb zelf de 1080 ti -> 2080 ti gehad en was van plan om de 3080 ti te gaan halen, maar gezien deze niet kwam voor een 6900XT gegaan. Deze weten te bemachtigen voor 980€ via amd.com. Als ik nu zo de benchmarks eens vergelijk is het 50/50 in de games waar de 3080 ti het beter doet. Totaal geen nut om nu 2000 euro extra neer te gaan tikken voor deze kaart. Normaal gesproken kon AMD niet eens in de buurt komen van de xx80 ti kaarten, maar deze ronde heeft AMD zeker gewonnen in mijn ogen.
RTX 3080 Ti;1;0.3554845452308655;Voor 2000 euro hoeft het dan weer niet daarom zei ik ook voor de FE prijs
RTX 3080 Ti;3;0.5790639519691467;Leuk dat word gesproken over een advies prijs, aangezien dat totaal niet haalbaar gaat zijn. Maar dit kaartje zal wellicht meer in trek zijn bij de mensen die eigenlijk een 3090 wilde aanschaffen al acht ik de kans net zo laag als alle andere kaarten om er een te bemachtigen. Daarnaast prima review mensen!
RTX 3080 Ti;1;0.39026474952697754;Stel ik ben zo'n idioot die misschien een 3080TI wil aanschaffen, rond welke tijd kan ik morgen online in de schappen verwachten? Heeft iemand daar wat meer ervaring mee?
RTX 3080 Ti;5;0.38817694783210754;I want to know as well! I am still awake hitting F5 already lol!
RTX 3080 Ti;1;0.7189887166023254;Het onverkrijgbare vlaggenschip en dan toch een review kunnen doen Kan ik jullie kaartje niet overkopen? Ik ben namelijk beperkt tot 2 slot dus veel keuze heb ik toch niet en op die 3070 die ik besteld had wacht ik nu al 5 maand... Altijd weer bijzonder dat kaarten lastig te krijgen zijn behalve voor degene die de reviews/marketing doen.
RTX 3080 Ti;4;0.4981735646724701;Normaal gesproken: heel gaaf! Maar nu in de huidige omstandigheden: lekker boeie.
RTX 3080 Ti;1;0.42529040575027466;Wat zijn de vram temps van deze kaart hoger of lager vergeleken met een 3080 non ti ? wil hier graag een antwoord op krijgen
RTX 3080 Ti;1;0.39222922921180725;Is die 3080 Ti ook nog ergens voor een normale prijs (lees: adviesprijs) te pre-orderen? Of kan ik dat vergeten?
RTX 3080 Ti;2;0.4186117947101593;Pre-orderen bij nVidia gaat niet op, als je de sites die ik een stukje eerder noemde in de gaten houdt (en uiteraarde de bekende Discords als Benelux Stock Checker) dan maak je nog het meeste kans.
RTX 3080 Ti;3;0.48882120847702026;Cool review maar geen upgrade waard vanaf me 3080, luttle frames meer maar goed voor mensen die er nog geen 1 hebben.. Als je er 1 kan krijgen natuurlijk
RTX 3080 Ti;1;0.2555863559246063;Ik ben dan geen miner, maar als je toch zulks achterlijke prijzen gaat neerleggen voor een gpu dan is LHR gewoon kul. Je betaalt al de hoofdprijs vanwege de schaarste in grondstoffen, mining en scalpers (wellicht ook vanwege OEM) en afhankelijk wat je doet kan je de kaart die je aanschaft terug verdienen wat gezien de prijs momenteel voor mij wel een optie zou zijn. Je zou ipv de redenen die vermeld worden simpelweg kunnen stellen dat je zoveel voor een kaart betaalt omdat bij msrp misschien binnen een maand terugverdiend kan worden.
RTX 3080 Ti;1;0.3519677221775055;Ik zou dirt5 toch even opnieuw testen met de 6900xt. Ik haal gemiddeld 154 FPS met laagste 138 en hoogste 199. In de test op 1440p ultra komt Tweakers uit op 135 FPS gemiddeld.
RTX 3080 Ti;1;0.6017265915870667;Wanneer komt hij uit? Wanneer kan ik F5 gaan spammen? Dat is het enige moment dat je de kaart op msrp kan krijgen. Hij zal alleen binnen een minuut uitverkocht zijn...
RTX 3080 Ti;2;0.44025731086730957;Slechts rond de 10% sneller dan een RTX 3080, en dat voor 66% meer geld qua adviesprijs.. Vrij kansloze GPU dus, net zoals de RTX 3090 al was t.o.v. de RTX 3080 qua prijs/prestatie verhouding.. De RTX 3080 blijft de beste keuze als highend Nvidia videokaart wat dat betreft deze generatie, als de prijzen normaal geweest waren.. Ik heb gelukkig nog een RTX 3080 kunnen kopen eind vorig jaar voor een normale prijs en ben daar nu alleen maar nog blijer mee dan ik toen al was.. Dat een mogelijke 3080Ti niet zo heel bijzonder zou worden kon je toen eigenlijk al weten omdat de 3090 ook slechts 14-15% sneller was en een 3080Ti waarschijnlijk niet sneller zou zijn.. Prijstechnisch had ik een 3080Ti verwacht rond de 1000 euro, dus 1200 valt ook nog eens tegen.. Wat dat betreft niks verandert t.o.v. de ook al idioot dure RTX 2080Ti.. De huidige marktsituatie voor videokaarten is een totale infarct, past wat dat betreft perfect in het Covid19 crisis tijdperk waarin we leven.. Hopelijk snel voorbij allemaal..
RTX 3080 Ti;3;0.27608251571655273;Als ik de fps zie bij tombraider en creed en stroomverbruik , is men beter af met een 6800xt en ook nog eens 500 goedkoper als de prijzen weer normaal zijn
RTX 3080 Ti;2;0.3838895261287689;"kaart dus 50% duurder maar performance in NVIDIA vriendelijke spelletjes liggen max 10% hoger. kaart wilde ik graag maar komt nu op de ""laat maar"" lijst te staan."
RTX 3080 Ti;1;0.7332214713096619;19 Pagina's om een nauwelijks snellere kaart die 70% meer kost aan te prijzen, totale afzetterij. En het is geen mindere 3090, maar een licht verbeterde 3080..... Hopelijk gaat Intel kaarten leveren voor een normale prijs in 2020, in plaats van, zoals nvidia, gebruik te maken van de tekorten op het moment bij TSMC en de absurde crypto nonsens.
RTX 3080 Ti;1;0.2756488621234894;Ga hem zeker proberen te krijgen vandaag haha. Als ik er 1 kan krijgen, gaat de rtx 3080 in de verkoop.
RTX 3080 Ti;1;0.3763146698474884;hé, wat een gekke 3080 zeg
RTX 3080 Ti;1;0.7225457429885864;Ze hebben de advertentie zelfs verwijderd hahaha.
RTX 3080 Ti;1;0.7622487545013428;"""Het onverkrijgbare vlaggenschip"" was nooit echt kloppend.. Webwinkel Azerty heeft er tientallen op voorraad en die voorraad wordt niet snel minder. Begrijpelijk voor 2500 euro per stuk..."
RTX 3080 Ti;2;0.45143336057662964;"Er zou eigenlijk geen aandacht aan die nieuwe kaarten gegeven moeten worden, zolang deze niet te verkrijgen zijn. Het is overigens ronduit belachelijk dat je gemiddeld 2500 EUR neer moet leggen voor een videokaart. Voor mijn 980 ti heb ik destijds 800 EUR betaald en dat vond ik al buitensporig veel. Deze kaart doet overigens nog prima anno 2021. Gamen op 2560 x 1440 gaat uitstekend op ""high""settings. Chiptekorten of niet. Hoe het er nu aan toe gaat is ronduit belachelijk. En aangezien de fabrikanten nu weten dat er mensen zijn die deze achterlijke prijzen betalen, zie ik de prijs ook niet meer dalen."
RTX 3080 Ti;1;0.6635936498641968;Rare titel. Is de RTX 3090 niet het vlaggenschip meer?
RTX 3080 Ti;1;0.6218493580818176;Die is nu gerebrand naar creators. Ik ben toch echt geen creator hoor
RTX 3080 Ti;1;0.3959982991218567;Ik ben ook geen vlaggenschip
RTX 3080 Ti;5;0.5208746194839478;Nvidia heeft de 3090 zo wel vanaf het begin genoemd in ieder geval, en de 3080 werd geadverteerd als flagship game kaart.
RTX 3080 Ti;3;0.2328425645828247;idd, 3090 was het extraatje die niemand zou moeten nodig hebben.
RTX 3080 Ti;2;0.385283887386322;Ik heb er anders veel plezier van. Gelukkig op tijd (begin dit jaar) besteld. De 3080 vond ik niet toekomst vast genoeg en de 3080ti liet te lang op zich wachten. Dat is wat mij betreft de beste keuze geweest, want de 3090 was op het moment dat ik hem binnenkreeg(3,5 week later) al 600 euro duurder, en het is nog maar de vraag wanneer je de 3080ti voor een lagere prijs dan ik mijn 3090 heb gekocht zal kunnen kopen. En anders kan ik alsnog mijn 3090 voor een dikke prijs kwijt en een 3080ti kopen. Aangezien de '90 wel fatsoenlijk mined en dus waardevaster is.
RTX 3080 Ti;2;0.3064630329608917;De meeste reviewers vonden de meerprijs van de 3090 niet te verantwoorden tegenover de 3080 gezien het kleine prestatieverschil. Maar idd, een dure 3090 die je kan kopen tegenover een mythe die een 3080 heet...
RTX 3080 Ti;1;0.5967139601707458;Ik zeg altijd beter duur dan niet te koop. Voor jouw beeld, mijn 3090 was 1900, een 3080 gaat nu al voor meer volgens mij...
RTX 3080 Ti;3;0.2912394404411316;Ach zolang max ict een 3090 voor 15k verkoopt is alles relatief: D linkje voor zij die graag nachtmeries krijgen
RTX 3080 Ti;5;0.3944065272808075;Aanbiedt != verkoopt.
RTX 3080 Ti;1;0.5689542293548584;Zonne steekje opgelopen? Wat zeg je nou allemaal?
RTX 3080 Ti;5;0.4802708327770233;underrated comment
RTX 3080;3;0.3496360778808594;Nog wat reviews om te bekijken: NVIDIA GeForce RTX 3080 with AMD Ryzen 3900XT vs Intel Core i9-10900K NVIDIA GeForce RTX 3080 PCI-Express Scaling How Much CPU Does the GeForce RTX 3080 Need? Voor de YouTube fans: Digital Foundry Gamers Nexus Hardware Unboxed Techtesters OC 3D JayzTwoCents Linus Tech Tips De sortering is losjes gebaseerd op de kwaliteit (lees: diepgang) van de reviewer(s).
RTX 3080;3;0.4410528242588043;"Geweldige post, kerel! +3 Echter is er nog 1 ding die ze niet kunnen reviewen en dat is RTX IO. Dat beloofd echt heel veel goeds voor laadtijden van games. Alleen RTX IO is wel een van de beste, zo niet de beste feature van deze kaart. RTX IO wordt dus volgend jaar via Direct X uitgerold en ik kan 'als ik naar mezelf kijk' bijna niet wachten op een nieuwe PC mét deze kaart. Mijn systeem is nu 13 maanden oud met een RTX2080 en dat was snel; nu is er de weg vrij om over een goed jaar over te stappen. Er is geen weg meer naar 4K gaming nodig, die is er sinds vandaag officieel. Deze RTX3080 zit te slapen op 4Kp60. Dus ik vermoed dat de G-Sync modellen op 4K nu meer op voorraad komen. Zelf game ik op 1440p144, dat is ook mooi hoor. Ik kom nog van het Commodore 64 tijdperk. Verder noem ik het geen upgrade ofzo, maar wereldnieuws voor de gamers onder ons. Ik heb genoten van de eerste aflevering van Tech hub, in het dubbele zelfs. PS: Een tijdje terug waren er nogal wat discussies over dat PC gaming dood zou bloeden. Nou, nu al helemaal niet meer"
RTX 3080;1;0.8807970285415649;HDMI 2.1 hebben zo ook helemaal niet getest. Helemaal nergens. Werkt de Auto Low Latency Mode, werkt VRR goed met niet standaard refresh rates, werkt HDR pass through? Eigenlijk weten we nu nog niets/veel nieuws. Bovendien zijn de founders edities slecht verkrijgbaar en hebben bovengemiddelde koelventilatoren, die niet gebruikt worden met andere edities of andere fabrikanten.
RTX 3080;2;0.4436276853084564;"Dat viel mij ook al op; ze reviewen allemaal dezelfde aspecten, alsof ze het bestaan van HDMI 2.1 en het belang ervan vergeten zijn."
RTX 3080;1;0.4348790943622589;RTX IO is geen exclusieve feature van deze kaart. Er is al bevestigd dat dit ook uitgerold gaat worden op RTX 2000 series.
RTX 3080;3;0.24506594240665436;Nou, als je RTX aan zet bij nieuwe titels ben je blij als je nog 60FPS haalt bij 4K
RTX 3080;1;0.2882068455219269;RTX is erg zwaar, weet ik. Nu The Witcher 3 komt met een RTX patch ben ik ook benieuwd hoe de prestaties gaan zijn. We wachten het af
RTX 3080;4;0.4622194468975067;De weg naar 4K gaming was er al jaren. Speel ik wel voornamelijk de wat lichtere titels (Trials, SOR IV, Mother Russia, War Thunder) op 75Hz maar dan nog. Enige wat me wel kan verleiden naar een upgrade is de laadtijden, ben wel benieuwd naar de winst op dat vlak.
RTX 3080;3;0.334896445274353;Hebben videokaarten invloed op de laadtijd?
RTX 3080;1;0.3232780396938324;Dit vraag ik mij dus ook af. want zover ik weet is over het algemeen hier het opslag medium de bottleneck.
RTX 3080;3;0.24972333014011383;Jep
RTX 3080;2;0.4534415900707245;"Staat in de review, onder RTX IO. ""gecomprimeerde textures rechtstreeks inladen naar het videogeheugen, waardoor knelpunten bij de processor en het opslagmedium worden omzeild"". Dus binnenkort wel ja. Nog niet. En als er grote winsten zijn, dan wil ik m'n oudere kaart wel inwisselen, maar voor amper meetbaar doe ik het niet."
RTX 3080;1;0.2441098690032959;Deze verdient ook nog wat extra aandacht: Op 4K ultra in het 99ste percentiel is de Ryzen 3900X gemiddeld 2 fps sneller dan de 3600. Voor 250 euro extra..
RTX 3080;2;0.3990439176559448;De 3900X is een CPU dat voordelen heeft voor encoding, video bewerking, compilatie, streaming+gaming enz. Voor de meeste games is de 3900X gewoon overkill en word er nauwelijks 1 CCD ( 6 Cores ) gebruikt. De andere CCD staat vaak idle bij gaming want cross CCD communicatie is duur. Die andere CCD doet dan andere taken ( je browser in de achtegrond, achtergrond taken, enz ). Die 250 Euro extra betaal je voor 2 maal 3600 in op je moederbord te hebben. Aka, zoals een Dual Socket CPU te hebben. Mensen dat enkel naar de cores kijken en dan klagen over gaming prestatie, zijn vaak die dat geen benul hebben van wat de echte troeven zijn van de 3900X/3950X. Als ik bijvoorbeeld compile, dan word de load over alle 12 cores verdeeld. Een voordeel tegenover de 6 cores dat de 3600 enkel heeft. En ja, te veel mensen kopen 3900X's met de gedachte meer cores = goed voor gaming, terwijl dat eigenlijk niets uitmaakt ( soms zelf een nadeel als data over de 2 CCD's gaat ). Zijn er games dat meer dan 8 cores kunnen gebruiken: Ja, maar het zijn er niet veel. Over het algemeen zijn de meeste games nog altijd geoptimaliseerd voor 4 Cores ( de grootte meerderheid van mensen zitten nog op oudere 4 cores cpu's. Zie Steam statistieken ). Ik denk met de nieuwe consoles, dat we meer naar de 8 Cores als standaard gaan maar dat is een inhaal beweging over jaren. Tegen dan zitten we mogelijk aan 6900X met 24 cores of zo.
RTX 3080;2;0.46420949697494507;ik zie idd in heel veel games dat mijn x3900 niet veel verder komt dan 2 cores en soms 4 cores. maar als je render taken gaat doen komt de kracht los en ben je zo een uur minder tijd aan renderen of code compilen. Unreal engine project compiled nu in 10 min ipv 2 uur op mijn oude I7.
RTX 3080;2;0.439217209815979;"""De 3900X is een CPU dat voordelen heeft voor encoding, video bewerking, compilatie, streaming+gaming enz."" Precies. ""Voor de meeste games is de 3900X gewoon overkill"" Nou, als je de door Blackice09 gegeven link ( ) nader bekijkt, dan lijkt de 3900X zelfs een bottleneck t.o.v. de 9900K."
RTX 3080;1;0.3768678903579712;De 3080 is toch echt een videokaart voor 4K gaming, en daar is het verschil tussen CPU's zo klein dat zelfs een i3 geen bottleneck is! Of ga jij wel een 3080 kopen om op 1080p medium te gamen? Edit: fps op 4K ultra, 9 game average Poeh poeh, wat een CPU bottleneck zeg... Of heb ik een aantal op 1080p gamende toekomstige 3080 eigenaren (more=better!) beledigd?
RTX 3080;1;0.3874308168888092;En waarom niet voor 1080p? Afgaande van de eindresultaten van Tweakers gaat op 1080p Ultra je fps van 85 naar 145 als je nu op de GTX 1080 zit. Lijkt me een goede bump wanneer je een 144+Hz scherm hebt.
RTX 3080;2;0.45020395517349243;Gemiddeld geven 1080p 144+Hz spelers weinig tot helemaal niks om ultra instellingen. Je hebt het dan al snel over bijvoorbeeld een competitive shooter zoals CSGO o.i.d. waarbij je 'met een aardappel als videokaart' al de 144 haalt... De hier (en ook in het geval van de meeste andere reviews) geteste games zijn grafisch flink zwaarder dan de gemiddelde game library van 1080p 'high fps' spelers, je vergelijking is dus krom. De GTX1080 haalt in de werkelijke game library van de 1080p 'high fps' spelers veel meer dan 85 fps gemiddeld omdat daar, in tegenstelling tot bij deze review, geen games als Control, TWS:Troy, SotTR en RDR2 bij zitten! Edit: Was Control nog vergeten...
RTX 3080;2;0.3398124873638153;maarja de 360hz schermen komen binnenkort uit en daar kan je deze kaart dan wel weer voor gebruiken op 1080p al heb ik liever de 1440p 360hz maar die zal er vast nog niet komen
RTX 3080;3;0.32871320843696594;"Competieve gamers mijden meestal 4K schermen vanwege: - hoge input lag - framerate - gebrek aan meerwaarde van 4K resolutie bij 144 Hz t.o.v. 1440p 144Hz 1440p en ja, ook 144 Hz 1080p schermen zijn daartegen zeer gewild. En als we naar het aantal minimum frames kijkt bij 2560*1440 9 game gemiddelde kijken zien we: i3 10100: 113.5 R5 3600: 120.4 R9 3900X: 132.0 i9-9900K: 146.5 i9-10900K: 150.0 Kortom CPU maakt een duidelijke verschil en ja er is sprake van een CPU bottleneck. ""Of ga jij wel een 3080 kopen om op 1080p medium te gamen?"" Nee, als ik al een 3080 koop dan niet primair voor 3D games maar vanwege de CUDA cores voor Lc0 e.d. Wat 3D games betreft, Diablo II LOD zit op 800*600 resolutie"
RTX 3080;1;0.4811604917049408;"De 3080 is toch echt gepositioneerd als videokaart voor 4K gaming. Vandaar mijn bottleneck opmerking, die is er dus niet op 4K, dat is gewoon 100% juist. Dat er op 1080p/1440p bij high fps (dikke videokaart) al snel een CPU bottleneck is weten we al jaren, da's niks nieuws. En wat is er nog relevant aan je opmerking ""competieve gamers mijden meestal 4K schermen vanwege: ..."" als je daarbij simpelweg voor het gemak eventjes vergeet dat 'competitieve gamers' de geteste games helemaal niet spelen? Dit artikel gaat over de 3080, niet over low res high fps gaming, de tests voor deze reviews (games) zijn daar duidelijk niet op gericht, daardoor niet relevant voor deze groep en discussie daarover is daarmee ook niet relevant... Waar wil je heen?"
RTX 3080;2;0.3559756875038147;"Dat zoals ik in mijn eerste post al aangaf de 3900X niet gewoon overkill is maar een bottleneck. ""De 3080 is toch echt gepositioneerd als videokaart voor 4K gaming."" Natuurlijk zet de Nvidia marketing de 3080 als 4K neer, daar is de prestatiewinst procentueel het grootst t.o.v. ander kaarten. En voor 1024 * 768 hoeft de gamer zijn huidige videokaart niet naar een 3080 te upgraden. "" niet over low res high fps gaming"" Sinds wanneer zijn 1080 en 1440P low res? En al helemaal niet met 21:9 schermen. Ik durf rustig te stellen dan van de mensen die de komende 12 maanden een 3080 kopen slechts een minderheid op de 4K monitor gamed. De veeleisende gamer heeft meer plezier van een 144Hz 1440p scherm dan van een 60Hz 4K monitor."
RTX 3080;2;0.4220275282859802;"waar noem ik 1440p low - res? dat verzin je ter plekke... en verder, jij trekt 1080p toch in de discussie onder een artikel over de 3080? daar ging uiteraard mijn "" low - res "" opmerking over. het antwoord op je vraag, maar dan voor 1080p, is ; sinds al een tijdje. het is simpelweg een feit dat het de laagst verkrijgbare resolutie voor gaming is tegenwoordig. hoeveel schermen kan je uberhaupt tegenwoordig nog nieuw kopen die op een resolutie lager dan 1080p zitten? het zijn er 380 tegenover 2391 ( 1080p of hoger ) in de pricewatch, en van die 380 is er voor zover ik kan zien niet 1 gericht op gaming. je kan het dus wel of niet leuk vinden, maar tegenwoordig is 1080p low -, 1440p mid - en 2160p ( 4k ) high - res. v. w. b. 16 : 9 ( 2179 stuks in de pw, dus duidelijk de norm ), voor ultrawide en ' ultrabreedbeeldmonitoren ' is dat uiteraard iets anders. waarmee ik nogmaals aan wil geven ; een rtx 3080 voor 1080p? dan is je doel duidelijk om de allerzwaarste nieuwe aaa games met hoge / de hoogste settings op relatief hoge framerates te spelen. de catch hierin ; dat is dus niet wat competitieve high fps spelers doen... die spelen toch echt veel lichtere games en gaan daarom nog een aantal jaren geen 3080 power nodig hebben om hun framerates te halen en daarmee competitief te blijven. en zeer waarschijnlijk ook niet als ze recent naar 1440p zijn overgestapt want ook daar haalt de 3080 met gemak zeer hoge framerates in competitieve games. de fout die je hier telkens maakt is dat je de lijst met geteste games, en de resultaten van de testen, van deze review behandeld alsof deze representatief zijn voor een andere doelgroep die andere games speelt ( al dan niet competitief ), en dat is dus, nogmaals, niet het geval..."
RTX 3080;1;0.7156059741973877;"Inderdaad te veel gamers die niet weten wat ze kopen. Het zal AMD overigens een worst wezen waarom iemand een CPU koopt. ""For the world's elite gamers"" staat bovenaan op AMD's officiële productpagina voor de 3900X. Ik vind het een slechte ontwikkeling, vandaar dat ik deze test graag wou delen. Daarnaast zal het één van de meest gestelde vragen zijn: ""moet ik mijn systeem upgraden""?"
RTX 3080;2;0.5427618026733398;Bij het werken met te veel cores krijg je by complexere physics berekeningen al snel timings problemen met andere processen.
RTX 3080;5;0.6105557084083557;wat is eigenlijk de beste keus kwa proc? { voor game's }
RTX 3080;3;0.4674854278564453;"Zelf ben ik een fan van Guru3D , ik vertrouw al zolang op hun reviews. Wat wel grappig is; is dat hun niet boven 38 dBA uitkomen met hun 100% load test kwa noise en T.net daar keihard overheen gaat 50.8 wat echt een significant verschil is."
RTX 3080;3;0.4985117018222809;Het ligt er ook aan van hoe dichtbij je de meting doet. Je moet het daarom ook meer in verhouding tot de andere kaarten zien, dan als absolute getallen.
RTX 3080;3;0.5104777812957764;Maakt qua info niet superveel uit, zolang je maar weet wat de variabelen zijn, wat in deze dus vooral afstand tot de ventilator is, op welke plek de meting wordt afgenomen en welke fans er allemaal nog meer draaien. Zolang je dat weet en dit een constante gelijke afstand is etc dan kun je daar redelijke conclusies uit trekken, of dat nu op 1m of 1cm afstand is. Ik heb liefst een op realistische afstand voor mijzelf, dus ongeveer 1 meter. Ik heb de kast niet naast mij staan dus
RTX 3080;3;0.4327182173728943;Houd daarbij rekening dat decibel (dB) een logaritmische schaal is. Dus verschillende afstanden tot de bron kan tot een hoger/lager verschil in dB tussen kaarten leiden.
RTX 3080;5;0.4446854293346405;Deze mogen er ook nog bij (OC3D):
RTX 3080;5;0.6224316954612732;Thanks!
RTX 3080;5;0.49362942576408386;Tering..was al blij dat ik deze (uitstekende) review uit had. Maar serieus, fantastische bronvermelding. Bedankt!
RTX 3080;2;0.32422128319740295;"Tweakers is er flink op vooruit gegaan door de ""merger"". Je ziet alleen dat ze bij bv TPU nog net wat meer tijd erin steken met o.a. een volledige teardown van de koeler. Vandaar dat die ook bovenaan staat."
RTX 3080;1;0.41249415278434753;Geen tech youtuber, maar wel iemand die veel gedetaileerde filmpjes heeft gemaakt over DLSS en daarmee waarschijnlijk zijn review kopie heeft weten te scoren: 2kliksphilip -
RTX 3080;1;0.4228358864784241;daar mist de digital foundry review bovenaan de youtube lijst
RTX 3080;1;0.4809124171733856;Ik begreep anders van het AMD fanboy meubilair in CJ's Radeon topic dat Digital Foundry absoluut niet geloofwaardig is. Althans, door de rood gekleurde bril heen @whyz
RTX 3080;2;0.42631208896636963;En dat klopte uiteindelijk dus ook. Zijn resultaten waren niet in lijn met de onafhankelijke reviewers.
RTX 3080;5;0.4651390612125397;Ook nog wat andere landgenoodjes hebben hun beste gedaan. Movie :
RTX 3080;4;0.2846148908138275;
RTX 3080;3;0.6173166632652283;Handig linkje, maar ik sorteer ze liever op kwaliteit dan op alfabet .
RTX 3080;1;0.3512893319129944;Hardware.info had bovenaan gestaan.....r.i.p.
RTX 3080;1;0.33860117197036743;Waarom zou je die site bovenaan willen hebben dan?
RTX 3080;1;0.2960324287414551;Omdat HWI reviews van zeer hoge kwaliteit hadden, en ook daadwerkelijk luisterde naar website bezoekers als die de volgende keer meer diepgang wouden op dit of dat, lees eens recencies op ''grote'' reviews op tweakers de afgelopen paar jaar, veel commentaar van veel bezoekers.
RTX 3080;1;0.5730348825454712;Helaas vind ik de reviews van HWi niks meer dan belabberd. Staan altijd vol met spelfouten en een zinsopbouw waarbij je gerust de schrijfkwaliteit Nederlands van de redacteur in twijfel kan trekken.
RTX 3080;1;0.45416226983070374;Newsflash: Dit is de review die anders bij Hardware.info had gestaan....
RTX 3080;1;0.36765778064727783;Nieuwsflash: Tomas en Reinoud zijn de Hardware.info redacteuren die altijd moederborden en videokaart reviews op hardware.info geschreven hebben.
RTX 3080;2;0.3502517342567444;Inderdaad, ik dacht dat o.a. Ch3cker de reviewer was aangezien die nogal veel in de ik en wij vorm hier reageerd.
RTX 3080;3;0.4277175962924957;"Ik ben benieuwd waarom Linus tech tips onderaan staat in kwaliteit, ja de reviews zijn minder in-depth dan die van Gamer nexus maar focussen meer op je gemiddelde consument die gewoon denkt ""hoe snel draait dit games "" en dit op een leuke manier gepresenteerd wilt hebben."
RTX 3080;3;0.3084327280521393;Hij moet eigenlijk Antony meer laten doen. Die gast is geniaal.
RTX 3080;2;0.37198978662490845;Ja en nee, Linus heeft meer ervaring met reviews maken en met hardware kloten terwijl Anthony Imo beter is in software en ook goed is met hardware.
RTX 3080;3;0.2416970580816269;"Het zal mij niks verbazen als de tests door Anthony is gedaan. Linus eeft meerdere malen laten weten dat hij het schrijven van scripts leuker vind en vind zich zelf een ""Writer"" en is dus ook de ""Senior Writer"" van zijn bedrijf, wel is hij nog vaak de host maar ook dit wil hij gaande weg veranderen en andere team leden een kans geven dit te kunnen doen"
RTX 3080;5;0.5555264353752136;Anthony idd, een baken van kennis. En presenteren kan hij geweldig.
RTX 3080;1;0.3256576657295227;oh laat maar was al gepost.
RTX 3080;2;0.5672623515129089;Niet erg leuk voor de redactie van Tweakers als de eerste comment een verwijzing naar vele andere reviews is..
RTX 3080;1;0.4374372959136963;Heeft er iemand een FE kaart kunnen bestellen, behalve wat reviewers?
RTX 3080;2;0.5178372263908386;Maken ze de verwachtingen waar? Antwoord nee. nVidia heeft de chips te veel gehyped en de prestaties zitten onder wat nVidia je wil doen geloven. Ja ze zijn efficienter dan Turing maar dat is slechts 8% tov de 2080 Ti dat is heel wat anders dan de 90% die mensen wilde geloven aan de hand van de slides al heb ik toen ook al meerdere keren geroepen dat mensen even goed moeten kijken naar die grafiek en hoe nVidia die claim maakt. Dit alles neemt niet weg dat het wel een mooie stap vooruit is (Avg 32% tov de 2080 Ti). Performance per watt is omhoog. De totale prestaties zijn beter en de prijs prestatie verhouding is beter dan Turing. (maar dat zegt meer over hoe overpriced turing was) Al met al is dit voor gamers wel goede generatie zeker als je al van plan was te upgraden. Nu hopen dat er genoeg zijn. De geruchten hebben het over kleine aantallen en dat ze lang niet genoeg hebben om aan de vraag te voldoen. Ik ben wel benieuwd wat de 3090 straks laat zien ik verwacht zelf 15-20% sneller dan de 3080.
RTX 3080;2;0.4717883765697479;"Verwachtingen van gebruikers die maar het halve verhaal vertellen kunnen ze (natuurlijk) niet waar maken. Deze review tegenover hun eigen verwachtingen heb ik weinig grote verrassingen in gezien. 90% efficienter zonder verdere condities is ook niet geclaimed. Ze hebben geclaimed dat voor hetzelfde performance level als Turing, dat Ampere 1.9x perf/w heeft. Echter bij hetzelfde vermogen volgens deze plot is dat nog maar 50%, en met 320W blijft er nog minder van over. Je kan dat ook narekenen. Volgens de gelinkte plot: Turing 60fps bij 240W (0.25fps/W), en Ampere 105fps bij 320W (0.33fps/W). Resultaat: 31% meer efficient. Pakken we de tweakers getallen er bij: de 3080 verbruikt 31% meer stroom dan een 2080S (250W TGP), terwijl deze 66% meer FPS behaald in Control 4K met RTX. Daarmee is de efficientie verbetering ca. 1.66/1.31 = 1.26x (26% meer efficient). Ditzelfde kan je voor de prestaties doen die bvb door DigitalFoundry op aankondigingsdag zijn vrijgegeven. Bvb 70% meer FPS in Shadow of the Tomb Raider 4K, wederom: 3080 vs 2080S +57.7% in dit artikel. 2080S vs 2080 +3.9% in Super kaarten review. Gecombineerde verbetering: +64% Er zijn wel enkele outliers te vinden (Doom Eternal volgens TechPowerUp review bijvoorbeeld), maar het ontloopt elkaar niet orde groottes. De generatie is een mooie stap voorwaarts, hoewel je wel een beetje terugziet dat de chips erg hard gepushed worden om een ""4K 60FPS"" experience te kunnen leveren. Ik vraag mij af hoe erg dat gaat zijn bij de 3090 om ""8K Gaming"" te kunnen waarmaken. Ik denk dat het leuk staat als ""First 8K GPU"" maar verder puur een parade paardje is.."
RTX 3080;3;0.43033653497695923;Het gaat er om dat nVidia focus legt op 1,9X performance per watt en dat is niet realistisch. Ze vergelijken Turining pushed to the max dus ver boven het meest efficiënte punt van die chip en vergelijken het met Ampere die in zijn sweetspot draait. Als je de meest efficiënte Turing GPU pakt: de 2080 Ti is dat verschil in de reviews ineens niet zo groot. En het is nog maar de vraag of de 3090 veel efficiënter gaat zijn gezien het TDP van die kaart. Dus ik vind dit wel een valide vergelijking. Zoals ik hier boven ook al aan geef ik zou willen dat de tech bedrijven dus niet alleen nVidia maar de andere ook een wat meer realistische presentatie geven. Daar is ook niets mis mee. Deze kaarten doen het veel beter dan de Turing generatie. Waarom moet je het dan nog mooier laten lijken dan het is terwijl de daadwerkelijke prestaties gewoon top zijn gezien de prijs van de kaarten? Dat vind ik gewoon storend en ze proberen gamers gewoon te misleiden.
RTX 3080;1;0.30551543831825256;Waar legt Nvidia de focus op 1.9x performance per watt, op een manier die gamers misleid? Ik heb die informatie alleen gezien in een presentatie waar in een grafiek duidelijk wordt uitgelegd wat ze er mee bedoelen?
RTX 3080;1;0.5347861051559448;Letterlijke woorden van Jensen: Beide claims zijn niet waar. En die grafiek laat het ook zien. Maar later in de slide komen ze ook weer met 1.9X en dat klopt gewoon niet. Het verschil bij vergelijkbaar verbruik volgens de grafiek is 50 vs 60 FPS bij 120 watt. En dat is ook een mooie verbetering als ze dat zo hadden laten zien en het hadden over 20% betere performance per watt helemaal goed en mooi maar de claims die ze maken kloppen niet. Ze zijn ampere in het beste licht aan het zetten en turing in het slechtste. En als je naar de reviews kijkt zie je inderdaad dat er helemaal niets klopt van de claim van Jensen. het is geen 2x efficiency en ook geen 2x performance. Not by a long shot.
RTX 3080;3;0.4004546105861664;"Ok, dit gaat dus om een presentatie waar ze gewoon exact uitleggen wat ze met 1.9x bedoelen. Duidelijk dan waar we het over hebben. Als je daar perse iets anders in wil lezen dan wat Nvidia bedoeld, inclusief uitleg van Nvidia, moet je dat verder zelf maar weten. En als je ""tot 2x"" wilt lezen als ""gemiddeld 2x"" ben je naar iets op zoek. Nvidia heeft in hun eigen presentatie al laten zien dat je dit lang niet in alle workloads hoeft te verwachten. Je wist al lang al dat dit cherrypicked resultaten zijn, op games/resoluties waar Ampere de extra floating point performance goed kwijt kan. Daar heb je de afgelopen week dagelijks over geklaagd. Wat ik eigenlijk nog wel interessanter vind. Hoe heb ik een paar minuten na je reactie al 2x een 2+ moderatie? Onder een nieuws item van een dag oud, met 400+ reacties, terwijl andere oudere reacties volledig genegeerd worden. En dat bij een post waar je woorden cherrypicked, veel meer nog dan Nvidia dat met test resultaten gedaan heeft."
RTX 3080;1;0.45058473944664;Zou je mij dan voorbeelden willen geven waar Jensen aangeeft dat dit gemiddeld anders is? ik zie dat in zijn presentatie niet terug. Ook zijn claims over de grote prestatie verbeteringen zijn extreem cherry picked waar ze games en settings gebruiken die meer dan 8GB VRAM gebruiken waardoor de 2080 helemaal onderuit gaat. Als je dan een 30 game benshmark gaat doen krijg je een totaal ander beeld. Ik wil er niet perse iets anders in lezen dit is wat Jensen zelf zegt en wat heel veel nieuws sites waaronder tweakers 1:1 overgenomen hebben. Ook mensen op fora namen die claims over 1.9x en 2x als waar aan. Of kwamen met hogere waardes dan we nu bij de uiteindelijke reviews zien. Dus ja ik heb toen aan verwachting managment gedaan en gezegd dan men even beter naar de grafieken en slides moet kijken want tov die slides kan het gaan tegenvallen. En dat is nu ook gebleken. Ik zeg dus niet dat Ampere slecht is. Integendeel. Maar ze hadden de cijfers niet op hoeven te blazen. Ampere is as is goed genoeg de performance per watt is duidelijk omhoog gegaan. De prijs is lager. De prestaties tov de vorige generatie zijn meer dan prima. Dus waarom het dan toch mooier proberen te maken dan het is? waarom niet de daadwerkelijke gemiddeldes noemen die mensen ook echt gaan zien? Wat probeer je hier mee te zeggen? En welke woorden cherrypick ik dan? ik quite Jensen gewoon letterlijk. Jij gaf aan dat nVidia dat niet gezegd heeft. Ik geef je de letterlijke quote van de CEO waar uit blijkt dat dat wel zo is. Zelf kom je vervolgens niet met bewijs. Dus ik nodig je uit net als mij met duidelijke slides, quotes en youtube timestamps te komen waaruit blijkt dat nVidia de boel niet mooier probeerde te maken dan het daadwerkelijk is.
RTX 3080;1;0.6942405700683594;Dat mag maar op deze manier zeggen je woorden hierboven dus niets meer. Je wil het bewijs niet leveren. Zonder bewijs kan iedereen van alles claimen. Dit gebeurd heel vaak ook en als je dan naar bewijs vraagt komt het niet. En het is niet dat ik perse mijn gelijk wil halen. Als ik het verkeerd heb wil ik dat met alle liefde toegeven ik ben ook maar een mens en kan wat over het hoofd gezien hebben. Dat zijn compute workloads geen games. Maar ook de geclaimde game percentages (dacht 80% tov de 2080) worden in reviews niet gehaald in praktijk zijn die 10-15% lager. Maar goed als nVidia dat met 2x bedoelt leuk maar totaal de verkeerde doelgroep dat is meer wat voor de quadro launch dit was gericht op gamers. Je hebt gelijk je zei: Het gaat mij om deze slide: Waar ze 1,9x Perf/W claimen. Wat er staat klopt wel maar kijk ook naar de curve van turing die begint af te vlakken omdat die tot de max gepushed wordt of tegen een bottelnek aan loopt. Als je bv een 120 of 200 watt vergelijking gaat maken krijg je een totaal ander beeld. Waarom ik dit ook zeg is dat veel nieuws sites en mensen op foa of in comments die claim van nVidia over namen en er in geloofden. Ik had gewoon liever gezien dat nVidia een wat meer realistische vergelijking had gemaakt dus bv at same power 20% faster compared to turing. Daarnaast laten ze niet zien welke GPU dit is ze geven alleen aan dat het Control at 4K betreft. Kijk je nu naar reviews en de power efficiency tests daar valt dat gewoon tegen tov de 2080 Ti. Dus ben daar wel beniewd of de 3090 dat beter kan doen. Dat soort mensen heb je helaas altijd. Je post heeft onterecht een -1 gekregen. Maar zo te zien is dat al weer recht gezet. Je posts zijn gewoon on topic.
RTX 3080;1;0.5841745734214783;Je hebt zeker een punt hoor. Als je kijkt naar de grafiek zie je dat Nvidia de winst claimt bij een (gecapte?) 60 FPS. De grafiek laat zien dat het energieverbruik inderdaad de helft is van de 2080. Maar dat scenario is helemaal niet realistisch. In de echte wereld is de Perf/W veel lager, eerder in de buurt van 20-30 %. Dus misleiding.... JA,daar kan je zeker van spreken in dit geval,is mijn mening. En ik denk dat de (misleidende) claim juist is gedaan om mensen zand in de ogen te gooien,juist omdat het het energieverbruik zo hoog is. Voor mij heel duidelijk marketing geneuzel,maar bij de gemiddelde gamer gaat het er in als roze koeken..... En daarvan is Nvidia bewust natuurlijk,juist daarom is het zo belangrijk dat sites als GoT dit checken,en mensen er op wijst als claims onzinnig zijn. (wat dus,ook bij GoT niet gebeurt).
RTX 3080;2;0.5197935700416565;Als je de 2080Ti qua prestaties per watt vergelijkt dan zijn de resultaten op zijn zachts gezegd niet bijzonder... Pak je bijvoorbeeld 1440p ultra gemiddeld van deze review en dan het stroomverbruik dan kom je op een verbetering van circa 2% wat imo erg slecht is gezien de node schrink. Bij de reviews die ik tot dusver heb gezien kom ik tot hetzelfde resultaat.
RTX 3080;2;0.4359884262084961;Helemaal eens. Prestaties zijn indrukwekkend en de prijs is aardig, maar daarvoor hebben ze echt ingeleverd op prestaties per watt. AdoredTV heeft net een video online gezet waarin hij speculeert dat Nvidia per se de prijs omlaag wilde hebben en daarom op Samsung gegokt hebben, waarbij NV waarschijnlijk niet van tevoren had kunnen bevroeden dat die 8nm node van Samsung zóveel energie zou slurpen. 320 watt, en daar een groot deel je kast van inblazen, dat lijkt mij niet het beoogde eindresultaat van een bedrijf dat zijn reputatie behaalt heeft op zuinig, stil en koel.
RTX 3080;3;0.25738903880119324;Hier is de video:
RTX 3080;3;0.43400901556015015;Dank. Tis nogal een lang verhaal en hij doet wel redelijk wat aannames. Hoewel ik het niet echt oneens ben met hem had hij feitelijk ook gewoon kunnen zeggen: 'De prestaties / watt zijn slecht, we weten dat Samsungs 8 nm node slechte prestaties / watt heeft maar super goedkoop is, ergo heeft Nvidia voor Samsung gekozen vanwege de lage kosten'. Dat had geen 32 minuten hoeven duren.
RTX 3080;4;0.5019683241844177;Mooie reactie erg informative. Ik vraag mij persoonlijk aff of ik voor een rtx 3070 ga of 3080. Ik ben well van plan op 4k 144hz te gamen. Mijn vraag aan u is zou ik met een 3070 cyberpunk op max settings kunnen spelen denkje of heb ik dan de 3080 of 3090 nodig. Ik weet dat job dat vast niet zekere weet maar als je een schatting deed wat zou je antwoord zijn?
RTX 3080;2;0.4365425705909729;Er zijn nog geen Cyberpunk reviews uit. Als je kijkt naar de huidige games is er nogal een verschil tussen de framerates bij games die RTX en DLSS gebruiken. Dus het is heel moeilijk te zeggen of je een goede framerate gaat halen. Control is bv super zwaar qua RTX. BF5 aanzienlijk minder. 3070 zie ik sowieso niet als een 4K RTX kaart meer als 1080/1440P RTX of 4K zonder. Als je echt alles max wilt en je hebt het budget zal de 3090 de beste keuze zijn. Maar qua prijs prestatie verhouding zijn de 3070 en 3080 aanzienlijk beter en dan zou ik voor de 3080 gaan. Maar goed nVidia kan zo maar met een 3070 Ti 16GB gaan komen en 3080 Ti met 20GB tegen de tijd dat AMD met RDNA2 chips komt. En dan kan het hele speelveld weer anders worden. Je kan daar op gaan wachten maar volgens mij komt Cyberpunk begin nov uit en gezien hoeveel mensen nieuwe kaarten willen is er dan een kans dat zowel de nVidia als AMD kaarten uitverkocht zijn en je dan niets hebt. Dus wil je zekerheid kan je beter zo snel mogelijk iets halen of een 2e hands 2080 Ti voor een leuke prijs. Zelf zou ik toch afwachten. Dan het spel maar wat later spelen. In het begin zijn er vaak ook storende bugs dus een paar weken wachten is dan vaak wel goed dan hebben ze het ea kunnen patchen. Alleen moet je dan niet te veel op internet gaan lezen want dan krijg je te veel spoilers ^^
RTX 3080;1;0.6084969639778137;Heel erg bedankt voor je tijd om dese reactie te schrijven. Is precies de soort reactie die ik wou😀. Ik weet nu zekere dat ik ga proberen de founders edition 3080 te halen, lukt dit niet dan wacht ik lekker aff. Heb namelijk gehoord (moores law) dat bij nvidia de prijzen in october flink omhoog geschroeved worden. Nou nogmaalgs bedankt voor de reactie en pray for me dat ik we rtx 3080 man krijgje aangezien nvdia een zogenoemde paper launch heeft.
RTX 3080;4;0.4655678868293762;Your welcome. Ja ik ben erg benieuwd naar de beschikbaarheid van de Ampere kaarten. En inderdaad of die prijs stelling wel klopt. We hebben het eerder ook al gezien met de 2080 Ti die heeft de MSRP amper gehaald heeft er meestal boven gezeten. Dat kan hier ook gebeuren en dat zou wel erg jammer zijn als dat het geval is. Het is dan nog steeds een goede deal tov Turing maar dan wel minder goed als je er niet meteen op dag 1 bij was.
RTX 3080;3;0.22832410037517548;Geloof het of niet ik het on het moment nog een 750ti er in zitten in combination met een and phenom😂😂😂. Dus mijn performance increase zall hoe dan ook erg merkzaam zijn denk ik zelf zodra ik ook mijn cpu upgrade uiteraard. Als het op cpu's aankomt way you je dan adviseren. Wachten op de nieuwe amd?
RTX 3080;3;0.5039854645729065;Dan moet je de CPU inderdaad wel upgraden ja anders kan die 3080 zijn ding niet gaan doen. De nieuwe Zen3 CPU's worden 3 oktober aangekondigd. Ik denk wel dat dat the way to go zal zijn op dit moment. Ik vraag me alleen wel af of ze ook meteen met een 6 core komen of in het begin alleen 8 cores en meer. Ik denk dat qua prijs prestatie verhouding de 3600 de beste zal blijven. En als je op 4K gaat gamen is de CPU iets minder belangrijk. Dan is de GPU de bottelneck. Dus als je haast hebt nu een B550 + 3600 halen je kan er altijd later een 4700 of 5700 cpu in gooien (geruchten hebben het er nu over dat Zen3 Ryzen 5000 gaat worden). Maar je kan natuurlijk ook 3 Okt even afwachten. Dan een 3600 kopen kan altijd nog die zijn er genoeg. Kan wel zijn dat er daar na een run komt op B550 en X570 borden als mensen willen upgraden naar een nieuwe AMD setup. Maar misschien valt dat ook wel mee omdat veel mensen toch al een B450 of nieuwer bord hebben en dus geen nieuw bord nodig hebben.
RTX 3080;2;0.4410734176635742;Het zou fijn zijn als zen3 Ryzen 5000 gaat worden voor de desktop, maar ik betwijfel het. Dat wordt gewoon 4000 anders heeft de consument te snel door dat de laptop cpus nog op de voorlaatste generatie gebaseerd zijn, en de 5000 serie voor laptops zal nog wel even op zich laten wachten.
RTX 3080;1;0.2535856366157532;In de VS hebben wat mensen problemen met een 3080 FE bestellen tot nu toe:
RTX 3080;1;0.869305431842804;Massive scam AMD HERE I COME
RTX 3080;4;0.33208149671554565;Ik denk dat je deze niet moet vergelijken met een 2080 Ti. Tuurlijk, dit heeft nVdia wel gedaan, maar als je kijkt naar de prijsklasse kun je deze beter vergelijken met een 2080 Super. Als je ziet dat de 3080 ongeveer 50% sneller is dan een 2080 Super (uitzonderingen daar gelaten) vind ik dat echt erg indrukwekkend. Ik ga mijn 2070 (niet-super) in ieder geval upgraden naar de 3070.
RTX 3080;5;0.4321848154067993;Klopt en de 3080 is ook gewoon de opvolger van de 2080S en niet van de Ti.
RTX 3080;3;0.2936737835407257;Persoonlijk zou ik (als je het bedrag kan ophoesten) voor een RTX3080 gaan of wachten op een RTX3080TI. Als je op langere termijn kijkt kan je zo vaak een generatie overslaan waardoor je op het einde van de rit even veel hebt betaald en dezelfde prestaties hebt behouden maar dan met 2 high end kaarten ipv 3 lower end. Overigens zit je met RTX2070 vs 3070 nog steeds met 8GB Vram..
RTX 3080;4;0.46094661951065063;Goed punt! Ga ik denk ik zeker doen.
RTX 3080;2;0.2948457598686218;Maat ook dat is niet wat nVidia je wilde laten geloven die strooide met persentages van 80% die niet gehaald worden. Dus mijn punt is en blijft valide. Ik zou willen dat al deze tech bedrijven wat meer realistische presentaties geven van hun producten en niet altijd maat extreem veel moeite doen om de extreme gevallen er uit te lichten.
RTX 3080;3;0.3176998198032379;Die 2x blijkt te zijn in full RT games (minecraft RTX) kinda verwacht van Nvidia om games te Cherry picken voor de performance nummers
RTX 3080;5;0.4267102777957916;Voor de mensen die de 1000 serie nog hebben is het een geweldige upgrade. Mijn 1080ti koste rond de 1100,-. Nu heb ik straks voor een stuk minder geld de dubbele snelheid. Want die magere 30% van de 2080ti was me niet nog eens 1100+ euro waard.
RTX 3080;5;0.48299023509025574;Helemaal mee eens. Het ligt er ook een beetje aan wat je doet. Als je nu 1440P120 of 144 hebt hoef je de 1080Ti eigenlijk nog helemaal niet te vervangen (of je moet perse RayTracing willen). Wil je 4K dan is het een ander verhaal en dan is het zeker de moeite om voor een 3080 of eventueel RX6900 te gaan. Dit zijn vooral hele interessante upgrades voor mensen met een 980/980Ti of zelfs 780Ti.
RTX 3080;5;0.2984856069087982;Ik heb een 780 ti. Wat maakt deze 3xxxx generaties so interessant voor mij?
RTX 3080;4;0.45452171564102173;Afhankelijk van de resolutie waar je op speelt en de games die je wilt spelen is het een grote performance upgrade. Als je nu een 1080 Ti of 2080 hebt is het verschil niet zo heel groot. De 2080 Ti is al meer dan 2x zo snel: Dus als je meer prestaties nodig hebt is de 3000 of AMD 6000 generatie interessant. Maar als de 780 Ti voor jou nog steeds moet wat hij moet doen en de games draaien soepel gewoon lekker blijven gebruiken natuurlijk
RTX 3080;2;0.43367111682891846;Voor de claims van Nvidia moet je eens kijken naar de resultaten in Blender en V-ray in de video van LTT. In games zijn de prestaties niet zo imposant als geclaimd, maar in Blender en V-ray is de 3080 2 tot 2,5 keer sneller dan de 2080. Die toename in prestaties is echt extreem
RTX 3080;2;0.4815015196800232;Mja dat is leuk maar gamers gebruiken dat niet. Dat is meer voor mensen die quadro en firegl kaarten gebruiken. Dus in mijn ogen niet relevant en misleidend in een presentatie die gericht is op gamers.
RTX 3080;1;0.5426565408706665;In de conclusie wordt onterecht gesuggereerd dat deze founders editie een referentiekaart is: Dat is onjuist. Nvidia heeft in de Founders Edition dit keer een custom PCB gemaakt, de PG133, terwijl de referentiekaart de PG132 is. Bron. Dat is belangrijk als je van plan bent om zelf een custom waterkoeler te kopen: deze gaan niet persé passen op het PCB in de Founders Edition.
RTX 3080;2;0.3977489769458771;Dat ze verbeterd zijn zie je niet terug in de benchmarks. De relatieve verschillen tussen met en zonder DLSS zijn niet groter dan bij de RTX 2080 Ti. Met andere woorden, de 2080 Ti behaalt met DLSS percentueel grofweg dezelfde winst t.o.v. zonder DLSS. Dit bevestigt ook Computerbase.de, vertaald vanuit Duits:
RTX 3080;3;0.384575217962265;Zijn hier het aantal tensor-cores hier ook bij meegenomen? De 3080 heeft er 272 waar de 2080ti er 544 heeft. Als er met de helft van de cores een vergelijkbare prestatie (percentueel) wordt gehaald is dat dan niet een enorme verbetering?
RTX 3080;1;0.3926563560962677;Terecht punt. Ik zie nu trouwens dat het ook in deze review van Tweakers wordt genoemd: Ze weten dus bij de RTX 3080 met de helft van de tensor cores dezelfde prestatiewinst te bereiken als bij een RTX 2080 Ti, inderdaad een verdubbeling van de prestaties per tensor core.
RTX 3080;3;0.3641829490661621;Jammer dat Flight Simulator 2020 niet is meegenomen in de review! Dè game bij uitstek om te testen hedendaags!
RTX 3080;2;0.4486401081085205;Zeker! Dat had een mooie toevoeging geweest. Helaas zit deze game op dit moment niet in onze benchmarksuite, en hebben we daar dus ook nog geen benchmark voor ontwikkeld. Ook is Flight Simulator een game die zeer sterk CPU afhankelijk is, en daarmee minder interessant om te gebruiken voor een 3d-chip review. Wellicht dat we in de toekomst nog eens in de benodigde hardware voor FS kunnen duiken!
RTX 3080;2;0.440755158662796;Wellicht? FS2020 is de game waarvoor reikhalzend naar de 3000-serie is uitgekeken omdat die erg verlegen zit om meer grafische rekenkracht. Dat gonst al heel lang op het internet. En als totale leek ben ik niet onder de indruk van deze benchmarks. Ik had echt geen verdubbeling in prestaties verwacht maar gemiddeld 50% meer had toch wel gemogen, zeker gezien de flinke toename in opgenomen vermogen van de kaart. Maar wellicht is het ook nog een kwestie van driver optimalisatie de komende maanden.
RTX 3080;3;0.465977281332016;In de benchmark van GamersNexus is die meegenomen en daar zie je op 4K maar een minimaal verschil door de enorme CPU bottleneck. Op lagere resoluties is er wel absoluut een sprong zichtbaar.
RTX 3080;3;0.2874678373336792;MSFS2020 presteert ruim beter, LTT.
RTX 3080;2;0.37257447838783264;Is toch 50% meer? Je moet het vergelijken met de RTX 2080, niet met de RTX 2080ti, die zal met de 3090 moeten vergeleken worden.
RTX 3080;2;0.37237441539764404;Nee, die 3090 zal met de Titan vergelijken moeten worden. De 2080Ti heeft geen directe opvolger. Waarschijnlijk komt er nog wel een 3080Ti, en dat wordt dan de opvolger van de 2080Ti.
RTX 3080;1;0.4684946835041046;Ook fout, aangezien er óók een nieuwe Titan in de maak is. De 3090 start een nieuw segment die tot nu toe niet bestond. De 3080 kun je het best vergelijken met de 2080. De verwachting is in het algemeen dat er nog een 3080ti komt... Maar misschien ook wel een 3090ti... We weten het simpelweg nog niet
RTX 3080;5;0.29911771416664124;De directeur van NVidia bracht het in zijn presentatie also de 3090 de Titan opvolgt. Wat er nog gaat volgen in de 30-serie... Zoals je al zegt, we weten het nog niet.
RTX 3080;3;0.4057437777519226;De correcte quote was, dat de 3090 'Titan like performance' krijgt - niet dat het de nieuwe Titan is. Groot verschil is de focus van de kaart - de 3090 is veel meer gericht op gaming dan de Titan.
RTX 3080;2;0.35659876465797424;Aan al de mensen die riepen dat FS niet cpu bottlenecked is.. Hier is het resultaat, de game wordt niet overal getest omdat het de gemiddelde cijfers doet kelderen..
RTX 3080;5;0.34385234117507935;10 mensen onder jouw posten het zelfde en krijgen +1 Dat vind ik behoorlijk onterecht.
RTX 3080;2;0.5082610249519348;Ik had het over de comments in de voorgaande artikels omtrent FS2020. Telkens weer opnieuw fans die uitriepen dat het een goede multi-threaded game is omdat al hun cpu cores benut worden. De traagste schakel is de simulatie berekening die helemaal niet geoptimaliseerd is, en waarschijnlijk zelfs draait op een aangepaste port van de FSX engine. Microsoft ESP is daar dan weer de grondslag van, die dateert al van 2007. Dat is het jaar dat Crysis uitkwam. MSFS is een topgame, maar ik wil meer performance zien en het doet me pijn in het hart dat de VR update uitbundig gebruik gaat maken van misselijkmakende frame creation truukjes.
RTX 3080;3;0.43354958295822144;Op mijn pc r5 3600/rtx2060s is die vooral gpu limited op 1440p. Dus het was wel interessant geweest hoe die zou draaien op een rtx3080
RTX 3080;3;0.34802260994911194;Klopt, wat sommige reviewers doen is FS2020 alleen op 4K Ultra reviewen, waar je wel een GPU bottleneck hebt zelfs met de 3080
RTX 3080;3;0.5417155027389526;Daar zijn al vragen over gesteld in het FS2020 topic, daar is de redactie mee bezig. Maar er zijn nogal wat factoren om een betrouwbare benchmark te doen. Die benchmark test komt dus, maar niet specifiek om de RTX3000 serie, maar meer met het idee 'hoe haal je het beste uit FS2020'.
RTX 3080;4;0.38389265537261963;Dat is goed om te horen. Tip: wacht echt even met het benchmarken zelf tot na de nieuwe patch. Anders kun je alles weer opnieuw doen
RTX 3080;1;0.5425608158111572;De verwachte patch gaat niks doen voor de prestaties
RTX 3080;3;0.4438976049423218;Zeker wel: As far as the patch is concerned, some of the significant offerings that will be arriving through the update are as follows:Performance improvementsATC updatesUI updatesAerodynamic updatesAircraft updatesCockpit visuals and animation updatesGeneral aviation system updatesGeneral aviation avionic updatedAirliner system updatesAirliner avionic updatesLive Weather updates – (e.g. 225/3kt wind fixed, persistency fixed, etc.)Upgraded multiplayer serversMarketplace updatesContent Manager updatesLocalization updatesAccessibility updatesCamera updatesBush Trip updates (e.g. completion trigger fixed/Completionist achievement fixed)World updates En hier nog een mooi plaatje:
RTX 3080;2;0.4092811644077301;Dat is allemaal content gerelateerd, wellicht heeft het indirect invloed op de prestaties. Maar het is niet het doel van de update
RTX 3080;2;0.355405330657959;En die indirecte invloed is wel belangrijk. Er wordt een heleboel gefixed. Spelers zullen per se deze upgrade willen. Als dat de prestaties erg naar beneden drukt, mag dat best bekend zijn.
RTX 3080;5;0.5203822255134583;40 FPS on 4K Ultra present. Aldus Linus
RTX 3080;3;0.5404068231582642;Oef, dat valt me eigenlijk wel tegen. Nou is FS2020 natuurlijk echt een grafische krachtpatser, maar ik had niet verwacht dat hij de 3080 op de knieeen zou krijgen. Ik lees in het FS topic op GoT echter ook dat hele hoge framerates voor FlightSims minder relevant zijn dan bijvoorbeeld shooters, en dat 30FPS prima speelbaar is. Het kan maar zo zijn dat ze daarom ech tot het gaatje gaan qua grafische pracht en praal omdat ze gewoon meer speelruimte hebben.
RTX 3080;3;0.35443565249443054;FS 2020 is ook heel erg cpu afhankelijk. Wellicht zijn de huidige cpu's gewoon een bottleneck voor de 3080.
RTX 3080;2;0.3946565091609955;FS2020 is vooral vooralsnog een DX11 game. De verschillen tussen DX11 en DX12 zijn huge, zeker als het op CPU single/multi threading aankomt. Eigenlijk zit je met DX11 zowel bij Intel als AMD aan een limiet, en zijn alleen de absurd single threaded CPU's een verbetering. Dagelijks leven draai ik een Threadripper en ik speel nu al ~15 jaar een game die van DX7 naar DX12 is gegaan. World of Warcraft. Blizzard heeft deze expansion vooral rond de ~8.2 tijd enorm veel gezet in betere GPU scheduling door juist DX12 en nieuwe CPU's te gebruiken. Ik zie dus nu dat WoW heel veel meer cores is gaan gebruiken, maar, belangrijker, m'n FPS is bijna verdubbeld zonder de settings te verlagen in een drukke zone als Boralus harbour. FS2020 zal ook nog een DX12 upgrade krijgen, en ik heb er goeie hoop voor. Dat terzijde... FS2020 is misschien het soort spel wat niet persé >100FPS nodig heeft, en nog controversiëler: misschien is 30 zelfs wel genoeg...
RTX 3080;3;0.5115111470222473;Volgens mij is dat echter niet bij iedere game zo, bij WoW is DX11 v.s. 12 inderdaad goed merkbaar in bepaalde situaties / configuraties, echter in een game als Total War: Warhammer 2 maakte het dan weer vrijwel geen verschil, sterker nog, DX11 was regelmatig een stuk sneller.
RTX 3080;3;0.3780106008052826;Omdat het daar meer een bolt on was. Bij de nieuwere versies van Total War maak het wel meer uit dacht ik (ik heb ze helaas niet liggen alleen Total Warhammer 1 en 2). Sowieso waren Total War spellen eigenlijk altijd CPU limited. Dat ze eindelijke de stap naar DX12 hebben gemaakt is gewoon goed. In context, mijn cpu heeft maar een paar cores die belast zijn met Total Warhammer 2 en mijn gpu zit uit zijn neus te vreten op de world map. Tijdens gevechten wordt hij meer belast, maar nog steeds maar max 80%. Getest met een 3950x en 5700XT.
RTX 3080;3;0.30338630080223083;30 genoeg? Potentieel. Meer = beter? dat is zeker. Dus waarom ook niet vernieuwen naar DX12?
RTX 3080;3;0.4736179709434509;60 FPS lijkt me wel beter want ook in een 'trage' sim wil je dat alle beweging er gesmeerd uitziet. Als met DX12 en andere softwarematige verbeteringen gemiddeld 60 FPS wordt gehaald op 4k ultra dan hebben we denk ik een sweet spot te pakken namelijk een fijne frame rate zonder absurd dure hardware te hoeven kopen.
RTX 3080;2;0.40846937894821167;Hier moet ik wel op toevoegen dat in dit soort spellen 30FPS al als behoorlijk smooth en vaak zelfs goed genoeg gezien wordt. Historisch is zelfs dat al erg lastig en is 40-60FPS in een FS die er zó goed uitziet al ongehoord knap.
RTX 3080;3;0.44608259201049805;Daar ben ik het mee eens, maar 50-60FPS op Ultra settings en 4K door alleen een update lijkt me niet erg bereikbaar
RTX 3080;3;0.26032572984695435;In deze review zitten ook twee DX11 spellen...
RTX 3080;3;0.4429647624492645;Hangt er natuurlijk een beetje vanaf hoe je FS gebruikt. Met een beetje stuntvliegen is het veel leuker om soepel, vloeiend beeld te hebben. Veel fps is nog belangrijker als je - in de toekomst - VR wilt gaan gebruiken. Dan is 30fps echt niet meer genoeg, dan wil je 8-90fps. Wanneer de Cpu geen bottleneck is, zou een 3090 dat toch wel moeten kunnen leveren.
RTX 3080;3;0.5570665597915649;Valt mee. Met de reviews van de 2080 Ti staat een 12 core CPU redelijk rustig aan te draaien. Af en toe loads naar 70% op alle cores, maar meestal wel rond de 30 tot 40%, dus ik betwijfel of een 3080 zoiets dan dicht kan trekken.
RTX 3080;3;0.37339916825294495;Het is prima mogelijk om in FS2020 50-60 fps te halen door de opties iets te tweaken. 99% van de mensen zal het verschil niet eens zien in de settings die er echt toe doen qua fps. Dus zo erg is het nou ook weer niet met dat spel . Zie een recent filmpje van Jayztwocents over welke settings het dan gaat.
RTX 3080;5;0.249740332365036;Vooral de bomen een klein tikje terug. :-)
RTX 3080;3;0.3950590491294861;40 FPS op 4k Ultra noem ik nou niet bepaald 'op zijn knieen'. Maar misschien heeft ie niet 's nachts over de windmolens in Nevada gevlogen.
RTX 3080;4;0.3220549523830414;Overdag op JFK met multiplayer aan en bewolkt/regen
RTX 3080;5;0.41510626673698425;Dat zal ook wel werken, ja!
RTX 3080;4;0.42169201374053955;Digital foundry heeft een erg goede analyse van MSFS, als je dat interessant vindt
RTX 3080;1;0.24309080839157104;En min > 33 FPS
RTX 3080;5;0.3525385558605194;Je moet niet alleen naar Linus kijken, kijk ook (vooral) naar GamersNexus. Die had rond de 52-53 fps met alle details die je nodig hebt.
RTX 3080;1;0.3415969908237457;Volgens mij hadden ze niet alles maxed maar geprobeert zoveel mogelijk de cpu bottleneck weg te halen. Dat behandelen ze o.a. in hun video daarvoor waar ze hun manier van testen uitleggen.
RTX 3080;2;0.46679720282554626;prestatiewinst in die game valt tegen, lijkt erg CPU limited?
RTX 3080;3;0.47082775831222534;Daar lijkt het op ja, maar FS2020 is ook niet zo heel goed geoptimaliseerd voor processors met veel cores en relatief lage kloksnelheden. Toch was ik heel benieuwd. Deze week verschijnt trouwens patch #2 die aanzienlijke performanceverbeteringen moet meebrengen. Dus wellicht handig om tegen die tijd opnieuw te benchmarken.
RTX 3080;5;0.3634468615055084;Die zijn ze nog aan het downloaden
RTX 3080;5;0.5518624186515808;Net boven de 40 fps in 4k Dus speelbaar 😊
RTX 3080;4;0.2846530079841614;In de review van LTT wordt hij besproken
RTX 3080;5;0.4333932399749756;Gamers Nexus
RTX 3080;5;0.230128675699234;Gamers nexus heeft wat tests, pas met 4K niet meer cpu limited op een 5.1 Ghz intel met supersnel memory.
RTX 3080;3;0.2874179482460022;flight sim is cpu limited ivm dx11, zelfs op 4k
RTX 3080;4;0.42379459738731384;Linus tech tips heeft een leuk grafiekje met 4K ultra:
RTX 3080;5;0.3733896017074585;eens
RTX 3080;1;0.40458035469055176;Ik denk precies hetzelfde! Deze nieuwe 4k game was perfect geweest! Doodzonde dit. Gauw op zoek naar een andere review
RTX 3080;1;0.8715906143188477;Zo slecht, de keuze om deze kaart niet met een 10900k te testen. Je wil toch de snelst mogelijke game cpu om deze game gpu maximaal van data per tijdseenheid te voorzien? (Zeker bij de sub-4k-tests.)
RTX 3080;2;0.25013911724090576;Er is een hele goede reden om deze kaart op Ryzen te testen, namelijk PCie 4.0 welke intel niet ondersteund. Je wilt de maximale performance van een GPU testen, dat gaat op 4K met PCie 4.0 en een CPU met genoeg performance.
RTX 3080;1;0.45470717549324036;tja, en ook in hetzelfde artikel tonen ze aan dat PCIe 4.0 er niet toe doet. Dus toch ook effe op een i9 lopen nu we weten dat PCIe 3.0 goed genoeg is?
RTX 3080;3;0.5066711902618408;Om dat te testen moet je het wel ondersteunen HardwareUnboxed heeft in hun review een vergelijking en geeft aan dat het verschil tussen Intel en AMD slechts een paar procent is, en ook alleen op lagere resoluties. Op 4K is de performance identiek.
RTX 3080;3;0.4017832279205322;"ja idd, dat was ook te verwachten. 4k ultra is nog altijd GPU limited (wat je hier wilt, voor een GPU test), alvast met deze kaarten (straks mss niet meer met RTX3090?). Voor lagere resoluties/settings, is het mogelijk om CPU limited te zijn. Gemakkelijker met DX11 trouwens. Het zou leuk zijn zou je een indicatie hebben ""CPU limited"" of ""GPU limited"" bij een resultaat, maar het staat er in de tekst soms. Voor je eigen PC thuis zou een tooltje handig zijn om het aan te geven."
RTX 3080;1;0.4075528085231781;Midden in het Ryzen Era? Dikke 3900, of een Threadripper als je echt de stoer mijnheer/mevrouw uit wil hangen.
RTX 3080;2;0.4050561785697937;Pci-E 3.0 vs Pci-E 4.0 maakt minder uit dan het verschil in cpu kracht tussen Intel en AMD. Een Intel platform is alsnog sneller qua cpu kracht. Edit: ik bedoelde dus in gaming.
RTX 3080;2;0.367939293384552;"Over de 3080: ""The gap between current AMD and Intel CPUs at 4K is basically nonexistent. In fact, minor differences in board design, firmware, or just 1% variability in benchmark runs all become factors."""
RTX 3080;2;0.46343734860420227;Tja, wel logisch op 4k... meer een gpu bottleneck dan een cpu bottleneck. Niet iedereen wil op 4k 60 fps gamen Dan kan de i7 4770k ook nog wel meekomen... Ik speel zelf op 1080p low (cpu bottleneck scenario) om zo dicht mogelijk bij- en stabiele 144fps (144hz monitor) te halen... 60 fps of 75 fps vind ik gewoon waardeloos. Dat heeft allemaal met input lag / latency te maken. Vanuit ervaring kan ik vertellen dat Intel gewoon merkbaar sneller is in dat scenario. (Hunt: Showdown... nogal een cpu heavy game.) En ook niet voor niets dat mijn 9700k op 5ghz all cores werkt en mijn 3600 cl15 ram op cl14 (beide overgeklokt). Alle beetjes helpen. Maarja dat had je aan de 1080p resultaten op de gelinkte website zelf ook kunnen zien Feit is dat Intel gewoon nog steeds sneller is in games... als je kijkt naar de gelinkte website dan zijn de verschillen tussen de AMD 3900x en de Intel cpu's op 1080p nogal significant!
RTX 3080;3;0.5464360117912292;Klopt maar we moeten dus wel degelijk onderscheid maken tussen 4K en lagere resoluties wanneer het gaat om verschillen in CPU's.
RTX 3080;1;0.23964093625545502;in gaming* In elk ander vak rekent een AMD van dezelfde prijs de intel eruit.
RTX 3080;1;0.3725423812866211;Dat klopt inderdaad, ik staar me daar op blind
RTX 3080;2;0.6034494638442993;dat valt dus wel tegen, ligt helemaal aan de software en geheugen gebruik. Ik heb een overstromingsmodel dat heel veel matrix berekeningen doet en de code is parallel gemaakt met openmp (sourceforge/openlisem). Het blijkt dat mijn overklokte 3900x met alle 24 threads bijna net zo snel is als de i7-1064 met 8 threads van mijn werk surface pro 7! En dat schaalt gelijk met 1,2,4,8 cores. Het blijkt doordat de code niet geoptimaliseerd is, de cores helemaal niet op 4200MHz draaien, de bottleneck is veel te veel lees en schrijf werk van en naar 2D pointer arrays met doubles. mijn code is misschien bagger hoor, maar mijn punt is dat het er helemaal aan ligt hoever de software geoptimaliseerd is voor parallele berekeningen. En dat is volgens mij lang niet alle software.
RTX 3080;2;0.4821917414665222;Tsja dat lijkt echt gewoon aan je geheugensnelheid te liggen en dus aan jouw code of manier van rekenen. Dat de cores niet omhoog klokken is omdat ze niks te doen hebben, dus daar gaat ook een snellere CPU je niet bij helpen. Waarschijnlijk haal je met een 3700x dan net zoveel performance. De meeste serieuze software waarbij dit een verschil maakt is daar goed genoeg voor geoptimaliseerd, anders kies je het pakket van de concurrent welke sneller is. Gelukkig is er in de benchmarks en real-world software beter over nagedacht dan jouw n=1. Beetje onterecht om op basis daarvan een algemeen correct bevonden claim als onjuist te bestempelen.
RTX 3080;1;0.48995891213417053;ik weet niet waarom je je zo aangevallen voelt, ik geef alleen maar aan dat de software een grote rol speelt. natuurlijk kan mijn code bagger zijn. maar ik geloof er niks van dat alle software geoptimaliseerd is voor parallelle computing en dus is het blind aannemen dat de 3900x rondjes draait om de intel niet terecht.
RTX 3080;2;0.4147617518901825;"ik voel me niet aangevallen, maar jij maakt een onterechte vergelijking. een ferrari is ook bloedsnel maar voor een "" race "" over de trajectcontrole op de a2 ( je memory - bottleneck, of misschien een beter voorbeeld : in de file ) tegen een eco - boodschappenkarretje zal in een behoorlijk gelijkspel eindigen. dus "" blind aannemen "" dat een ferrari sneller is dan een random eco - boodschappenkarretje is niet terecht? gaming is vaak minder geoptimaliseerd voor parallelle computing, dus daar zie je inderdaad dat intel vaak iets beter presteert met een lagere corecount. kijk nog eens naar de benchmarks die gebruikt worden en wat ze testen. daaruit volgt dat de claim dat de nieuwe ryzens over het algemeen beter presteren dan de nieuwe intels. natuurlijk hangt dat van je workload af, dus in sommige gevallen kies je inderdaad voor intel. in jouw geval maak je dus geen keuze ( of boeit die niet ) en dat is helaas puur te wijten aan de kwaliteit van je software. het klinkt alsof jouw probleem veel baat kan hebben bij een hoge corecount, maar dan moet je iets aan je memory - bottleneck doen, waarschijnlijk door je berekeningen op een iets andere / slimmere manier uit te voeren zodat de data waarmee je werkt in de cpu - cache kan in plaats van elke keer uit het geheugen moet komen waar je met alle cores tegelijk aan het strijden bent om bandbreedte. als je dat issue gefixt krijgt weet ik zeker dat "" de 3900x rondjes draait om de intel "". late edit : ik kan ook slechte software maken die bijvoorbeeld op een geforce rtx 3080 20fps haalt en op een geforce 256 ook. met die software zeg je dan ook helemaal niks nuttigs over de prestaties van beide kaarten ten opzichte van elkaar. daarom test je ook met fatsoenlijk geschreven software die op een bepaald werkgebied de maximaal mogelijke prestatie uit de aangeboden chip zal halen, en gebruik je verschillende van dat soort stukken software om een gewogen oordeel te kunnen vellen over veel verschillende werkgebieden."
RTX 3080;1;0.5232421159744263;Gebruik je een Intel compiler? In dat geval mag je de performance niet vergelijken met AMD: Intel weigert sinds jaar en dag SSE2+ instructies te ondersteunen voor concurrerende processors in hun compiler (laat staan AVX+) waardoor software op concurrerende hardware geniepig een stuk slechter werkt. Zie bijvoorbeeld: of de piepkleine disclaimer onderaan (Intel's compilers may or may not optimize to the same degree for non-Intel microprocessors for optimizations that are not unique to Intel microprocessors...)
RTX 3080;3;0.2887599766254425;ik gebruik mingw g++, met Qt? ik weet niet of dit geoptimaliseerd is voor intel?
RTX 3080;2;0.40722745656967163;Zonder de Intel compiler ben je meestal veilig voor de cripple AMD functies, dan is dat waarschijnlijk niet de reden voor je performanceverschil. Tenzij je ergens in je project of keten werkt met de Intel Math Kernel Library (MKL). Het wordt verdacht als je mkl*.dll/lib of libiomp5md.dll/lib tegenkomt
RTX 3080;3;0.4428931176662445;Ligt er een beetje aan of je AVX512 gebruikt of niet. Zoja, dan wint Intel, anders wint AMD. Nou is AVX512 voor een hoop rekenwerk best de moeite waard.
RTX 3080;2;0.4440867304801941;Het maakt voor een RTX 3080 niet uit of deze in een pci 3 of 4 slot zit, het verschil is maximaal 1% of minder. Intel blijft performancekoning op alle resoluties, alleen ligt die op 4K zo dicht tegen AMD aan dat het niet meer uitmaakt welk merk cpu je hebt waardoor AMD de interessantste wordt.
RTX 3080;3;0.5051750540733337;Dat wisten we pas nadat dit getest is. Makkelijk praten weer. Ik vind het heel slim van Tweakers om dit zo te testen. Kunnen ze ook meteen ervaring op doen en dat kan alleen maar voordelen opleveren bij komende producten die hier wel gebruik van maken.
RTX 3080;3;0.36822640895843506;Er zal vast wel ergens een review komen waar men met een Intel platform op 1080p test. Ik gok op een review van een 360Hz monitor.
RTX 3080;2;0.4174564480781555;Voor 4K maakt het sowieso niks uit bij deze kaart, en voor 4K had het wél uit kunnen maken of we PCI-e 4.0 of 3.0 hadden gebruikt. Daarom is de keuze gemaakt om met de 3900XT icm PCI-e 4.0 te testen. Uiteraard betekent dit dat er bij een gering aantal games, vooral bij Far Cry New Dawn, een CPU-bottleneck optreed bij de lagere resoluties. We wilden liever een eventuele bottleneck op 4K voorkomen, dan een bottleneck op 1080p.
RTX 3080;2;0.28316351771354675;"Interessant genoeg als we de cijfers van mindfactory.de ( de grootste PC winkel in DE ) bekijken, dan is AMD verkoop 78% tot 91% ( ) vs Intel's 9% tot 23% in het afgelopen jaar. Blijkbaar kopen mensen Intel niet enkel meer voor die ""gaming"" CPU en hebben ze geen enkel probleem om die paar procent extra of te offeren voor goedkopere prijs of meer cores. Voeg daarbij dat AMD de enige zijn dat PCIe 4.0 aankunnen. En ja, zelf al geeft PCIe4.0 weinig voordeel, dat ontdekt je enkel als je ... AMD gebruikt in je tests."
RTX 3080;4;0.49274110794067383;"Mee eens! Op 1080p maakt Intel (ook de 9900k) gehakt van de 3900x. Enkel in Red Dead Redemption 2 zijn de verschillen ""klein""."
RTX 3080;2;0.5334656238555908;"ik zal het er geen doekjes om winden ; ik vrees dat een deel van deze review linea recta de prullenbak in kan : f1 2020 @ 1440p techpowerup : 136. 9 fps 2070 super vs 240 fps @ 3080 = 175 % tweakers 137. 5 fps @ 2070 super vs 181. 1 fps @ 3080 = 131 % dat zijn enorme verschillen! hiermee onstaat overduidelijk een verkeerd beeld. de oorzaak is snel gevonden : beetje jammer dit. leuk idee dat pci 4. 0 wellicht belangrijker is dan pci 3. 0, maar dit kun je testen. sterker nog het is al getest en de conclusie is ook duidelijk ; het is nu niet belangrijker. 3900xt pci 3. 0 220. 6 fps3900xt pci 4. 0 224. 4 fps ( + 1. 7 % ) 10900k 236. 3 fps10900k ( max pl ) 238 fps9900k oc 240. 1 fps ( + 8. 8 % ) dit zou verder geen verrasing moeten zijn. er is geen reden om aan te nemen dat dit met pci 4. 0 anders gaat zijn, en het blijkt ook dat dit niet anders gaat dan met pci 2. 0 en pci 3. 0. tegelijkertijd hebben we regelmatig periodes gehad waarbij de cpu de bottleneck werd. zo lang het gros van de games niet perfect schaalt met een hoog aantal threads blijft het testen van de nieuwste generatie videokaarten dan ook voorbehouden voor de modellen met de beste performance op ( semi ) single threaded. wat bedoel ik dan met semi single threaded? dat is de hoogste uitkomst van ghz * ipc maar dan met in acht neming van een minimum hoeveelheid threads. dat minimum lijkt in 2020 8 threads te zijn. jammer! testen van de snellere gtx 3090 met een cpu die op 4. 2 ghz is vastgezet zal natuurlijk nog problematischer worden. dan is dat maar alvast gezegd. laten we hopen dat amd opschiet met ryzen 4xxx, dit is het moment om het ( gaming ) gat met intel te dichten. geeft meteen kopers van ryzen 3xxx een reden te upgraden."
RTX 3080;2;0.45226144790649414;Wij testen F1 2020 met de slechtst mogelijke weersomstandigheden, zoals ook in de review aangegeven. Dat heeft uiteraard invloed op de scores. Ook het gekozen circuit heeft invloed op de scores. Gezien TPU al deze settings achterwege laat is het vrijwel onmogelijk om de resultaten eerlijk te vergelijken. Op 4K, waar deze kaart voor bedoelt is, is er geen verschil tussen de 3900XT en de 10900K, ook niet volgens TPU. Omdat er wél een verschil had kunnen zijn tussen pci-e 3.0 en 4.0 op de 4K resolutie, de resolutie waar we deze kaart op willen beoordelen, is er voor gekozen om voor de 3900XT te gaan. XFR is daarbij uitgeschakeld omdat dit extreme variaties geeft in de scores tijdens het benchmarken. Intel heeft simpelweg nog geen platform beschikbaar met pci-e 4.0. Gezien de 3090 nóg sneller is dan de 3080, zou het zomaar kunnen dat deze kaart wel profijt heeft bij pci-e 4.0. De keuze was dus : gaan we voor de hoogst mogelijk scores op de lagere resoluties, of gaan we voor de hoogst mogelijke scores op de hogere resoluties. Omdat dit kaarten zijn die je wilt beoordelen op basis van de hogere resoluties, zijn we voor dat laatste gegaan en hebben we twee systemen met pci-e 4.0 gebouwd, met dus een AMD processor.
RTX 3080;2;0.3647877275943756;Is dat zo? Of kwam die bewering Nvidia goed uit gegeven de huidige generatie processoren? Ik tel op 6 van de 10 door jullie geteste games resultaten onder de 120/144 Hz op 1440p (al dan niet door die cpu bottleneck!). Low latency gaming is het voorlopig aan het winnen van 4K. Natuurlijk is het mooi dat we straks 144 Hz op 4K kunnen gamen maar voorlopig is dat nog een lastig verhaal, al is het maar door de prijs van een dergelijke monitor die begint bij een euro of 900 een doorloopt tot dik in de 2000 euro. Er zijn al boze tongen die beweren dat deze kaart te weinig vram heeft voor een toekomst in 4K. Maar goed wat dat betreft ben ik meer van eerst zien dan geloven. Maar het punt was nu juist dat die hypothetische trade off tussen pci 4.0 en 3.0 niet bestaat. Ook op 4K laat TechPowerUP zien dat het Intel systeem sneller is op 4K. Dus je keuze was helemaal niet 4K resultaten óf lagere resuloties. Er was geen reden om de 1440p (en lagere) resultaten te compromiteren. Al zou dit de meest waarschijnlijke uitkomst zijn, dat is het wat mij betreft niet, dan nog ga je dat dat éérst vaststellen vóór je een algemene review gaat maken. Sterker nog, dan heb je meteen materiaal voor een review die uitzoekt wat het nut is van PCI 4.0. Verder gaat het niet om Intel vs AMD, het gaat om het platform waar je de snelste resultaten mee haalt. TPU heeft laten zien dat voor élk scenario dat momenteel 'toevallig' het platform van Intel is, ondanks PCI 3.0. De kans is aanzienlijk dat dit met Ryzen 4xxx niet langer het geval is, niet wanneer de boel daar 300 Mhz stijgt in combinatie met een +15% IPC. Vervoglens is de kans aanzienlijk dat wanneer Intel in 2021 met rocket lake komt de boel weer omdraait (nieuwe architectuur). Dat hoort er een beetje bij. edit: Naast TPU heeft eurogamer (digital foundry) ook getest met pci4 vs pci4 vs intel 10th gen en ook die heeft geconcludeerd dat ook voor 4k de prestaties op het Intel platform beter waren:
RTX 3080;3;0.25069499015808105;Er komt nog een 3080 met 20GB geheugen, en een 3070 met 16GB geheugen. Dus als je je zorgen maakt kan je nog even wachten op die uitvoeringen.
RTX 3080;4;0.6366309523582458;Goed punt inderdaad, bij een videokaart test willen we vooral zien wat die kaart kan zonder dat andere hardware een beperkende factor is. Maar als je kijkt naar de resultaten van Techpowerup met een AMD 3900XT op 1440 en met PCIe 4 dan halen ze daar ook 222,4 fps. Dus ze zullen de resultaten ook op een iets andere manier berekend hebben dan ze hier bij Tweakers/HWI hebben gedaan.
RTX 3080;3;0.26861119270324707;Dat komt hoogst waarschijnlijk omdat ze daar de cpu lekker laten boosten (3900XT is max 4.7) en niet kunstmatig vertragen. Edit: Bevestigd AMD Ryzen 9 3900XT 3.9 to 4.7 GHz (Zen 2, 64 MB Cache) Intel Core i9-10900K 3.7 to 5.3 GHz (Comet Lake, 20 MB Cache) Intel Core i9-9900K @ 5.0 GHz (Coffee Lake, 16 MB Cache) De 10900K op een vaste x52/x53 OC zal op dit moment de beste resultaten geven.
RTX 3080;2;0.3732195794582367;De 3080 haalt maar 13GB/s op PCI-e 3.0? Zouden ze PCI-e 3.0 afknijpen om vervolgens te kunnen zeggen dat PCI-e 4.0 in toekomstige titels beter werkt?
RTX 3080;1;0.5070077180862427;Dit is gelukkig niet het geval, we hebben dit op een native 3.0 gedubbel checked. Op 3.0 native platformen haal je ook 13+-GB/s
RTX 3080;1;0.5474310517311096;Ik ben sowieso over. Voor 750 euro een 2080TI on steriods. Ik kom van de 1080TI dus dat zal sowieso een grote stap zijn.
RTX 3080;4;0.5047517418861389;Voor een 1080ti is het zeker een goede overstap. Met mijn 2080ti op 1440p is het een stuk minder...en denk ik dat ik het zo even hou.
RTX 3080;3;0.23739556968212128;"Ik snap ook echt die mensen in V&A niet. Heb je de beste videokaart die er te krijgen was, ga je die met dik verlies na een jaar verkopen, want je MOET de RTX30xx hebben. Kan je beter de 2080Ti ""opgebruiken"" totdat die vervangen moet worden. Je kunt toch zeker nog 2 a 3 jaar heel veel plezier hebben van een 2080Ti. Je zou wel gek zijn om die voor €500 te dumpen, terwijl je €1200 hebt neergelegd."
RTX 3080;3;0.5141476988792419;Sommige mensen kunnen de extra fps toch gebruiken. Ik speel bijvoorbeeld veel race sims in VR waarbij ik afhankelijk van de game toch nog aardig wat settings omlaag moet zetten. Met de HP Reverb G2 wat een veel hogere resolutie heeft zal dit nog veel meer het geval zijn. Het is dus zeker niet altijd een kwestie van per se het nieuwste moeten hebben. Ik weet overigens nog niet of het mij waard is om van een 2080 ti naar een 3080 te upgraden, hier ga ik me nog eens over bedenken. Het is maar net wat je er voor over hebt en wat je te besteden hebt he.
RTX 3080;3;0.3532788157463074;Precies. Ik game zelf op 1440p en als ik zie dat deze kaart daar maar gemiddeld 18% meer performance levert, is dat eigenlijk teleurstellend. (ze gaven aan dat die 2x zo snel als een 2080 zou zijn, maar dat haalt die bij lange na niet) Ik heb gelukkig niet 1200 maar 870 bij de outlet betaald, maar dan nog ga ik niet upgraden naar de 3080. Je zou maar je rtx2080ti hebben verkocht en voornamelijk op 1080 of 1440p gamen, dan ben je wel sneller natuurlijk...maar of het dan de moeite waard is?
RTX 3080;3;0.5279884338378906;Die 200 euro dat je extra moet investeren krijg je deels ook weer terug als je later naar de 3080ti of 4080 upgrade Maargoed ik laat mijn 2080ti nog wel even zitten, toch wel nieuwsgierig naar AMD want de 3080 valt mij iig wat tegen en de 3090 is het geld niet waard (want dan heb ik over 2 jaar weer hetzelfde als nu met de 2080ti).
RTX 3080;1;0.5702716708183289;Als je gaat praten over zinnig geld investeren in videokaarten dan koop je per definitie geen kaart van 1200 euro. De prestatie per euro van de tragere kaarten zijn altijd veel beter. Een dure kaart kopen in de hoop er vervolgens 4-5 jaar met top of the line performance te zitten is gokken tegen de ontwikkeling in. Wie rationeel een 2080Ti koopt wil persé de beste prestaties en is bereid daarvoor een X bedrag aan uit te geven. Diezelfde groep kan prima upgraden zodra er iets anders nóg sneller is.
RTX 3080;2;0.4849492311477661;980TI hiero op 1920x1200..., als ik kijk dan wacht ik nog wel een generatie... 4K hoeft van mij echt niet. Speel toch meer minecraft, GPU snelheid is best nutteloos hiero en RPG's... Waar FPS niet echt een super-must is. Als het maar boven de 30 blijft.
RTX 3080;4;0.38979142904281616;En met die resolutie kan ik met een vrij hoge zekerheid wel stellen dat het bij de 60 FPS ook wel ophoudt voor je monitor
RTX 3080;5;0.4780102074146271;8 jaar oud beestie, *aait*, doet het gewoon goed...
RTX 3080;3;0.48764732480049133;Het is best mooi hoor, om Minecraft te spelen met 144hz en een render distance van 64 chunks Hoewel daar het geheugen/processors ook enorme bottlenecks zijn.
RTX 3080;3;0.3874978721141815;Of iets slimmer zijn zoals mij en voor de persconferentie €900 vangen voor de TI. Ik ga er lekker makkelijk op vooruit😁
RTX 3080;4;0.3512405455112457;Die gozer voelt zich een kneus nu zeg. Maarja goed gedaan! je gaat er 25% op vooruit en je kan van het verschil paar dikke titels kopen of 2x met je vriendin lekker uit eten.
RTX 3080;2;0.4691777229309082;Hm... niet helemaal! Bekijk het anders: voor minder dan 25% (€250 o.i.d., in ieder geval onder de €300) extra haal je 25% hogere prestaties. Of je dat dan ook moet doen is een keus, ik kan me ook best voorstellen dat de hard-core gamer met grote beurs van de 2080ti naar de 3090 gaat. Dat dat dan wellicht wat duur is interesseert die mensen dan minder...
RTX 3080;1;0.40461209416389465;Eindelijk iemand die verstandig antwoord, hoeveel 2080ti's ik hier wel niet zie @vraagenaanbod voor de premium hoofdprijs is echt te gek voor woorden...
RTX 3080;3;0.32085683941841125;Ik zou even wachten totdat Nvidia een 3080Ti uitbrengt. Dat zal voor 2080Ti gebruikers veel interessanter zijn.
RTX 3080;2;0.3711051642894745;Idem hier. De 2080 (super) en Ti's waren héél duur voor een marginale winst. Op 1440p is deze gemiddeld (als ik het grafiekje hier goed gelezen heb) zo'n 63% sneller. Dat is zéker de moeite. De OC kaarten van de 3rd parties zijn mogelijks zelfs nog ietsje sneller.
RTX 3080;5;0.529146671295166;Dit dus inderdaad. Heb zelf een 1070 TI dus een 3080 wordt dan echt wel weer een beste upgrade . Nou nog hopen op goede beschikbaarheid de komende weken.
RTX 3080;4;0.40072911977767944;Heb een 1070, dus ik sta ook in de rij om te upgraden. Ik wacht nog even testen van de verschillende varianten af, bijvoorbeeld de MSI kaarten. De rest van de pc is vorig jaar geupgrade, dus zal een aardige sprong gaan maken, excited!
RTX 3080;5;0.6575358510017395;Zelfde hier! Ook een 1070 maar vorig jaar een 3950X gekocht dus kan ook niet wachten. Veel plezier alvast
RTX 3080;2;0.45662999153137207;Jammer de Palit kaarten nu van die decoratiemeukjes boven de fangaten hebben gemaakt. Dat was altijd mijn favoriet. Ik ga deze ronde eens voor zo'n KFA2 kaart.
RTX 3080;5;0.3699014186859131;Ik kom van 960 4GB....wie wat bewaart heeft wat!
RTX 3080;2;0.29706329107284546;Ik heb een R9 380 uit 2015. Telkens als ik teleurgestelde commentaren lees moet ik mij er aan herinneren dat men het met de 2080 Ti vergelijkt, en een upgrade naar de 3080 voor mij eigenlijk altijd het geld wel waard zal zijn. Het is immers niet erg nozel om een 2080 Ti nieuw of afgeragd te kopen, en ik zou niet weten welke andere optie in de buurt komt. Ik ga morgen in ieder geval een poging wagen om een FE te bemachtigen. Lukt dat niet, dan kijk ik wat AMD doet, maar zelfs als ze iets soeps op de markt brengen, lijkt het me stug dat ze Nvidia dermate weten te overtreffen dat je spijt krijgt van je 3080. Men heeft het over CPU bottlenecks, en ik ben benieuwd wat er nodig is om dat zoveel mogelijk te voorkomen. Ik kijk op dit moment uit naar de Zen 3 opvolger van de 3900XT, dat zal hoe dan ook wel een verbetering zijn ten opzichte van mijn huidige Phenom II X6 1090T
RTX 3080;2;0.45305323600769043;Wacht nauw eerst nog even de AMD aankondiging af. Je mag verwachten dat ze daar ook grote stappen hebben gezet qua performance maar de Radeon kaarten worden gegooid ik op 7nm van TSMC gebakken, wat nogal wat zuiniger zou moeten zijn dan het 8nm procedé van Samsung. Sowieso is het altijd goed even af te wachten wat de concurrentie doet.
RTX 3080;5;0.2436402589082718;Heb vorig jaar december de 2080 Gaming gekocht voor bijna e 900, als ik dit zo lees is die geen fluit meer waard tov de 3080... Maar goed as kerst maar de 3080 aanschaffen :-)
RTX 3080;5;0.5321775078773499;Joh, je hebt nog steeds een heel prima kaart waar je nog jaren mee vooruit kan. Upgraden met een 5080, over een paar jaar.
RTX 3080;2;0.6017094850540161;Gelukkig heb ik mijn 2080ti nog niet verkocht, en heb ik eerst dit afgewacht. Ik heb het al eerder gemeld, dat ik vermoedde dat ze een AMD'tje zouden doen, en dat blijkt zo! Natuurlijk krijg je veel voor een stuk minder geld. Een 3080 zal zeker zijn geld waard zijn, maar als ik kijk naar 1440p waar ik op game is net 18% t.o.v mijn 2080ti niet schokkend! Nvidia heeft dus eigenlijk flink zitten te liegen met hun dubbele prestaties...terwijl het meer naar net 50% gaat...dat is toch wel een flinke hoop minder. Tevens is het verbruik ook erg hoog...320 watt ingame...dat is zeer fors! Tegenvallend op het punt dat ze het beter doen geloeven. Ik ben benieuwd naar de AMD kaarten, want daar verwacht ik nu nog meer van eerlijk gezegd! Een 3070 zal dus helemaal niet sneller zijn dan een 2080ti, en ook nog eens 3gb minder ram hebben... Lol, je zou je 2080ti maar verkocht hebben voor een lage prijs...wordt je dubbel genaaid door ze. Wel een mooie kaart verder, maar het stroomverbruik is wel eng aan het worden.
RTX 3080;2;0.43808943033218384;De vergelijking van Nvidia is tussen de reguliere 2080, niet de 2080 super of ti. De 3080 is wel een heel stuk sneller dan de 2080. De overstap van de Ti naar de 3080 is niet bijster groot. Enkel de 2080ti is volledig zijn waarde kwijt met de komst van deze kaart. Laat staan als de 3070 het waar maakt om even snel of nipt sneller te zijn als de Ti.
RTX 3080;2;0.502766489982605;Klopt, dan zitten ze 55 tot 60%...maar dat is niet 100% zoals Nvidia beloofde... Ik ben blij dat ik gewoon heb afgewacht en niet mijn 2080ti heb verkocht. De 3080 vreet stroom...nog een stuk meer dan mijn 2080ti, en heeft nog meer warmte. Dus moet je eigenlijk al een waterkoeling hebben om dit fatsoenlijk te koelen tijdens gamen, of accepteren dat je veel lawaai hebt...en dat laatste is voor mij onacceptabel..
RTX 3080;2;0.4689750373363495;Je springt hier wel erg snel op conclusies, hoor. van r/nvidia GamersNexus review thread: En met de AIBs die beweren dat ze verbeteringen hebben aangebracht aan hun cooling design, gaan de temps en noise reproduction op die hun modellen misschien en hopelijk niet meteen pijlsnel de hoogte inschieten.
RTX 3080;2;0.4315641224384308;Dit soort tests worden tegenwoordig nagenoeg altijd in een open omgeving gedaan. Daarmee bedoel ik dat ze op een open testbench staan, en niet in een case! En de cijfers worden heel anders als je ze in een case plaatst! Zeker weten. Ik begrijp wel dat ze dit doen omdat er zoveel cases zijn, maar eigenlijk zou men het ook moeten testen in een wat mindere kast. Ik heb zelf een crystal x570 van Corsair, maar als ik game, haalt die totaal niet de beloofde DB. En mijn 2080ti heb ik zelfs met een lagere powerlimiet ingesteld. In een open omgeving kan die zijn warmte wel kwijt, maar in een case is het dus een ander verhaal. En als je dan opeens 320 watt moet afvoeren, gaan die leuke theoretische cijfers gelijk de deur uit helaas.
RTX 3080;1;0.37489792704582214;"Tjah, ik zou zeggen: bekijk eens de video review van o.a. Gamers Nexus en Linus eens. Daarin testen ze in een closed chassis, en is er geen groot verschil in heat output tussen de 3080 en 2080 Ti. Nogmaals, niet zomaar tot conclusies komen zoals ""je hebt waterkoeling nodig voor de 3080 te koelen!"" of ""ze testen enkel en alleen in open benches!""."
RTX 3080;3;0.5742664933204651;"Er is wel een groot verschil in output, want het TDP en verbruik is gewoon hoger. De RTX 3080 heeft gewoon een betere koeler dan de RTX 2080 Ti. Ik denk dat je met het FE model prima zit; of dit ook zo zal zijn met de partner modellen is nog even afwachten."
RTX 3080;3;0.41567444801330566;Dat dit een 4k-gerichte kaart is was al wel duidelijk. Ik had op 1440p ook geen gigantisch verschil verwacht
RTX 3080;2;0.4653974771499634;Klopt, dat is zo, maar laten we eerlijk zijn, er zijn maar weinig mensen die op 4K gamen.. dat is nog steeds maar een 5% of zelfs minder. Ik had al verwacht dat ze het goed opbliezen...en als je kijkt naar performance per watt, dan doen ze eigenlijk het gewoon slecht. 320 watt onder load, en maar 25% sneller dan de 2080ti die een stuk minder verbruikt...dan is de Samsung 8NM niet echt een grote stap vooruit, of ze hebben de kaart gemaakt in de hoop dat er veel mee gemind gaat worden.
RTX 3080;1;0.549807608127594;Dan heb je niet doorgedacht. deze kaart wordt misschien aan 1% verkocht, laat die 5% nou net een doelgroep zijn, maar meer nog mensen die een nieuw systeem en monitor aanschaffen om weer helemaal bij te zijn. Dat AMD nVidia gaat overtreffen in het hoogste segment lijkt me erg onwaarschijnlijk. Ik denk eerder de nVidia zijn voorsprong heeft vergroot. Geen nood voor AMD omdat hier de grootste verkopen niet liggen. We zullen het zien als het op de dag van de aangekondigde aankondiging bekend wordt. Dit soort ontwijkend gedrag lijkt er op te duiden dat AMD nog druk bezig is na te denken hoe ze hun nieuwe kaarten moeten positioneren na dit nVidia-geweld.
RTX 3080;3;0.40739545226097107;"Denk je niet dat deze kaart populairder gaat zijn? De prijs is vergeleken met eerdere generaties toch veel aardiger. Ik vraag me ook af wat AMD bekokstooft; aan de ene kant zou je denken dat ze toch eerder met iets naar buiten zouden willen komen als ze echt wat moois te tonen hadden, maar aan de andere kant weten ze bij AMD ook wel dat ze niet overhaast hun roadmap moeten versnellen alleen maar om het niet te laten lijken alsof ze geen weerwoord hebben. Voor ze het weten zitten ze bij AMD dan weer met een stuntelige presentatie waar de demo crasht of iets dergelijks. Uiteindelijk worden toch de meeste kaarten voor de kerst verkocht; dan kunnen ze beter op 28 oktober volgens plan aankondigen dan hals over kop met een mindere presentatie komen. Daarnaast zag ik een aardige berekening over het verbruik van de RDNA 2 GPU+RAM in de Xbox Series X, waar uit kwam dat 170 watt daarvoor geen onrealistische schatting is. Dat is iets meer dan de helft van het verbruik van de RTX 3080. Nu zal de RTX 3080 ook een stuk sneller zijn dan de GPU van de XSX, maar bijna twee keer zo snel? Daarnaast gebruikt AMD nu het TSMC proces waar Nvidia met de vorige generaties zulke fantastische prestaties per watt mee neerzette. Hoewel ik niet verwacht dat AMD Nvidia voorbij zal streven heb ik toch wel goede hoop dat ze het gat aardig dichtlopen."
RTX 3080;2;0.3885709047317505;Nvidia schreef 60% tot 100% sneller dan de originele 2080. En natuurlijk dat is voor reguliere spellen meer rond de 60%, terwijl RTX zaken grotere verschillen hebben. Dat mensen dan denken dat alles 100% sneller zou zijn, tja, dan zet je jezelf ook wel erg op voor teleurstelling.
RTX 3080;2;0.42106175422668457;Ja maar mensen blijven de 3080 maar wegzetten tegenover de 2080 Ti......zet de 3080 eens eerlijk weg tegenover de 2080/2080s. Dat zijn het toch wel echt forse upgrades. Ik heb bewust de 2080 Ti overgeslagen omdat ik 1300 euro echt krankzinnig vind voor een videokaart. En toen was het echt een mega dikke kaart nu koop ik voor de helft van een 2080 Ti een kaart die ook nog eens 25% sneller ks. Ik vind het zelf een knappe prestatie. Het is zuur voor de 2080 Ti bezitters, maar eigenlijk hebben ze zich wel beetje laten naaien door Nvidia omdat er totaal geen concurrentie was.
RTX 3080;3;0.4485551714897156;Same, had hem bewust veel te duur te koop gezet, biedingen liepen wel uiteen van 250 tot 600 euro. 550/600 lijkt mij vrij redelijk, ben ook overtuigd dat ie het beter doet dan de 3070 dus hij mag ook duurder zijn Tegenover de 2080, niet de 2080 ti. Niet dat ze dat waar kunnen maken zonder DLSS/Raytracing, maar dat maakt de overdrijving wel wat kleiner.
RTX 3080;2;0.48780256509780884;Klopt, maar zelfs op een 2080 is het verschil niet eens zo groot als ze zeiden...de 100% wordt nooit gehaald. En dat is wel misleidend. Tevens is raytracing in de nieuwe kaart eigenlijk ook niet zo veel beter. Enkel met dlss en op 4K. En natuurlijk is de prijs zeker de kaart wel waard. maar ik heb zelf een 2080ti, die doe ik hier de deur niet voor uit. Het heeft dan niet zoveel zin lijkt mij. En een referentie model zie ik zelf niet zitten met de gekke powerplug in het midden. Dan is de vraag wat doen de andere fabrikanten...
RTX 3080;2;0.43494027853012085;De 3070 wordt waarschijnlijk iets van 15% langzamer in de meeste games, maar ongeveer even snel in compute heavy of raytracing games. (als ik uitga van 25% sneller dan 2080Ti en 48% sneller dan 3070) Ik zou als 2ehands koper toch een groter verschil in bang/buck willen zien voor een kaart met veel kortere garantie. Maar ik zou sowieso nooit zo veel besteden aan een 2ehands computeronderdeel.
RTX 3080;3;0.3990214467048645;Met 3 jaar garantie waarvan een klein jaar af is zit ik waarschijnlijk boven de meeste nieuwe kaarten, behalve degene waar je veel meer voor gaat betalen (asus rog strix), maar dan is mijn prijs weer veel lager
RTX 3080;2;0.31929001212120056;De perfomance per wat is ten opzichte van de 2080ti maar met 8% gestegen. Gemiddeld 25% meer fps tegen een verhoging van 17% in gebruik. Nu de beloofde 2x 2080 prestaties bij lange na niet gehaald wordt ben ik zeer benieuwd wat Amd te brengen heeft. Kwa prijs is de 3080 leuk, maat had zelf ook erg gehoopt op FS2020 benchmarks
RTX 3080;2;0.39267656207084656;"Stroomverbruik is inderdaad zeer opvallend! Je kan de 3080 dus eigenlijk niet gebruiken zonder waterkoeling of zonder (een deel van) de hitte de kast uit te blazen! Dus alle standaard AIB koelers zullen de temperaturen in je kast flink opdrijven. In de praktijk zou de 3070 wel eens een interessantere kaart kunnen zijn aangezien het verbruik daarvan ongetwijfeld een stuk lager zal liggen; 250 watt? Stel je voor, het is 2020 en een midrangekaart verstookt 250 watt in game! Dat is Titan-verbruik! Ik hoop dat AMD dit beter doet, al is dat normaalgesproken natuurlijk juist hun zwakke punt."
RTX 3080;5;0.5270628929138184;Nee hoor, waait gewoon lekker met de airflow van de kast mee naar buiten.
RTX 3080;2;0.41416871547698975;Echt impressed ben ik niet. 50% meer transistors voor gemiddeld 25% meer performance en 17% meer verbruik. Een 2080Ti defeater is het wel ja, maar na twee jaar is dat wel het minste wat het zou moeten zijn. Een heuse killer/vermorzelaar en een “ikmoetmijn2080TiechtNUgaanverkopen” kaart is het zeker niet.
RTX 3080;3;0.4119362533092499;De 3080 is dan ook niet de opvolger van de 2080Ti maar van de 2080S
RTX 3080;5;0.3602827191352844;Ik heb van Jensen anders niet gehoord dat de 3080 niet tegenover de 2080Ti zou komen te staan. De 1080 was sneller dan de 980Ti, de 2080 was sneller dan de 1080Ti en de 3080 is sneller dan de 2080Ti. Dat past allemaal precies in dit rijtje. De 3090 is (voorlopig) gewoon de nieuwe Titan. Ti's komen later want dat deden ze altijd al en nu is daar zeker een goed zichtbare plek voor tussen de 3080 en de 3090 qua prijs, en mogelijk qua performance.
RTX 3080;1;0.4618683457374573;Als je van generatie tot generatie bekijkt is dit wel de grootste sprong in performance winst die ze ooit gemaakt hebben. Ik ben anders wel onder de indruk dat een kaart van 700 dollar msrp 25% beter presteert dan een kaart die 500 duurder was. Hij verslaat ook gewoon de RTX titan op zijn sloffen en die was rond de 2000.
RTX 3080;2;0.4631294906139374;"Nee, er zijn grotere stappen gemaakt in het verleden. ""Prior to 2010, and with the exception of the NVIDIA GeForce 9800GTX, PC gamers could expect a new high-end graphics card to be faster by 70% than its predecessor (that’s average percentage). Not only that, but in some cases, PC gamers got graphics cards that were more than 100% faster than their previous generation offerings. Two such cards were the NVIDIA GeForce 6800Ultra and the NVIDIA GeForce 8800GTX. However, and as we can see in the following graph, these days the performance gap has been severely reduced. New high-end GPUs no longer offer a performance increase over 50% (thankfully the GTX1080 was an exception to that), with the performance increase of these past few years averaging around 30%."" bron:"
RTX 3080;1;0.487496018409729;Artikel uit 2017, toen bestond deze generatie nog niet. En met predecessor bedoelen ze de directe tegenhanger van de vorige generatie, dus in dit geval zou je moeten vergelijken met de rtx2080 FE (dus niet eens de super of ti). Dan spreken gemiddeld over 90% en meer. En dat is volgens vele techsites dus toch de grootste sprong die ze ooit gemaakt hebben sinds de geforce 256 met zijn TnL engine.
RTX 3080;5;0.4065151810646057;Je hoeft niet heel lang te zoeken naar veel grotere sprongen in performance hoor, maar dan moet je wel meer dan 10 jaar terug. Qua performance / watt is dit juist een opvallend kleine sprong! Bij de vorige generaties waren die sprongen niet alleen groter maar volgden de generaties elkaar ook sneller op. Er is ofwel iets misgegaan of Nvidia heeft de strategie losgelaten om de zuinigste, stilste en duurste te willen zijn. Bij deze generatie laten ze zuinig en stil helemaal los maar de prijs gaat ook omlaag. Ik denk opzich dat de meeste gamers daar wel blij mee zullen zijn, al is 320 watt erg veel hoor! Als je dat moet koelen op een hete dag dan gaan alle fans in je PC staan loeien.
RTX 3080;2;0.42851385474205017;Nah, Titan's zijn altijd gebind op performance / watt en meer gericht op compute en dat is de 3090 niet. Huang zegt nadrukkelijk dat het een gaming kaart is met 'Titan class performance'
RTX 3080;3;0.3875527083873749;Titan class performance betekent niet dat het dus een Titan klasse is. EN.. de 3090 is welzeker gebinned, de 3080 ook.
RTX 3080;2;0.45730355381965637;Dan zijn we het daarover eens maar in de post van jou waar ik op reageerde zeg je letterlijk: De 3090 is (voorlopig) gewoon de nieuwe Titan. Dat gaat dan toch niet echt op omdat Titans zich altijd hebben onderscheiden door prestaties / watt, en dat doet de 3090 nu juist niet. [edit] Je kon eigenlijk altijd elke volgende generatie Titan voor elkaar omwisselen in je kast en je had gewoon veel meer performance en verder niks, terwijl je voor de 3090 een nieuwe voeding en extra koeling moet aanschaffen en daarna ook geheid met een bak extra herrie en hitte zit.
RTX 3080;1;0.2845410108566284;En juist DAT kunnen we nog niet zeggen... alleen speculeren.
RTX 3080;1;0.3363528549671173;Dat heeft Huang zelf gezegd...
RTX 3080;1;0.3399335443973541;Hij zei ook dat de 3080 dubbel zou scoren van de 2080... Iets met WC eend
RTX 3080;3;0.5810199975967407;Ja dat klopt, maar als hij al zegt dat de prestaties / watt van de 3090 een stuk minder zijn dan van de 3080, dan mag je wel al aannemen dat ze in de praktijk niet beter uit zullen vallen en dat de 3090 dus niet echt een goede plaatsvervanger zal worden van de Titan kaarten.
RTX 3080;2;0.45064857602119446;Ik begrijp niet hoe de prestatie/watt opeens positiebepalend is. Dat blijkt nergens uit en is een metriek die je er zelf nu aan hangt.
RTX 3080;2;0.44214534759521484;Dat is het segment waar de Titan altijd op gericht is geweest. Even hoge of iets betere prestaties als de snelste GTX kaart met betere prestaties / watt en meer gericht op professioneel gebruik. Deze 3090 heeft veel hogere prestaties en veel slechtere prestaties / watt dan de 3080 en lijkt daarmee gewoon niet op eerdere Titans. Je kan wel zeggen: 'het is gewoon een Titan die minder zuinig is' maar dat was nou net wat alle vorige Titans anders maakte dan de GTX kaarten.
RTX 3080;3;0.5333058834075928;Mee eens, maar de 3080 is de opvolger van de 2080S. Hij is inderdaad sneller dan de Ti, zoals je zegt in lijn met de vorige generaties. Maar het blijft de opvolger van de 2080S en niet van de Ti. Volgens mij zijn we hetzelfde aan het zeggen
RTX 3080;3;0.34085342288017273;Ik zou nog eerder zeggen dat dit de opvolger is van de gewone 2080. Aangezien dit de eerste uitvoering is van de 3080 zou ik die vergelijken met de eerste uitvoering van de 2080. Hier zullen de komende 2 jaar ook wel weer snellere uitvoeringen van komen die IMO dan een directere opvolger van de 2080s en/of Ti zijn.
RTX 3080;3;0.44899046421051025;Denk je? Hebben ze inderdaad gedaan met de 2xxx serie maar nooit ervoor. Dat er een Ti komt ben ik wel van overtuigd,
RTX 3080;3;0.38031673431396484;Ik wil geleidelijk mijn systeem upgraden, nu wil ik als eerste mijn videokaart vervangen (gtx 1080 naar rtx 3080) maar mijn huidige mobo heeft pcie 2.0... gaat dat wel lukken?
RTX 3080;2;0.4279845654964447;zou dat niet doen.. het verschil tussen pcie3 en pcie4 is das niet heel groot, maar de stap tussen pcie2 en pcie3 zal dat wel zijn. En als je een mobo en cpu hebt met pcie2 zou ik eerst die upgraden
RTX 3080;3;0.44718483090400696;Bij de 2080ti scheelt het 3% in de praktijk (pcie3x16 vs pcie2x16), ik zou me daar niet door laten tegen houden. Ik zie de prijzen (tenzij AMD met wat heel moois komt) ook niet snel wat doen dus daar hoef je ook niet op te wachten...
RTX 3080;2;0.2867256999015808;Ik denk dat het tijd is om je moederbord inclusief cpu te gaan vervangen voordat je aan een nieuwe videokaart begint. Een goedkoop eerste of tweede generatie Ryzen systeem zou ruim voldoende moeten zijn. Een pc is op zijn best als het in evenwicht is......niet je focus alleen maar op de videokaart richten
RTX 3080;3;0.31302106380462646;Met een GTX 1080 op een PCIe 2.0 bordje zou ik misschien eerst eens kijken of een CPU upgrade niet meer voorrang zou moeten krijgen. Ik weet niet welke CPU je er nu in hebt zitten maar ik zou er niet raar van staan te kijken dat die nu al de beperkende factor is. Tenzij je een i7 2600 lekker overgeclockt hebt en een 4k scherm gebruikt. Dan zal je misschien meer baat hebben bij een videokaart upgrade. Maar op lagere resoluties zou ik eerst naar de CPU kijken. Check anders even op deze link, de Bottleneck Calculator, daar kan je je CPU, GPU en ram invullen en dan laat ie zien waar het systeem het eerst tegen een beperking aanloopt.
RTX 3080;4;0.3622213900089264;Follow-up: mijn cpu is een i7 6700 K @4000 mhz, ik game op 1080p native resolutie van m'n monitor. (dank voor de bovenstaande reacties allen)
RTX 3080;3;0.32607734203338623;Nou daar gaat mn 1080 TI , hallo 3080 een 550 watt 80+ PSU zou dus voldoende moeten zijn
RTX 3080;2;0.43820539116859436;hmm dat denk ik niet. NVIDIA beveelt 750w psu aan, maar 650w zou ook genoeg zijn, tenzij je heel veel overclockt. De GPU zelf verbruikt meer dan 350w onder load.
RTX 3080;5;0.2967761158943176;350 watt gpu 65 watt cpu 5 watt hdd 5 watt nvme Wat mis ik dan nog?
RTX 3080;2;0.5251460671424866;ram/moederbord/cpu fans (of aio cooler enz.) Een psu moet minstens 100-150w extra ruimte hebben ivm overhead en efficientie. Net genoeg is niet voldoende en daarmee riskeer je niet maar je gpu maar je hele pc. Naarmate een psu ouder wordt, wordt ie minder efficient. Hier kun je berekenen hoeveel energie je nodig hebt, maar geen garanties: Naar mijn mening is minder dan 650w niet aanbevolen.
RTX 3080;3;0.4115138649940491;Je moederbord trekt onder load natuurlijk ook wel wat. Daar zou ik toch een 50 watt voor rekenen.
RTX 3080;5;0.35998648405075073;Mijn simpele conclusie (adhv de techpowerup prestatiescore op 1080p, 1440p en 4k). Op 1080p presteert de 3080 13% beter dan de 2080 Ti. Op 1440p presteert de 3080 19% beter dan de 2080 Ti. Op 4k presteert de 3080 24% beter dan de 2080 Ti. Als je de 2080 Ti overclockt kom je in de buurt van 3080 stock prestaties. Voor competitieve gamers die voornamelijk op 1080p gamen (esporters) is de 2080 Ti momenteel een hele slimme koop. Gemiddeld onder de 600 euro, waar je voor de goedkoopste 3080 720 euro of meer betaalt. 20% duurder voor 13% meer prestaties. Voor 1440p en 4k is deze kaart echt een mooie stap. Goedkopere videokaarten worden duidelijker goed in hogere resoluties. Prachtig.
RTX 3080;1;0.638346254825592;Je hebt het natuurlijk over een tweedehands 2080Ti met 80% van de gevallen geen garantie meer hebben of bijna vervallen, dan is 600€ en zeer dure zaak, als ze die al verkocht krijgen voor 600€ want als straks een 3070 net de prestaties van de 2080Ti weet te evenaren voor 500€ is die 2080Ti in het beste geval nog maar 350€/400€ en dan nog zijn er mensen zoals ik, die dan liever 100€ bijleggen voor een nieuwe kaart.
RTX 3080;2;0.40598419308662415;Uiteraard moet je letten op garantie en prijs. 500 euro voor een 3070? Nee, dat zul je dit jaar niet kunnen krijgen, misschien Black Friday? De goedkoopste 3080 is 520 en de vraag is nog maar hoe snel dat leverbaar is. Bovendien verwacht ik niet dat de 3070 het beter gaat doen dan de 2080 Ti, zeker niet op competitief gebied (13% van de 3080 Ti tov de 2080 Ti is ook niet om naar huis te schrijven). Een 2080 Ti met garantie voor 550+- is m.i. nog een hele goede deal, zeker als je op 1080p gaat gamen. Op 1440p en 4K zijn de nieuwe kaarten een no brainer.
RTX 3080;5;0.43173813819885254;mee eens
RTX 3080;1;0.5293318629264832;Waarom zou je een verouderde kaart kopen die minder performt en achterloopt op raytracing en DLSS en aan zijn max zit met driver optimalisaties ten opzichte van een nieuwe kaart waar nog veel meer prestatie optimalisaties te halen zijn voor 100 euro verschil. En dan heb je ook nog over een tweedehands kaart tegenover een gloednieuwe kaart. In mijn ogen valt er helemaal geen case te verzinnen om nog een 2080 TI te kopen. Ook niet tweedehands.
RTX 3080;1;0.5987926125526428;Of je koopt een 3070 voor 520 euro en hebt nog steeds vergelijkbare prestaties tot een 2080ti met een lager stroomverbruik. Een tweedehands 2080ti kopen voor meer dan 400 euro vind ik mometeel een bijzonder slecht idee tenzij je met zekerheid kan stellen dat de kaart nog geen 3 maanden oud is en niet is misbruikt.
RTX 3080;1;0.41037097573280334;Dat zijn Nvidia's woorden. Een 3070 met vergelijkbare prestaties als de 2080 Ti op 1080p? Ik moet het nog zien (daar had ik het over in mijn post). Op 1440p en 4K zijn de nieuwe kaarten een no brainer.
RTX 3080;1;0.7178394198417664;Al zit hij er 10% onder, dan ga je toch geen 100 euro meer uitgeven aan een 2e hands kaart waar je elders een nieuwe voor minder geld kan krijgen? Wederom, meer dan 400 piek voor een 2e hands 2080ti vind ik een enorm slecht idee
RTX 3080;2;0.4053187072277069;Geen vergelijking met de 2080? Gemiste kans, dat is de vergelijking die Nvidia zelf heeft gemaakt. Maakt dit overzicht een stuk ingewikkelder om te volgen. Een 2080 is immers niet hetzelfde als een 2080 Super danwel een 2080Ti.
RTX 3080;3;0.2910679876804352;De 2080 is een kaart van twee generaties geleden. De 2080 Super was de opvolger van de 2080, de 3080 is de opvolger van de 2080 Super. De 2070 Super is nagenoeg even snel als de 2080, en ook nog eens goedkoper. Vandaar dat we voor dit prijs/prestatie punt voor de 2070 Super hebben gekozen, en niet voor de 2080.
RTX 3080;5;0.4455590546131134;uit jullie eigen 2080 super review De 2080 is gewoon vorige generatie...... de super een kleine specbumb na betere yields.
RTX 3080;1;0.4456354081630707;Ondanks je uitleg, dank daarvoor, de daadwerkelijke claim van nvidia checken is toch zo gek niet? -- Toegevoegd -- Worden de resultaten van deze kaart geüpload in de Hardware Info database? Daarmee zouden we zelf alsnog de vergelijking op kunnen vragen.
RTX 3080;3;0.5475630760192871;Precies, nu is het verschil tussen 2080 en 2080S niet enorm, maar als ik het me goed herinner deelde Nvidia dat de 3080 tussen de 60% en 100% sneller zou zijn dan de 2080. En natuurlijk, dit is Nvidia, dit is marketing, en dan kan je beter wat eraf halen om een realistisch idee te krijgen. Maar alleen vergelijken met de 2080S, en in je conclusie enkel en alleen die 100% sneller noemen, en niet 60%-100%, is wel imo een vlekje op een verder prima review.
RTX 3080;2;0.39122486114501953;Mbt PCI 3.0/4.0: Er is wel een groot verschil tussen PCI 3.0 en 4.0 als je naast de videokaart nog een ander PCI device op je moederbord hebt. Dan gaat bij een standaard Intel of AMD moederbord (muv de x399/x299 versies), het aantal PCI-e lanes voor de videokaart naar beneden van 16 naar 8. Bij PCI-e 3.0 en 8x verliest een 2080ti al 3-5% performance, bij de 25% snellere 3080 zal dit 30% zijn. Dus als je naast je videokaart ook nog een ander PCI-e device in je pc hebt (SSD kaart of 2e videokaart), dan gaat de performance erg zwaar gethrottled worden bij een PCI-e 3.0 moederbord.
RTX 3080;2;0.4320935904979706;Het is jammer dat dit steeds minder het geval is, behalve bij Intel, waar de CPU enkel lanes heeft voor de GPU, niet voor de SSD (16 vanaf CPU intel, 20 bij AMD op consumentenplatformen). Dit wordt vaak opgelost door de PCH te gebruiken voor de NVME drives, maar daarmee concurreren ze weer rechtstreeks met USB, en heb je aan DirectStorage weer niet eens zo veel...
RTX 3080;2;0.5160663723945618;Wat je hier beschrijft klopt niet helemaal (x399) 1. Meeste CPUs hebben sh*tload aan PCIe lanes. Wat je beschrijft klinkt als een geval waar 16 lanes worden verdeeld via een mux over 2 slots. Dus je gebruikt 1 slot 16x of 2x slots met ieder 8 lanes. Met andere worden, de moederboard is niet geweldig of je gebruikt verkeerde slots. 2. Het verminderde gebruik wat je beschrijft is heel erg situatie afhangkelijk. Dus dat precentage zegt helaas niet erg veel. Als je puur commandlists afvuurt over DMA die alleen dispatch en draw calls hebben, dan heb je daar zo goed als geen perf penalty. Als je contineu buffers pompt zo als textures die aan de max van je pcie bandswidth zit, dan ga je het zwaar voelen (waar een samen spel ligt tussen de limiten van CPU of GPU gebruik).
RTX 3080;2;0.38036900758743286;Jammer dat de 2080 niet meegenomen is in de review, konden we exact zien hoeveel nvidia gejokt heeft met hun presentatie.
RTX 3080;3;0.5660580396652222;Beetje jammer dit maar er stond duidelijk in de presentatie up to 2x zo snel. Niet 2x zo snel..
RTX 3080;2;0.46843382716178894;Ik kom hem nergens tegen die 2x zo snel niet 1maal, daar zit hem het probleem.
RTX 3080;2;0.39942628145217896;"Hij is 1x sneller, of 2x zo snel. ""Sneller"" (meer snel) geeft aan dat je de benchmark als startpunt neem om bij op te tellen. ""Zo snel"" geeft aan dat je rekent vanaf 0 dus benchmark (2080) is misschien 45, en de 3080 is dan 2x45 = 90. 2x zo snel is dan wel goed. Zoals @biggydeen2 aangeeft, up to, ofwel, tot 2x, niet overal gegarandeerd."
RTX 3080;1;0.623181164264679;Dat zeg ik ook niet, het komt nu echter NERGENS voor, ook op andere websites niet te vinden.
RTX 3080;4;0.2779086232185364;"Nvidia heeft gezegd dat de ""Up to 2x 2080"" voorkomt in Quake 2 RTX en Minecraft RTX. Zie deze review voor benchmarks:"
RTX 3080;1;0.5073549747467041;Aah, begreep je opmerking verkeerd. Excuses! Er staan mogelijk nog meer dingen in de NDA naast de review release datums, dit 'riekt' daar wel naar.
RTX 3080;1;0.5377224683761597;"Dus wanneer ze hadden beweerd dat 'ie tot wel vijf keer zo snel zou zijn, zou dit ook correct zijn? Om een analogie te gebruiken: wanneer elektronicahandel X een actieweek houdt en adverteert met ""kortingen tot wel 80%!"" en vervolgens komen er hele mooie aanbiedingen waarbij er producten zijn die met 10, 30% korting worden aangeboden, soms zelfs 60%, maar geen enkel product wordt daadwerkelijk met 80% korting in de winkel gezet... dan heeft X toch gejokt...? De vergelijking gaat niet volledig op, daar kortingen uit een simpel rekensommetje bestaan, maar ik snap @Mschrijver88 heel goed: de bewering van N-Vidia zal gestaafd moeten worden met minstens één benchmark die op die verdubbeling uitkomt. Zo niet, dan hebben ze inderdaad gejokt."
RTX 3080;1;0.40554478764533997;"1) had de opmerking over gelezen als ""2x zo snel niet, een maal"" mijn fout, vandaar doorgestreept. 2) Wat betreft de ""tot"", dan moet je wel de test uitvoeren zoals nvidia dat gedaan heeft. Als nvidia inderdaad dergelijke resultaten heeft weten halen, kunnen ze dit stellen. De vergelijking met een winkel is niet echt representatief omdat wij de meet opstelling en resultaten niet tot onze beschikking krijgen, maar de uiteindelijke aanbiedingen van je elektronicahandel X wel, nvidia kunnen we in die zin dan ook niet controleren. Dus X jokt omdat hij te controleren is, nvidia kunnen we dat niet van stellen. Aangezien nvidia een bedrijf is met aandeelhouders en simpelweg veel winst maakt het liefst, moeten we bij alle claims er van uit gaan dat ze de minst representatieve meting aan ons geven (die 1 op 60 meting die superpositief uit valt voor henzelf ;-) ), nergens geven ze 'een minimum winst ten opzichte van' op. Dat zal altijd op basis zijn van vage voorwaarden. Misschien halen de custom kaarten dat wel, of ging dit specifiek over de FE variant?"
RTX 3080;3;0.4479008615016937;Dat wordt ook lastig want de vergelijking was met de 2080 terwijl in de review van tweakers staat dat de 3080 de opvolger van de 2080Super is. Dat klopt dus niet zoals je zelf ook al aangeeft. Dan nog is het up to 2x dus in een enkele game zal dat wel gehaald worden.
RTX 3080;4;0.3680269122123718;De nieuwe Doom met textures op max zodat de RTX 2080 uit geheugen loopt is een situatie . Zet je de textures een stand lager dan is het verschil vergelijkbaar met de rest van de spellen.
RTX 3080;3;0.2910679876804352;De 2080 is een kaart van twee generaties geleden. De 2080 Super was de opvolger van de 2080, de 3080 is de opvolger van de 2080 Super. De 2070 Super is nagenoeg even snel als de 2080, en ook nog eens goedkoper. Vandaar dat we voor dit prijs/prestatie punt voor de 2070 Super hebben gekozen, en niet voor de 2080.
RTX 3080;3;0.44692400097846985;De 2080 is gewoon de vorige generatie, dat er nou een super tussen is gepropt met een kleine specbumb doet daar niet aan af. 2080 met 3080 is appels met appels vergelijken, ze zitten ook veel meer in dezelfde prijsrange. ik vind het jammer, maar goed ik heb het niet voor het zeggen.
RTX 3080;4;0.32276710867881775;We hebben met beperkte tijd een afweging gemaakt en gekozen voor de 2080 Super omdat deze een completere TU104 bevat. Uit eerdere prestatiescores blijkt de 2080S 5 procent sneller te zijn dan de normale 2080. Vanwege Nvidia's claims van de 3080 ten opzichte van de 2080 hadden we laatstgenoemde ook zeker graag meegetest.
RTX 3080;4;0.3583950400352478;Prestaties van een 2070s zijn vergelijkbaar met een stock 2080, dus deze zou je als referentie kunnen aanhouden
RTX 3080;3;0.3152753412723541;Ik gebruik momenteel een PC met een i7 8700k, 32gb ram en een GTX 1080ti... Het kriebelt om die 1080ti te upgraden, maar heeft het daarbij ook zin om ook een voor een nieuwe CPU te gaan denken jullie? (Ik zie helaas niets over hoe de RTX 3080 presteert met verschillende CPU's)
RTX 3080;2;0.3527727425098419;Volgens de reciew van jayz op youtube gaat jou 8700 een bottleneck vormen voor de rtx3080, je haalt grofweg 20% minder performance uit de kaart dan met een nieuwere cpu.
RTX 3080;1;0.2617785334587097;Oh wow dat is een groot verschil. Heb je een linkje voor me?
RTX 3080;1;0.38704177737236023;Weet effe niet meer wanneer hij het over zijn oude test systeem had met een 8700k waarbij zijn 2080ti een behoorlijk verschil laat zien met de testen die hij op zijn nieuwe cpu uitgevoerd heeft, ergens vlak na de slides met fps resultaten geloof ik dat het was
RTX 3080;5;0.27834904193878174;Het is precies op 12:20
RTX 3080;3;0.4505923390388489;Thanks. Het blijkt mee te vallen op 1440p, vooral in 1080p is er verschil
RTX 3080;3;0.28021326661109924;Bottlenecks door de CPU zijn altijd het meest aanwezig op de lage resoluties Hoe hoger je gaat in de resolutie, hoe minder de CPU van belang is en hoe meer je GPU van belang is
RTX 3080;3;0.5007102489471436;bottleneck alleen op 1080p en bij sommige gevallen 1440p. Hoe hoger de resolutie hoe minder de bottleneck. En het verschil is niet zo groot. Dus de bottleneck zou meer te merken zijn bij games waar je heel hoge frames per seconde krijgt (bvb e-sport games)
RTX 3080;3;0.2964223027229309;Hangt ook af van of je stock draait over overclocked.
RTX 3080;3;0.2849690914154053;Tomshardware heeft een mooi resultaat. Op 4k gaat het niks uitmaken, op 1440p gaat het in de enkele procenten zitten en op 1080p zou ik überhaupt niet weten waarom je een 3080 zou willen .
RTX 3080;3;0.46238845586776733;Ja ik zag het, valt idd wel mee op 1440p en 4k
RTX 3080;3;0.3469446003437042;"Hardware Canucks heeft daar iets over: ""We talk about the impact of an RTX 3080 upgrade for people who have older systems and GPUs."""
RTX 3080;5;0.43891552090644836;Thanks man
RTX 3080;2;0.5106016397476196;Zo dat valt tegen.... Het verschil tussen de 2080ti en de 3080
RTX 3080;3;0.3970348536968231;Valt wel mee, 25% sneller en dit krijg je nu voor 499$ 699$ vs 1200$ bij de 2080Ti.
RTX 3080;1;0.40950214862823486;De 3070 is toch 499?
RTX 3080;3;0.4709179401397705;Jup al aangepast. Denk wel dat de 3070 nog net iets meer bang for the buck zal zijn...
RTX 3080;3;0.302947998046875;Denk het ook. Misschien tijd om mijn 970 eens te upgraden.
RTX 3080;2;0.3640592694282532;25% sneller, en 17% meer opgenomen vermogen. En een flinke lel luider dan een 2080 Super.
RTX 3080;5;0.6601004600524902;Idd de Ti was ook het topmodel. De 2080 Ti kun je qua positionering (en prijs) ook beter met de 3090 vergelijken straks.
RTX 3080;3;0.5940759778022766;Wat tegenvalt is dat er meer van verwacht was kwa performance, lijkt me stug dat hij op de prijs doelt Voor de mensen zonder 2080Ti is de 3080 natuurlijk de betere keus, al waren ze gelijk geprijsd, dus of het verschil nou 501 dollar is of 1 dollar maakt dan weinig uit. Voor de mensen met een 2080Ti is de upgrade weer een beetje matig want maar 20% sneller en daar moet je dan 200+ voor bijleggen, en het alternatief is dan de 3090 waarvoor je alsnog weer 1200+ mag bijleggen, en die gaat wss dus ook niet meer dan 50% sneller zijn... Zelf heb ik een 2080Ti met aftermarketkoeling (ROG Strix), die daardoor bijna 10% sneller is dan de 2080TI FE in de review. Met andere woorden daar is het gat nog kleiner! En gezien de koeling nu wel stock al goed is is dat ook niet iets waar aftermarketkoeling (muv watercooling misschien) wat aan gaat veranderen.
RTX 3080;2;0.38739052414894104;Waarom zou je als 2080Ti eigenaar willen upgraden naar de 3080? Ik als 2080Ti eigenaar zit juist meer te wachten op de 3080Ti. En ja de 3080Ti gaat er komen, je gaat mij niet wijsmaken dat Nvidia niet nog een kaart of twee tussen de 3090 en 3080 gaat plaatsen want het gat tussen 1400USD en 700USD is te groot om dat niet te doen.
RTX 3080;3;0.38954538106918335;Om dan 10% extra performance te krijgen en misschien wat meer geheugen, dus ipv 10GB nu 20GB? Ik denk dat deze generatie de stap naar de Ti een heel stuk kleiner is dan normaal. Mijn beredening de RTX 3090 is qua specs +/- 20% sneller dan een RTX 3080 dan hou je vrij weinig ruimte over voor een RTX 3080 Ti tenzij Nvidia er voor gaat kiezen om deze sneller te maken dan een RTX 3090 (wat mij niet slim lijkt).
RTX 3080;2;0.3967055082321167;Als nog delen disabled zijn van de GA102 voor de 3090 zou er 3090Ti kunnen komen. de grote prijs stap maakt iets 3080 super of Ti mogelijk. Mogelijk zijn de yields te slecht om te veel variaties nu te releasen.
RTX 3080;3;0.45646002888679504;Over de performance/prijs verhouding van de 3080 kan je weinig negatiefs zeggen, 25% sneller dan de 2080 Ti en 40% goedkoper. Zelfs als je na 5 jaar een nieuwe auto koopt kan je die prestatie/prijs toename vergeten. Het opgenomen vermogen is naar mijn mening het grootste minpunt, dit maakt de relatief lage geluidsproductie van de koeler wel weer indrukwekkend want er moet een stuk meer warmte afgevoerd worden.
RTX 3080;3;0.34082233905792236;Ik daag je uit 1 iemand te vinden in alle reacties hier die dat wel doet Snap de reactie niet helemaal, waarom is dat een reactie op mij? Heb ik daar wel wat over gezegd?
RTX 3080;3;0.49370065331459045;Dan zal ik het doen. Die kaart zou maar €400 moeten kosten. Ik vind dat de 5700XT het goed doet in de benchmarks.
RTX 3080;3;0.34768176078796387;Ik denk niet dat de vergelijking zo makkelijk te maken is. Met de nieuwe consoles welke RTX gaan ondersteunen gaan we RTX veel vaker tegenkomen in games. Daarin is de 3080 toch ruim sneller. Daarnaast zijn dit de eerste de beste drivers die beschikbaar zijn. Als je de vergelijking maakt met de eerste drivers voor de 2000 serie vs de laatste kan dat ook oplopen tot 10% extra prestaties.
RTX 3080;3;0.5006706714630127;Nu ga ik niet alle games nalopen maar 3Dmark Firestrike Ultra (totaalscore) is van ~8102 naar ~8379 gegaan, dat is 2%, misschien 3? reviews: RTX 2080 en 2080 Ti - Een nieuwe, maar dure prestatiekroon
RTX 3080;3;0.2892357409000397;Waarom zou je je eigen strix vergelijken met de reference? die kan je straks vergelijken met de strix 3080 die ook weer sneller zal zijn dan de reference
RTX 3080;4;0.2931775450706482;Marketingtechnisch gezien volgt de RTX 3080 dan ook de 2080 (Super) op. Je zal nog even de 3090, of een eventuele 3080 Ti, moeten afwachten om meer prestaties bovenop de 2080 Ti te krijgen.
RTX 3080;3;0.44771429896354675;Prijs technisch gezien zit deze 3080 dan weer op het prijs niveau van de vorige 1080Ti. De 20 serie was alleen achterlijke duur.
RTX 3080;1;0.27446839213371277;Ik vind het helemaal niet tegenvallen als je kijkt naar de prijs waarvoor de 2080 Ti te koop was zoals @Clemens123 al aangeeft. Ik heb de review vluchtig gelezen op werk, maar ik heb in het artikel hier niks over teruggelezen. Ik vind het een win-win voor NVidia, 20-30% prestatiewinst voor bijna 30% minder €€€. Nu ben ik benieuwd wat NVidia gaat kunnen leveren met de 3070 qua prijs-prestatie.
RTX 3080;1;0.6322017908096313;Als ik kijk naar de prijs van 2080ti dan kijk ik naar een 3080Ti en enigste wat daar in de buurt komt van iets nextgen is de 3090. Zou ik nV fan zijn en 2080Ti hebben. Dan kijk ik naar een 3080Ti iets en een 3080 is dan stap te laag want dat is 2080 original. Je krijgt altijd bij dieshrink bij nextgen of refresh meer performance voor minder en dan kijk je naar de release prijs van 2080 vs 3080. Een high-end gamer zou 2080Ti al van af release hebben. Iemand die recent zoiets gekocht heeft volgt de markt slecht. of boeid het niet en kijkt niet meer om en games 3 jaar verder. niet iedereen upgrade bij elke scheet van nV.
RTX 3080;2;0.4663742780685425;Meer prestaties voor (aanzienlijk) minder geld. Hoezo valt dat tegen?
RTX 3080;1;0.47472336888313293;Nvidia had een verdubbeling van prestaties beloofd volgens mij. Iedereen werd gek
RTX 3080;3;0.40415096282958984;Ten opzichte van de 2080. Niet te opzichte van de 2080ti
RTX 3080;3;0.7197270393371582;"Correct. Helaas is ook dat niet waargemaakt. Prijs-prestatie zijn deze nieuwe kaarten toppers; zeker op 4k! Maar performance per watt zien we helaas maar een kleine verbetering. Verder is het is goed om te zien dat de koeling verbeterd is, maar met slechts 1 exhaust fan ben ik wel benieuwd wat voor effect deze 320watt topper heeft op de temperatuur van het hele systeem. Misschien moeten kopers van de founders edition meteen een setje casefans bijbestellen 😇 Ik kijk in ieder geval enorm uit naar de nvidia 3070, amd 6000 serie en misschien zelfs 3060(ti) benchmarks de komende periode."
RTX 3080;2;0.4788508713245392;Valt tegen als je van de Nvidia cijfers uitging.
RTX 3080;1;0.314194917678833;Nvidia cijfers waren vooral gebaseerd op games met RT aangezet.
RTX 3080;2;0.47569578886032104;nvidia heeft behoorlijk een behoorlijk betere prestaties beloofd als dat meetbaar gehaald is, dat valt ''tegen''
RTX 3080;3;0.6885033845901489;Vind het aardig een flink verschil, sommige games wel 40%, dat is gigantisch. Wat had je dan gedacht, 2x zo snel? Wel realistisch blijven he.
RTX 3080;3;0.3859952986240387;Dat vind ik dus ook! Ik game op 1080p144hz en wil een overkill kaart in me pc voor de frames. Beste gevallen: 10fps beter dan een 2080ti op 1080p, valt dat even tegen.
RTX 3080;2;0.29782646894454956;Zal eerder een CPU bottleneck zijn dan toch?
RTX 3080;1;0.5017967224121094;8 oktober misschien, komt AMD met de game kroon CPU en dan op PCI-4.0
RTX 3080;5;0.32846373319625854;Ik hoop het
RTX 3080;1;0.47685977816581726;Omdat de cpu de bottleneck wordt in 1080p met zulke krachtige kaarten, dat snap je toch zelf ook wel? Dan kan je beter voor een snellere cpu kijken, als je de hoogst mogelijke fps wilt op 1080p. De 3080 heeft helemaal niets meer te maken met 1080p gamen.
RTX 3080;3;0.434885174036026;Zeker op 1080p, denk toch is een beetje na. Dat kan je toch zelf zien in de benchmarks dat de cpu de bottleneck is, dat is precies de reden dat de fps zo weinig scheelt in 1080p, en het verschil veel groter is in 4k, want pas in 4k worden deze gpu's de bottleneck. Common sense.
RTX 3080;2;0.5015943050384521;Ja, dus op jouw cpu zal het verschil in 1080p tussen de 2080ti en 3080 dus groter zijn, en is die 3080 dus een stuk minder slecht dan wat jij in eerste instantie ervan vond, kijk vanavond anders even op Youtube, dan zullen er vast benchmarks zijn met snelle intel cpu's, misschien staan ze er nu al op. Ook is de keuzen voor een Ryzen cpu geen slechte keuze, aangezien er meer mensen een ryzen cpu gebruiken bij een zelfbouw systeem dan een intel cpu.
RTX 3080;2;0.3105941116809845;Is toch duidelijk aangegeven PCIE 4.0 is exclusief AMD In principe hadden ze ook de CPU schaling moeten meten door 2 klokken te pakken voor CPU 20% verschil in klok en hoeveel invloed dat heeft op performance in 1080p En dan in de meer in de e-sport en online titels.
RTX 3080;1;0.4269498288631439;De meeste mensen die kijken om de 3080 te kopen zullen niet op 1080p gamen, want dat is onlogisch, aangezien je gpu dan nooit volledig wordt benut. Op 4k, waarvoor deze gpu gemaakt is, is de cpu geen bottleneck. Je gaat toch geen 3080 kopen om op 1080p te gamen als die niet de 50% gebruik voorbij gaat? 4k gamen kost 400% meer gpu power dan 1080p, deze gpu kan in veel 4k games al over de 130 fps, schaal dat terug naar 1080p en je hebt 520 fps, dat gaat geen één cpu die vandaag bestaat bijbenen in moderne titels. Zelfs een 3070 is al zwaar overkill voor 1080p, ik game zelf ook op 1080p ( 240hz) , en ik wacht lekker op de 3060.
RTX 3080;2;0.4275358319282532;4k gamen is niet 4x zo zwaar als 1080p, misschien 2.5x zo zwaar ofso maar het is echt niet zo simpel en scheelt per titel...
RTX 3080;3;0.7570505738258362;oke laat het 3x zijn, mijn punt blijft dan nog steeds hetzelfde.
RTX 3080;2;0.422212153673172;De komende jaren zullen er geen games uitkomen waar een rtx3080 dicht bij een bottleneck uit zullen komen, want cpu's worden maar heel langzaam sneller, het is laatste jaren iets sneller gegaan dankzij AMD, maar als je rtx3080 op 1080p games een bottleneck moet worden, dan moeten cpu's letterlijk 2x sneller worden in aankomende jaren, wat natuurlijk niet gaat gebeuren. Ook is het niet gek om een Ryzen test systeem te gebruiken aangezien de meeste gamers die zelf een pc bouwen op een Ryzen cpu gamen.
RTX 3080;1;0.41542816162109375;Waarom zie je dat niet gebeuren ? Intel zit op 14nm++++ en met 2/3jaar is de planning dat ze op 7nm gaan zitten. Oftewel, het wordt een huge jump. En dan willen ze ook nog met een nieuwe architectuur aan de gang gaan. STEL, dat ze daar nog flinke prestatie's eruit halen, vind je het dan niet zo snel gaan ? Kom op man... AMD heeft geluk dat Intel op nm achterloopt anders waren de resultaten heel wat anders. No fanboy, heb zowel beide. Gelukkig dat er nu concurrentie is en gun het AMD van harte. Het is heel gek om een ryzen cpu te gaan gebruiken op 1080p 120/144hz. Elk fps telt gezien je minfps 120/144fps is en niet 60. Singlecore performance daar is AMD nog niet. GamersNexus test de kaart met een intel cpu en er zijn flinke verschillen... Het is dus heel gek dat Tweakers met een amd cpu benchmarked...
RTX 3080;2;0.4460787773132324;Dat hoefde je mij niet te vertellen dat een intel cpu sneller zal zijn. Maar de meeste mensen hebben een Ryzen, dus dan is het toch verstandig om benchmarks met een Ryzen cpu te laten zien? 14nm naar 7nm betekend niet 2x zo snel, integendeel, hun 10nm was zelfs langzamer dan 14nm, daarom is die ook gecanceld, dus geen garanties voor een '' huge jump'',
RTX 3080;2;0.4743771255016327;Het is juist slim om een cpu te pakken die het merendeel gebruikt, als je dat niet doet dán sluit je pas het merendeel uit.
RTX 3080;3;0.25396648049354553;Wat ben jij een Intel fanboy zeg, heb je soms aandelen?
RTX 3080;3;0.2323857992887497;Nee, een 1080p 144hz monitor.
RTX 3080;4;0.2846148908138275;
RTX 3080;1;0.6765467524528503;nope, 1080p 144 monitor hier en dan gaat tweakers met een amd cputje benchen. Raar,heel raar.
RTX 3080;1;0.4155017137527466;Beste geval? Eerste game die ik aanklik, control zie ik al meer dan 10fps verschil op 1080.... wat is dan het beste geval? Het beste geval om je punt te maken? of het beste geval van wat jij bekeken hebt? Plus als je een overkill kaart wil ga je dan toch voor de 3090.
RTX 3080;1;0.5655593276023865;dus je hebt net zo hard gelogen als nvidia? Jij riep in het beste geval... misschien dat nvidia dat ook deed?
RTX 3080;3;0.2852992117404938;Kijken we wel naar dezelfde review Jeroen?
RTX 3080;2;0.4310961663722992;Maximaal 60% sneller dan een 2080 gemiddeld gezien en dat op 4K. Op 1440P is dat al 50%. Ik had echt meer verwacht en heb medelijden met 2080ti eigenaren die hun kaarten voor minder dan 550 hebben verkocht. Ik wacht wel wat AMD te bieden heeft en zal wachten op de 3070.
RTX 3080;3;0.3891143202781677;Percenten ..... klinkt inderdaad weinig , maar zet die percenten om in FPS en dat maakt dan weer een groot verschil uit. De 3080 is de opvolger van de 2080S en niet van de 2080Ti, toch verslaat de 3080 de 2080S en 2080Ti zowel in prijs als prestaties. Ik snap niet echt wat mensen hadden verwacht, iedereen weet dat zowel Nvidia als AMD als Intel hun cijfers opblazen.
RTX 3080;2;0.2825119197368622;Als je rekening houdt met het feit dat je een 2080 al voor 400 euro kan krijgen is het onzinnig om 80% meer te betalen voor 50% meer performance op 1440P. Ik zelf ken niet zoveel mensen die op 4K gamen en meeste gamers gamen nog altijd op 1080P waar de verschillen nog kleiner zijn. Is een leuke kaart als je een high refreshrate 4K scherm hebt!
RTX 3080;5;0.5520737171173096;In mijn geval is dit een zeer goede upgrade, ik game nu nog op een 1080Ti en op 1080P dus een 3080 met een nieuwe Monitor 1440 zal een wereld van verschil maken voor mij. Ik had bewust de 2xxx serie overgeslagen omdat de prijzen gewoon belachelijk waren 1500€,1600€,1800€ voor een 2080Ti. Dus in feite voor mensen in mijn situatie is deze kaart gewoon ideaal
RTX 3080;4;0.4406246244907379;Jouw 1080ti is niet veel langzamer dan een 2080. Als je een 2080 hebt kunnen scoren voor minder dan 450 euro in de laatste twee weken is een upgrade naar een 3080 niet echt zinnig denk ik. Veel plezier iig
RTX 3080;3;0.42003777623176575;Valt te zien hoe je het beziet, ik wil graag een paar jaartjes zoet zijn
RTX 3080;4;0.45176950097084045;Ik snap jou ook. Als je het waard vind is het zeker geen slechte upgrade vooral als je later misschien op 4K gaat gamen. Al zou ik in jouw plaats misschien wachten op wat AMD doet want Nvidia zal er zeker op reageren. 3080 super/ti is zeker een mogelijkheid in de komende paar maanden
RTX 3080;3;0.3219057619571686;Dat is misschien ook wel een optie, bedankt
RTX 3080;1;0.43726325035095215;Hij zou nu toch te koop moeten zijn hier?
RTX 3080;1;0.46653902530670166;Ik denk dat het een Bug is. Want alternate, megekko en azerty hadden er moeite mee.
RTX 3080;1;0.6058911085128784;wat is dat nou weer voor een rare link.. .waarom zie ik daar wel uitverkocht.. wat een drama-launch was dit zeg
RTX 3080;5;0.5392407178878784;Deze link is direct van Nvidia's twitter account. staat al uitverkocht sinds 15:00
RTX 3080;3;0.376345694065094;Nou helaas dan maar
RTX 3080;1;0.47119927406311035;"Ach, je kunt nog altijd terecht op ebay.com. Staan er verschillende ""te koop"" voor $10.000+"
RTX 3080;1;0.7638855576515198;Ja ik hield het in de gaten vanaf 14:50 Kaart is geen seconde online geweest voor zover ik weet. Ook de mailnotificatie niet gehad waarvoor aangemeld. Geweldig geregeld hoor...
RTX 3080;2;0.3115519881248474;Kan iemand me vertellen wat PEG1 en PEG2 is? Ik snap dat het moederbord stroom kan leveren, maar waarom zou je de 2x 8-pin PCI-E stroomkabels splitsen? Of wordt dat niet bedoeld met PEG1/2?
RTX 3080;3;0.36697617173194885;Per kabel hebben ze waarschijnlijk een 8-pins tussenstuk met meetapparatuur geplaatst. Dit zal wel makkelijker/beter/accurater werken dan beide kabels op 1 meetapparaat aan te sluiten. Op deze manier meet je namelijk altijd ongeveer hetzelfde per kabel, ongeacht of de kaart 1 of 2 kabels heeft of een 6 of 8-pins aansluiting. Ze geven vervolgens de gegeven waarde van beide kabels apart op, exact zoals ze die per kabel hebben gemeten. De optelling kan men dan zelf doen. Bij PEG2 mist 1 kaart, die kaart zal 1 8-pins aansluiting hebben.
RTX 3080;5;0.38715049624443054;@Tylosion precies zoals @MrOleo hier beschrijft, ik kan er maar weinig aan toevoegen We gebruiken een riserkaart van Adex voor het PCI-E slot en de Tinkerforge Bricklet voor de PEG/PCI-E stroomkabels, zoals ook beschreven op de pagina testverantwoording. Het totaalverbruik is daarbij voor de meeste gebruikers/lezers het meest relevant, de meting per slot/kabel doen we in verband met nauwkeurigheid én om te controleren of specificaties niet overschreden worden, zoals bij enkele videokaarten in het verleden wel eens is gebeurd.
RTX 3080;3;0.292655885219574;Iedereen die zijn 2080ti nu voor 400 euro heeft verkocht..... valt de prestatie winst tegenover de ti toch wel mee.
RTX 3080;1;0.3965074419975281;Voor bijna de helft van de prijs. Dat scheelt nogal. Edit: Waarom wordt dit gedownvote?!
RTX 3080;5;0.4328700304031372;Dat is zeker waar!
RTX 3080;1;0.4857153296470642;Tja dat krijg je. Met zulke hoge verwachtingen is het al snel een teleurstelling. Nvidia kennende houden ze nog een paar procenten achter de hand voor het moment dat AMD hun kaarten op tafel legt, maar wereldschokkend is het niet. Ondanks dat het mooie prestaties zijn hoor! Mijn 1080ti word redelijk van de kaart geveegd op 1440p maar mijn behoefte om direct een 3080 te halen is redelijk verminderd nu
RTX 3080;2;0.5522916316986084;Ik heb een GTX 1080 en speel op 1440p. Als ik naar het prestatietoename kijk dan is dat zo'n 93%. Maar de toename in opgenomen vermogen is zo'n 77%. Wat zouden de prestaties zijn als je het opgenomen vermogen gelijk zou houden? In twee generaties tijd lijkt de performance per watt niet bijzonder te zijn toegenomen. Eerlijk gezegd zit ik niet te wachten op een kaart die 77% meer warmte de kamer in blaast als je zit te gamen. Ik was destijds juist blij dat de GTX 1080 zuiniger was dan de kaart die ik daarvoor had.
RTX 3080;2;0.2779553234577179;"En nu de benchmarks allemaal vrijgegeven zijn en de belofte niet helemaal wordt waargemaakt voor diegene die te snel op de hypetrain gesprongen zijn, zal je zien dat het AMD-argument ineens naar boven komt en dat zij ""toch wel"" een goede kans maken t.o.v. nVidia? Die kans is altijd aanwezig en reëel geweest."
RTX 3080;2;0.4538010060787201;Inderdaad, als je ziet dat de 5700XT met 40 cores grofweg op 50% scoort van deze kaart, zal big navi met 80 cores zeker in de buurt komen met evt. verbeteringen van RDNA 1 naar RDNA 2 Ben totaal niet onder de indruk van deze kaart eigenlijk, iets meer performance dan de 2080 Ti (25%) maar ook meer stroomverbruik (17%). Ook de Die grootte is iets kleiner maar vergelijkbaar met de 2080 Ti. Wat eigenlijk ongeveer betekent dat ze nu de 2080 Ti met een fatsoenlijke prijsstelling op de markt brengen, niet zo onrealistisch duur als de vorige serie.
RTX 3080;1;0.577707827091217;Nope, 2x core count != 2x performance.
RTX 3080;2;0.40975067019462585;Waarom vergelijken mensen het toch steeds met de Ti. Juist in het verleden was het altijd zo dat de de xx80 versie in een nieuwe generatie gelijk was aan de vorige xx80ti versie. Dus de 1080 was vergelijkbaar met de 980 ti. En de 2080 was vergelijkbaar met de 1080 Ti. .....nu hebben ze de 3080 die ineens juist 25% sneller is dan de vorige generatie Ti en het is niet indrukwekkend? Het is juist een onwijze stap vooruit als je het vergelijkt met voorgaande versie performance releases.
RTX 3080;3;0.5450520515441895;Omdat deze veel vergelijkbaarder is kwa hitte ontwikkeling en die grootte. Er is dus tov de top van de vorige generatie weinig gebeurt met de prestatie. Het is op een kleiner procedé gemaakt en verder opgevoerd kwa opgenomen vermogen en wat meer performance. Niet echt heel spannend, welk naampje het beestje ook heeft. Enige interessante is dat de prijs omlaag is gegaan, en gezien de die grootte pakken ze gewoon minder marge op het product toch de 2080 Ti. Ik zou niet zoveel kijken naar welk naampje Nvidia eraan geeft naar kijken waar hij mee moet concurreren
RTX 3080;2;0.47462448477745056;Mijn 5700XT valt vies tegen qua prestaties als je het naast de niejuwe kaart zet. Maar ik wacht nog tot Big Navi meer details uitbrengt.
RTX 3080;3;0.6781442165374756;Valt wel mee als je de prijzen ernaast legt. Het is misschien ongebruikelijk, maar toch niet al te vreemd dat een grofweg twee keer zo dure kaart in het beste geval ongeveer twee keer zo goed presteert.
RTX 3080;3;0.35448095202445984;Kan er een graph van prestatie per watt bij komen?
RTX 3080;4;0.2846148908138275;
RTX 3080;3;0.47205057740211487;Goed punt, we gaan dit voor de 3090 review zeker implementeren. Voor deze review kwam dit net te laat helaas..
RTX 3080;3;0.5537323951721191;Als ik de prestaties van een aantal games en het stroomvebruik erbij neemt, kom ik tot de conclusie dat deze RTX 3080 er altijd als beste uit komt qua prestaties per watt. De overige resultaten zijn om-en-om, maar meestal staat de RTX 2080Ti op de tweede plaats. De andere kaarten lijken niet echt een vaste positie te hebben. Heb steekproefgewijs een aantal games meegerekend en enkel in 1440p en 4k, niet in 1080p.
RTX 3080;1;0.40394091606140137;Vraagje voor de redactie: Komen de AIB reviews morgen om 15:00 online?3
RTX 3080;1;0.4185117781162262;Als antwoord op je vraag: morgen verloopt het embargo op de partnerkaarten op basis van de RTX 3080 inderdaad Wel moet ik daarbij zeggen dat vanwege de verschoven NDA van de Founders Edition en het testwerk voor de AIBs onze review nog niet op dat moment zal komen.
RTX 3080;1;0.5373433232307434;Ik weet niet of tweakers deze kaarten hebben gekregen zoja dan komen ze 16:00 uit ipv 15:00.
RTX 3080;3;0.6506248712539673;Oke. Ben benieuwd..
RTX 3080;1;0.3501499593257904;Zijn er al ergens testen met VR beschikbaar?
RTX 3080;3;0.2936750650405884;Arstechnica geeft geen hele concrete cijfers, maar dit wordt er wel vermeld: en: Edit: 2e quote toegevoegd
RTX 3080;3;0.46421730518341064;Jammer dat Horizon zero dawn er niet bij staat bij de pci vergelijking, die game lijkt sterk afhankelijk van de pci snelheid. Ik was daar eigenlijk wel naar benieuwd.
RTX 3080;4;0.2894729673862457;Zie review op techspot
RTX 3080;2;0.3942476809024811;Dat was de enige game inderdaad die bij upside-down-steve (gamerunboxed) bij de PCIe 3.0 VS 4.0 vergelijking bij de 5700XT naar boven kwam als een enorme PCIe 4.0 nut-hebber. Overigens gok ik dat de DirectStorage API óók een forse impact kan bieden, zeker als je je storage op dezelfde PCIe controller hebt (CPU of in het geval van HEDT: zelfde CCX) als de storage.
RTX 3080;3;0.48045220971107483;@Trygve Zijn de kaarten waarmee vergeleken wordt ook nVidia FE edities? Wel nuttig om te weten zeker bij geluids metingen.
RTX 3080;1;0.29542797803878784;Klopt, alle kaarten zijn referentiemodellen tenzij anders aangegeven. Zal het in de review ook nog even verduidelijken.
RTX 3080;2;0.4543313980102539;Als dit al ergens genoemd is dan sorry, maar het is al erg druk in de reacties . Na net stukje over DLSS gelezen te hebben... ben ik verbaasd dat daar absolute framerates staan? We wisten toch al dat de 3080 sneller is dan de 2080Ti uit de vorige pagina's. Wat we hier willen weten is hoeveel de kaart _zichzelf_ weet te verbeteren door DLSS te gebruiken. Dus eigenlijk moet er een run worden gedaan met bepaalde (4k?) instellingen met DLSS uit, en dan exact dezelfde instellingen _op dezelde_ kaart met DLSS aan. Bereken de verbetering in procent voor die kaart. Doe nu hetzelfde voor de tweede kaart. Laat ons die twee getallen zien: Weet de 3080 zich relatief meer te verbeteren met DLSS dan de 2080ti (omdat de 3080 snellere Tensor cores zou moeten hebben). algemen aanvulling: wordt er rekening mee gehouden dat een tal van spellen tegenwoordig dynamisch resolutie en settings aanpassen om een bepaalde framerate te halen? Dit soort zaken moeten uitgezet worden (of er in elk geval zeker van zijn dat ze niet actief zijn in een benchmark run).
RTX 3080;5;0.5893201231956482;Goed punt! We hebben de grafieken met DLSS aangepast, zodat nu ook de resultaten zonder DLSS er in staan Uiteraard letten we altijd op settings andere settings die dynamisch instellingen aanpassen, en zetten deze altijd uit tijdens het benchmarken.
RTX 3080;1;0.403975248336792;weet iemand vanaf hoelaat deze kaarten beschikbaar zijn morgen?
RTX 3080;1;0.26455411314964294;15:00 😉
RTX 3080;3;0.2915554642677307;Ik heb vorige maand een nieuwe pc gebouwd (i7 10700K) en game nog met mijn 780 gtx. Deze kaart zal zeker een hele stap voorwaarts zijn. Zit alleen nog even te twijfelen wat AMD gaat doen.
RTX 3080;3;0.570601761341095;Zou zeker wachten, gemiddeld is de 3080 op 4K 84% sneller dan de 5700 XT. De verwachting is dat het topmodel van AMD 2x zo snel is als de 5700 XT. Dus dan kom je als het goed is iets boven de 3080 uit
RTX 3080;4;0.2846148908138275;
RTX 3080;4;0.38942134380340576;Mooie review, maar waarom hebben jullie Microsoft Flight Simulator niet meegenomen in de benchmarks? Super actueel, zeker nu een hoop mensen voor een investering staan vanwege de grafische pracht en praal van de sim.
RTX 3080;4;0.2846148908138275;
RTX 3080;2;0.5057863593101501;"Prima stap, de Ampere, maar ik ga deze toch nog overslaan. Turing bracht raytracing maar eigenlijk geen (noemenswaardige) verbeteringen in FPS/watt of FPS/euro. Ampere is een grote stap op gebied van FPS/euro, maar het (absolute) verbruik laat te wensen over; er is niet zoveel verbetering als gehoopt. Helaas werpt het nieuwe procede bij Samsung dan toch nog niet die vruchten af. Ik verwacht dat voor de 4000 serie de architectuur en optimalisaties prioriteit krijgen en ze dan weer een goeie stap maken op gebied van FPS/watt. Dan ga ik overstag."
RTX 3080;4;0.5364187359809875;FPS kan nog beteren zoals bij vorige series door driver optimalisaties In begin was de 2080 helemaal niet sneller dan de 1080Ti, na een paar maanden wel. Is een beetje afwachten wat ze gaan doen, maar wat de prijs betreft ben ik al aangenaam verrast.
RTX 3080;2;0.4028090834617615;dat er geen verschil lijkt te zijn tussen pcie 3 en 4 in games is natuurlijk niet gek. deze games zijn allemaal gebouwd op engines die ontwikkeld zijn lang voor pcie 4 een ding was. en ze zullen dus ook niet geschreven zijn om gebruik te maken van de extra bandbreedte die de nieuwe standaard bied. als je de engine schrijft voor een geheugen bandbreedte die er simpel weg niet is zal de engine niet heel erg populair zijn. dus ik denk dat de kleine verschillen geheel te verklaren zijn. maar nu de standaard er is er kaarten zijn die er gebruik van kunnen maken technologien zijn die juist bedoeld zijn om die bandbreedte nog beter te benutten en nu de nieuwe generatie consoles beide een zelfde techniek gebruiken is het een kwestie van tijd voor game engines de mogelijkheid zullen krijgen om hier gebruik van te maken en voor we echt grote verschillen gaan zien in de performance tussen systemen met en systemn zonder de nieuwe standaard. het verhaal dat nvidia 2x de snelheid beloofd zou hebben is een beetje raar. 9x performance per watt verbetering zien. ik heb het niet nagerekend maar op het oog lijkt het er op zijn minst dicht in de buurt te zitten. hoe dan ook de nieuwe kaarten zijn indrukwekkend wat betreft de vele verbeteringen en de rtx trukendoos lijkt nu eindelijk nuttig voor andere dingen dan het maken van prachtige screenshots. ik wacht nog lekker even op het antwoord van amd want ook daar veracht ik veel goeds. of ze het nvidia moeilijk kunnen maken deze ronde... geen idee met de prijzen van nvidia lijkt het prijs prestatie argument dat amd gebruikte om veel mensen over te halen om voor hun hardware te kiezen een lastig iets te worden. maar wie weet de benchmarks waar van men vermoed dat het navi gpu ' s zijn wijzen ook bij amd op een flinke stap vooruit. dat ik een nieuwe kaart zal kopen deze generatie staat bijna voor 100 % vast of er een groen of een rood labeltje op zit weet ik nog niet mar op het moment lijkt het er mij op dat het een groene stikker gaat worden.
RTX 3080;5;0.5539470911026001;Ik snap eerlijk gezegd niet waarom er zo vaak negatief gedaan wordt over de prestaties van Raytracing op de RTX 2000-serie. Alle titels die RTX ondersteunen bieden ook DLSS om de prestaties drastisch te verbeteren, en met de qua beeldkwaliteit sterk verbeterde DLSS 2.0 (bij bijv. Control, Death Stranding, Deliver us the Moon) is dat ook qua beeldkwaliteit erg goed. Ik heb een RTX 2060 Super en ik speel Deliver us the Moon op 2560x1440 met RTX aan en DLSS volledig vloeiend op een 60 Hz monitor. Dus Raytracing is i.c.m. met DLSS zeer bruikbaar op een RTX 2000-serie.
RTX 3080;2;0.5266980528831482;Ik zeg niet dat het niet speelbaar is wat ik wel zeg en wat talloze tests bewezen hebben is dat ray tracing met de 20 serie resulteert in een drop in frame rates die bij sommige games niet al te erg is en bij andere gewoon echt vervelend. DLSS v1 is helaas nog niet echt mooi o te zien objecten worden een stuk minder scherp er door wat resulteert in het gevoel dat je aan een bril moet omdat alles er wat wazig uit begint te zien. Natuurlijk zal het allemaal bij sommige titels veel meer merkbaar zijn dan bij andere en zullen er ook heus wel titels tussen zitten die het best nog goed doen maar over het algemeen is het toch echt duidelijk merkbaar dat het een eerste generatie is van deze nieuwe technieken die om do die reden helaas nog niet echt helemaal lekker uit de verf komen. Ik ben blij te horen dat je tevreden bent met de kaart en de beelden die deze produceert maar helaas geld dat niet voor veel mensen die ondanks dat de kaarten zeker niet slecht zijn ray tracing nog niet goed genoeg vinden om het ook altijd maar aan te hebben staan. Om die reden heb ik dan ook nooit overwogen om de eerste generatie RTX kaarten te kopen. Het is gewoon nog te nieuwe technologie voor me om me daar aan te wagen en ik wacht liever op de volgende versie die een heleboel beter zal zijn. En zie hier de 30 serie lijkt dat zeker te doen al helemaal als je zo als ik een 4k monitor hebt om op te spelen dan is een 20 series kaar simpel weg niet instaat om de resultaten te leveren die ik graag zou willen zien.
RTX 3080;3;0.4227961003780365;Prestaties zijn goed maar totaal niet wat Nvidia beloofd heeft, grofweg een 20-25% performance winst tozv de 2080 Ti. Wel een mega stap in prijs prestaties natuurlijk!
RTX 3080;2;0.3312865197658539;De belofte van 2x was in full-blown path tracing (raytracing) games was, zoals Quake II RTX / Minecraft RTX. Zoals te zien in de presentatie was dit ook een haalbare belofte, gezien de nieuwe kaart op dit vlak dubbel zo veel cores had, met daarboven op de nieuwe / verbeterde architectuur. Dus.. niet algemeen over alle games heen, dat was uiteraard ondenkbaar. De vergelijking was dan ook nog eens tov een 2080 en géén 2080 Ti.
RTX 3080;2;0.34194818139076233;De specs waren gebaseerd op de gewone 2080 niet de ti
RTX 3080;2;0.34082120656967163;Ik ben bang dat je of niet goed hebt opgelet of hoorde wat je wilde horen. Nvidia was heel duidelijk met select games (Minecraft en Quake RTX) waar de 2x improvement ook echt gehaald wordt... Ook denk ik dat veel mensen de 1.9x vebetering gezien hebben maar vergaten te lezen dat er duidelijk bijstond dat men het over performance per watt had op dat moment niet over overall performance. Nvidia heeft in dit geval maar weinig echt overdreven claims gemaakt en ze zijn zeer zorgvuldig geweest met de uitspraken. Is in het verleden wel anders geweest met een sh*tstorm als resultaat ik denk dat men juist daarom heel goed heeft opgelet dat men wel de mooie praatjes hield maar er ook steeds een correct reference bij plaatste die aangaf met welke kaart men vergeleek en op welke resolutie men vergeleek. Al met al van alle reviews die ik tot nog toe heb gezien is er niet een die negatief is over de enorme prestatie verbeteringen de meeste merken op da de prestaties inderdaad een stuk beter zijn en dat zeker de ray tracing performance ook op hogere resoluties er enorm op vooruit is gegaan.
RTX 3080;4;0.43586647510528564;ik geloof dat ze zeiden up to 2x zo snel als de 2080 (dus NIET de ti). Volgens mij hebben ze dat niet helemaal waargemaakt, maar wel een hele mooie prestatie boost ten opzichte van de vorige generatie.
RTX 3080;2;0.3712143003940582;"""De gulzige gpu wordt uitstekend koel gehouden bij een redelijke geluidsproductie, terwijl er ook voor het eerst een semipassieve modus is toegevoegd. De verscheidene Nvidia-partners zullen ongetwijfeld modellen kunnen bouwen die onder belasting nog stiller zijn, maar of ze dat ook voor een soortgelijke prijs kunnen doen… Wij vragen het ons af. Nvidia verkoopt de Founders Edition echter alleen via zijn eigen website en van de vorige generatie weten we dat voorraad daar absoluut geen gegeven is. In hoeverre fabrikanten van custom-RTX 3080's zich zorgen moeten gaan maken, valt dus te bezien."" Dit is voor mij het belangrijkste stukje, dat wordt morgen om 15:00 klaar zitten en hopen dat ik er tussendoor kom."
RTX 3080;3;0.3765726089477539;Waarschijnllijk komen er wel aftermarket-koelers die nog stiller zijn...
RTX 3080;3;0.4242739677429199;De vraag gaat alleen zijn op welk prijsniveau. Als die modellen richting de €800 is het maar de vraag of dat wel heel interessant is, tenzij ze ook nog eens 10-15% beter presteren. Als de AIB modellen op een gelijk prijspunt betere koeling en gelijke of betere prestaties kunnen leveren is dat natuurlijk wel interessant.
RTX 3080;5;0.5057035088539124;Van een heleboel zijn de prijzen al bekend
RTX 3080;3;0.3781021237373352;Of dat de prijzen zijn moet natuurlijk nog maar blijken, dat kunnen ook placeholders zijn. Wat niet onwaarschijnlijk lijkt, goedkoop uitziende kaarten gelijk geprijsd aan de FE's, al zal dat denk ik altijd wel zo blijven. Maar kijk ook bijvoorbeeld maar eens naar die blijven van bepaalde Asus modellen, die moeten wel heel erg speciaal zijn willen de meerprijs van tot €160 t.o.v. een FE waard zijn. Het zou mij weinig verbazen als die prijzen nog flink zullen gaan schuiven, zeker bij de hoger geprijsde kaarten, of ze moeten wel echt veel meerwaarde bieden over de FE's.
RTX 3080;3;0.3448270261287689;Daar ben ik inderdaad ook benieuwd naar. Weet je wanneer die reviews van andere kaarten online komen?
RTX 3080;1;0.510844886302948;"Ik neem aan binnen enkele dagen nadat de kaarten verkocht worden. Alternate heeft er al een hoop staan, maar nog niet wanneer ze beschikbaar komen. Ik wacht sowieso nog de reviews van de 3090 af, voordat ik met vooruitwerkende kracht mijn dertiende maand naar NVidia ga brengen. ;-)"
RTX 3080;4;0.44945913553237915;Toch mooie prestaties.
RTX 3080;2;0.3292875289916992;Heb trouwens van een andere site begrepen dat reviewers die zich richten op Linux, de kaart niet hebben gekregen: De verwachting is wel dat de drivers vandaag ook openbaar worden voor Linux, maar wees dus gewaarschuwd dat reviewers op Linux de kaart niet hebben kunnen benchmarken.
RTX 3080;5;0.49418672919273376;Het lijkt erop dat de 3080 de verwachtingen zo goed als waar maakt. Wat een mooie sprong in prestatie! Nu maar hopen op degelijke levertijden.
RTX 3080;4;0.4234222173690796;ook wel interessant.
RTX 3080;2;0.3417474925518036;Dat is nogal een understatement.Mooi voorbeeld van sluwe buisness tactieken.
RTX 3080;4;0.40101009607315063;Leuke stap en helemaal voor dat geld. Dus in die zin hoor je mij niet klagen. Moet wel zeggen dat ik niet 'mega' enthousiast ben. Had toch wel 90% (ok, stiekem misschien 80%) in performance verschillen te gaan spotten. Resumé top kaart en prima prijs in vergelijk met de 2080Ti. Maar een über kaart en verschil vind ik het dan ook weer niet.
RTX 3080;5;0.3825889527797699;En ik dacht nog met 800 euro gek te zijn om een 1080ti te kopen 3 jaar geleden. Dat ding kan nog goed mee komen .
RTX 3080;2;0.3204094469547272;Hier exact hetzelfde inderdaad. Denk dat ik mooi nog even aankijk waar AMD mee komt en dan komt Nvidia wel weer met super of Ti versies..
RTX 3080;2;0.5009319186210632;Ik heb een vermoeden dat enige Super varianten alleen voor de 60 en 70 zijn, de 80 zou misschien nog een Ti kunne krijgen maar veel ruimte zal die niet hebben met de 90 op de hoek. De 90 kan ook nog een Ti versie kunne krijgen maar dan kunnen we de 400w wel verwachten. Een titan zit er niet in, niet op de samsung node in ieder geval, te inefficiënt, de Titan varianten hebben in het verleden 250-280w max in verbruik, die kunnen ze niet even opschroeven naar 350-450.
RTX 3080;2;0.3016156554222107;Ik hoop dan ook op een 3070 super of Ti tegen de tijd dat AMD hun kaarten beschikbaar heeft.
RTX 3080;3;0.46036556363105774;Behoorlijk indrukwekkende prestaties. Maar toch, als een 2080 TI zou hebben, zou ik hem toch niet zo gauw vervangen voor een 3080.......zegt iemand met slechts een 1660 super.
RTX 3080;4;0.4028432369232178;Het is dus vooral een 4K Ultra gaming kaart, echt een stuk sneller, maar voor 1080p, kijk in de vraag en aanbod voor een nu nog voordelige 2080 Super of Ti
RTX 3080;2;0.5637995004653931;Toch ben ik het hier niet mee eens. Er zijn genoeg mensen die 1440p willen spelen met alles op z'n hoogst , en dan het liefst 120-144 fps . Daar komt zelfs de 3080 eigenlijk niet aan. Voor 4k op zulke instellingen zou ik , als je van hogere framerates houdt , mss kijken naar de 3090. Voor mij lijkt de 3080 een goede kaart voor 1440p > 100 fps gaming.
RTX 3080;5;0.3943009674549103;Dat ben ik met je eens, als je inderdaad +120 fps wilt, kun je beter voor de 3090 gaan, of misschien heeft AMD nu ook een flinke stap gezet, ik ben benieuwd naar waar ze mee komen.
RTX 3080;3;0.4417339563369751;Complimenten dat er ook wat simulatiespellen zijn behandeld, en dat het niet heel extreem gefocust is op 'loop door een doolhof en schiet op dingen'-spellen zijn. (Die heb ik sinds Wolfstein3D en Doom wel gezien)
RTX 3080;4;0.3433469235897064;"Ik kijk zo uit naar de Raytracing! Ik werk nu met een Quadro P6000 in Unreal, en ja raytracing werkt, maar nog niet echt optimaal. Ik kan niet wachten tot ik dit op volle kwaliteit kan gebruiken. Overigens heeft Raytracing nog een lange weg te gaan, het is niet alsof je ineens met een RTX kaart in elke scene raytracing kunt aanzetten met top kwaliteit tot gevolg. Het is goed te weten dat er verschillende soort dingen zijn die je kunt raytracen, met verschillende aantal bounces en samples per pixel. Hoe meer bounces, hoe realistischer, hoe meer samples, hoe minder ruis / noise zichtbaar. - shadows: dit is vrij licht om te raytracen. - global illumination (indirect illumination):hoe meer bounces hoe zwaarder. Hoe meer samples hoe zwaarder, helemaal met brute force. een RTX 2080 krijg je snel op zijn knieën met 3 bounces en genoeg samples om geen noise meer te zien. - reflections: een scene vol glossy reflection materialen is nog veel zwaarder dan GI om zonder artifacts (=dus veel samples) te renderen. Dit krijgt gegarandeerd de 3090 ook op zijn knieën. - geluid kun je ook raytracen (wave tracen?). Een dikke reverb kost daarbij veel ""samples"" Hier een video die mijn gebrabbel op een betere manier overbrengt met wat voorbeelden in Unreal4 op een RTX 2080"
RTX 3080;2;0.4067687392234802;Nou dat wordt toch even upgraden zo tegen de feestdagen. M'n GTX1060 lag daadwerkelijk te zweten terwijl ik de review las. Moet er alleen aan denken dat er ook een betere PSU bij moet. Ik denk niet dat ik het met 500W ga redden
RTX 3080;3;0.2783028185367584;Als je ze voor de winter koopt heb je er ineens een extra verwarming bij.
RTX 3080;4;0.4141468107700348;Ooh handig! M'n oude AMD FX pc staat bij mijn ouders dus ik kan wel een vervanging gebruiken
RTX 3080;5;0.4289897382259369;iNvidia geeft ook aan dat min 750 PSU nodig is.
RTX 3080;2;0.5499258637428284;"En wederom een geval van ""geloof niet zomaar iets van de presentaties van een bedrijf"". Natuurlijk, het is een mooie upgrade over de 2080Ti, maar het komt hij lange na niet in de buurt van wat beloofd werd; 20 tot 30% sneller bij zo'n 20% meer stroomverbruik. Vooral dat laatste stelt teleur en is mogelijk toch een bevestiging dat NVidia's 8nm ontwerp niet zo'n denderend succes is als ze wellicht gehoopt hadden. Het is wel positief voor mensen die deze nieuwe generatie NVidia (en/of AMDs aankomende reeks) willen aanschaffen: de prijzen zijn weer wat lager. Je krijgt meer performance dan een 2080Ti voor enkele honderden euro's minder. Helaas alleen dat de RTX3090 juist nog duurder is geworden (begrijp niet dat ze het niet een Titan hebben genoemd) en dat SLI niet meer mogelijk is bij de RTX3080 en lagere modellen. Niet dat SLI en Crossfire het einde zijn, verre van, maar wellicht dat een tweetal RTX3070 kaarten nog een interessant performance alternatief had kunnen zijn voor de RTX3090. Goed, en nu even wachten op wat de RX6000 serie te bieden gaat hebben. Iets zegt mij dat, met in het achterhoofd houdende wat ze RX5700XT al biedt en de RTX3080 nu blijkt te bieden, dat de topend RX6000 kaart (indien de prijs binnen de perken blijft) een veel serieuzere concurrent kant worden voor de RTX3080, dan dat de voorgaande AMD kaarten (Vega 64) waren t.o.v. de RTX2080/2080Ti. Want ja leuk die RTX3090, maar met die prijs is het echt een halo-product, iets wat alleen de top 0.x% kan en wil betalen."
RTX 3080;3;0.5518844723701477;320 watt, dat is toch wel heftig niet? Ik snap dat die een stuk sneller is dan een 2080ti, maar hij gebruikt dus ook een stuk meer stroom, lastig.
RTX 3080;2;0.4359671473503113;"Wauw, wat is de nieuwe manier van reviewvideo's maken storend zeg. Ik verwacht ieder moment dat ze gaan zeggen: ""We schakelen nu over naar Den Haag voor een live verbinding"". Waarom is ervoor gekozen om het te brengen als het nieuws? Verder wel mooi kaartje hoor!"
RTX 3080;2;0.5426961779594421;Eens. Ik vind de huidige presentatoren ook behoorlijk storend. Niks tegen hun persoonlijk, maar de stijl van reviewen haalt het niet bij de concurrentie.
RTX 3080;5;0.5186993479728699;Zeer mooie prestatie winst, niet geheel wat NVIDIA heeft geschetst maar het komt in de buurt. Fabrikanten kiezen vaak de meest gunstige uitkomst en brengen dat dan naar buiten uit als gemiddelde over de hele linie. De al eerder uitgelekte benchmarks waren natuurlijk helemaal door NVIDIA beheerd, de settings etc waren zo ingesteld zodat de kaart er gemiddeld gezien zo goed mogelijk uitkwam. Dit gezegd hebbende zet deze kaart en verwacht ik ook de andere kaarten in de 3000 serie voor het geld mooie prestaties neer TOV de vorige generatie. Je kan zeggen dat de 2000 serie eigenlijk te hoog is ingezet kwa prijs waardoor nu lijkt dat de 3000 serie een veel betere prijs/prestatie heeft. Laat de 3070-3060-3050 maar komen. Ook ben ik zeer zeer benieuwd naar wat AMD Hier tegen over gaat zetten. Voor gamers is het hoe dan ook heel positief. En nog even wat anders.Na dit lang lopende forum gedeelte , CPU, GPU en PSU reviews zie ik nu dan echt de enorme verbeteringen terugkomen. Het samen gaan met hw.info werpt nu al zijn vruchten af.Natuurlijk is het niet perfect en mis ik de 2080 bv maar dat is echt een detail.De kwaliteit boost is duidelijk zichtbaar en bedankt voor jullie werk en deze in mijn ogen hele goede en uitgebreide review. Dit moet de nieuwe standaard gaan zijn kwa inhoud diepgang en kwaliteit.@Eric van Ballegoie @Trygve @Tomas Hochstenbach @Ch3cker en al jullie collega's bedankt voor deze samenwerking want ondanks sceptische geluiden(ook beetje van mij) zie ik duidelijk verschil in een positieve zin . Ook dit mag wel eens gezegd worden!
RTX 3080;2;0.43018797039985657;Geen 2080 getest, geen performence per watt, geen overclocking, een ryzen 3900xt terwijl de intel 10900k beter is voor games. Er valt nog genoeg te verbeteren.
RTX 3080;5;0.40302789211273193;Zijn stuk voor stuk zaken die we zeker geen meenemen! Thanks voor je feedback in ieder geval!
RTX 3080;3;0.4806392788887024;Ik mis nog benchmarks qua compute (tensorflow en co), was wel een extra kers op de taart geweest.
RTX 3080;3;0.44879966974258423;Dat is zeker interessant, misschien dat we daar nog ooit een losstaand artikel aan kunnen wagen! In deze review is de focus vooral gericht op het game gedeelte.
RTX 3080;2;0.374464213848114;Misschien kan het meegenomen worden als er een review komt van de fabrikant-kaarten Jammer genoeg lijkt phoronix geen review kaart te hebben gehad, die is in iedergeval wel van plan om de nodige computer benchmarks te draaien.
RTX 3080;1;0.3088451623916626;Inderdaad. Ik zoek reviews icm Davinci Resolve. Het enige dat ik zo kon vinden is
RTX 3080;5;0.44626498222351074;Als Big Navi idd 50% performance increase heeft per watt en ze weten het verbruik op 320/325w te krijgen dat wordt dat een beest.. Wanneer je dat berekend dan kwam je ongeveer uit als net zo snel als een 3080 maar toen gingen we er nog vanuit dat de 3080 30% sneller zou zijn dan de 2080 ti dus mogelijk presteert Big Navi tussen de 3080 en 3090 in (3080 Ti level) Ben heel benieuwd
RTX 3080;3;0.39579614996910095;Hoewel de prijs/performance natuurlijk weer een heel stuk beter is dan met de vorige generatie is het jammer dat Nvidia toch wel een vertekend beeld heeft gegeven en onjuiste informatie. Ik ben blij dat ik mijn 2080ti niet heb verkocht, want op 1440p is de winst niet zo overtuigend als ik had gehoopt. Maar dat ze de prijs zo verlaagd hebben, lijkt mij maar 1 ding waarom dit zo is, en dat is dat AMD dadelijk echt een snelle kaart heeft. Denk dat ze goed de 3080 voorbij gaan. Op 4K is de 3080 een monster...en hoewel die geen dubbele prestaties heeft t.o.v de 2080 is een 55 tot 60% natuurlijk nog steeds erg goed voor de opvolger van de 2080.
RTX 3080;1;0.572784423828125;Ik wil niet heel veel zeggen, ben echt een Tweakers mens, dus weinig tot amper op hardware.info. Maar wat een talk-show is die video... Wat een geklets. Lijkt wel een podcast. Niet mijn ding, ben meer van maak een video, actie. Hup laat dat ding zien, laat dat ding zien draaien. Ieder zijn smaak natuurlijk, maar ik zal dan voortaan het maar met de text moeten doen, want de video heb ik niks aan.
RTX 3080;3;0.5760292410850525;50 dB onder load is dat nu niet veel luider dan bijvoorbeeld de 2080 TI van MSI met 3 fans onder load ? Ze vergelijken het met de oudere FE kaarten, maar die waren nou niet echt stil wat dat betreft.
RTX 3080;3;0.2945600152015686;Ik ga nog even wachten wat de andere RTX 30xx kaarten gaan doen, ik heb nu een RTX 2060 en wil wel gaan upgraden dit jaar, de vraag is welke gaat het worden..
RTX 3080;5;0.43356168270111084;Ik ben toch toe aan een gehele upgrade van mijn pc dus ik ga er bizar op vooruit. Volledig nieuw systeem dus nog even de nieuwe AMD CPU's en GPU afwachten en gaan. Na jaren op oudere hardware gespeeld kan ik amper wachten. Nog een maand te gaan.
RTX 3080;5;0.5596581697463989;Goede upgrade voor mijn GTX 1080!
RTX 3080;3;0.559797465801239;Dit is wel een mooie kaart, verschil tussen RTX 2080 Ti is mooi, maar niet huge. Maar het verschil tussen de RTX 3080 en mijn GTX 1080 Ti is echt mega.
RTX 3080;1;0.4535714387893677;offtopic: ik kan niet aan die accent stem wennen. Kinda rip voor mijn 2080ti.
RTX 3080;3;0.41557544469833374;Beieuwd wanneer er een waterblok.voor komt. Ik zet de kaart wel.in een oude setup (rog mobo met een 3750k) dus zal.niet de maximale snelheid halen. Maar ten opzichte van de gtx970sli verwacht ik er wel wat van, in 1440p :-)
RTX 3080;1;0.6216306686401367;??? De ventilator heeft een ring aan de buitenkant en doet daarmee juist niets van wat een blower fan doet. Of mis ik hier iets?
RTX 3080;5;0.6290651559829712;De MSI GeForce RTX 2080 Super Gaming X Trio had ik vorig jaar december gekocht voor E 825, - Tot voor een upgrade.... :-)
RTX 3080;2;0.4390452206134796;Dus de kaarten zijn met stock-koeling getest? Ik heb een nzxt kastje staan zonder waterkoeling o.i.d. Zou ik dan een FE 3080 erin kunnen ploppen zonder al teveel warmte problemen? Vind het design van die FE namelijk wel erg strak!
RTX 3080;3;0.4414520263671875;Iederen vergelijkt de 3080 Founders Edition met hun eigen aftermarket 2080ti. Wacht maar tot de aftermarket 3080 uitkomen, dan wordt t verschil toch iets groter. En anyway, hoe 'klein' t verschil is, hij is wel 500 euro goedkoper bij launch.
RTX 3080;1;0.7301194667816162;Bla bla bla,, video skippen naar 10:45... Wat een lulverhaal zeg, sorry dat ik het zeg.
RTX 3080;3;0.37857505679130554;"Indrukwekkend. Maar wat ik mij nu afvraag... gaan de oude RTX 20X0 nu uit de handel of mogen we ons verwachten aan een prijsdaling van die ""oude"" modellen?"
RTX 3080;3;0.6167468428611755;Had toch wat meer verwacht door de marketing van Nvidia. Ziet er uit als een hele goede kaart, maar komende van een 2080ti is het niet zo heel spannend. Price to performance is natuurlijk wel gigantisch beter dus top voor mensen die van de 1xxx serie komen, maar ik blijf toch bij me 2080ti, ookal was ik helemaal van plan om een 3080 te halen. +-25% extra performance voor +-30% stroom vind ik niet de moeite. Maar voor de mensen die van de 10 serie of zelfs de 9 serie komen : have fun!
RTX 3080;2;0.4304591417312622;"Niet eens Metro op EXTREME settings.. facepalm Zon beetje meest belangrijke benchmark want hamert echt op de GPU;.. Dat is geen tweakers maar noob"
RTX 3080;2;0.6420661211013794;Overpromised, underdelivered. Doet nVidia niet vaak, maar toch wel wat met hun marketing hier. Het enorme stroomverbruik mee in acht nemend, is deze kaart heel snel, maar zeker geen absolute perfecte kaart. Ik snap waarom de reviews zijn verschoven tot vlak voor de release... Ondanks de minder fraaie kanten proberen om FOMO wat mee te laten spelen.
RTX 3080;2;0.6496444344520569;80% meer verbruik en iets meer dan 120% meer performance t.o.v. een GTX 1080 in conventionele games vind ik toch een flinke tegenvaller. Het is nog geen herhaling van Fermi/Thermi, maar heel enthousiast word ik er niet van. Had meer verbetering verwacht t.a.v. het verbruik. Mocht AMD ervoor kiezen de komende generatie weer voorbij een power/performance sweet-spot te duwen dan lijkt het weer een jaar met hete, hongerige kaarten te worden.
RTX 3080;5;0.2621001601219177;wat ik me afvraag, kan ik morgen ook gewoon de third party cards bestellen zoals een msi of asus rog strix?
RTX 3080;2;0.3935052752494812;Resume: Nvidia verkoopt een hoop gebakken lucht. Resultaten zijn veel lager dan wat zij zelf voor ogen hadden. AMD gaat hier slim op in spelen door tussen 2080ti en 3080 te zitten met performance maar dan tegen een lagere prijs verwacht ik.
RTX 3080;5;0.575614869594574;Beste referentiemodel tot nu toe wordt er genoemd. De 3080 Founders Edition is echter geen referentiemodel. Het is een custom build. Het referentiemodel is anders en waterkoeling die reeds is gemaakt voor de rerefentie PCB zal dan ook niet op de FE passen (zonder aanpassingen).
RTX 3080;1;0.4761582314968109;Wat is er mis met het filmpje zelf? De hoofden zijn wazig van tijd tot tijd op 1080p. Je ziet duidelijk op de wangen en de nek dat deze wisselen van scherpte. Maar dit gezegd zijnde, hardware tv en tweakers zijn nu samen gesmolten. Hopelijk nog vele vette jaren toegewenst!
RTX 3080;2;0.5507170557975769;Ik vindt deze serie toch wel tegenvallen, na ruim 2 jaar had ik persoonlijk meer verwacht. Ook het het bij lange na niet de performance die Nvidia beloofde in hun presentatie. Het verbruik vindt ik ook te hoog en valt tegen, wat zeker hoger wordt bij de AIB kaarten. Even wachten op AMD RDNA2 kaarten dus.
RTX 3080;1;0.35217973589897156;hahhaa
RTX 3080;5;0.4156324863433838;Wat een azijn pisser Ben jij zeg pffff :-)
RTX 3080;1;0.3756037950515747;Wat is het grote verschil tussen de non founders edition en de founders edition. Heeft het zin om te wachten op de non founders edition van de 3090?
RTX 3080;1;0.5384297966957092;Ik begrijp niet dat men nog altijd met de 2080 TI vergelijkt deze kaart staat tegenover de gewone 2080 en deze is hier zelfs niet in opgenomen ter vergelijking. Nogmaals voor alle duidelijkheid: Is dus ook nooit beweert geweest dat de 3080 50 percent sneller zou zijn dan de 2080 TI.
RTX 3080;3;0.3358455002307892;En nu op een Intel core. Blijkbaar gaan die toch beter om met een Nvidia chip
RTX 3080;1;0.5127565264701843;Ik was enthousiast tot ik de Guru3D review las dat de 3080 last heeft van coil whine onder load. Zelfs met een dichte kast is het nog hoorbaar. Berichten komen uit meerdere bronnen nu binnen, ook op Youtube met video. Heeft tweakers hier dan niets van gemerkt? Verschrikkelijk dat geluid, dat is dan gelijk een deal breker voor mij.
RTX 3080;1;0.2696104645729065;Iemand een linkje naar een site met benchmarks voor games als warzone en apex legends..
RTX 3080;2;0.3884775936603546;Hoe verhoud het geluid van de RTX3080FE fans zich tot de huidige RTX 2080 Ti kaarten van Asus/MSI/Gigabyte/etc. Bij eerdere reviews van Hardware info werd het geluid gemeten op 10cm, bij tweakers op 30cm, dus dat is niet 1-op-1 te vergelijken. Bij Hardware.info had je ook een koeler efficientie test (fixed at 40db @10cm), ik mis die eigenlijk wel. Zeker nu Nvidia qua koeler ook meer gaat concurreren met de andere kaarten bouwers. Ik las in andere reviews dat de kaart ook wat last had van coil whine. Hebben jullie dat ook gemerkt? P.S., Ik zie wel een tegnstrijdigheid tussen de fan noise resultaten van Tweakers en Hardware.info Tweakers fan noise @30cm = 50.7 dB(A) Hardware.info fan noise @10cm = 48.5 dB(A) Dat kan niet kloppen, wel? link:
RTX 3080;3;0.5548747777938843;Ik vind het wel een beetje te gortig worden allemaal. 320 watt om een beetje te gamen. Waar gaat dit heen? Misschien wordt het tijd dat de EU eens wat maatregelen gaat nemen.
RTX 3080;1;0.8856332898139954;Stel je niet aan, wat een onzin, weet je dat je overheid al het plastic afval tegenwoordig gewoon verbrandt? En jij maakt je zorgen over je energieverbruik, er zijn ergere dingen aan de hand..
RTX 3080;5;0.5296373963356018;Deze kaart kan veel sneller lopen gezien de specs. Ze hebben nog zeker 20% performance achter de hand als AMD sneller zou zijn. Dan brengen ze gewoon een TI of Super uit.
RTX 3080;5;0.3403291404247284;Logisch dat iedereen klaagt dat de kaart niet zo goed presteert als nvidia beweert. Dat de hardware krachtiger is betekent niet dat hudige games daar optimaal gebruik van maken. Benieuwd hoe deze kaarten presteren na een jaar updates en nieuwe gamereleases.
RTX 3080;3;0.2468722015619278;Bedankt voor de review, ik blijf lekker bij mijn evga 2080ti ftw 3. Voor die ~15% verbetering hoef ik geen nieuwe kaart voor m'n 1440p monitor. Ik snap de paniek ook niet van mensen om hun 2080ti nu massaal te verkopen.
RTX 3080;3;0.2706202566623688;"Bedankt voor de review en de video Ik heb zelf 1 jaar geleden mijn PCtje in elkaar gezet 3700x 2070s OC 27"" 1440p @165Hz Ik speel COD en FN op medium settings, met shadows off. En haal met FN 165FPS en COD tussen de 100 en 130FPS. Temps CPU - 65 en GPU 72 Gezien de testen zou ik met de 3080 hogere settings kunnen spelen en met COD ook stabiel 130FPS halen. Die 3080 gaat me geld kosten Ik ben wel erg nieuwsgierig wat RTX IO gaat doen en wanneer je daar echt iets aan hebt. Bedoel met een BR game maakt het geen kudt uit, omdat je in een lobby komt waar je moet wachten tot iedereen er is."
RTX 3080;3;0.46424639225006104;GPU vs GPU Vergelijking Benchmark misschien handig voor sommige:
RTX 3080;2;0.3940451741218567;Hmmm ik was even van plan om mijn GTX 1070 door een 3070 te vervangen en over te stappen van een 1080p scherm naar 1440p maar de kans zit er dik in dat ik mijn voeding ook moet upgraden en van een stil systeem (zelfs onder full load) naar een lawaaierige kast ga. Dilemma.
RTX 3080;2;0.2886882424354553;De nieuwe 30 series betreffen gfx kaarten met een GPU die op 8nm is geproduceerd door Samsung, wat deze GPU een chip maakt welke op een half procedé (halve node) is geproduceerd, welke beschikt over ~ 44,56 Miljoen transistors op één mm² en nu heeft de chip een totaal chip oppervlak van 628,4 mm², wat een totaal maakt van 28 miljard transistors, waar wanneer ze dezelfde chip op 7nm door TSMC hadden laten produceren, er op datzelfde chip oppervlak (628,4 mm²) er 54 miljard hadden kwijt gekund. Alleen nu hat TSMC geen plekje meer vrij voor NVidia wat ik er van begreep, waardoor NVidia vervolgens op Samsung aangewezen was, alleen de chip zo wel aanzienlijk meer stroom nodig zou zijn geweest, ze moesten daardoor immers van 7nm naar 8nm over, en een groter productie procedé zorgt nou eenmaal voor een hoger verbruik. Dus heb dan ook zo'n vermoeden dat mede aangezien de 30 series in de basis iets lager geklokt zijn dan hun voorgangers, wat het verbruik van deze chip ten goede komt, en om ze zo toch nog aanzienlijk sneller te kunnen krijgen dan hun voorgangers, ze om die reden er een sh*t-lode meer aan CUDA/Shader cores aan hebben toegevoegd, 2,5 keer zoveel in over één generatie is niet niks namelijk En voor de liefhebbers bij deze nog even de Whitepaper genaamd: NVidia's Next Generation CUDA Compute Architecture FERMI Gehele link: Edit: nog even de Whitepaper van deze nieuwe FERMI architectuur toegevoegd.
RTX 3080;2;0.3953823745250702;Gamers nexus heeft echt veel betere scores vooral met RDR2, daar zitten ze met 4K op 90fps en 1440p op een whopping 127
RTX 3080;3;0.29541638493537903;Wij hebben Red Dead Redemption 2 met Vulkan getest, Gamers Nexus met DX12. Dus dat daar verschillen inzitten zou best kunnen
RTX 3080;1;0.38561201095581055;En Shadow of the Tombraider? Die wijkt ook mega veel af? Nette review hoor maar wellicht een idee om DX12 te fixen op jullie test systeem want in principe testen jullie op een systeem wat dus problemen heeft?
RTX 3080;3;0.40747109055519104;Wij testen bij 'Ultra' niet de 'Very High' preset, maar alle instellingen maxed out. Dat zou het verschil kunnen verklaren. Overigens zag ik dat er bij de medium settings wat rare resultaten waren, dus we hebben SotTR op alle kaarten net opnieuw getest, de resultaten staan inmiddels in de review.
RTX 3080;4;0.2571820914745331;Gamernexus runde RDR2 op andere settings. Stond boven aan de benchmark
RTX 3080;5;0.3454079031944275;Zoveel pagina's? Die gaat even wachten tot vanavond! -- toegevoegd -- ~+30% op de 2080ti, call me impressed
RTX 3080;3;0.5045315027236938;is toch echt max 25% verschil tussen 2080ti en 3080 en dat op 4K. op lagere resoluties worden de marges kleiner
RTX 3080;3;0.4355143904685974;4k Ultra, 3080 FE tov de 2080 Ti: Control: +32,1% Doom: +35,4% F1 2020: +32,3% Shadow of the Tomb Raider: 27,1% Er zijn wel degelijk games waarvan de testresultaten wel meer dan 25% hoger liggen. Dat ze voor jou niet representatief zijn, zegt niet dat ze niet bestaan. En als een game FPS-capped is, of de CPU simpelweg meer moeite gaat krijgen omdat er niet voldoende grafische pracht en praal gevraagd wordt, is dat nog nooit anders geweest.
RTX 3080;3;0.42043477296829224;Ik bedoelde gemiddeld gezien.. Er zijn ook spellen waarin verschillen van minder dan 20% te waarnemen zijn toch
RTX 3080;2;0.3102476894855499;Eindelijk hebben we een GeForce RTX 2080 Ti killer, die VEEL goedkoper is op de moment dat hij (GeForce RTX 3080) uit kwam, sta verteld hoe snel hij is, wel gemiddeld 90% sneller dan een GeForce RTX 2080 in 4K, dat is erg bijzonder, en dat de prijs het zelfde is als toen de GeForce GTX 1080 uit kwamen, vanaf €700,- Met de GeForce GTX 980 en GeForce GTX 1080 was het het zelfde en was de GeForce GTX 1080 ook 90% sneller dan de GeForce GTX 980, alleen was wattage tussen de GeForce GTX 980 en GeForce GTX 1080 bijna het zelfde 161w vs 184w. Vind het HEEL erg jammer van Tweakers dat de GeForce RTX 2080 (non super) niet mee genomen zijn in de benchmarks, en dat Tweakers er maar weinig verschillende andere grafische kaarten bij hebben staan met de GeForce RTX 3080. Ben nu helemaal benieuwd hoe de Radeon RX 6000 series gaat woorden, en wacht nu eerst tot die uit komen, en maak dan een beslissing of ik voor AMD ga of voor Nvidia. Edit spelfoutjes
RTX 3080;1;0.6905472874641418;Wat een lauwe conclusie, de daadwerkelijke presentaties liggen heel veel lager dan dat Nvidia claimt en daar wordt zo langs gefietst. Geen korrel zout maar een enorme bak met zout is tegenwoordig nodig bij de presentatie van een product. Keiharde leugens als je het mij vraagt, zo ingedekt dat het te vaag is om aangeklaagd te worden. Triest. Ik ben niet onder de indruk, 2 jaar later, heel veel meer verbruik, enorme kaarten die amper in een pc passen, een prijs waarvoor je straks never nooit een custom kaart gaat kunnen kopen, en bij lange na niet de beloofde prestaties, alles behalve indrukwekkend.
RTX 3080;2;0.5139959454536438;Overdrijven is ook een vak......Leuke kaarten en leuke evolutie voor 800€ krijg je meer prestatie dan een 2080Ti, in sommige games 12% en andere 60% maar toch zeker 600€ goedkoper voor meer prestaties dus waar zit je probleem eigenlijk? Deze kaart is dan ook nog eens de opvolger van de 2080S en niet van de Ti. Qua grootte, sorry hoor maar die zijn niet echt groter dan een 2080Ti van Asus of MSI dus waar zit het probleem? Keihardeleugens blah blah... doen ze allemaal, ook AMD en in principe niet eens leugens, ze schetsen enkel hun best case scenario en ongetwijfeld zal die nog verbeteren met drivers updates, dat was ook niet anders met de 2xxx serie. Prijzen van Aib's zijn bekend op alternate etc .... zijn niet extreem veel hoger.... Ik begrijp je razernij niet echt en eerlijk gezegd boeit het me ook niet, maar om zomaar iets af te breken zonder enige geldige argument ...
RTX 3080;1;0.5089325904846191;Haha mooie vergelijking maken met een 2080ti die Nvidia idioot geprijsd had. Stel ik verkoop je nu een Mars voor 10 euro, klinkt een beetje idioot, wie doet dat? 2 jaar later bied ik je dezelfde Mars aan, met 30% meer inhoud voor 6 euro. Wat een fantastische deal! Hype! Hebbehebbehebbe! Als je dat niet begrijpt, dan kan ik je niet helpen. Trouwens, ik onderbouw het heel goed, Nvidia zegt 80%+ performance, zelfs tot 2x de performance. Wijs mij maar aan waar die winst zit dan. Juist, die is er niet. Ja, 2x zo snel als een 2070? Zo ken ik er nog wel een paar.
RTX 3080;2;0.3361402153968811;Dat het een zure pil is voor de 2080Ti eigenaars besef ik maar al te goed, ze hebben daar gigantisch veel geld voor betaald om nu eigenlijk ingemaakt te worden door een kaart die 800€ kost. Het is inderdaad ook een feit dat heel de 2XXX serie overpriced waren en nog steeds. Maar langs de andere kant een high-end nu is binnen 2 jaar geen high-end meer (afhankelijk waarvoor jij hem gebruikt natuurlijk en op welke resolutie etc...), de 2080S is ook sneller dan de 1080Ti. Ze kunnen moeilijk de 3080 hoger in prijs plaatsen om de 2080Ti eigenaars niet tegen de borst te stoten. Ik in mijn geval ben zeer blij dat de prijzen weer een beetje normaal zijn, ik bezit een 1080Ti en heb bewust heel die 2xxx klucht serie overgeslagen. In mijn ogen is de 3080 een zeer mooie kaart en in lijn van mijn verwachtingen qua prestaties tegenover zijn voorganger de 2080S. Ik snap nog altijd niet uw raging, tenzij je een 2080Ti eigenaar bent. En daar kan ik je ook niet bij helpen.
RTX 3080;1;0.7362831830978394;Doe het maar opnieuw ik had een lijstje gemaakt en niks komt erin terug.
RTX 3080;1;0.5251741409301758;Volgens mij is dat schromelijk overdreven, veruit het merendeel van jouw 'lijstje' komt in deze review aan bod.
RTX 3080;2;0.5067971348762512;Sorry hoor maar ik vind de video en de review matig. Had uitgebreider gemogen. Nu moet ik gaan zoeken naar deze onderwerpen waar ik/ wij info wil over hebben. Is niet erg maar had er meer van verwacht. Airflow,Temps van cpu en vga Temps van de chipset als je een riser kabel gebruikt 8k VR Pci-e 3.0/ 4.0 x8 en x16 Games: Dayz, GTA 5, COD, Microsoft Flight Sim, Forza Horizon 4, DCS World video encoding cpu: 8700k, 9600k, 3600x, 3900x, 10850k, 3950x Overclocking van de kaart
RTX 3080;1;0.4318023920059204;klopt vond ik ook. offtopic..... Wat ik ook echt irritant vond is die man .....met zijn handen praten.
RTX 3080;3;0.28824955224990845;Ik draai momenteel nog een GTX970 op een Intel(R) Core(TM) i7-2600 CPU en een Asus Maximus Extreme III moederbordje. Gaat deze upgrade überhaupt wel lukken of moet het mobo en CPU ook mee vervangen worden? Die I7 2600 is al die tijd nog zo verdomd goed gebleven.
RTX 3080;3;0.34238097071647644;Ik zou zeker mobo-cpu-geheugen upgraden mocht je een 3080 willen, die wordt behoorlijk gebotlleneckt met een 2600 ( ondanks dat die niet verkeerd is )
RTX 3080;3;0.6280224323272705;Je gaat zeker vooruit op frames, maar niet zo veel als het mogelijk is, omdat de cpu een grote bottleneck gaat worden. Ik had hetzelfde met een AMD fx 8350 en een 1080.
RTX 3080;1;0.32093682885169983;hoe test je dat? en hoe kwam je daar achter?
RTX 3080;2;0.324104368686676;Ik had geupgrade van een 770 naar een 1080 in mijn vorige pc, en dan heb ik een hoop spellen getest. Bij sommige spellen ging ik een beetje vooruit (Dying Light) maar bij Attila Total War bijvoorbeeld helemaal niet. Dat zag ik aan de gemiddelde frames van de benchmark in de game. Zelfde met pc van mijn vriendin. Die heeft nog een tijdje mijn AMD FX 8350 gebruikt, eerst met een Radeon R9 280 en erna een upgrade naar een RX 580. Assassins Creed Odyssey draaide heel slecht met de R9, met de 580 haalde ze 30 gps gemiddeld. Ongeveer een boost van bijna 10 frames. Nu heeft ze mijn vorige Ryzen 2600 en draait alles super vlot. Dus ik zou alleen upgraden als je ook binnen een jaar of zo ook je pc gaat upgraden/vervangen.
RTX 3080;5;0.45282623171806335;Ik heb een tijdje een 2070 Super op een i7 2600 gehad. Gaat als een tierelier. Je haalt alleen niet alles uit je videokaart natuurlijk. Een 3080 zal ook prima gaan, maar daarvoor geldt dat al helemaal.
RTX 3080;1;0.9177132248878479;RIP AMD. Totaal geen concurrent meer. De performance per Watt kroon hadden ze al niet en nu zijn de bang for buck kroon ook al kwijt.
RTX 3080;1;0.26110389828681946;Als AMD geen concurrent meer was, waarom denk je dat ze de prijzen verlaagd hebben!
RTX 3080;1;0.5228663086891174;Laten we de rollen niet omdraaien. NVIDIA dicteert de markt. NVIDIA dwingt AMD om hun prijzen laag te houden omdat anders niemand AMD kaarten zal kopen. Als NVIDIA dan ook zijn prijzen verlaagt (na een hele periode goed verdiend te hebben) dan is helemaal game over voor AMD. De nieuwste kaarten worden nu zelfs van in het begin een betere bang for buck. AMD kan de prijzen niet nog verlagen zonder verlies te maken maar NVIDIA zal hen ertoe dwingen omdat ze anders met een hele voorraad blijven zitten.
RTX 3080;1;0.7206393480300903;Er zitten geen producten van Nvidia in de huidige en nieuwe consoles dus zo domineert Nvidia helemaal niet.
RTX 3080;5;0.46316564083099365;Precies op tijd!
RTX 3080;3;0.2579628825187683;Hopen dat ik de 3080 FE te pakken kan krijgen, dat is de enige 3080 kaart die goed in mijn mITX kastje past.
RTX 3080;4;0.4005894064903259;Fijn dat er geen turbo gedoe is. De GPU kan gewoon volle bak blijven knallen, zonder dat hij tegengehouden wordt door zijn koeling. Dat gepruts hebben we nu al te veel bij CPU's, maar gelukkig niet bij deze videokaart.
RTX 3080;3;0.4343680441379547;Waarom staat er in het artikel dat de 3080 de opvolger van de 2080super is? Het is de opvolger van de 2080. Daarnaast ook vreemd dat de 2080 niet in de review is meegekomen om de claim (up to 2x) een beetje naast elkaar te leggen. Desondanks toch een behoorlijke prestatie als je kijkt naar de prijzen van de vorige generatie.
RTX 3080;4;0.3606451451778412;De 2080 is een kaart van twee generaties geleden. De 2080 Super was de opvolger van de 2080, de 3080 is de dus opvolger van de 2080 Super.
RTX 3080;2;0.3294983506202698;Dat lijkt mij een rare vergelijking. Waar is de 3080 super/ti dan de opvolger van als deze straks uitkomt?
RTX 3080;1;0.3288038969039917;Van de 2080ti ?
RTX 3080;1;0.31771382689476013;Dat is de 3090 toch?
RTX 3080;3;0.3099423050880432;De 3090 zou de compute kaart worden ... we zullen moeten afwachten.
RTX 3080;3;0.4073101878166199;Wie zegt dat er een 3080 super komt? Er was ook geen 950ti. Wat was dan de opvolger van de 750ti? Wat is de opvolger van de GTX 790? Mijn punt is, soms maken ze een speciale kaart om een bepaalde gat op te vullen. De 2080 super was een soort refresh van de 2080, net zoals de Ryzen 3800 XT dat is van de 3800X. Dus een soort van opvolger. Toch was een vergelijking met de gewone 2080 wel fijn geweest.
RTX 3080;5;0.4455590546131134;uit jullie eigen 2080 super review De 2080 is gewoon vorige generatie...... de super een kleine specbumb na betere yields.
RTX 3080;2;0.4439579248428345;Niet echt. De 2080Super was de opvolger van de normale 2080 (i know zelfde chip, maar het was de opvolger van de 2080), dus logisch dat de 3080 de opvolger is van de 2080Super.
RTX 3080;2;0.40814366936683655;Is mijn overzicht kapot of staat de normale 2080 niet in het lijstje? Mag aannemen met de 3070 straks ook nog in review, die er wel bij komt te zitten? Ergens dus in lijn van verwachting, hij doet het beter. Ik kijk verder 0,0 naar DLSS en RT, 99% van de games die ik speel hebben geen enkele vorm van ondersteuning, dus mij is het puur framerate.
RTX 3080;3;0.2910679876804352;De 2080 is een kaart van twee generaties geleden. De 2080 Super was de opvolger van de 2080, de 3080 is de opvolger van de 2080 Super. De 2070 Super is nagenoeg even snel als de 2080, en ook nog eens goedkoper. Vandaar dat we voor dit prijs/prestatie punt voor de 2070 Super hebben gekozen, en niet voor de 2080.
RTX 3080;1;0.43886077404022217;Ik ken de namen niet van al die Nvidia's, ik kijk puur naar namen: Ik zie hier gewoon '2080' staan en '1080' staan, geen idee hoe dat verhoud naar Super (klinkt als sub naam zoals een Ti) of andere generatie.
RTX 3080;1;0.5409637689590454;Dit snap ik ook echt niet, in de conclusie spreken ze er wel over maar misschien mogen ze de vergelijking niet maken van Nvidia? Want als je beloofd 2x zo snel te zijn als kaart X verwacht ik in de reviews dat ie JUIST tegen die kaart afgezet wordt...
RTX 3080;2;0.3500487506389618;Ik maak me behoorlijk zorgen over temperaturen in een 'echte' case. Ik ben al geruime tijd aan het prutsen om m'n vega 56 + 2700x in een meshify c optimaal te laten presteren zonder te veel lawaai. Voorlopig nog niet tevreden (5 noctua fans). En dat is met 'maar' 250 wat verbruik voor de gpu. Waarbij thermals de limiterende factor zijn. Kan een 3080 in een normale pc case dezelfde prestaties neerzetten?
RTX 3080;3;0.43091005086898804;/offtopic: Als je het in een meshify met 5 van die fans al niet voor elkaar krijgt ben ik wel benieuwd wat je cpu cooler is en je temps.
RTX 3080;3;0.4914078414440155;Wel, 'niet voor elkaar krijgt' maar dan wel bij een geluidsdruk die ik fijn vind, wat natuurlijk bijzonder subjectief is. Wat objectieve cijfers: - 2700x met NH-U14S tijdens een stevige game sessie: 63°C - Vega 56 UV @1550/900 core/hbm2, gerapporteerd verbruik +/- 245w: core 81 max Het lijkt me dus echt een uitdaging om in vergelijking met m'n systeem nu nog een extra 100 watt aan warmte de kast uit te krijgen @ een voor mij acceptabel geluidsniveau.
RTX 3080;3;0.3960346281528473;Wellicht dat ik het gemist heb, maar hoe zit het met de zorgen om de warme luchtstroom die de FE over de cpu blaast? Wat voor verschil maakt dat voor de cpu temperatuur vergeleken met andere video kaarten?
RTX 3080;5;0.5026086568832397;Kunnen de Skyrim Modders weer aan de slag!
RTX 3080;3;0.3176274001598358;Vooral SkyrimVR. Nu haal ik met een 2080S en een Valve Index (6 451 200 pixels) op 90hz buiten Riverwoord gemiddeld een frame timing van 10ms. Ik verwacht dat het met deze kaart richting de 8ms zal gaan.
RTX 3080;2;0.32436928153038025;Ze moeten toch ook vanaf vandaag te bestellen zijn? Ik kan ze nog nergens vinden.
RTX 3080;3;0.3024936318397522;Volgens mij vanaf de 17e
RTX 3080;3;0.40994197130203247;Deze zin uit de conclusie valt nog te bezien jep.. In dat kader is deze vid. ook wel interessant. (Prijs en leveringstaktieken)
RTX 3080;5;0.4372774064540863;ik heb nu een 1080 maar dit is indrukwekkend
RTX 3080;4;0.45630306005477905;Verbeteringen voor motion blur. Als er een grafische optie is die ik per definitie uit zet dan is het dat wel. Goed om te zien dat de geluidsproductie niet de rampzalige proporties heeft aangenomen zoals de panelen voorspelden op basis van het verbruik. Nu nog een maandje wachten op AMD's antwoord.
RTX 3080;3;0.272614061832428;Die Nvidia drivers voor in de test... Beta-drivers???
RTX 3080;4;0.5692433714866638;Nice. Nog even wachten op de 3070, en dan maar eens upgraden. Die 1070 van mij is wel wat aan verbetering toe zo te zien..
RTX 3080;3;0.581985592842102;De verwachte prestaties dus voor de RTX 3080, 25% sneller dan een RTX 2080Ti.. Wel valt de koeling weer beetje tegen van de FE, zeker qua geluidsproductie.. 50+dB is voor mij gewoon teveel, dus dat wordt gewoon weer een 3rd party met betere stillere koeling, ook daar niks verrassends eigenlijk.. Ik ga er dit jaar wel een kopen voor minder dan 800 euro sowieso..
RTX 3080;1;0.2642796039581299;Zelfs als AMD-fanboy moet ik toegeven dat dit écht een hele stap vooruit is qua prestaties. En zie ik AMD dit niet evenaren. Het topmodel vd rx6000 móet dan 100 tot 125% betere prestaties hebben dan de rx5700xt. Dus Nvidia heeft weer de snelste GPU
RTX 3080;3;0.5161400437355042;Ik zie AMD er eerlijk gezegd wel overheen gaan. De prestaties van de RTX 3080 zijn toch wat minder dan verwacht in deze benchmarks. Goed, blijft voorlopig speculeren natuurlijk. Ik zie die 100-125% waar jij het over hebt overigens niet terug in de benchmarks. Welke benchmark is dat?
RTX 3080;5;0.27821215987205505;In die van Daar wordt de 3080 met de 5700xt en readon vii vergeleken.
RTX 3080;3;0.5434184670448303;Het is natuurlijk niet eerlijk om een uiterste als gemiddelde te nemen Hier op tweakers.net zie je dat in de ideale situatie (4k gaming) de 3080 maar liefst 84% sneller is dan de 5700 XT. Nog steeds een groot verschil, maar wel een verschil dat valt goed te maken met de plannen van AMD zelf.
RTX 3080;2;0.3336528241634369;RDNA2 is totaal anders dan RDNA1, plus de grotere die-size en de 7nm+ en dubbel zoveel CU`s en shaders... 100%+ moet redelijk makkelijk haalbaar zijn.
RTX 3080;5;0.3002029061317444;De vorige generatie kon ook niet tippen aan de gtx kaarten. Het wordt weer een upper midrange kaart van AMD, netals de 5700xt
RTX 3080;3;0.3090610206127167;De 5700XT was ook nooit bedoelt om de 2080TI aan te vallen, een bewuste keus van AMD dus.. Dat zullen we met de reviews gaan zien, voor zover het er nu naar uitziet (betrouwbare geruchten) zit AMD tussen de 3080 en 3090 in met iets minder verbruik.
RTX 3080;5;0.4083617925643921;Ja, laten wij het hopen, dat is goed voor de concurrentie. En onze portemonnee
RTX 3080;5;0.3045442998409271;Waarde opvolger van mijn 1070 : D Al wacht ik nog heel even AMD af... maar verwacht dat 't een 3080 wordt
RTX 3080;2;0.43314990401268005;Kleine gains op 1440p en 1080p. Het nieuwe geheugen werkt kennelijk beter op hogere resoluties. En dit zijn de best gebinde chips, en naar verwachting zijn er maar heel weinig FE kaarten. Genoeg geruchten dat de 3rd party ook nog eens slechtere chips (makkelijker te maken) en duurder zijn. Deze generatie is in ieder geval goedkoper dan de belachelijke prijzen van de RTX20 en met de boost is dat in ieder geval goed. Nu hopen dat AMD iets fatsoenlijks kan laten zien. Dat zorgt ervoor dat Nvidia hun beste beentje voor moet zetten.
RTX 3080;3;0.6833063960075378;Valt toch een beetje tegen, bij lange na niet die 100% verbetering
RTX 3080;4;0.4190807640552521;Ik zal wel minnetjes krijgen om deze opmerking. Maar kan ook iemand anders dan Koen de video's begeleiden... dat accent en stemgeluid werkt geen wonderen voor mij. Verder top dat HWI en Tweakers nu samenkomen
RTX 3080;4;0.4129272997379303;Mooie kaart van nVidia en als ik ging upgraden dan zou ik hebben gekozen voor de RTX 3080. Hoe zit het met 144Hz op 4K resolutie is de nieuwe RTX 3080 daar snel genoeg voor?
RTX 3080;3;0.45605966448783875;Kan ik voor wqhd ultra settings op 60fps (80fps is meer comfortabel om de komende jaren ook wat speling te hebben). Beter voor de rtx 3070 gaan?
RTX 3080;1;0.33417996764183044;1,9x prestatie/watt claimen en dan met 50% prestatiewinst tegen 30% meer verbruik = 1,15x prestatie/watt aankomen zetten. Marketing noemen ze dat dan, oplichting is een accurater woord. Jammer dat in deze maatschappij dit de norm is.
RTX 3080;1;0.7522826790809631;Hahahha Wat een onzin ! :-)
RTX 3080;1;0.36895546317100525;De onzin van het woord 'onzin' in een discussie.
RTX 3080;3;0.33339473605155945;But does it run Star Citizen?
RTX 3080;3;0.37223875522613525;Natuurlijk moeten ze nog uitkomen, maar kan iemand hier met ervaring inschatten of de 3080 FE beter koelt dan de koeler van de partners of juist andersom bij stocksnelheden?
RTX 3080;3;0.547973096370697;Van wat ik tot nu toe voorbij heb zien komen halen de kaarten van de AIB partners nog een paar graden af van de FE kaarten. Maar dat kan natuurlijk weer per merk en model verschillen. Maar ik verwacht wel dat de meeste iets betere full load temps weten vast te houden. Misschien wel ten koste van meer lawaai, we zullen het zien.
RTX 3080;1;0.2919442355632782;Misschien valt dit wel helemaal buiten topic, maar welke tijd kunnen we in Nederland een FE bestellen ?
RTX 3080;1;0.32084712386131287;17/9 @15:00
RTX 3080;2;0.333522230386734;Goedemorgen, sorry, maar ben er niet zo in thuis, wat is nu het verschil tussen de kaart die je vanmiddag om 15.00 uur kan bestellen, de Founders editie of geen founders editie? Ik begrijp dat de kans dat je er vanmiddag één scoort klein is, wat dan waar kan je hem dan bestellen? En wanneer? Bvd gr Rob
RTX 3080;1;0.7618708610534668;Dat klopt. Het was gekkenwerk. In 20 minuten ongeveer was alles weg. Het was een pain in the A hole om erdoor te komen alleen al..
RTX 3080;1;0.49062660336494446;Poef... alles weg in een tijd van 20 minuten ongeveer. Heb gelukkig wel nog me kaart weten te bestellen. Nu hopen dat dat allemaal goed zit.
RTX 3080;2;0.4377288222312927;Ik heb toch wel wat bezwaren, de voornaamste is toch wel het energie verbruik waar ik mij toch stiekem aan stoor. Ik zelf denk dat het oa komt door de nogal nieuwe niet bepaald energie efficiente GDDR6 X . Nog een irritatiepunt is dat 8NM procede van samsung, wat eigenlijk een 10 NM procede is, Volgens mij gebruiken ze nog de oude chipmachines hiervoor en niet de nieuwe van ASML. Ja het is een goede kaart maar dit had veel beter gekund als ze naar TSMC waren gegaan, deze hebben waarschijnlijk gewoon nog niet genoeg capiciteit voor de productie van 7 NM chips voor NVIDIA. Er is een kans dat we volgend jaar de TSMC 7 NM rtx 3080 super krijgen die opeens sneller en zuiniger is, omdat er dan meer UV machines door asml zijn geleverd en de GDDR6x opeens efficienter is. Lekker hoor zit je dadelijk met zo'n energie sluper opgezadeld. Nauw ik bedank ervoor, bovendien blijkt uit de benchmarks dat vooral bij 4K winst is te halen, 1440p ook maar al minder. Voor 1080p hoef je het uiteraard al helemaal niet te doen tenzij je een pro gamer bent. Ik houd mijn rtx 2080 nog even, aangezien die waarschijnlijk goed gaat presteren met DSSL 2.0 op 4k en ik daarmee op medium-high settings nog wel 60 fps eruit kan persen met een beetje geluk. Zodra NVDIA 100 watt aan verbruik reductie heeft ga ik een nieuwe kaart halen. Ik heb al uitschieters naar bijna 400 watt verbruik gezien in de benchmarks ik vind dat toch wel raar in deze tijd van efficientie en energiebesparing/reductie. Deze nieuwe kaart lijkt meer op een rtx 2080 ti op steriods, dan een kaart met een echt verbetererd procede en dat is mijn mening.
RTX 3080;1;0.6971574425697327;Die 2 euro aan extra stroom kosten ga je echt niet merken...
RTX 3080;3;0.4345517158508301;Jammer dat qwhd niet als standaard in de benchmarken zit heden ten dage
RTX 3080;3;0.580115020275116;Mooie review, maar ik mis aandacht voor VR performance. Zeker i.c.m. de Valve Index en haar ondersteuning voor 120Hz en 144Hz waarbij de nieuwe generatie videokaarten meerwaarde kunnen bieden als zij een consistente framerate kunnen leveren (voor de niet-ingewijden: in vergelijking met een normaal scherm is het voor de VR ervaring van veel groter belang dat de frametimes consistent laag blijven). Zo ben ik benieuwd naar hoe games zoals Alyx en Elite Dangerous presteren, evenals menig racing en flight sim.
RTX 3080;5;0.27383366227149963;Volgens mij is dit de enige 3080, die daadwerkelijk leverbaar is in Nederland op dit moment:
RTX 3080;1;0.4311679005622864;linkje komt nergens op uit...
RTX 3080;1;0.5884246230125427;Heeft er iemand een idee waarom 699$ zich vertaald naar 719€? Zijn we hier in europa echt zo veel welvarender tov Amerika? Omgerekend met de huidige koers 699$=590€ dit is dus een verschil van 130€?
RTX 3080;1;0.36965760588645935;Amerikaanse prijs is zonder BTW. (Hoe lang zal het duren tot men dit eens doorheeft)
RTX 3080;2;0.36037173867225647;Ow thnx daar was ik me niet van bewust
RTX 3080;5;0.6623077988624573;man man man, ikheb net geupgrade naar een gtx 2060 voor een bak geld. een waardig upgrade van mijn 1050 TI, over 3-4 jaar dan maar upgraden naar de 30 series 😮
RTX 3080;4;0.24250099062919617;Over een tijdje een 3080 variatie die een stuk energie zuiniger is en een klein beetje sneller, tussen beide modellen in geprijsd. Kijken of ik gelijk heb
RTX 3080;2;0.5507511496543884;Jammer dat nVIDIA geen ruk lijkt te geven om vergroening en juist precies de omgekeerde weg opgaat. Hopelijk doet AMD dit beter. Teleurstellende prestaties afgezet tegen de bak meer energie.
RTX 3080;5;0.9448639750480652;Top kaarten Nvidia ! alweer nummer 1! test ook even de rtx 3090 aub :-)
RTX 3080;1;0.39384597539901733;Je bent al snel nr1 als de concurrentie hun kaarten nog niet heeft uitgebracht. *Nvidia.
RTX 3080;3;0.28884488344192505;Dan ben je nog erg jong en ga je nog niet zo lang mee in hardware land. Maar nogmaals, je kan geen “nr1” (wat dat ook mag betekenen) zijn als de concurrentie hun nieuwe serie nog niet uitgebracht heeft. *Nvidia.
RTX 3080;5;0.6990184783935547;Sinds Nvidea sli over pakte van Voodoo 3d zijn ze al top en blijft een top fabrikant ! ze hebben geen ondersteuning nodig van een chip fabrikant AMD om het verder te schoppen zoals Ati toendertijd :-) Dus hou het lekker bij Nvidea. Volgende maand lekker de RTX 3080 erin :-)
RTX 3080;3;0.5926553010940552;Oké duidelijk, je bent nog niet oud genoeg en gaat maar zeer kort mee in hardware land. Overigens heb je een flinke “voorkeur” voor een merk wat niet erg slim is. (Daar is een naam voor....fanb..) Ik zou zeggen verdiep je er eens in, dan zul je merken dat je onzin uitkraamt. *Nvidia.
RTX 3080;1;0.45104315876960754;nieuws: Aankoop ATI bezorgt AMD nieuwe tegenvaller
RTX 3080;5;0.614311933517456;Je hebt helemaal gelijk jongen. I rest my case, succes verder.
RTX 3080;5;0.3675108850002289;Tsja.. wat is hoog :-) Nvidia is gewoon goed ! zie hier alleen maar mensen janken etc.. Je moet hem niet kopen het mag :-)
RTX 3080;5;0.23721742630004883;Het is nvidia. Vroeger 'nvidia
RTX 3070 Ti;3;0.5208579301834106;Prima kaartje als je er een weet te bemachtigen. Wat mij wel opvalt is dat de RTX 3000 serie enorm veel energie verbruikt vergeleken met AMD. De RTX 3070ti verbruikt zelfs meer energie dan de RX 6900XT voor beduidend minder performance. Laat vooral zien wat voor enorme stap AMD gemaakt heeft met de 6000 series. Ik mis in deze review wel echt de vergelijking met de RX 6800. Gemiste kans.
RTX 3070 Ti;4;0.38424935936927795;"Ik heb alleen testresultaten van de 6800 met de 21.2.2-driver (februari). De kaart staat inmiddels aan met de 21.5.2-driver (mei). Ik zal de 6800 toevoegen met de resultaten van de oudere driver; zodra het testen klaar is vervang ik ze door de nieuwe resultaten. Compromis, maar het beste wat ik kan doen op het moment."
RTX 3070 Ti;2;0.40433385968208313;"Wellicht dan ook deze regel aanpassen bij het overzicht en de 6800 toevoegen in ""Prestatiescore games"", want toen ik dat las vond ik dat missen Een echte concurrent uit de rode hoek is er niet in dit segment: de 6800 XT is een brug te ver. Overigens, kijkend naar de imaginary adviesprijzen, dan scheelt de 6800XT maar 4 tientjes met de 3070Ti. Bij de scores van Assassin's Creed: Valhalla staat nu: De 3070 Ti heeft in Valhalla een prestatiewinst van ruim 11 procent op de 3070 op 4k-resoluties, maar dat voordeel slinkt tot slechts een paar procent op lage resolutie, als de game meer cpu-gelimiteerd wordt. Dat laatste lijkt echter niet te kloppen, want andere kaarten halen nog wel 50% tot 60% hogere framerates. De vraag is: welke bottleneck remt de 30xx kaarten op die lage resoluties?"
RTX 3070 Ti;2;0.3408494293689728;scherp, ik heb het stuk in de prestatiescores even aangepast, en bij Valhalla beetje aangepast... ik denk namelijk dat er wel degelijk een bottleneck zit, als je kijkt dat de 3090 niet sneller is dan de 3080 (Ti) bv... ergens kunnen alleen de dikke Navi 21-kaarten performance vandaan halen, mss de mem-bandbreedte? Ben benieuwd wat de gewone 6800 doet hier...
RTX 3070 Ti;5;0.6099382042884827;Super! Dank voor de review! Wellicht ook interessant om in de toekomst te kijken wat de performance van verschillende kaarten is met oude vs nieuwe drivers.
RTX 3070 Ti;2;0.426230788230896;Ik heb vorig jaar gedaan inderdaad: reviews: Tien maanden driverupdates getest - Welke winst kun je verwachten? Heel lastig om conclusies eruit te trekken: het hangt nogal van de games en resoluties af wat drivers doen... dat gezegd hebbende, bij AMD leek er minder te verschuiven dan bij Nvidia (al kan dat natuurlijk ook weer per kaart verschillen....)
RTX 3070 Ti;3;0.3760058581829071;Is het wellicht een idee dit zelfde soort review te doen maar dan met VR? De Nvidia Drivers sinds November 2020 schijnen een 'bug' te hebben waarbij PCVR toestellen vreemde 'hiccups' vertonen tijdens het spelen van VR. Nvidia is er druk mee bezig, maar hun oudere drivers hebben deze problemen schijnbaar niet. Wellicht ook een test waardig... Want waar Nvidia in 'monitor' performance vaak wel excelleren, lijkt het er op dat het in VR performance het er soms gigantisch op achteruit gaat...
RTX 3070 Ti;2;0.5038718581199646;Prima kaartje voor wat? De prijs? Ik vindt het helemaal geen prima kaartje, de prijs/kwaliteit verhouding is hier ver te zoeken. Ook als je de huidige tekorten en prijzen niet in rekening neemt. Die 10% meer performance is gemiddeld, ik heb andere reviews gelezen en gezien waar men net 4% winst behaald. De Readon 6800XT en 6800 zijn prijstechnisch veel interessanter. De XT is bij de adviesprijs maar €30,- duurder, maar je hebt wel een behoorlijke prestatie boost. Als je het gemis van de DLSS variant en raytracing buiten beschouwing laat dan is de 3070 Ti alleen maar goed voor Nvidia en de partners. Hopelijk is FSR van AMD straks ook echt interessant en doet het niets onder dan de dlss van Nvidia. En dan denk ik dat Nvidia echt aan de bak moet als de tekorten opgelost zijn.
RTX 3070 Ti;2;0.4035271406173706;Maar waarom zou je de huidige markt, dlss en Ray tracing buiten beschouwing laten als je gaat vergelijken? Adviesprijzen zijn totaal nutteloos als criterium aangezien geen enkele kaart voor die prijs te krijgen is. Dlss en Ray tracing zijn juist 2 technieken waarvoor gamers een Nvidia kaart willen kopen en het nieuws rondom FSR is (vooralsnog) niet heel positief. Dus stellen dat de AMD kaarten de betere keus zijn om vervolgens te stellen dat je voor die conclusie de realiteit buiten beschouwing moet laten heeft weinig zin volgens mij. Als doorontwikkeling op de 3070 is deze Ti gewoon een prima upgrade.
RTX 3070 Ti;2;0.5050050020217896;Hoeveel mensen of games maken echt gebruik van raytracing? Ik ben absoluut een voorstander van deze techniek, maar de implementatie en de impact op de performance is nog te groot om hier daadwerkelijk gebruik van te maken. De alternatieven en huidige technieken doen qua belichting niet veel onder. Adviesprijzen zijn niet nutteloos, omdat van zowel AMD als Nvidia de reference kaarten gewoon voor die prijs verkocht worden. Of je er een kunt bemachtigen is een ander verhaal. Ga je kijken naar de huidige prijzen dat is de 6800XT nog veel interessanter. Ik laat de realiteit juist niet buiten beschouwing, dlss en raytracing zijn erg interessante technieken, maar een native resolutie draaien is nog steeds het beste. Dus als je met de 6800XT 10-20% meer prestatie hebt dan een 3070 Ti voor 30 euro meer dan is de realiteit gewoon dat je een veel mindere kaart krijgt voor je geld. Helemaal als de 3070 maar gemiddeld 10% minder presteert, maar wel 100 en op dit moment enkele hinderden euro’s goedkoper is en ook dlss heeft. De 3070Ti is dus helemaal geen prima upgrade, maar gewoon een overtrokken 3070 die maar weinig prestatie winst boekt, voor veel meer geld.
RTX 3070 Ti;3;0.402405709028244;Voor de prijs dat ik heb betaald voor de RTX 3070 TI kan ik echter net een 6700 XT krijgen. De 6800 XT is 330,- duurder.
RTX 3070 Ti;2;0.46082019805908203;AMD is goed bezig de laatste tijd, maar met dingen als Optix, RTX en DLSS loopt Nvidia mijlenver voor. AMD komt dan een tijd later met een alternatief, maar tegen die tijd hebben veel ontwikkelaars de oplossing van Nvidia al geïmplementeerd, die vaak ook nog eens beter werkt. Dit is zeker niet zo'n makkelijke strijd als die ze met Intel voeren, dat mag duidelijk zijn.
RTX 3070 Ti;4;0.3679393529891968;Als AMD de groeicurve vast kan houden die ze afgelopen jaar hebben laten zien verwacht ik alleen maar goede dingen. FidelityFX is veelbelovend en voor een eerste generatie zeker geen tegenvaller. Over de implementatie in games ben ik het met je oneens. Waar DLSS gebruik maakt van een zelflerende AI server bij Nvidia waar de ontwikkelaar gigantische datasets moet toeleveren om de AI te trainen, maakt AMD gebruik van een techniek die direct in de processing van de kaart plaatsvindt. Ik ben geen expert maar dit maakt het toch juist toegankelijker voor ontwikkelaars om voor FidelityFX te kiezen? Daarnaast ondersteunt FFX zowel AMD als Nvidia kaarten en beslaat dus een veel groter deel van de markt. Hoor graag of ik het bij het juiste eind heb. Ik leer graag
RTX 3070 Ti;1;0.2623838484287262;Dat is sinds DLSS 2.0 niet meer het geval. Het is inderdaad zo dat AMD's techniek ook op Nvidia kaarten gaat werken. Hoe waardevol dat is moet zich nog bewijzen.
RTX 3070 Ti;1;0.4953843057155609;AMD hoeft nu eigenlijk niet meer te doen dan zorgen dat er genoeg videokaarten worden geproduceerd. Dat wil nog niet vlotten. Volgens Jon Peddie Research is de meest recente verdeling 82% Nvidia en 18% AMD. Zie
RTX 3070 Ti;2;0.5762743353843689;Om nog maar niet te spreken van de gebrekkige software support van AMD. Mijn 6900XT heeft veel last van microstutters in verschillende titels zoals BFV en PUBG. Dan moet ik mij door allerlei bochten wringen m.b.t. configuratie om dat te verhelpen. Even snel een 3070 ingeprikt en alles werkt gewoon zonder problemen. Jammer, want AMD bied betere prijs/performance en performance/watt ratio's, alleen is mijn praktische ervaring dus anders.
RTX 3070 Ti;2;0.46381816267967224;"ik herken me hier dus helemaal niet in met mijn 6900xt. de kaart en software van amd zijn prima, ik kan alle spellen ( tot nu toe ) spelen zonder enig probleem ( incl. bfv en de bij mij niet aanwezige microstutters ). ik denk toch echt dat er ( nog steeds ) iets mis is met je systeem of instellingen ( te ver overgeclocked? ). maar hier hebben "" we "" het al over gehad op het forum. daarnaast, is het vermelden dat jouw 6900xt niet goed werkt nog wel relevant voor je? het lijkt een beetje een rare melding aangezien het lijkt dat je deze op 8 juni verkocht hebt, gezien je advertentie en kopersfeedback? als je problemen opgelost zijn met de 3070 prima toch, waarom dan amd of hun software afkraken? je schreef zelf in de advertentie dat de kaart prima is maar teveel van het goede. ik vraag me nu af het goede van wat... microstutters? link : v & a aangeboden : amd radeon rx 6900 xt mensen zijn snel "" overtuigd "" van hun eigen gelijk. dit zijn enkele voorbeelden die je zo op het forum kan vinden. amd is gewoon lekker bezig de laatste tijd wat goed is voor de concurrentie en dus voor ons als consument. ik koop persoonlijk alle merken en fabrikanten en heb dan ook zowel 3dfx, intel, cyrix amd als nvidia gehad in het verleden ( en heden ) en kijk gewoon wat de beste "" bang for buck "" is op dat moment. overal is iets van te zeggen maar vaak liggen problemen toch echt bij de eindgebruiker. ook ontbreekt bij mensen regelmatig de zelfreflectie en zie je veel "" fanboy "" gedrag wat jammer is. en als laatste, problemen zijn een uitdaging waar we als tweakers groot mee geworden zijn, ook ik heb van alles liggen prutsen met soms een negatief of matig resultaat, dit is juist leuk en geeft voldoening als je het oplost. merken afbranden omdat ze niet voldoen aan je eisen zonder enige vorm van onderbouwing / achtergrond is echter niet echt gewenst."
RTX 3070 Ti;1;0.6287711262702942;Na jaren nVidia, een 5700 gekocht, wilde toch weer eens AMD supporten. Wat een ellende mee gehad. Random gamestutters, lockups en blue screens, zelfs in Photoshop en 3D edit pakketten. Bij gaming eens een lockup of screenfreeze, ja das niet fijn. Maar tijdens mijn reguliere werk is het verschikkelijk. Ook omdat er geen enkele relatie tussen de problemen bleek te zijn. Soms al op de desktop, of tijdens een youtube terwijl er ook een game draait. Dagen bezig geweest, allerhande fixes geprobeerd, zet dit aan, dat uit enz... Alle fora afgestruind. Schoon systeem, 6 verschillende drivers, iedere keer tot op registry niveau volledig verwijderd. Begon zelfs te twijfelen aan mijn super stabiele 850 watt platinum voeding of mijn Mobo. Dan liep het weer een halve dag goed, bam.. uit het niets. Handoek in de ring gegooid, terug gestuurd. Toen voor zelfde geld een iets minder snelle 2060 gekocht. Driver geinstalleerd en het werkte weer allemaal zoals het hoort 24/7. Geen enkele issue (tot op heden, afkloppen), super stabiel. Ik was weer even klaar met AMD voor een paar generaties.
RTX 3070 Ti;2;0.4613613784313202;"Offtopic: Klopt de 5700 (vooral de XT) was een behoorlijke dramakaart, de hardware op zich was best goed maar de software in combinatie met die kaart was inderdaad om te huilen Daarentegen waren bijvoorbeeld de 9700 pro , R9 serie en VII serie best prima kaarten en softwarecombinaties. Hetzelfde geld voor nVidia, de geforece 3 was een topper maar de 4 en 5 heb ik best veel problemen mee gehad. Bij nVidia heb ik nog het meeste bang for buck gehad met de 8800 GTS 512. Ontopic: Mijn persoonlijke mening is dat op dit moment een 3070, 3080, 6800 en 6800 XT de beste prijs prestatie leveren met toch wel AMD iets meer in de lead. Het is absurd dat ik blij ben met een 6900xt van 1000 euro omdat dit de enige kaart is die ik voor msrp kon krijgen. Als je dit 2 jaar geleden zou zeggen (dat je dit van plan was) dan verklaarde ik je voor gek maar nu is het normaal geworden. De 3070/80 Ti is gewoon te duur voor wat je krijgt. nVidia is de prijs aan het vragen die ze toch wel betaald krijgen. Dit vind ik niet echt netjes/ethisch van een bedrijf (ondanks dat een bedrijf bestaat om winst te maken zijn ze eigenlijk een soort scalpers geworden met de 1200 euro voor de 3080 Ti). AMD doet dit toch iets netter door gewoon ""normale"" prijzen te vragen (ook al is de leverbaarheid ruk)."
RTX 3070 Ti;1;0.45648160576820374;"Behoorlijk subjectief maar ik probeer het toch maar in perspectief te plaatsen (eenzijdige herkenning ), Ik als eindgebruiker ervaar deze problemen en ik ben niet de enige. Het internet staat er vol mee. Ja het is anekdotisch, so what. AMD staat bekend om driver problemen en slechte ondersteuning bij spellen, daar kun je gewoon niet omheen. Over fanboyism gesproken. Hier kan ik wel ""omheen"" want het internet staat ook vol met fanboys die halleluja roepen over AMD. Het is een feit dat er voors en tegens zijn. Om op basis van jouw mening gelijk te roepen dat AMD slecht is omdat jij microstutters ervaart gaat mij te ver. Verder zit de Radeon software vol met allerlei toeters en bellen zoals 'Anti Lag' en 'Radeon sampling' die het probleem alleen maar verergeren. Doe het er dan niet in. Op verschillende tech fora's is het eerste advies dan ook om al die 'extra' features van de software suite uit te zetten, mooi product. Die toeters en bellen, zoals jij dit noemt, verergeren jouw problemen ? Hier insinueer je dat deze opties nutteloos zijn gebaseerd op, wederom, je eigen ervaring. De vraag is, is dit wel zo ? De opties werken voor andere prima. Als laatste hoef je deze opties niet eens te gebruiken. Ja ze zitten er in net zoals andere fabrikanten ook opties aanbieden, het blijven echter, zoals het woord zelf al aangeeft, opties. Daarnaast, sinds wanneer is alles wat op internet (en vooral op forums) staat waar ? Verder zie ik niet in waarom het verkopen van een 6900XT relevant is in deze discussie, mijn persoonlijke ervaring wordt er niet beter of slechter van. Dit is zeer zeker wel relevant omdat het nu lijkt of op elke post welke gaat over nVidia of AMD ik jouw, zeer uitgesproken, anti-AMD mening tegenkom. Dit terwijl je zelf in je V&A aangeeft dat de kaart prima is ? Als je zo nodig je mening wil geven over zaken welke niet goed zijn doe dit dan met onderbouwing zonder dat je verzaakt in het ""AMD is slecht boehoe"" verhaal."
RTX 3070 Ti;3;0.48017042875289917;"Relax DiscoNootje, Klopt dit is jouw mening, op zich ben ik het met de inhoud niet eens of oneens. Ik vind het alleen wel jammer hoe mensen een stelling poneren zonder onderbouwing maar met de stelling ""ja maar half internet/4chan (of elk willekeurig forum) is het met mij eens"". Meningen mogen gedeeld worden maar blijf wel netjes en probeer niet zomaar te roeptoeteren."
RTX 3070 Ti;2;0.4356499910354614;Misschien omdat veel mensen dit niet gewenst en offtopic vinden ? Niet echt relevant in een 3070 ti review topic.
RTX 3070 Ti;2;0.41205382347106934;Dit ligt daarom toch niet altijd en enkel alleen aan kaarten van Amd of software van Amd ? Ik heb ook problemen gehad met kaarten van Nvidia hoor in het verleden software matig of hardware matig. Soms geven drivers wel eens conflicten van andere hardware en dan los ik dit nogal veel op met drivers volledig verwijderen of terug gaan naar een vorige stabiele versie. Zo heb ik afgeleerd beta's te installen.
RTX 3070 Ti;3;0.7016009092330933;Lijkt me wat overdreven. Als ik bij techpowerup kijk is de 3060Ti bijvoorbeeld efficienter dan de 6700XT.
RTX 3070 Ti;3;0.47357890009880066;Klopt. En die staat net 4% lager qua performance dan de 6700XT. Maar een 3070ti en een RX 6900XT verschillen dusdanig veel qua performance dat het kleine verschil in vermogen opmerkelijk is.
RTX 3070 Ti;3;0.559391975402832;Het valt misschien meer op omdat de 6900XT in absolute zin ongeveer hetzelfde verbruikt als deze 3070Ti, terwijl de 6900 de top end is, maar de 6800 is duidelijk de meest efficiente kaart. Zullen ze wel gedaan hebben om de 6800XT minder in de weg te zitten qua performance.
RTX 3070 Ti;4;0.2757270336151123;Digital Foundry heeft die vergelijking opgenomen als je die wil bekijken:
RTX 3070 Ti;1;0.7900200486183167;Hoe kom je überhaupt bij deze onzin ? Dit is echt weer zo’n rood v groen reactie. 6900XT verbruikt rond de 400w, met powerspikes naar +600w.
RTX 3070 Ti;1;0.6448920369148254;Sorry zal voortaan een bronvermelding bijsluiten. Info komt uit deze review en de review van techpowerup. Verbruik van zowel de 3070ti als rx 6900XT zitten rond de 300watt waarbij de 6900XT ongeveer gelijk presteert met 3080ti/3090. Meer nodig?
RTX 3070 Ti;5;0.48882508277893066;Kijk nog maar eens goed naar oa de powerspikes die ik benoem. Om een 6900XT goed te voeden heb je echt een goede 850w voeding nodig, of zwaarder. Ik heb zelf een RTX3080 met 760w iON+, en dat gaat prima. Heb zat ruimte over qua wattage verbruik. Zal je opweg helpen
RTX 3070 Ti;2;0.43919041752815247;Klopt. Powerspikes zijn behoorlijk maar niet enkel aanwezig bij de amd kaarten. Daarnaast verbruikt mijn kaart vaker 250w dan 300w. Maar volgens mij was dat niet het punt dat ik eerder maakte. Ik zei toen dat beide kaarten ongeveer 300w verbruiken en dat de 3070ti daar niet naar presteert. The Verge zegt zelfs dat de ti 30% meer verbruikt voor maar 10% extra performance. Zeker interessant te noemen toch? Beetje jammer dat je gelijk denkt dat ik team Red hier enorm zit te verdedigen. Heb zelf een 2070 en 3070 gehad en dat beviel prima.
RTX 3070 Ti;5;0.3995586037635803;klopt, mijn RTX3080 heeft spikes van 400w +-, dat is nog 200w minder dan 6900XT. Maar goed, het is een mooie opvolger van 2070 Super, met betere prestaties dan RTX2080Ti
RTX 3070 Ti;1;0.3406611680984497;Genoeg reviews zeggen dit, maar het is een rood v groen reactie? Zal wellicht niet altijd he geval zijn, maar dat het zo dicht of zelf over de 6900XT gaat voor beduidend minder performance is gewoon een feit
RTX 3070 Ti;2;0.3880496025085449;ga maar een een 6900XT voeden met 750w voeding, dan spreek ik je later wel. Zie je topic met problemen dan wel verschijnen op GoT. Vergeet niet dat het verbruik weer wat zal verbeteren, met nieuwe drivers, zoals dat altijd het geval is. power spikes(20ms) van 620W(6900XT) v 350w (RTX3070Ti) denkt niemand aan....
RTX 3070 Ti;1;0.574015200138092;Hoe kom je überhaupt bij deze onzin ? Heb je dan wel een 6900xt? stock gebruikt mijne 250-280 watt (met +15% powerlimit)
RTX 3070 Ti;5;0.5993744134902954;Ik heb de afgelopen tijd 4 builds gemaakt met 6900XT, en zowel 5800X, 2x 5900X en 1 5950X. Werken allemaal prima, mooie clocks en goede temps. niks afgeknepen, maar goed ingesteld. twee AMD versies, en twee Sapphire Nitro+ OC. (ik ben dus niet van het groene kamp te noemen, en heb mijn kennis niet van YouTube filmpjes )
RTX 3070 Ti;3;0.31955674290657043;Het is een review, geen round-up.
RTX 3070 Ti;1;0.5728582739830017;Klopt! En wat doe je bij een review? Je bekijkt de waarde van het product vergeleken met soortgelijke producten van dezelfde prijsklasse. De directe competitie van de 3070 ti is de rx 6800.
RTX 3070 Ti;1;0.34901168942451477;Een vergelijking hoeft niet per se onderdeel te zijn van de review, dat is waar ik op doelde. Een vergelijking doe je in een round-up, daar is het immers een round-up voor, want dan leg je meerdere producten op de (pijn)bank. Bij een review bekijk je een product tot in de puntjes, daar hoeft niet per se een vergelijking met een andere videokaart bij in te zitten.
RTX 3070 Ti;1;0.3402598798274994;dus in jouw review lijst je de fps waarden op. En wat kan de lezer daar dan mee? Die zeggen niets zonder een vergelijking van een andere kaart op hetzelfde testsysteem.
RTX 3070 Ti;2;0.36484798789024353;"Ik zeg dat in een review ga je de diepte in, daarmee hoef je niet per se een vergelijking te maken, dat is aan de reviewer. Een vergelijking zou ik echt niet missen oid. Bij een round-up verwacht ik wel onderlinge vergelijkingen, het is maar net hoe ""strikt"" je daar in zit, denk ik dan maar."
RTX 3070 Ti;3;0.6845773458480835;Het is ook best interessant dat deze nieuwe kaart niet direct sneller is als de RX6800, terwijl de 3070Ti bijvoorbeeld wel het mooie snelle GDDRX heeft. Er is toch ergens iets mis gegaan wat dat betreft. Qua prijs, wattage, performance etc is het gewoon allemaal niet overtuigend.
RTX 3070 Ti;5;0.3098699450492859;Inmiddels (gistermiddag) staan de resultaten van de 6800 met driver 21.5.2 erbij.
RTX 3070 Ti;2;0.5459182262420654;Meh - 50% energieverbruik ingame erbij voor 10% prestaties bovenop de 3070 FE ... en dan toch 20% meer vragen. Als je deze tegenover elkaar zet in frames per watt (FPW) is het echt geen mooi plaatje Goed dat de RX6800 toegevoegd wordt nog - niet omdat merk A beter is dan B, maar omdat die zowel in FPS als FPW aantrekkelijker is ... maar ook niet te verkrijgen dit jaar. Nou dan gaan we van de vakantiecentjes dit jaar gewoon extra ver en lang weg, wel even lekker !
RTX 3070 Ti;1;0.377424418926239;Zelfs als het 20% meer kost, is het nog steeds een goede deal. Met de kanttekening dat je er één kan krijgen voor msrp. Ditzelfde geld net zo hard voor de 3080Ti.
RTX 3070 Ti;3;0.35000893473625183;Hoe is het dan een goede deal? De vergelijking van Cassius gaat toch nog steeds op voor alle kaarten op adviesprijs? Een vergelijking moet wel gebeuren met gelijke situaties, of alles op adviesprijs of alles op marktprijs.
RTX 3070 Ti;3;0.2809913456439972;"Omdat je een 3080 niet voor adviesprijs krijgt. Als je een 3080TI met adviesprijs kan kopen, ookal ""betaal je er meer voor dan de prestaties waard zijn"" is het een goede deal. Het is wat lastig uit te leggen. Maar dat is nu eenmaal de chip wereld waar we nu inzitten."
RTX 3070 Ti;3;0.33188268542289734;Dat iets ten opzichte van andere producten een relatief goede waarde heeft in de huidige markt, staat niet gelijk aan een goede waarde ten opzichte van de voorgaande markt. Daarnaast neem je nu ook aan dat we hier met goed fatsoen een 3070Ti kunnen krijgen voor de adviesprijs, wat nog te bezien valt. De 3070Ti voor adviesprijs is dus niet de slechtste deal momenteel, maar nauwelijks een goede deal te noemen. \\Edit, ter ondersteuning: Gamers Nexus over de 3070Ti:
RTX 3070 Ti;2;0.4218292236328125;Yep, veel mensen hebben niet door dat een RTX3070 gelijk presterend is aan RTX2080Ti. Dan moet men ook kijken naar de prijs bij uitkomen, toen en nu. Door verschillende omstandigheden zijn veel hardware moeilijk verkrijgbaar. Corona, chip tekorten Miners die ineens 90 stuks kopen voor hun miningtafel etc. @willemdemoor jullie testen een Gigabyte GeForce RTX 3070 Ti Gaming OC 8G. Dat is niet zoals titel en foto laat zien Nvidia FE. (tenminste die indruk word gewekt)
RTX 3070 Ti;2;0.33604684472084045;Dat leggen we hier uit: we testen de kaart als stock en als we een foto van de GB-kaart gebruiken, leidt dat weer tot verwarring (we testen immers niet de aib-kaart hier)
RTX 3070 Ti;1;0.649405837059021;Helemaal eens. Ik denk eerlijk gezegd dat we deze generatie kaarten beter kunnen vergeten. Tegen de tijd dat deze weer normaal verkrijgbaar zijn, zitten we bijna tegen een volgende release aan. De 3000 kaarten zijn voor gamers echt een verloren generatie.
RTX 3070 Ti;4;0.41343191266059875;Fpw lijkt me een mooie metric om kaarten met elkaar te vergelijken.
RTX 3070 Ti;3;0.34681203961372375;Bedankt voor de mooie review! Qua prijs/performance in het huidige landschap is dit zo'n beetje de opvolger van de 2070 Super in de line-up, maar prijstechnisch meer de opvolger van de 2080 Super. Vreemde kaart, gezien die beide 2000-serie kaarten betere performance/watt boden, echter deze kaart het omgekeerde. Geen verbeteringen in het procedé nu de volledige GU104 is ingeschakeld, maar eventuele instabiliteit compenseren met domme voltages! Dat relatieve stroomverbruik.... is dit een Vega64 kaart in vermomming? Echter, lieve auteur, ik mis iets heel belangrijks. Gezien de FE-edition kaarten nog het meest verkrijgbaar lijken (én heel gewild zijn qua looks) mis ik in deze review uitgebreide eyecandy-plaatjes, maar belangrijker nog de temperaturen/geluidsniveau van deze kaart. Zijn deze (net zoals de 3070) prima in orde, of hebben deze een duik genomen door het hogere verbruik? Ook weten we ondertussen met z'n allen dat undervolten veel goeds brengt bij zowel RTX 2000 als 3000 series kaarten (met de vergelijkbare 3060Ti/3070 als uitschieter in posieve zin), én van de RX5000 en RX6000 serie. Wat mij betreft mag dit echt niet meer in een review ontbreken. Zeker bij deze kaart met obsceen stroomverbruik!
RTX 3070 Ti;3;0.30774250626564026;Komt de founders edition kaart dit keer wel beschikbaar voor Nederland?
RTX 3070 Ti;1;0.38034579157829285;Ben ik nou de enige die het allemaal niet meer zo boeit? Er is toch niets te krijgen voor normale prijzen.
RTX 3070 Ti;2;0.3625960946083069;Ben je zeker niet de enige in. Ik snap dat Tweakers wel deze review online moet zetten (buiten sponsors etc om nog), alleen haal ik m'n schouders op en lees even de conclusie. Voorheen was ik wel geïnteresseerd in alle grafiekjes en fps-counters, maar wat voor nut heeft het? Enige dat ik hoop is dat m'n videokaart er niet ineens mee stopt, want dan heb je als PC-gamer echt een probleem.
RTX 3070 Ti;1;0.4220196604728699;Niet eens zozeer als gamer, zou niet weten wat ik in m'n PC zonder igpu zou moeten stoppen als mijn oude 1060 er nu ineens mee op zou houden
RTX 3070 Ti;2;0.3982865512371063;Founders Edition kaartjes zijn te krijgen voor normale prijzen. En nee, je hebt verre van een zekerheid dat je die bij de eerste drop die je meevolgt meehebt, maar een beetje opletten en het is verre van onhaalbaar. Ik heb zelf sinds maart 5 FE's kunnen kopen (2 3070/1 3080/2 3090). Zonder bots, soms gewoon via smartphone. En nee die worden niet gescalpt - daar help ik anderen mee verder.
RTX 3070 Ti;3;0.29486674070358276;Sharing is caring: hoe/waar heb je dat gedaan? Trouwens, waarom 5, daar maak je de krapte toch alleen maar groter mee? (ook al is het fijn te horen dat je niet scalpt, dat zullen we dan maar voor waar aannemen)
RTX 3070 Ti;1;0.4780694842338562;Er zijn verscheidene discords, telegram groepen, Twitter accounts, ... waar FE drops worden aangekondigd. Als Belg kan je die dan kopen bij LDLC, als Nederlander bij NBB. Snel zijn is absoluut de boodschap en de slaagkans is verre van 100%, maar na een drietal keer zou je toch iets moeten hebben. Omdat ik wel wat mensen ken die ook graag gamen voor een normale prijs... En de krapte wordt er niet groter van he. 5 gamers die geholpen zijn en niet verder zoeken naar een kaartje.
RTX 3070 Ti;1;0.6391250491142273;Toch raar dat je dan een risico neemt van 5 dure kaarten te kopen om anderen te helpen ? Als de prijs ineens inzakt zoals de prijs kan doen van crypto's ben je zwaar gejost om er niks op te verdienen . Je kon toch maar 1 kaart bestellen per klant om zo iedereen de kans te geven een kaart te kunnen kopen ? Of je koopt op naam van een bedrijf veronderstel ik dan. Zodra ETH overstapt op POS zakt gans het kaarten gedoe in mekaar als pudding.
RTX 3070 Ti;1;0.8154422640800476;Founders Edition kaartjes zijn niet duur, die gaan gewoon aan MSRP. Ik koop die ook niet zomaar om ze nadien door te verkopen - er zitten in mijn dichte kring al meer dan genoeg mensen die schreeuwen om een kaartje aan MSRP vast te krijgen. Ik koop niet op naam van bedrijf neen.
RTX 3070 Ti;1;0.5533954501152039;Je hebt helemaal gelijk en ik snap ook niet waarom ze elke paar maanden een nieuwe versie uit brengen terwijl de kaartjes niet eens bestelbaar of leverbaar zijn.
RTX 3070 Ti;5;0.5899186730384827;Ze zijn leverbaar vanuit het standpunt van Nvidia, ze worden in grotere aantallen geproduceerd en verkocht dan eender welke vroegere reeks GPU's die Nvidia al op de markt heeft gebracht.
RTX 3070 Ti;2;0.3845836818218231;Die kaarten zaten waarschijnlijk al lang in de pijplijn. Het ontwikkelen duurt gewoon een tijd.
RTX 3070 Ti;4;0.31561389565467834;Het is mooi om te hebben voor als de boel ooit weer eens normaliseert
RTX 3070 Ti;1;0.5388025641441345;Sterkte aan mensen die momenteel een nieuwe PC willen/moeten bouwen, of aan first-time builders. Er is helemaal niets leuks aan nu een PC samenstellen. Gelukkig zijn er genoeg games die op een aardappel draaien om nog een jaartje door te komen.
RTX 3070 Ti;1;0.5102668404579163;"ik had dat sinds kerst willen doen; inmiddels half jaar later ... nog steeds buitensporige prijzen en nauwelijks aanbod. Leuk die nieuwe GPU's, maar aanbod haalt het niet met de vraag. Daarom nintendo switch gekocht (had dat eerder moeten doen ) dan maar even geen FPS games meer"
RTX 3070 Ti;5;0.5243191123008728;En er is altijd nog Google Stadia
RTX 3070 Ti;5;0.5445433259010315;Ik hoor ook veel goede dingen over GeForce Now van vrienden! Je kunt het een uur gratis uitproberen
RTX 3070 Ti;1;0.4135015606880188;Zelfs beter: je gratis account kan 1 uur per keer gamen. Na een uurtje stopt je spel en nadien kan je terug een uurtje gamen. Op zich wel rond te werken door kleine pauze in te lassen en terug in de queue te schuiven .
RTX 3070 Ti;3;0.4258517026901245;lol ik kom niet eens aan 1 uur per dag gamen 😆 klinkt dus wel goed voor mij, hoe werkt het? 🙃
RTX 3070 Ti;4;0.41818374395370483;Op zich kan je gewoon GeforceNow downloaden en dan een Nvidia account aanmaken. Je koppelt dan je Steam library en als je dan een game wil spelen dan start je die gewoon uit Geforce Now op. Gratis zit je denk ik op een uurtje FullHD 60FPS, wat echt wel dik ok is. Latency is ook goed te doen.
RTX 3070 Ti;3;0.37247368693351746;De game die je wil spelen moet wel beschikbaar zijn op GeforceNow, anders ben je alsnog t haasje xD
RTX 3070 Ti;5;0.40727248787879944;Je kan ook gewoon op uw oude gpu gamen dan gezien geforce now niveau gtx1080 is.
RTX 3070 Ti;2;0.36381444334983826;Naar mijn mening is stadia niet voor bestaande pc gamers omdat je niet je eigen game library kan meenemen.
RTX 3070 Ti;3;0.33051660656929016;been there done that, voor mij niet meer, toch veel lag !
RTX 3070 Ti;3;0.387742280960083;Grappig hoe dat zo verschilt. Hier op het limburgse platteland met ziggo is de latency tussen de 12 en 25ms. (Wat prima speelbare latency is voor de meeste games.)
RTX 3070 Ti;5;0.34120458364486694;Ik zie nog steeds geen enkele reden om mijn 2080Ti in te ruilen voor één van deze kaarten, ok ja de 3080Ti en 3090 persen veel meer fps uit maar eerlijk gezegd voor de huidige prijzen zit ik nog steeds zeer goed hoor.
RTX 3070 Ti;1;0.46661657094955444;Mooi nog even een jaartje wachten inderdaad Wellicht komen ze met de 4-serie ook met een acceptabel stroom verbruik. Gaat deze serie ook helemaal nergens meer over.
RTX 3070 Ti;1;0.6206565499305725;Maar echt hoor, de 1650 kwam 2+ jaar geleden uit, en AMD heeft al bijna 4 jaar geen 75 watt kaart uitgebracht (rx 560)
RTX 3070 Ti;1;0.635819673538208;Is dit nu een midrange kaart? Of hoe positioneert nvidia deze eigenlijk? Ik vond mijn 2070 al teringduur (bijna 450 euri) maar 619 of weet ik veel wat ze zich er nu voor vragen is toch echt belachelijk.
RTX 3070 Ti;1;0.5281829237937927;Vandaag de dag is een 1050ti (1 fan) ond de 300 en een 1050ti oc (2 fans) rond de 500. een 2060 zit op 600 en daar boven kom je op een kleine 900eu uit. Het is gewoon niet leuk meer zo.
RTX 3070 Ti;2;0.40257301926612854;Wel om te onthouden is dat de 1050ti nu voor 500 te verkopen is vanwege scalping en te kort aan kaarten. De 3070ti is 619,- van Nvidia zelf, zo de officiele prijs. Zo de vergelijking is niet helemaal juist. De 1050ti was voor de crisis te koop voor 150 euro,- wat een prima prijs is voor zn kaart. Maar de prijs voor kaarten momenteel is belagelijk
RTX 3070 Ti;2;0.33212634921073914;De MSRP prijzen waren nog 'ok', imo, niet schitterend, maar wel een degelijke performance upgrade tov van vorige generaties. 3080 is ~50% (soms een stuk meer) sneller dan een 1080 Ti, 4 jaar later voor dezelfde MSRP. Het eerste probleem is gewoon dat de huidige schaarste ongezien is (bestel maar eens een wagen, gigantische wachtijden ook door tekort aan chips). Ik heb inmiddels 4 jaar geleden mijn RX 580 8GB gekocht en ik zou die vandaag de dag waarschijnlijk nog met lichte winst tweedehands kunnen verkopen als ik zo de prijzen zie op tweakers. 4 jaar later! En ik heb hem notabene destijds al wat overpriced gekocht door de eerste (denk ik?) crypto mining boom. Het tweede probleem is dat AMD en Nvidia het bewust vertikken om low end kaarten op de markt te brengen (een RX 580 killer, zeg maar). Nvidia komt nog het dichtste met de 330 dollar MSRP 3060. Dat zou ik mid range noemen. Maar MSRP lijkt dan ook meer en meer een leugen. Blijkbaar was er nogal wat geknoei met Nvidia en de 3080 'MSRP' omdat Nvidia in het begin de AIBs korting gaf op de chip en later niet meer. Het meest trieste, vind ik dat veel reviewers hier in mee gaan. Als ik '3060 msrp' google zie ik een review die het omschrijft als een 'budget gpu' fucking 300+ knotsen voor een 'budget gpu'. Bitch please. De hele range is naar boven geschoven. 'budget' is het vorige midrange, midrange is de vorige high end, de nieuwe high end is de vorige gestoorde categorie. Laat ons hopen dat het beter wordt, ik zou alvast zeker niet upgraden vanaf een 2070. Heb het zelf wel gedaan vanaf mijn 580.
RTX 3070 Ti;3;0.2666822373867035;"De xx70ti is in de regel wel high-end te noemen, of is in ieder geval de absolute top van de midrange. Zeker ook vanwege de ti toevoeging. Traditioneel vormen de xx50 en xx60 kaarten de midrange en xx80 de high-end. Daar is natuurlijk nog de xx90 bovenop gekomen als ""ultra high-end"" ter vervanging van wat vroeger het SLI/X-Fire segment was. Daar gebruikte men twee kaarten in plaats van een enkele of had men een enkele, gigantische kaart waar dan twee GPU chips op zaten. Low-end kaarten die je zonder externe voeding kan draaien lijken wel een zeldzaamheid geworden. Ik heb ook het gevoel dat de vraag naar high-end / premium producten de afgelopen 10 jaar exponentieel is toegenomen al dan niet door exponentieel toegenomen eisen (verviervoudiging van resolutie, minimaal stabiele 60 fps, liefst 100+, ultra high settings, geen stutters, etc.) waar review outlets graag op inhaken. Mede daardoor voelt het voor mij inmiddels ook alsof een xx70 kaart slechts midrange is, maar dat is enkel omdat men amper nog spreekt over wat er onder de xx60 range uitkomt. Als er überhaupt al iets voor op de markt wordt gebracht dankzij de chiptekorten."
RTX 3070 Ti;3;0.3180215656757355;Meer reviews NVIDIA GeForce RTX 3070 Ti Founders Edition Review MSI GeForce RTX 3070 Ti Suprim X Review Palit GeForce RTX 3070 Ti GameRock OC Review Zotac GeForce RTX 3070 Ti AMP Holo Review Gigabyte GeForce RTX 3070 Ti Eagle en Inno3D GeForce RTX 3070 Ti X3
RTX 3070 Ti;3;0.3105943500995636;Selling point van deze kaart is het stroomgebruik . Edit: Dit zou je misschien zelfs als positief kunnen beschouwen omdat miner liever een zuinige kaarrt hebben. Hoewel je dat misschien met undervolten kan bereiken.
RTX 3070 Ti;1;0.39258918166160583;Mining is niet het probleem van de schaarste, en tevens al zouden ze zo n kaartje scoren dan gaan ze die undervolten waardoor ze alsnog een zuinige kaart hebben.
RTX 3070 Ti;2;0.536059558391571;Uitgaande van de adviesprijs, vind ik deze kaart behoorlijk duur voor wat je krijgt. Hij is nagenoeg net zo duur als een AMD 6800XT, maar qua prestaties en stroomverbruik scoort deze minder. Alleen DLSS is volgens mij een plus op deze kaart. Los daarvan is het nu natuurlijk pakken wat je pakken kunt, maar eigenlijk vind ik dit een hele rare positionering door Nvidia.
RTX 3070 Ti;3;0.33326780796051025;Zou hem wel eens getest zien in MSFS 2020
RTX 3070 Ti;2;0.4676460027694702;Wat ik niet begrijp aan deze kaart is nog de zeer beperkte hoeveelheid ram die de kaart heeft. Dit gaat in de zeer nabije toekomst problemen opleveren op 1440p en 4k. Ik had zelf een 3070 van MSI die ik heb verkocht en draai nu een rx 6800 van MSI. Goedloper en veel meer toekomst bestendig! 16gb vs 8gb! Dat is een wereld van verschil. Begrijp Nvidia om die reden niet, of wellicht wel want dan kan je straks weer een nieuwe kaart kopen om bij te blijven. Een 3080ti met 12gb voor 1200 msrp? Kom op zeg, AMD heeft al met de 6700xt 12gb en Nvidia zelfs met de zwakke 3060. waarom dan niet bij de betere? Verder hele mooie kaarten en de beschikbaarheid begint wel te verbeteren. Ook zakken de prijzen. Een 3070 staat nu voor 1100 te koop en een 3080ti voor 2100 en een 3080 voor 1800 wat nog steeds veel en veel te duur is, maar er is wel een zakkende lijn te zien doordat de mining aan het inzakken is. laten we hopen dat deze trend voortzet.
RTX 3070 Ti;3;0.4514915347099304;Door de hash rate limiter zouden deze kaarten wel eens een stuk beter verkrijgbaar kunnen zijn.
RTX 3070 Ti;3;0.5467570424079895;Ik ben wel enthousiast op zich en wil wel proberen er 1 te krijgen maar zou niet weten waar ik als simpele consument naartoe moet om de bij de eerste lichting mee te kunnen proberen..
RTX 3070 Ti;1;0.3162659704685211;"In de titel staat: ""Nvidia GeForce RTX 3080 Ti Review"". Dit zou volgens mij ""Nvidia GeForce RTX 3070 Ti Review"" moeten zijn."
RTX 3070 Ti;5;0.4285305440425873;al aangepast
RTX 3070 Ti;3;0.3427768647670746;Leuk dat Nvidia wat nieuws aankondigt, maar is deze dan wel te kopen of niet?
RTX 3070 Ti;3;0.5181066989898682;ja, maar zeker niet voor de prijs dat hier aangegeven staat
RTX 3070 Ti;3;0.3765052855014801;Dan laat maar zitten.
RTX 3070 Ti;4;0.6525373458862305;Heel mooi, ik ben zelf al een tijdje uit PC gaming hoewel ik nog wel WOW speel op mijn laptop. Mijn laatste Gamer PC had een 1080 GTX toen hij net een paar maanden uit was. Als de nieuwe Battlefield een modern jasje krijgt ala BF3/4 dan kriebelt het wel weer om voor mij om naar een goeie gamer PC te kijken. Alleen een gemiddeld prijskaartje van ~1700 euro voor een high endgame bak met deze kaart staat me dan wel weer even tegen. Ik ben wel heel benieuwd hoe dit er nu allemaal in het echt uit ziet in 4K in de huidge games. Ik vind BF3 er namelijk nog super strak uitzien Dus ik loop wat achter zeg maar
RTX 3070 Ti;2;0.5575176477432251;Aan de review te zien is de enige interessante reden om deze kaart te kopen mining, dankzij het snelle geheugen.. en juist dat wordt kunstmatig beperkt. Jammer. Ik ben al tijden op zoek naar een kaart om mee te minen als ik niet aan het gamen ben (90% van de tijd). Ik begrijp de achterliggende reden, maar het zou gewoon fijn zijn als je als potentiële koper een keuze had, eventueel tegen een meerprijs.
RTX 3070 Ti;1;0.3580315411090851;eerlijk gezegd ben ik dit jaar gestopt met het lezen van die reviews van Videokaarten, aangezien ze toch niet te verkrijgen zijn of onbetaalbaar. Dus ik sla wel een jaartje over
RTX 3070 Ti;1;0.4218052923679352;Blijft raar een review over iets wat bij niet te koop is of onbetaalbaar.
RTX 3070 Ti;4;0.3446219265460968;Leuk zo'n review van een kaart die niet eens echt te koop is.
RTX 3070 Ti;3;0.37914416193962097;Als ik kijk naar de MSRP van Nvidia voor zo een kaart en de prijs in de winkel, door wie worden we dan nu een poot uitgedraaid? Beetje vreemd als dat dan een paar x over de kop gaat, waarom worden die prijzen dan vermeld.
RTX 3070 Ti;4;0.32013651728630066;Heb er een via Alternate kunnen kopen voor 819,-. In deze markt vind ik dat toch best een koopje (200 euro boven het adviesprijs). M'n pre-order van de 3060 ti voor 519,- maar geannuleerd omdat die dingen toch niet meer worden geproduceerd/geleverd... Goeie kaart, leuke prestaties. CoD Warzone, wat ik toch het meeste speel haal ik toch mooi rond de 120 fps op 2k (zo goed als alles op high/ultra).
RTX 3070 Ti;1;0.38165026903152466;Waarom is de keuze gemaakt om niet de RX6800 mee te nemen in deze review? Dat lijkt me juist een directe concurrent voor deze kaart.
RTX 3070 Ti;1;0.641640305519104;Ja heel vreemd. En de GeForce RTX 3070 Ti verbruikt bijna net zo veel als de GeForce RTX 3080, in ieder geval VEEL meer dan de GeForce RTX 3070, en dat de GeForce RTX 3070 Ti maar 8 a 15% sneller is dan GeForce RTX 3070, en dat allemaal voor €100 meer, als hij te koop was voor die prijs als hij uitkomt, is hij niet echt de moeite waard, dan zou als je die ook kon kopen en voor normale prijs voor voor de GeForce RTX 3080 gaan.
RTX 3070 Ti;2;0.43878084421157837;Eens, en tegen de tijd dat je ze wel kan krijgen zijn er al zoveel nieuwe modellen (of zelfs een 4080) dat de vergelijkingen vrij zinloos zijn.
RTX 3070 Ti;1;0.3238380253314972;Dit artikel bestaat ook nog wanneer de kaarten weer beschikbaar zijn. Als over een jaar bv alle kaarten weer normaal beschikbaar zouden zijn, zouden alle tech reviewers dan opeens reviews moeten schrijven van alle kaarten die het afgelopen jaar uit zijn gebracht?
RTX 3070 Ti;1;0.4135730564594269;Waardoor denk je dat deze kaarten volgend jaar goed verkrijgbaar zijn? De vorige generatie kaarten is niet eens leverbaar, tenzij je 3x de adviesprijs betaald.
RTX 3070 Ti;1;0.6635920405387878;Het was een voorbeeld. Net als velen, heb ook ik geen idee wanneer deze crisis ophoud. En tuurlijk is de RTX 2000 serie en RX 5000 serie niet meer leverbaar, ze worden niet meer geproduceerd. van de RX 5000 serie ben ik niet zeker. Maar de RTX 2000 serie (buiten de 2060, door de crisis is die weer terug gebracht) wordt al sinds augustus volgens mij niet meer geproduceerd. Dus voor de 3080 launch.
RTX 3070 Ti;1;0.6208491325378418;Als ETH overstapt op POS en ze niet meer nodig zijn om mee te minen ! Dan ga je eens zien wat een overvloed aan kaarten er op de markt gaan gedumpt worden omdat ze niet meer nodig zijn. 2dehands gaat overspoeld worden met kaarten die nog rap willen verkocht worden aan de gekke prijzen. Ik wil al meer als een jaar een nieuwe kaart kopen maar vertik het om scalpers en opkopers om te minen een dubbel of 3dubbel bedrag te betalen.
RTX 3070 Ti;2;0.41653257608413696;"Ik weet natuurlijk niet hoe het gros miners hun kaarten fine-tunen om de best mogelijke hashrate/watt te verkrijgen, maar ik zie regelmatig vermeldingen in advertenties als ""met deze kaart is NIET ge-mined"". Dit doet mij vermoeden dat dergelijke kaarten niet altijd goed in de smaak vallen als ermee is ge-mined."
RTX 3070 Ti;1;0.8040340542793274;Ikzelf heb zeer slechte ervaringen met kaarten die gebruikt zijn om te minen. 2 x kaarten gekocht van een miner en 2 x kaarten waar problemen mee waren. Ventilatoren die beste van hun tijd hadden en artifacts. Vaak worden die kaarten ook geflashed met andere bios of OC voor betere opbrengst. Voor mij nooit nog ex mining kaarten. Je betaald bijna de nieuwprijs zodat je ze beter nieuw besteld met gratis games of vouchers en volle bak garantie. En zeker van je garantie wat niet kan gezegd worden van mining kaarten.
RTX 3070 Ti;3;0.4857880175113678;Wat komt er op gang dan? Buiten de 3080/3090 zijn de kaarten niet zo heel interessant als je al een 2070/2080 hebt. Als je puur naar verbruik kijkt dan kun je beter nog even een generatie wachten. Dat hebben ze echt niet voor elkaar nu, geldt overigens ook voor AMD.
RTX 3070;5;0.29812851548194885;@Trygve, looking good! Omdat meer dan 1 review bekijken nooit verkeerd is: YouTube: Gamers Nexus Hardware Unboxed Techtesters JayzTwoCents Linus Tech Tips
RTX 3070;2;0.47025588154792786;Wellicht geen overbodige luxe. Dezelfde fout als bij de 3080 review is opnieuw gemaakt: er is getest met een AMD cpu die handmatig is vastgezet op een lage clock (dus de boost is uitgezet). Opvallend want tijdens de 3090 review heeft tweakers zelf de verschillen al gepubliceerd. Ze geven hier zelf aan dat testen met hogere clockst 20% op 1440p scheelt bij een 3080. In deze review op 4.2 Ghz kom je bij f1 2020 1440p tot de conclusie dat de 3070 bijna even snel is als de 3080 (14%). Maar dat is hoogst waarschijnlijk te rooskleurig omdat de 3080 meer dan 20 fps extra haalt op 5 Ghz, aldus tweakers. In welke mate de 3070 ook last heeft van de 3900X op 4.2 Ghz valt zonder testen niet te zeggen. Dus je kunt wat mij betreft beter kijken naar de resultaten van andere reviewers. Bovenstaande is binnen de context van Ryzen 5000 nog opmerkelijker. Daarvan weten we nu al dat deze voorbij Intel streven op single threaded performance (IPC * clock). Er zijn al wat passmark resultaten gelekt. Over een maand gaat de boel nóg beter schalen dankzij de combinatie van hogere performance én PCI 4.0. (Heerlijke tijden momenteel voor de hardware liefhebber ). Kitguru: Intel Core i9-10900K Overclocked to 5.1GHz on all cores techtesters: Intel Core i9-10900K (MCE) Gamernexus (video): 10700K 5.1 Ghz Techpowerup: Intel Core i9-9900K @ 5.0 GHz (Coffee Lake, 16 MB Cache) Hardwareluxx: Intel Core i9-10900K Guru3d: Core i9 9900K Tomshardware 9900K Overclockersclub AMD 3800X 4.3GHz - All-Core (1.325v) FLCK 1:1 wccftech: Ryzen 9 3900X 4.3GHz All Core Lock (disable one CCD for 3600X Results) Computerbase: AMD Ryzen 9 3900XT, nicht übertaktet Tweaktown: 3800X edit: nuance 'even snel'
RTX 3070;1;0.4408649802207947;Dank voor je feedback, dat is altijd welkom. Dat is geen fout maar een keuze. AMD's boost-algoritme werkt opportunistisch en met het handmatig vastzetten elimineren we een variabele die anders testresultaten kan beïnvloeden. Je hebt het over een lage clock maar de 4,2 GHz all-core op de 3900XT is een overklok, stock komt hij krap boven de 4 GHz uit. We hebben dit nergens in deze review geconcludeerd. In de Prestatiescore is op 4K Ultra de 3080 ruim 30 procent sneller dan de 3070.
RTX 3070;2;0.3817821443080902;Ik zie dat mijn originele tekst niet heel duidelijk was. Ik keek bij de f1 2020 1440p grafiek en zie op tweakers 14% (197.7/172.9) tegen bij 27% (239.1/187.4) bij techpowerup. Ik heb er alle vertrouwen in dat het een prima review wordt als jullie testen met een 10900K op 5.1 Ghz. Ik ben er ondertussen ingedoken en zie dat de stock 3900X met een theoretische boost clock van 4.6 Ghz vrijwel identiek presteert aan een vaste overclock van 4.3 Ghz op all cores aldus gamernexus. Wat dat betreft zal die 4.2 Ghz all core dus inderdaad niet de fout zijn. Wat blijft staan is de keuze voor de 3900X. We weten al dat de keuze voor deze 3900X een drastische impact heeft op de onderlinge verschillen tussen de verschillende videokaarten. Zie jullie eigen test reviews: Nvidia GeForce RTX 3090 - De overtreffende trap van Ampere Er ontstaat een vertekend beeld juist door deze cpu. Dat is het laatste dat je wil als onderzoeker en wanneer je daar (zelf) achter komt dan valt dat eigenlijk niet meer recht te praten om dat door te gaan met de 3900X op 4.2 Ghz. Het liefst test je met een onbeperkt snelle cpu zodat je puur de gpu kunt testen, helaas bestaat die niet. Het beste alternatief is voorlopig om een 10900K met een maximale stabiele overclock te pakken. Je hebt dan een stukje prestatie verlies in de vorm van een theoretische PCI bottleneck máár die is constant en zeer beperkt (aldus overige onderzoeken). Een hertest met Ryzen 5xxx zal het probleem vermoedelijk ook oplossen.
RTX 3070;2;0.5318113565444946;"Je beschuldigt ons van wetenschappelijk onverantwoord bezig zijn maar je kijkt zelf naar één game op één resolutie, dat levert een gigantische bias op. Wij kijken voor ons oordeel van 3D-chips naar 10 games waaruit we de Prestatiescore berekenen. En in de review van de RTX 3090 die je aanhaalt zie je daar dat het verschil in de Prestatiescore op 4K Ultra nihil is tussen de 10900K en 3900XT (let op; geen 3900X zoals je stelt) is met zowel de RTX 3090 als de 3080. Zelfs op 1440p Ultra (naast 4K de andere relevante resolutie voor het prestatieniveau van de RTX 3070) is het verschil maar zo'n twee procent, allerminst drastisch lijkt me. Omdat voor deze snellere videokaarten de cpu belangrijker is dan bij de RTX 3070 en we daar dus al geen verschil zagen, hebben we er voor gekozen de RTX 3070 in combinatie met ons 3900XT-testsysteem te benchmarken."
RTX 3070;2;0.4381386637687683;hetgeen een prima initiatief is. ik zag in die vorige test naast de 20. 9 % bij f1 ook 21. 8 % bij far cry new dawn. dat zijn er al 2 van de 10. samen hebben die een gemiddelde afwijking van 21. 35 %. de overige 8 games zitten tussen de 0. 2 en 3. 8 % verschil. het gemiddelde verschil van deze 8 games komt op - 0. 88 %, de 3900xt is soms sneller. dat verschil is zo klein dat het vermoedelijk binnen de foutmarge van jullie tests valt. met andere woorden zou je kiezen voor de 10900k dan verlies je niks op die 8 games ( die 0. 88 % gemiddeld ) terwijl je op twee games 21. 35 % winst boekt. dan lijkt de keuze voor de 10900k mij duidelijk. als we iets verder uitzoomen in plaats van in de cijfers te duiken dan is wat mij betreft het doel van deze test om de gpu te testen en de impact van overige invloeden zo veel mogelijk te minimaliseren. gezien het bovenstaande is dat nu niet goed gelukt en daar gaat mijn feedback over. dat is hoe het werkt. lijkt me meer iets voor de betaalde staff om dat op basis van mijn kritiek te checken. wat verder het aangekaarte probleem met het ontwerp van de test ook niet oplost. zou ik zelf betaald reviewer zijn dan zou ik zelf uberhaupt mijn resultaten gaan checken met die van een paar anderere reviewers en daar bij grote verschillen waar nodig contact mee leggen om te kijken hoe die verschillen zijn te verklaren dan wel zijn op te lossen. is verder niet erg. zoals te zien doe ik ook niet altijd alles in een keer goed. is niet erg zo lang je de dialoog aan kunt gaan. tot slot welllicht nog zinnig om te toe te voegen dat sinds de hwi / tweakers merge de kwaliteit van de reviews hier sterk heb zien toenemen. nu nog de cpu bottleneck uitsluiten. * control - 3. 1 %, doom 2. 8 %, f1 20. 9 %, fc new dawn 21. 8 %, ghost recon breakpoint - 3. 6 %, metro 2. 7 %, pc3 - 3. 8 %, rd2 - 0. 2 %, shadow 0. 1 %, total war - 2 %,
RTX 3070;3;0.5387575626373291;Op zich heb je volgens mij valide punten. Ik kan me echter voorstellen dat wanneer alle andere videokaarten met een andere cpu getest zijn, het erg veel werk is om die allemaal opnieuw te testen puur met een nieuwe cpu. Dus kan me voorstellen dat je dan besluit de 3070 ook met die cpu te testen, puur omdat je dan niet alles opnieuw hoeft te doen. Als je enkel voor de test van de 3070 een andere cpu pakt zou je anders appels met peren aan het vergelijken zijn. Maar dan nog zou je natuurlijk enkele losse tests toe kunnen voegen met een snellere cpu waarbij je die bijvoorbeeld alleen bij de belangrijkste benchmarks tegen de andere referentiekaarten (3080 en 2080Ti bijvoorbeeld) afzet. In ieder geval als je denkt dat de cpu in sommige scenario's de bottleneck zou zijn.
RTX 3070;2;0.3066437244415283;Zijn er dan problemen of limieten met die Ryzen 3900X ?
RTX 3070;2;0.4155007302761078;Het 'probleem' is dat ook nu nog veel games zwaar op één thread leunen. De 3900X is in dat scenario wat zwakker dan de 10900K. Dit door een combinatie van ontwerp keuzes maar vooral door de gelijkwaardige rekenkracht per clock tik (IPC) terwijl de frequentie bij Intel veel hoger ligt. AMD heeft dit onderkend in hun presentatie van Zen 3 (Ryzen 5xxx). Ze hebben hun ontwerp aangepast om om te gaan met games door de door de L3 cache samen te voegen van 2x 16 naar 1x 32MB. Daarnaast is ook de latency verlaagd:
RTX 3070;5;0.5838505625724792;"Helder ;-) Dank je wel"
RTX 3070;2;0.42582467198371887;Terechte kritiek denk ik, maar zijn de resultaten bij 4k zoveel anders dan?
RTX 3070;3;0.4008844196796417;"Kijken wij naar andere grafieken? Ik zie in de game benchmarks op 1440p zo goed als altijd een verschil van 20-30 fps tussen de 3070 en 3080, dat is volgens mij niet ""bijna even snel"". Alleen in Far Cry New Dawn komt de 3070 heel kortbij, en die titel kennende is daar inderdaad waarschijnlijk de CPU de bottleneck, zeker gezien de framerate op 1080p nauwelijks beter is dan op 1440p. Er zijn nog een paar andere games waar je op 1080p ook wel de CPU bottleneck ziet, dat is inderdaad wel niet ideaal."
RTX 3070;2;0.43847379088401794;Ah ik zie waar het fout is gegaan. Mijn alinea had betrekking op de f1 2020 1440p test, niet op alle games op elke resolutie. Ik pakte deze game er als eerste uit en ben toen de cpu's van elke review gaan bekijken. Voor de volledigheid: op 1440p bij f1 2020 kom ik op een verschil van 14% op 3070 vs 3080 bij tweakers. Bij Techpowerup is dat 27%. Dat vind ik een te groot verschil. Die 14% heb ik beschreven als bijna even snel, daar valt natuurlijk alsnog over te discussiëren of je daar 200 euro voor wil neerleggen. Wellicht interessant om voor meer games directe vergelijkingen te maken tussen tweakers en techpowerup. Maar dat gaat behoorlijk wat tijd kosten.
RTX 3070;3;0.31072789430618286;Techtesters - Intel Core i9-10900K (MCE). Had het in de beschrijving van de video staan, maar zal het ook even netjes op de site erbij vermelden, wel zo handig. Mbt AMD vs Intel. De RTX 3080 hadden we op zowel i9-10900K en Ryzen 9 3950X getest, maar de Intel was structureel wat sneller (1080p zeker, maar gemiddeld genomen ook op 1440p/4K) en daar deed PCIe Gen 4 weinig aan. Dat was één reden om bij Intel te blijven, de andere reden was dat in verschillende tests van andere producten door de jaren heen Intel platformen vooralsnog toch echt net even wat stabieler/consequenter zijn in de prestaties. Vooral als je kijkt naar SSDs, geheugen, etc, maar ook GPUs in een handje paar games die op AMD toch echt significant nekken. Of dat vanaf volgende week anders gaat worden moeten we nog maar zien
RTX 3070;4;0.43799668550491333;Aangepast . Meestal duurt een nieuwe pci generatie inderdaad járen voor dat meer dan 1% verschil geeft. Op basis van de 5600X/5950X passmark single threaded resultaten heb ik goede hoop: Ik heb even gekeken maar met mijn 8700K haal ik die 3400-3600 punten niet. Verder is 4.6 Ghz (boost) x 1.19 IPC effectief 5.47 Ghz dus ook dat geeft hoop. Maar we gaan het zien.
RTX 3070;2;0.30009353160858154;"Het moet gek lopen als ze in single thread benchmarks Intel geen pak slaag geven idd. De vraag is echter of dat betekent dat ze ""gaming"" nu echt van 1080p tot 4K ook echt Intel voorbij gaan"
RTX 3070;2;0.4714321494102478;Ik weet niet wat er zo opmerkelijk aan is. Ryzen 3000 klokt niet tot 5GHz. Een 3950X komt al niet eens voorbij de 4.7GHz. Over welke CPU heb je het nou? Vergeet ook niet dat de klokfrequenties van 2 CPU families met elkaar vergelijken sowieso appel en peer is. De Intel 10000 generatie is nog steeds beter dan Ryzen 3000 in CPU-limited game titles. Ryzen 5000 is nog niet uit, dus kan nu nog niets over gecommuniceerd (laat staan geconcludeerd worden). Dus krijg je reviews waarin je ziet dat de helft voor Intel kiest (peak gaming/1080p performance), en de andere helft voor AMD (populair, PCI-e 4.0) Ik denk dat je in het algemeen wel kan concluderen, dat als je deze kaart koopt, je niet op 1080p medium settings moet gaan gamen.
RTX 3070;2;0.5379791855812073;Ik snap de score van je reactie niet echt want ik vond dit zeer informatief. Slecht dat dit zo getest wordt, dat zou je toch eigenlijk beter verwachten als er zo'n uitgebreide test wordt gedaan.
RTX 3070;2;0.5258323550224304;"Bij de 3080 review werd een verklaring gegeven door tweakers; 1) Ze wilden geen Intel systeem omdat ze er vanuit gingen dat PCI 4 op 4K belangrijker was dan extra rekenkracht. Die gedachte was onjuist, ook op 4K, zie reviews: Nvidia GeForce RTX 3090 - De overtreffende trap van Ampere. Hadden ze vooraf kunnen testen, hebben ze niet gedaan. Dat is jammer, kan gebeuren. Echter nu weten ze het al meer dan een maand dus nu gaat die redenatie niet meer op. 2) Ze wilden de herhaalbaarheid van de test verbeteren door random invloeden van de boost clock uit te sluiten. De 3900X kan hoger boosten (4.6 Ghz) dan de overclock van 4.2 Ghz. Dus het kan zijn dat de performance zelfs lager uit komt in sommige games. Lijkt een groter probleem dan ik dacht; Op 4.3 Ghz is het ongeveer hetzelfde als stock aldus gamernexus. Het doel van de review is ons zo goed mogelijk te informeren. Natuurlijk is daarbij een test die betrouwbare resultaten oplevert belangrijk. Echter wanneer reeds is aangetoond dat je keuze voor 3900X @ 4.2 Ghz een vertekend beeld oplevert en de lezer daardoor juist slechter wordt geïnformeerd dan gaat er gewoon iets mis. Op het moment dat je al weet dat je resultaten niet representatief voor de werkelijkheid zijn ben je al niet meer wetenschappelijk verantwoord bezig. Vol blijven houden aan die 3900X op 4.2 Ghz maakt het dan niet alsog wel een zinnige test puur omdat je één variabele een constante factor hebt gemaakt. Aan het eind van de dag is deze cpu te traag voor deze kaarten."
RTX 3070;2;0.3512757122516632;Maar waarom niet testen op zowel de AMD als Intel platformen en dan het gemiddelde van de twee test systemen aanhouden? En voor de liefhebbers de raw test data aanbieden..
RTX 3070;2;0.40506041049957275;Ik vermoed dat dit teveel tijd kost omdat je elke test dan 2x moet doen en ze uberhaupt elke test meerdere keren moeten doen om een betrouwbaar resultaat te krijgen. Bij de 3090 is dat overigens precies wat is gedaan: reviews: Nvidia GeForce RTX 3090 - De overtreffende trap van Ampere Je bent met één test systeem al zo een paar dagen bezig.
RTX 3070;1;0.5288216471672058;Ach even zen 3 afwachten en alle benchmarks worden weer overgedaan.
RTX 3070;1;0.2942461669445038;Ik vind dat ze gewoon moeten testen op de meest gekochte processor, Dus dat het een amd is, is dan logisch. En video kaartjes testen heeft niets met wetenschap te maken hoor. Ik denk dat behaalde resultaten heel erg representatief voor de werkelijkheid zijn, omdat de meeste mensen op amd zitten en het op de hoge resoluties toch al geen reet uitmaakt. Al stop je er een 8350 bulldozer in, net zoveel fps op 4K.
RTX 3070;1;0.3467644453048706;Hoe kom je erbij dat de meeste mensen op AMD zitten op dit moment? AMD systemen zitten in de lift, dat klopt. Maar dat er meer gamesystemen zijn met een AMD basis is nog niet waar volgens bijvoorbeeld Steam. En niet specifiek games: Link
RTX 3070;5;0.30492764711380005;Toevoeging Youtube: Linus Tech Tips
RTX 3070;3;0.3170506954193115;Nu nog de review van de nieuwe AMD kaarten afwachten en daarop je keuze baseren. Mits de kaarten voor de daadwerkelijke retailprijs verkocht worden.
RTX 3070;3;0.40046441555023193;Ik denk dat 8gb vram moeilijk te verkopen is eind 2020... als de prijs/prestaties gelijk zijn blijf ik dan mooi in het rode kamp als je daar 16gb vram krijgt. Alhoewel deze RTX 3070 kaart dus goed presteert.
RTX 3070;2;0.3606615960597992;Ik heb niet het idee dat meer Vram voor games echt nodig is? Ik heb nog nooit 8gb gecapped ahoewel ik alleen 1080p speel lijkt me het nogsteeds iets wat onnodig veel hoger te willen gaan?
RTX 3070;2;0.35837122797966003;"Dit hangt echt van de games af die je speelt. Ik merk bij sommige games dat 8GB totaal niet genoeg is, en dat op 1080p, laat staan op hogere resoluties. Maar ik heb ook games die amper 2 tot 4GB verbruiken. Beetje ""cheesy"" voorbeeld zou Skyrim met mods zijn. Gooi een goeie lading texture mods er in en de 8GB VRAM zit redelijk snel vol en stroomt over naar systeem geheugen."
RTX 3070;3;0.3029567301273346;Ik heb geen enkele game gezien dat legit meer dan 8gb nodig gehad. Ik speel een heel hoop games. Allocation is niet hetzelfde als verbruik. Cod mw gooit het ook vol, of je nou 8gb of 24gb hebt. Maar de prestaties zijn identiek.
RTX 3070;3;0.29981666803359985;"1080p high gaat vast nog een tijd goed met 8GB, maar ga je naar hoge resolutie en very high-ultra settings dan is 8GB inmiddels een beetje een gok. Zie deze link voor discussie en wat voorbeelden. Doe je maar 1-2 jaar met een kaart -> restwaarde gaat omhoog met iets meer VRAM, kan daarmee alsnog een goede beslissing blijken om nu al voor 10-16GB te gaan ipv 8GB, afhankelijk van het prijsverschil. Doe je er langer mee -> kans zit er dik in dat je er in toekomstige games zelf steeds meer plezier van gaat hebben. Houd je van grote getallen ""niet omdat het moet, maar omdat het kan"", of is er straks zo'n koper in de 2e hands markt als jij doorverkoopt, dan win je geheel los van de techniek altijd met meer RAM. De portemonnee verliest het vandaag natuurlijk wel, maar die is dat dan vast al gewend geraakt ook"
RTX 3070;2;0.5344910621643066;Ik speel op een 4K achtige 32:9 resolutie met alles setting sop high/ultra (geforce 3080RTX) Dit zijn allocaties van geheugen geen gebruik. Dat is zoals ik al eerder vermelde niet hetzelfde. Vram gaat veel verder dan dat, hoe snel kan dit geheugen gevult worden, hoe snel is de GPU zelf, wat is de bandbreedte etc. Beetje zoals de eerste AMD HBM based GPU welke maar 4GB had in de tijd van 8GB kaarten, omdat het geheugen zo rete snel was maakte het per saldo weinig uit omdat het gigantisch snel gevuld kon worden. Een langzame GPU met veel ram is nogsteeds nutteloos. Zoals vrijwel alle RX4XX/RX5XX GPU's met 8GB welke nooit op resoluties vlot genoeg zijn waar dit geheugen nodig is.
RTX 3070;3;0.33296576142311096;Iets al in VRAM hebben is stukken sneller dan het moeten inladen. De snelheid van een PCI-E bus om iets uit systeemgeheugen is waar er meer dan een klein beetje data in moet altijd de bottleneck, vergeleken met een PCI-E bus bandbreedte dan ondergeschikt of het GDDR6 of HBM geheugen is. ID software ontwikkelaar hieronder raadt toch echt aan minimaal 8GB VRAM en liefst net wat meer richting de toekomst. Ik begrijp dat je net een nieuwe kaart hebt met 8GB VRAM, wees daar blij mee en geniet van die games op de ultra. Laat graag wel wat discussieruimte over voor mensen zoals die hieronder welke de games van de toekomst maken en je wat goedbedoeld advies meegeven. Ooit was 4GB VRAM ook voor alles op dat moment op de markt genoeg, een paar jaar later niet meer. Op een bepaald moment in de tijd verschuiven zulke grenzen toch gewoon? Voor zeker niet in 2020, dus geniet van je nieuwe kaart
RTX 3070;2;0.4693302512168884;"De GPU moet snel genoeg zijn om het ook te kunnen verwerken. Alleen is zelfs bij de 3090RTX de PCI-E bus niet eens een limiterende factor in welk scenario dan ook. Dat heb ik niet, ik gebruik een 3080RTX, heb een laptop met een Geforce 2070MQ, had een Geforce 2070 Super. In geen enkel realistisch scenario wordt de volle 8GB benut en ik speel dan op 4K achtige resoluties qua pixel count. Zodra de huidige GPU's die nu met 8GB verkocht worden uiteindelijk meer nodig gaan hebben, dan is de GPU zelf al niet vlot genoeg meer om de graphics op de benodigde snelheid te tonen. Zoals je zelf zegt verschuiven de grenzen, maar een ""langzame"" 16gb in combinatie met een verouderde GPU qua performance over 3 4 jaar gaat niet opeens de game beter draaien omdat de GPU het niet meer snel genoeg kan tekenen."
RTX 3070;3;0.3516891896724701;Ik ga ook geheel mee in het allocation verhaal. Een game die waar het kan 11GB alloceert met verdere identieke fps en frame times vergeleken met een kaart die slechts 6GB VRAM heeft is uiteraard een informatief verhaal. Historisch ligt het toch wat genuanceerder. Er zijn de afgelopen paar jaar nog kaarten met snelle GPU verkocht waar de 4GB nu uiteindelijk toch een bottleneck wordt. Het is allemaal niet zwart-wit natuurlijk, zijn stukjes performance of minder vloeiende texture pop-in. Ik zou geen PCI-E bottleneck verwachten bij een RTX 3090, want die heeft juist wel meer VRAM. Net als de RTX 3080, ook al 10GB. Je krijgt zo'n bottleneck juist bij een kaart met GPU die vlot genoeg is voor hoge resolutie ultra setting, echter met relatief klein eigen geheugen. Dan ga je veel textures uit systeem RAM over de PCI-E bus trekken. Bij een RX5500 4GB speelt een PCI-E bottleneck, anders waren die benchmarks identiek aan de 8GB variant. Call of Duty Modern warfare, 54 fps om 66 fps. Shadow of the Tomb Raider 59 fps om 65 fps. Een 3070RTX is qua GPU zodanig veel sneller dan een RX5500, koffiedik kijken of je in 2023 in elke titel nog met 8GB genoeg hebt, zelfs op lagere resolutie, want de games zullen dan in de basis al veel meer VRAM vragen. Van de andere kant, het scheelt je dan misschien net als in dit verhaal 10-15% performance vergeleken met een 16GB variant in een paar titels. Er zijn ergere dingen, is tegelijk ook niet nul en hetzelfde.
RTX 3070;4;0.3467651307582855;Dit zijn identieke GPU's met identiek geheugen en een identieke architectuur. De 3070RTX bijvoorbeeld vergelijken met de 6800 is dat niet. Of de 3080RTX met 10gb versus de 6800X is dat ook niet Het is inderdaad koffiedik kijken, maar je kan er wel vanuit gaan denk ik als dat de consoles maar 16gb tot hun beschikking hebben voor de GPU en de rest van het systeem dat dit niet snel zal veranderen.
RTX 3070;2;0.33332112431526184;Mijn 580 van AMD heeft al 8 gb en die begint ook al een dagje ouder te worden. Maar videogeheugen is niet alles...
RTX 3070;5;0.4334433674812317;Klopt, ik heb ook al 8gb sinds een RX 480 hiervoor
RTX 3070;2;0.36282116174697876;Zolang er een tekort is zullen de prijzen hoger liggen dan de adviesprijzen.
RTX 3070;1;0.4366409182548523;Dat staat volgens mij in verhouding of de kaarten tegen retailprijs verkocht worden, afwachten dus... Ik zie 3080 nog steeds te koop voor 1000 euro te koop staan...
RTX 3070;1;0.7910287380218506;Er zijn mensen die op marktplaats meer voor een tweedehands artikel vragen dan dat het nieuw kost. En mensen betalen het nog ook, omdat ze denken goedkoper uit te zijn en niet weten wat de waarde is. Boycotten die handel.
RTX 3070;2;0.3588913679122925;Zou zelf altijd wachten op de “normale” prijzen. Wat ik niet snap is dat de tweedehands 2080ti’s voor nog ruim boven de €500 verkocht hebben, voorkeur voor mij zou zijn een nieuwe kaart van de laatste generatie.
RTX 3070;1;0.453164279460907;Waarom wordt HDMI 2.1 niet getest? Waarom niet? Of waarom wordt er vanuit gegaan dat het (goed) werkt? M.i. vragen Variable Refresh Rate (VRR), Auto Low Latency Mode (ALLM), Quick Frame Transport (QFT) etc. erom om getest te worden.
RTX 3070;3;0.27876338362693787;Redactie: zouden jullie bij de games ook FS2020 op hoge resolutie willen meenemen.
RTX 3070;2;0.32281163334846497;@Trygve De 8GB is inderdaad een punt van discussie, wellicht dat de Ti versie meer geheugen gaat krijgen. Maar wat ik eigenlijk wilde zeggen of vragen is waarom je dan niet naar het geheugengebruik van de videokaart heb gekeken tijdens het draaien van de benchmarks. Hoeveel van de 8GB wordt gebruikt en is er ruimte over of is het al krap. Is de 8GB voldoende voor de resoluties waarop je spellen kan spelen met deze kaart? Voor sommige spellen zal dat op 1440p zijn, met andere spellen is zelfs 4K mogelijk. Nou zal een spel op 1440p hoogstwaarschijnlijk wel genoeg hebben met 8GB, maar hoeveel wordt er gebruikt en is er over, zal dat voldoende zijn voor de komende jaren met 1440p gaming. En hoe zit dat op 4K, de spellen die op 4K speelbaar zijn, hoeveel geheugen wordt er gebruikt en is er over en is dat voldoende voor de komende jaren? Er zal een stukje speculatie in zitten omdat we niet weten hoeveel geheugen spellen in de komende jaren gaan gebruiken en welke invloed de consoles hierop gaan hebben die iets meer geheugen tot hun beschikking hebben. Maar pc's hebben weer systeemgeheugen als backup ondersteuning. Misschien hierover eens iets schrijven, voor zover mogelijk. ?
RTX 3070;3;0.31809887290000916;De uitdaging zit 'm hier in het verschil tussen geheugenallocatie en daadwerkelijk geheugengebruik. Moderne games alloceren vrolijk vele gigabytes aan vram maar dit is lang niet altijd noodzakelijk om het spel goed te kunnen draaien. Nvidia zelf stelt dit ook in hun informatie over het uitlezen van de framebuffer.
RTX 3070;2;0.3865039050579071;"Precies dat. Sterker nog, je memory ""Usage"" kan hoger zijn dan de ""Budget"". Alles wat op dat moment niet binnen de budget lijntjes van de gpu past, wordt ge-""evict"" (demote) naar de cpu. Alleen als je dat stuk op eens nodig hebt, wordt de blok ge-promote ten kosten van een andere blok die op dat moment niet wordt gebruikt (die wordt dan ook als evicted gemarkeerd en verplaatst naar de cpu)... en zo krijg je een heen en weer spelletje ten kosten van de performance."
RTX 3070;2;0.5251291394233704;High-end prestaties, dat vind ik overdreven. Naar mijn idee is het dit jaar pas echt mogelijk om in 4K Ultra te gamen zonder een extra hypotheek af te sluiten. De 3070 valt hier echter door het ijs. In diverse games onder de 60 fps. Is het net niet, en als je nu een upgrade uitvoert, dan zit je met die 3070 al snel tegen de limiet aan, dus ook niet erg future proof.
RTX 3070;1;0.5844571590423584;Echt een klassiek tweakers antwoord. Suggereren dat de 3070 door het ijs zakt omdat 4K Ultra niet altijd 60fps draait is echt hilarisch. Sorry hoor, maar je pakt een ontzettende edge case, plakt die op een upper mid range GPU en concludeert dat de 3070 een slechte upgrade is. Iedereen die tevreden was met een 1070 is dat nu ook zeker weten met een 3070. Over 4 jaar draait de 3070 prima mee, net als mijn 1070 dat nu nog doet. Want weet je, niet iedereen hoeft per se alles op 4K Ultra te draaien.
RTX 3070;3;0.3950749635696411;Ik speel nog 1080P op een GTX960, dus alles is in dat opzicht al een goede upgrade, maar ik reageer op de subtitel, high-end prestaties. Voor mijn gevoel is high-end gamen zonder compromis ten aanzien van fps en beeldkwaliteit op 4K. Iedereen moet natuurlijk zelf de afweging maken wat goed genoeg is, en inderdaad zal niet iedereen alles op ultra 4K willen spelen, maar een kaart die net uitkomt, bij courante spellen al de 60 fps op 4K ultra niet haalt, is naar mijn idee geen high-end
RTX 3070;2;0.355417937040329;De titel slaat op het feit dat hij even snel is als de 2080Ti en minder dan de helft zo duur, niet op high-end gamen an sich. In andere markten is high-end niet alleen de aller beste, maar alles aan de bovenkant van de markt, dus waarom dat nu ineens anders doen? De RTX 3070 is duidelijk de bovenkant van de markt aangezien niet heel veel mensen 500 voor een GPU neer willen leggen en de kaart qua game performance in de top 3 snelste game kaarten op de markt staat. Dus de 3070 lijkt me dan wel een high-end game GPU. Maar goed, als we jouw definitie aanhouden is alleen de RTX 3090 high-end en moet je dan gamen op 8K Ultra, niet 4K Ultra.
RTX 3070;1;0.47505486011505127;Rare zin, mist er iets? Maar los daarvan, 'het gevoel van aouwe' en 'wat wel/niet high-end is' staat v.w.b. feitelijkheid (is) uiteraard compleet los van elkaar. 'High-end' is hoe dan ook een rekbaar begrip en daarmee altijd aan interpretatie onderhevig. Als ik zeg dat 8K@60fps (of 1440p ultrawide @ 240 fps, of letterlijk 'wat dan ook' flink hoger dan gemiddeld...) voor mijn gevoel nu het minimum is voor high-end gaming is dat net zo waardevol (of waardeloos?) in een discussie.
RTX 3070;3;0.5383650064468384;"Enige minpuntje; de voedingsconnector in het midden van de kaart. Dat is qua bekabeling toch niet heel handig."
RTX 3070;3;0.6412585973739624;Wel veel beter dan aan het uiteinde van de kaart. In dat opzicht is het wel iets beter. Maar ideaal is het zeker niet.
RTX 3070;3;0.38894933462142944;Meestal zitten de mogelijkheden tot fatsoenlijk cable management aan de rechterkant van het moederbord of in de onderkant van de kast. Er loopt dus altijd een kabel voor of over je GPU heen, ook als je hem verticaal monteert. Ik heb een GTX 1080 nu verticaal gemonteerd en ik kan de kabel (op de connector na) compleet verbergen. Als je looks belangrijk vindt is het gewoon een minpunt, maar ik kan het goed begrijpen als men dat niet met mij eens is.
RTX 3070;2;0.3533042073249817;Ja ik deed vroeger wel aardig wat itx builds en kleine formfactors. Dan is uiteinde verbinding weer een paar MM erbij op die enorme kaarten. Argh! Maar ik begrijp je punt.
RTX 3070;2;0.5186460614204407;Op die manier ziet het niet in de weg voor de airflow, als je deze recht door het midden, tussen de twee ventilatoren naar achteren plaatst. Of het praktische is dan aan de achterkant dat weet ik niet, maar ik vindt het niet eens zo heel erg slechte plek.
RTX 3070;3;0.6380322575569153;Tweede minpuntje: toch weer minder zuinig dan zijn voorganger. Scheelt weer 30W. Natuurlijk ook een stuk sneller, maar ik had liever nog wat meer focus op efficiëntie gezien.
RTX 3070;1;0.37519246339797974;derde minpuntje: het ding is spuuglelijk. edit: belangrijker: maar 8 Gb vram
RTX 3070;2;0.5112794041633606;Voor 500 euro haal je dus heel wat power in huis, maar ik kan daar niet zoveel geld aan uitgeven, doet teveel pijn in mijn portemonnee. Vlak voor de RTX 3000 serie release heb ik een RTX 2060 gekocht, het meeste geld dat ik ooit in een gpu heb geïnvesteerd. In het verleden had ik te lang gewacht totdat de videokaarten afgeprijsd waren waardoor ze opeens niet meer leverbaar waren of opeens vreemd genoeg duurder werden. En ik moest echt een ander kaartje hebben omdat ik onverwachts was geupgrade naar een 4k scherm (was een mooie deal), met mijn gtx 960 kon ik alleen de sims 4 op ultra en (tot mijn grote verbazing) gta 5 op normal settings mee spelen. Hopelijk krijg ik geen spijt van mijn keuze, maar we moeten nog even afwachten tot de kaartjes in het lagere segment worden gepresenteerd.
RTX 3070;3;0.3649734556674957;je had beter kunnen wachten tot na de introductie en bijvoorbeeld op Black Friday je slag te slaan er komen leuke deals aan.
RTX 3070;5;0.2749287486076355;Tja, en zo kun je natuurlijk altijd wel blijven wachten..
RTX 3070;2;0.4009132981300354;"De 2060 Super FE is mijn duurste tot nu toe (€425) en ik vind dat eigenlijk al nauwelijks meer mainstream te noemen, terwijl de review de 3070 van €500+ als ""efficiënte mainstreamer"" bestempelt. Ik zie dat toch echt als de onderkant high-end...ik zou zelf rond de €450 de grens trekken tussen bovenkant mainstream en onderkant high-end. Nou moet ik wel zeggen dat ik een 4K scherm ook geen mainstream vind...1080p is er nog heel veel, maar 1440p is tegenwoordig prima betaalbaar, die beschouw ik op dit moment als mainstream resolutie. De 1060 was de eerste kaart die over het algemeen de 60 FPS op 1440p wel heel dicht benaderde, de 2060 komt er altijd wel boven. Wil je 4K gaming doen op redelijk frames, zal je toch in het high-end segment GPUs moeten kijken."
RTX 3070;2;0.4173707067966461;Ik heb een philips BDM4037U voor 250 euro op de kop kunnen tikken, een 40'' curved 4K monitor uit 2017. IVM ik mijn game pc ook als werk pc gebruik, had ik die extra bureaublad ruimte gewoon nodig. Dit betekende wel dat mijn GTX960, die altijd prima op full hd resoluties werkte, met pensioen kon. Ik heb nog een maandje een AMD RX570 gehad met 8gb, maar dat was het toch ook niet helemaal. Uiteindelijk bood de RTX2060 het meeste waar voor mijn geld, ik heb nog getwijfeld om voor de GTX1660TI te gaan, maar dat is een te groot risico ivm de restwaarde voor verkoop omdat mijn RTX2060 nu al op het randje is wat 4k betreft, dus heb ik besloten om voor dat prijsverschil van 30 euro maar voor een RTX te gaan.
RTX 3070;3;0.45778459310531616;Tja, die RTX gaat het verschil niet maken, dat kun je toch niet aanzetten met een 2060 op 4K. Voordeel van 4K is wel dat je bepaalde settings omlaag kan gooien omdat het er toch wel mooi uitziet door de extra pixels. Maar voor 60 FPS+ op 4K met ultra settings, ja, dan gaat de kaart waarschijnlijk een stuk meer kosten dan het scherm. Aan de andere kant is er met een redelijk budget prima te gamen op 4K, alleen dan niet op Ultra settings.
RTX 3070;3;0.454276978969574;Met monitoren is het inderdaad je eigen keuze, nog prima te doen op 1080p/1440p voor een schappelijke prijs. Voor TV's is 4K echter wel mainstream. Dus wil je daarop gamen zul je een high-end GPU nodig hebben, of genoegen nemen met een lagere resolutie maar dat is toch een beetje zonde. Met 4K is de markt in een aantal jaren toch een beetje scheef getrokken (video vs. gaming) en je ziet nu pas echt goede oplossingen zoals DLSS 2.0. Moest ook wel want de nieuwe consoles kwamen eraan en die moeten wel werken op 4K (althans de klant moet het gevoel hebben dat zijn mooie TV ten volste benut wordt door zijn console). En nu kan Nvidia geen 1500 euro meer vragen voor die exclusiviteit, want een console van 500 euro kan het ook (redelijk).
RTX 3070;3;0.4000558853149414;de 2070 super zit nu nog rond de 500, beetje minder soms. Ik ben heel benieuwd of 500 de prijs wordt, zou best meer kunnen zijn.
RTX 3070;2;0.42173710465431213;Door de leveringen zal het evt al duurder beginnen, maar ook de 3080 begon vrij laag en is toen vrij snel gestegen. Ben je 1 van de eerdere kopers heb je grote kans dat je hem nog voor vrij goedkoop kan krijgen. Mja afwachten hoe goed de leveringen zijn natuurlijk. Echter als de leverbaarheid goed is ben ik benieuwd wat het met de markt gaat doen. Immers een 2080Ti heeft wel meer geheugen, maar presteert ongeveer gelijk. Dus waar prijs je die kaart dan?
RTX 3070;1;0.3777390420436859;Wow sta versteld hoe een ENORME stuk minder stroom de opgenomen vermogen ingames zijn met de GeForce RTX 3070 Founder, met max 215w, dat is 103w minder dan de GeForce RTX 3080 Founder, en de GeForce RTX 3070 Founder gebruikte bij Guru3D 208w, en de GeForce RTX 3080 Founder gebruikte 338w. En de GeForce RTX 3070 is in sommige spellen maar ietsjes sneller dan de GeForce RTX 2080 Ti, maar dan voor bijna 1 derde van de prijs wat de GeForce RTX 2080 Ti koste toen die uit kwam.
RTX 3070;1;0.4769401252269745;Ik sta er versteld van hoe blind bepaalde mensen staren, kijk eens naar de prijzen en positioneringen grafiek in de inleiding. De 2080Ti schiet er ruim voorbij, omdat men dacht dat Raytracing het helemaal ging worden en mensen daar wel even een premium voor gingen betalen. Ook het gebrek van concurrentie heeft niet meegeholpen, maar de prijs van de 2080Ti was gewoon belachelijk t.o.v. de vorige generaties. Je kunt het zien als geweldig waar voor je geld, of je kunt het zien hoe Nvidia de consumenten de afgelopen jaren een oor heeft aangenaaid met hun belachelijke prijzen. Maar zolang mensen het kopen en er geen concurrentie is op dat niveau kunnen ze doen wat ze willen. Maar om nu net te doen alsof Nvidia de kaart voor een prikkie aanbied gaat mij te ver. De prijs zal hoger liggen dan die van een complete next gen console. vindt je het dan nog steeds goedkoop? Edit: de waarheid mag ook niet eens meer gezegd worden zonder dat je door fanboys gedwonmod wordt. reageer dan gewoon met een goed onderbouwde reactie.
RTX 3070;2;0.5586326122283936;Mensen realiseren zich niet dat de chip die's van de 20XX series veel groter waren door de RTX en Tensor cores in vergelijking met hun voorgangers en daardoor dus letterlijk de chips een heel stuk duurder zijn om te produceren. Mensen missen dus eigenlijk een beetje de inzichten om een goed oordeel te kunnen vellen. Of RTX het afgelopen 2 jaar de beloftes heeft waargemaakt valt te discusseren, uiteindelijk gaan de aankomende consoles allemaal iets met raytracing doen dus naar mening wel. Maar de prijs is te rechtvaardigen enigzins. 7.2miljard transistoren voor de Geforce 1080 versus 13.6 voor de 2080. Dat is dus bijna een verdubbeling.
RTX 3070;2;0.549815833568573;Productieprocessen verbeteren ook en worden geautomatiseerd. Ik weet niet in hoeverre jij inzichten hebt in productiekosten van de kaarten, maar ik geloof nergens dat dit in verhouding staat met de prijsverhoging. Je betaald voor de nieuwe techniek en de ontwikkelingskosten, maar dit is bij elke nieuwe generatie het geval. En Nvidia kon vragen wat ze willen in het top segment want er is gewoon geen concurrentie. RTX heeft zijn beloftes totaal niet waargemaakt, net zoals al die games die uit zouden komen volgens Nvidia met ondersteuning van ray tracing. De games die het wel hebben werden vooral door Nvidia ondersteund om ervoor te zorgen dat er in ieder geval titels waren die het ondersteunen. Het prestatieverlies met ray tracing weeg niet op tegen het minimalistische grafische pracht die je er voor terug krijgt. Raytracing is daarbij niet een nieuwe techniek, ja Nvidia heeft het mogelijk gemaakt om dit op thuis pc's in real time te kunnen gebruiken. Maar de implementatie ervan is nog steeds beperkt. Ontwikkelaars moeten nog steeds trucjes verzinnen om het te kunnen schalen zodat de prestaties niet te veel naar beneden gaan.
RTX 3070;1;0.5141662359237671;De wafers zijn naar mijn weten net zo duur gebleven de laatste jaren en daar is geen winst in geboekt en het productie process was niet enorm verbeterd tussen de 10XX en 20XX series. Minimalistisch wil ik het zelf niet noemen, het toont alleen aan hoe goed bijvoorbeeld screen space reflections etc het konden faken maar het is een doodlopend spoor, raytracing niet. Dit heeft denk ik ook niemand beweerd.
RTX 3070;1;0.5124472379684448;Ik staar niet blind naar de GeForce RTX 3070, ik koop hoogst waarschijnlijk geen eens een Nvidia grafische kaart als die van AMD de Radeon RX 6800XT tussen de GeForce RTX 3070 en de GeForce RTX 3080 in komt te zitten, en een stuk goedkoper is dan de GeForce RTX 3080, dan ga ik zeker weten voor de Radeon RX 6800XT, maar dat zullen we morgen allemaal zien. En ik doe ook niet net of Nvidia ze voor een prikkie aan bied, want dat doen ze niet, maar dat AMD nu eindelijk bijna gelijk komen met hun Grafische kaarten zorgt weer wel dat de prijs hoogstwaarschijnlijk eindelijk weer redelijk normaal gaan woorden bij Nvidia, en ik staar niet blind op een merk. En waarom ga je nou weer Game-PC (onderdelen) vergelijken met consoles, dat is zo stom, iedereen weet dat een Console goedkoper is, daarvoor kopen we niet een Game-PC.
RTX 3070;2;0.4310553967952728;Ik zeg ook nergens dat je de kaart zou kopen of blind zou staren op een merk. Maar je vermeld wel dat de prijs maar voor 1/3 is van de prijs waar de RTX 2080Ti voor uit kwam. Hoewel ik het helemaal mis kan hebben, maar ik neem aan dat je daarmee bedoelt dat je voor weinig geld veel krijgt in vergelijking met de RTX2080TI. En dat is nou juist mijn punt, dit is niet eens eerlijk vergelijkingsmateriaal want de RTX2080TI was gewoon extreem duur, helemaal als je het vergelijkt met vorige generaties. Ik maak de vergelijking met een console, omdat er genoeg gamers zijn die straks de keuze gaan maken tussen een next gen console, of eventueel wel een nieuwe grafische kaart. voor de RTX 2xxx kaarten zaten de mid-range kaarten van Nvidia altijd rond de €400,- wat goedkoper is dan een console, maar de prijzen zijn tegenwoordig al ruim boven de €500,- wat voor mij persoonlijk geen mid-range meer is. Als de consoles straks makkelijk 4k gamen mogelijk maakt dan bieden die gewoon veel en veel meer waar voor hun geld.
RTX 3070;2;0.5080108046531677;4K gaming op consoles voor 500 euro komt vooral neer op lage framerates en/of upscaling. De 3070 en 3080 zijn zoveel krachtiger dan de PS5. Of jij dat nodig hebt mag je zelf bepalen. Consoles worden ook tegen kostprijs verkocht, videokaarten met winst. Daarnaast natuurlijk het grote gebrek aan functionaliteit en flexibiliteit vergeleken met een PC. Ik heb alleen daar al een meerprijs voor over.
RTX 3070;2;0.4532955288887024;Ik vindt het al knap dat je weet hoe hoog de framerates op 4K zullen zijn op de next gen consoles. Men heeft hier verschillende uitspraken over gedaan en heeft laten weten dat 60 fps gewoon mogelijk is voor bepaalde games. Waar in mijn opmerking haal je er uit dat ik zeg dat de RTX 3070 niet krachtiger is dan de grafische kaart in de Playstation 5? Maar alleen maar rekenkracht is niet alles, kijk eens naar de kwaliteit die men nog uit de huidige consoles kan halen met erg verouderde hardware. Daarbij ben je bij de console helemaal klaar voor gamen, met alleen een grafische kaart niet. Men heeft al aangegeven dat men toch een recente CPU moet hebben wil men optimaal gebruik maken van de snelheid van de nieuwe GPU’s. Toch iets om rekening mee te houden. De functionaliteit van een pc heeft men niet nodig anders kocht men wel een computer. Ik speel toevallig op beide platforms, maar dat moet iedereen toch zelf uitmaken. Daarbij worden consoles bij de launch zelfs met verlies verkocht. Mensen die van mening zijn dat de hardware veel slechter is zal zich eens wat meer moeten verdiepen in een console. Juist door de grote aantallen en het gebrek aan verschillende configuraties kan men deze voor zulke lage prijzen aanbieden. Wil je de hardware met die van desktop pc’s vergelijken dan zit je al snel boven de 1000 euro.
RTX 3070;5;0.2890264391899109;Je zegt het zelf: iedereen moet het zelf uitmaken. De 3070 is krachtiger en wordt met winst verkocht, dus kost meer, dat is gewoon logisch. Als we kijken naar de PS4 Pro dan zien we al verschillende trucs om 4K gaming mogelijk te maken. Als een 3070 maar net de 60fps haalt op Ultra settings dan zul je concessies doen op een console. Als we het over 4K gaming hebben kun je zelfs een 3080 prima combineren met een AMD 3600 van nog geen 200 euro en een bijpassend moederbord van rond de 100. De 3600 is enigzins recent maar zeker niet duur. Beetje RAM erbij en je hebt een PC voor rond de 1000 euro die op 4K ultra 60 fps haalt zonder DLSS of raytracing. Dat was vorig jaar nog high-end.
RTX 3070;2;0.40034177899360657;"Laten de we 3070 in deze vergelijking er ff bij nemen: cpu - ~200,- gpu - ~500,- mem - ~100,- mobo - ~100,- psu - ~80,- ssd - ~50,- kast - ~80,- muis - ~20,- toetsenbord - ~40,- headset/speaker - ~50,- windows - 145,- Totaal: ~1365,- Monitor reken ik niet mee, console heeft immers ook een tv nodig, maar die tv komt wel meteen met speakers ingebouwd vaak. Maar.... 1365 of 500 voor het hele ""4k gaming"" pakket? En dan ook nog eens half, zo niet minder, het stroom verbruik? Ook nog eens minder storage dus daar komt nog eens een lading boven op in kosten wil je dat uitbreiden. hmmmmmm"
RTX 3070;2;0.3192598223686218;Dan ga je ervan uit dat je nooit een PC hebt gehad. Dat is inderdaad een grotere investering. Maar de laatste vier items kunnen voor de meesten er wel af. Zelf ook mijn huidige kast en opslag ook al voor meerdere systemen gebruikt. En als ik een upgrade doe verkoop ik het oude spul en krijg ik weer wat terug. Games zijn ook vaak goedkoper dan op console, en veel meer keus. Maakt ook niet uit, voor ieder wat wils. Ik zou mezelf niet beperken door alleen een console te kopen waar een PC voor zoveel meer gebruikt kan worden.
RTX 3070;5;0.6421592831611633;En je configureerd hier een systeem wat dus op papier veel sneller dan de ps5. 100 euro voor geheugen, 16gb dus, dan nog 8gb video ram erbij. Dat biedt dus meer. Een gpu dat een heel pak sneller is etc. Dat in een machine dat je kan uitbouwen en meer kan dan alleen gamen.
RTX 3070;3;0.5088707804679871;Tis niet veel sneller. 6 cores ipv de 8 in de console, wss iets betere gpu prestaties voor de pc, 16gb ram idd, maar is ddr terwijl de consoles geheel op gddr draaien, geoptimaliseerde ssd in de consoles tegen een generieke in de pc.... beide kanten hebben voor en nadelen, al met al denk ik dat het redelijk vergelijkbaar zou zijn.
RTX 3070;3;0.5641608238220215;Iets betere GPU prestaties voor de PC? Maak daar maar een flink pak van. GDDR6 heeft meer latency maar hogere throughput, het is dus nadeling voor game logic, voordelig om je textures er in te knallen. SSD performance wordt aangepakt door middel van de Directstorage API in Windows, dezelfde API dat zij voor de Xbox Series X gaan gebruiken. De aangekondigde consoles zitten qua GPU performance rond de Geforce 2070 Super niveau volgens een Epic medewerker welke vragen had beantwoordt rondom de performance op de PS5, de Xbox series X is misschien een tikkeltje sneller. De 2070 Super zou sneller moeten zijn dan wat er in de PS5 te vinden is. Dat is nu 2 jaar oude technologie. PC's prijziger maar veelzijdiger, games zijn goedkoper. Consoles hebben vooral een plek dat het compromis goed gemaakt wordt door de inleg kosten. Maar praten alsof de prestaties vergelijkbaar zijn is niet helemaal correct omdat de propositie anders is.
RTX 3070;3;0.2563956081867218;Het is al meerdere malen aangekaart dat met name de xsx een gpu prestatie heeft van tussen de 2080/2080ti.
RTX 3070;2;0.4146746098995209;Dat is een aanname gebasseerd op de TFLOPS en de prestaties van Gears5 wel in de tussentijd flink geoptimaliseerd is geweest. Het is echt geen 2080TI achtige GPU wat er in zit.
RTX 3070;1;0.3566829562187195;Overdrijven is ook een kunst bij je zie ik. Aerocool Prime-G-BK-v2 Zwart Midi Tower Behuizing Seasonic Core Gold GC 650 PSU / PC voeding Gigabyte B550M DS3H moederbord AMD Ryzen 5 3600 processor Corsair DDR4 Vengeance LPX 2x8GB 3200 C16 Geheugenmodule Gigabyte AORUS Gen4 1TB M.2 SSD Logitech Desktop MK330 Qwerty US GeForce RTX 3070 Totaal €1100 En als je de Gigabyte AORUS Gen4 1TB M.2 SSD vervang met de Intel 665P 1TB M.2 SSD ben je: Totaal €1000 kwijt. En Windows 10 Professional CD Key koop je voor heel weinig legaal bij veel winkels. En een headset/speakers hoor je niet er bij te rekenen, aangezien die ook niet bij een Console zit. Dat is al weer €350 goedkoper dan wat jij totaal opgaf. En de Xbox Series X zal ook niet zuinig wezen, de Xbox One X gebruikte al 172W bij spellen, en dat zal bij de Xbox Series X weer een stuk hoger zijn. En de opslag is bij de Xbox Series X ook 1TB M.2 PCI-e 4.0 x4 SSD, niks meer dan in een PC die ik boven aan gaf.
RTX 3070;2;0.3705759048461914;Bij gebrek aan beter kon Nvidia idd bruut hoge prijzen vragen. Daarom heb ik sinds de gtx970 (heb sli opstelling) geen upgrade gedaan, vond het simpelweg te duur. Nu begint het toch weer aantrekkelijk te worden (500 euro is nog wel toe doen, mss over ene half jaartje wat minder) (al vraag ik me af in hoeverre de processor etc. niet de bottleneck wordt, 3570k)
RTX 3070;5;0.6763386726379395;Heb hier ook een 3570k die jaren terug op 4.6 GHz is vastgezet. Best value ooit. Voor huidige games verwacht ik met de RTX3070 weinig tot geen CPU bottleneck op 4K / 60 FPS. Meer FPS op 1440p wordt wel een probleem. Komend jaar zullen er meer games komen die baat hebben bij een octacore, dan ga ik rustig kijken naar een nieuw Intel of AMD platform. Kijk nu ook de GPU frenzy nog even toe, heb liever een goed uitontwikkelde chip (yield) en kaart.
RTX 3070;3;0.46178698539733887;Ik zou er niet teveel van verwachten. Ookal draai je die CPU op 5GHz, het blijft een oud beestje van 22nm met een kleine cache en traag geheugen, en die zal behoorlijk last krijgen op 4K. Ik heb mijn 4670K vervangen door een 3600 omdat die toch behoorlijk begon te kraken op hogere resoluties (weliswaar icm een 1070Ti, geen 3070). Ik gok dat het met een I5 3570k vs Ryzen 3600 zo'n 10-15% FPS verschil maakt op 4K.
RTX 3070;3;0.3505798876285553;Klopt, voor mij is ~10% lagere FPS in 4K te doen voor komend jaar. Kan die 12 jaar oude HX620 voeding ook nog laten zien wat hij ooit waard was, nooit aan SLI toegekomen. Eind 2021 heb ik wel een knappe dodecacore die weer >8 jaar meekan net als de legendarische 3570k
RTX 3070;1;0.44424739480018616;Ik begrijp het nog niet helemaal, is de 3070 vandaag vrijggegeven, of toch pas op 29 okt.?
RTX 3070;1;0.4145529866218567;De kaarten zijn te koop vanaf de 29e, de reviews mochten vandaag om 14:00 worden gepubliceerd.
RTX 3070;1;0.2794564366340637;De release is verplaatst na AMD keynote, om op het laatste moment nog de prijs omlaag te kunnen schoppen indien nodig. De reviews zijn voor vandaag vrijgegeven, om voor in het gunstigste geval (dat er geen goedkoper alternatief zou zijn van AMD), mensen te laten wachten als morgen AMD direct in de verkoop zou gaan.
RTX 3070;2;0.4400547444820404;Prijzen zullen niet naar beneden gaan verwacht ik. AMD kondigde zijn release van de RX6900 aan (8 oktober) nadat de datum van de 3070 is verzet (Delay RTX 3070 the Verge). Dit lijkt er bij mij dus meer op dat AMD gebruik heeft gemaakt van de problemen van NVIDIA en hierop heeft gereageerd. Is natuurlijk ook afwachten wanneer de kaarten van AMD beschikbaar zijn om te bestellen.
RTX 3070;5;0.7008024454116821;ook natuurlijk geniaal van Nvidia, vandaag reviews van de RT3070 Founders edition, en 2 dagen later weer reviews van de Aftermarket RTX3070, zo houden ze de focus op hun eigen producten over meerdere dagen en is natuurlijk een mooie stoorzender voor de RX6xxx Serie die AMD gaat aankondigen
RTX 3070;1;0.5144726634025574;"Concurrenten proberen elkaar altijd af te troeven. De ene keer is het aandacht afleiden (zoals bij AMD en Nvidia nu), de andere keer een naam van de concurrent kapen (zoals AMD die plots een chipset x399 noemt, na Intel's x99 en x299). Als je Nvidia als ""pestkop van de klas"" ziet zegt dat volgens mij veel meer over je eigen denkbeeld en voorkeur dan over Nvidia."
RTX 3070;1;0.6046674251556396;Tsja ben je het hele gameworks debacle vergeten? Ze gebruiken vaker slinkse methoden van marketing en vaak vind ik het manipulatief. De x399 chipset van amd ken ik niet eens. Dat valt echt in de marge ivm wat intel en nvidia flikken.
RTX 3070;1;0.3854072690010071;"Als AMD ergens goed in is, is het wel mobiliseren van hun meer extreme fans. In elk willekeurig Nvidia artikel, je ziet keer op keer op keer de zelfde mensen die alles wat met Nvidia te maken heeft zo negatief mogelijk willen zien/draaien, en alles wat AMD doet met de mantel der liefde bedekken. En daar enorm veel tijd besteden. En genoeg mensen die gevoelig zijn voor dat overdreven negatieve gedrag worden daar in meegenomen. Zo blijft ""het gameworks debacle"" nog jaren iets waar je op terug kunt vallen, zonder enige nuance."
RTX 3070;1;0.4429830014705658;Tsja, ik denk dat je exact hetzelfde kan zeggen van de nvidia fans. Ik vind nvidia echt de über van de hardwarebedrijven. Dubieuze marketing, gekke incentives, enorme prijzen en winstmarges. Amd doet echt niet alles goed, zo hebben ze met vega rare keuzes gemaakt en ook de rdna1 drivers waren niet op tijd af. Ik heb nu een 1080ti maar zou graag weer over naar AMD. Heb ook altijd veel AMD videokaarten gehad in het verleden. Hoop dat Intel erbij komt, dat we straks weer 3 partijen hebben voor high end gpu's. Gameworks is echt niet het enige, het is wel een van de duidelijkere illustraties van hoe nvidia zaken denkt te moeten doen.
RTX 3070;2;0.40609729290008545;Dat kun je zeggen, maar het is niet waar. Er is een groot verschil tussen geen AMD kopen totdat ze zich bewezen hebben met drivers voor een generatie of twee, of je best doen aan alles wat Nvidia doet een negatieve draai te geven. Ik zie eigenlijk niemand die een negatieve draai aan AMD nieuws probeert te geven. Ik ben persoonlijk geen fan van doorgeslagen kapitalisme, maar winst marges is iets raars om een specifiek bedrijf op aan te kijken. Winst marges optimaliseren is het doel van alle bedrijven. Dat is AMD ook aan het proberen, en dat lukt ook aardig. En als AMD dat doet is iedereen lyrisch. Prijzen hangen af van de doelgroep. In het verleden was gaming iets voor jongere mensen met niet heel veel geld. Die generatie is nu ouder en voor genoeg mensen is gamen nog steeds een hobby, en is 1500 euro niet heel veel geld voor een hobby. Zolang er goede minder dure alternatieven zijn zoals 3080 zie ik het probleem niet. En zelfs als er een keer een generatie is die niet heel veel toevoegt, zoals 2000, zie ik het probleem ook niet zo. Kun je zonder problemen een keer een generatie overslaan. Met marketing bedoel je waarschijnlijk die onzin waar het Nvidia kwalijk werd genomen dat ze geen marketing budget beschikbaar stellen voor merknamen die ook voor AMD producten gebruikt worden? Ik heb nooit begrepen waarom dat een probleem is. En over gameworks... De meeste effecten werken prima op AMD GPUs, als de tessellation levels wat beperkt worden. Dat is een optimalisatie die game devs moeten doen, of een limiet die AMD in de drivers kan instellen. Alleen physx was in het begin echt een probleem. Maar sinds physx 3.0 draait dat prima op CPU, en dat is sinds 2011. Na 9 jaar is het tijd om dat een keer los te laten.
RTX 3070;1;0.7488608956336975;"Ik zie eindeloos berichten van mensen die AMD in een negatief daglicht proberen te zetten. Vwbt gameworks was het wel in de categorie, we betalen ontwikkelaars dik om een heel specifieke functie van onze kaarten te gebruiken, zonder dat dat dat er mooier uit ziet, maar gewoon omdat onze concurrentie er dan slechter opstaat. Je kan dat goedpraten met, oh dat kan je uitzetten, maar dat heeft best een tijd geduurd voordat we daar achter waren met elkaar. En al die tijd leek AMD gewoon trager. Nee qua marketing bedoel ik alle marketing. Net zoals ze nu allemaal review embargo's opheffen rond de launch van AMD kaarten (is echt tactisch), dat ze zeggen ""overspoeld zijn door de onverwacht hoge vraag"" (oh echt had je verwacht dat in europa minder dan 1000 mensen 2x 2080 performance wilden voor 750€? Ik geloof dat niet, zeg gewoon eerlijk dat je er niet meer kan maken). In veel communicatie zit iets wat niet klopt, je weet gewoon dat je (gedeeltelijk) op het verkeerde been wordt gezet. Misschien komen daar de gefrustreerde reacties vandaan (ik weet dat ik het daarom doe, zo van, zie je dan niet dat ze ons proberen te bedotten). Als een bedrijf een DSM label zou kunnen krijgen scoren ze hoog op narcisme denk ik."
RTX 3070;2;0.3477460741996765;"Eerste de beste artikel over AMD: reviews: AMD wil Xilinx: de strijd met Nvidia en Intel om datacenters en supe... Ik zie niet tot nauwelijks iets negatiefs, en de ene reactie die twijfels uit over RND budget voelt zich genoodzaakt om daar bij de vermelden dat hij een AMD fan is. Heb je 1 voorbeeld van een Nvidia nieuws artikel zonder gezeur over Nvidia? En Gameworks is gemaakt om sterke punten van Nvidia uit te buiten. Net zoals Tressfx is gemaakt om de sterke punten van AMD uit te buiten. Ook Tressfx had in het begin een veel sterkere invloed op FPS van Nvidia, en het heeft zeker niet heel lang geduurd voordat duidelijk werd dat de zwakkere tesselation de oorzaak was van lagere FPS bij gebruik van hairworks op AMD GPUs. Beide bedrijven sponsoren devs om spellen voor hun tech te optimaliseren. Waarom is dat alleen bij Nvidia een probleem? Over beschikbaarheid van 3080 heb ik geen relevante cijfers gezien. Wat is de totale vraag en productie? En hoe verhoud zich dat tot eerdere generaties? Dat hele ""ze liegen"" en ""ze doen het gewoon expres"" is volgens mij alleen maar gebaseerd op onderbuik gevoelens, of hoogstens geruchten. De reden waarom dit met de 3080 zo hoog oploopt is volgens mij alleen maar dat de vraag veel hoger is dan GPUs/CPUs die in het verleden moeilijk verkrijgbaar zijn geweest."
RTX 3070;1;0.49292850494384766;Want amd deed niet hetzelfde met benchmarks te releasen net na de release can de 3080?
RTX 3070;1;0.5643945932388306;'net na' ? 01/09/20 : officiele aankondiging RTX 3000 17/09/20 : Start verkoop RTX 3080 24/09/20 : Start verkoop RTX 3090 08/10/20 : teaser RX6000 27/10/20 : opheffen embargo RTX 3070 FE 28/10/20 : officiele aankondiging RX6000 29/10/20 : Start verkoop RTX 3070 **/**/20 : opheffen embargo RTX 3070 AIB **/**/20 : start verkoop RTX 3070 AIB De data rond opheffen embargo én start verkoop 3070 FE werden bovendien ook verplaatst NADAT AMD 28/10/20 had aangekondigd als datum waarop de RX6000 uit de doeken word gedaan. Lijkt me duidelijk dat Nvidia elke vuile truuk uit de kast haalt om AMD's launch te dwarsbomen. Beetje triest als je het mij vraagt.
RTX 3070;1;0.3650301396846771;De RX6XXX serie was geteasted mid september De eerste benchmarks verschenen rond het moment van hun Zen 3 presentatie.
RTX 3070;1;0.257276326417923;Om heel eerlijk te zijn zie ik het als iets willen zien omdat je het merk gewoon neit ziet zitten. Kan ook andersom gezegd worden dat AMD de RX serie ging aankondigen net rond het tijdstip dat Nvidia zijn launch had wat al in Augustus aangekondigd was. Alle bedrijven in deze industrie proberen de overhand te krijgen. Nvidia doet dit, AMD doet dit. Was overigens niet alleen die foto, ze releasden ook met mondjesmaat benchmark resultaten voor games als COD MW op 4K.
RTX 3070;2;0.3074040710926056;Ik had eerlijk gezegd niet verwacht dat hij daadwerkelijk even snel zou zijn als een RTX 2080 Ti. Nouja erg knap om deze nu voor hopelijk EUR 500 te kunnen kopen t.o.v. de kosten van de RTX 2080 Ti. Hopen dat de beschikbaarheid ietsjes beter is dan die van de 3080/3090.
RTX 3070;3;0.46359768509864807;Helemaal mee eens maar ik vind de prijzen van zowel vorige als deze generatie nog steeds te duur...Het wordt hoog tijd dat AMD zich ermee gaat bemoeien.
RTX 3070;2;0.3948988914489746;Die was echt wel een stuk duurder geweest, veel grotere chip (letterlijk 2x zo groot), meer geheugen (wat toen nieuwer/duurder was) betere/grotere stroomvoorziening, nvlink chip/aansluitingen. 2080ti zal vergelijkbare kosten hebben als een 3090, niet de 3070...
RTX 3070;1;0.41779884696006775;@Trygve, Hebben jullie ook de geluids- en warmteproductie getest van deze kaart? Deze staat wel op de testverantwoording maar nergens in de review zelf.
RTX 3070;5;0.527850866317749;Klopt, deze gegevens zijn we nog aan het verwerken en worden zsm toegevoegd!
RTX 3070;4;0.4386945366859436;dank je wel voor de snelle reactie
RTX 3070;4;0.3799920380115509;De pagina is inmiddels bijgewerkt met de resultaten voor geluidsproductie en temperatuur.
RTX 3070;4;0.2846148908138275;
RTX 3070;2;0.3923228979110718;Zouden jullie voor de volgende Vkaart AMD6000/Nvidia3000 vergelijkende review ook de resultaten voor VR mee willen nemen. Ik heb de vraag ook hier geplaatst met wat meer toelichting. Ik denk dat de meeste race en flysimmers, een genre waarin VR wel populair is, dit zeer kunnen waarderen.
RTX 3070;4;0.4322775900363922;Het lange wachten op de release date en vervolgens op de levering hebben mij doen besluiten maar compleet op big navi te gaan bouwen. Leuk, compete AMD Build, net als vroeger met mijn eerste athlon. Ik kan me niet voorstellen dat deze 3070 wel ineens eenvoudig leverbaar wordt.
RTX 3070;3;0.534156858921051;Maar je kan je wel voorstellen dat Big Navi eenvoudig leverbaar gaat zijn? Ik zou dat optimistisch noemen . Ik wacht nog wel even af waarschijnlijk. Sowieso tot de 29ste . Maar ik verwacht nog wel wat langer, wanneer er daadwerkelijk reviews van RDNA2 komen enzo.
RTX 3070;2;0.3971553444862366;Valt nog te zien... We zijn ondertussen anderhalve maand na de release van de 3080 en die is nog altijd gewoon niet te vinden. Dat is toch uitzonderlijk hoor. Volgens de geruchten zijn er toch serieuze problemen bij Samsung om de yield en/of productiecapaciteit op punt te houden. Dat gaat geen probleem zijn bij AMD, die ondertussen al jaren ervaring hebben met TSMC 7nm (wat ook ondertussen een heel volwassen proces is). Dat betekent niet dat je de eerste twee weken zomaar een kaart gaat kunnen vast krijgen, maar het lijkt me niet dat het maanden gaat aanslepen zoals bij Nvidia. Ach, we zullen wel zien zeker? Dit is exact waarom ik nooit plan om te kopen vlak na release. Plan op een paar maanden na de release, dan kan je veel rustiger je keuze maken...
RTX 3070;1;0.62110435962677;Diezelfde geruchten die stellen dat er een 3080Ti komt op basis van dezelfde chip, wat volstrekt onmogelijk zou zijn met slechte yields? Stel je voor dat ze een absoluut desastreuse yield van 50% hebben voor de 3080. Dan met een perfecte yield, en als dat de beperking was voor beschikbaarheid, waren er twee keer zoveel 3080s geweest. Denk je dat met twee keer zoveel 3080s er niet nog steeds enorme tekorten zouden zijn? Bij 5700XT release was het een maand wachten voor er uberhaupt AIBs kwamen. En ook die hadden wel tijd nodig voor ze echt beschikbaar kwamen (granted, niet zo lang als Nvidia nu). Uiteraard hoop ik dat bij AMD de beschikbaarheid beter gaat zijn, maar ook daar lijkt het alsof de AIBs pas op laatste moment uberhaupt samples hebben ontvangen, dus ik moet het nog maar zien. Edit @Niosus bedoelde inderdaad 3080ti
RTX 3070;2;0.4694138765335083;Ik neem aan dat je 3080 Ti bedoelt. En ik ga akkoord dat dat moeilijk te verklaren valt. Gewoon puur qua performance trappen snap ik niet wat een 3080 Ti eigenlijk zou moeten inhouden. De 80Ti is vaak 30-40% sneller dan de 80. Maar nu zitten ze met de 3090 die op ~15% boven de 3080 zit. Hoe ga je daar nu nog een kaart tussen krijgen? Ik snap de strategie alleszins niet. Ik weet gewoon dat we het nog niet vaak gezien hebben dat de kaarten zo moeilijk vast te krijgen zijn voor zo lang, met uitzondering van de Bitcoin craze misschien. Veel vraag hebben is leuk, maar als je in de verste verte niet aan de vraag kan voldoen dan laat je ook gewoon geld op tafel liggen. Als AMD nu wel gewoon redelijk kan leveren op een paar weken tijd, dan kan je er gewoon zeker van zijn dat er een hoop mensen een AMD kaart gaan kopen omdat ze gewoon geen Nvidia kaart vast krijgen. Niet het einde van de wereld voor Nvidia, maar dat kan toch het plan niet geweest zijn...
RTX 3070;5;0.681557297706604;Je hebt een punt en dat verontrust me enorm!
RTX 3070;5;0.5256343483924866;Alle signalen wijzen erop dat AMD deze RDNA2 kaarten goed kan leveren.
RTX 3070;2;0.37551605701446533;Welke signalen dan? Dat de xtx alleen door AMD in beperkte oplage wordt geleverd? Of dat net als bij Nvidia ook hier de aibs pas op laatste moment samples hebben gekregen? Ik heb nog niks gelezen wat specifiek op goede levering wijst, en ik volg het toch redelijk.
RTX 3070;5;0.2848619818687439;Wat er bij leakers beschikbaar is. Ook reacties van AMD personeel via diezelfde route. Veel beter beschikbaar dan de 30x0 serie. Afwachten natuurlijk maar ik zou niet van een paper launch uitgaan.
RTX 3070;1;0.5093529224395752;Wat heb je aan een kaart die toch nergens te koop is?
RTX 3070;3;0.2892792522907257;Haha, meer een preview voor de lezers ja.
RTX 3070;5;0.4312985837459564;Ooit. Op een dag ver ver in de toekomst. Dan kan je, heel misschien, deze grafische kaart kopen. Net een introductie van een sprookje.
RTX 3070;5;0.3330266773700714;"""In a galaxy far far away..."""
RTX 3070;1;0.5691726803779602;Waarom kopen mensen überhaupt nog die dure losse gpu's? Met de ps5 en Xbox series x opkomst heb voor 599,- € 4k@30-60fps
RTX 3070;3;0.4555974006652832;Omdat mensen van PC gaming houden, mooiere graphics en meer fps. 4k@30/60fps op een console is leuk maar is wel met settings turned down. Met een high end gaming PC zet je alles open en dat ziet er gewoon een stuk mooier uit. Ik vind het wel chill om soms even op de bank te hangen, lekker lui en de console aan te slingeren maar als ik gewoon echt flink wil gamen is dat achter mijn PC.
RTX 3070;3;0.34182730317115784;Sommige mensen vertellen zichzelf dat ze de GPU nodig hebben voor machine learning projecten… en video games.
RTX 3070;2;0.4058852791786194;Omdat je honderden games in je Steam lijst hebt staan, graag met muis en toetsenbord speelt en al je vrienden ook op PC spelen? Misschien speel je graag genres die je niet (veel) op de console kan vinden (RTS, MMO, VR, sim)? De consoles zijn niet slecht, ik heb thuis ook gewoon een PS4 Pro staan. Maar voor mij is het niet of/of. PC is mijn prioriteit, en binnen een paar jaar koop ik wel één van de nieuwe consoles voor een fractie van de prijs wanneer ik meteen ook een hoop goede games ervoor kan kopen. Laatste puntje: vergeet ook niet de prijs van de games. Je gaat deze generatie 80 euro mogen neerleggen voor nieuwe AAA spellen voor je PS5. Natuurlijk, tweedehands is een ding en die prijzen zakken ook wel in sales... Maar toch. Prijzig. Op PC is het toch een stuk makkelijker om goedkoop aan goede games te geraken, en de sales op digitale platformen gaan een stuk dieper dan op de consoles. Om nog niet te spreken van de gratis games die Epic uitdeelt. Voor een fervente gamer is de TCO toch een stuk meer dan de aankoopprijs van de hardware. En dat weten MS en Sony natuurlijk ook. Waarom denk je dat die console überhaupt zo goedkoop zijn?
RTX 3070;1;0.31123611330986023;Omdat een spelcomputer nog steeds een fundamenteel ander product is dan een pc.
RTX 3070;5;0.37743446230888367;omdat je dan ook kan werken , wat je niet kan met console , alsook uitbreiden (ram ssd )
RTX 3070;3;0.33364415168762207;Omdat PC gaming niet 1 op 1 te vergelijken is met console gaming..
RTX 3070;3;0.3282524347305298;Denk dat je het over 8K zal hebben
RTX 3070;3;0.2726500630378723;Ik weet nog niet hoe dit bij PS5 gaat zijn , maar bij PS4 werdt er om 4K of 1080p op 60fps te behalen, veel kwaliteit en render settings een stuk omlaag gehaald om die resolutie soepel te behalen. Daarnaast, als je al een bestaand systeem hebt, al is het nog een 4th gen i5 met 8-16GB RAM gooi je er nu een €500 kaart erin, heb je al mogelijkheid om de meeste games op maximale settings te spelen met 1440p op 120fps (ja er is 4k@120, maar ik denk persoonlijk dat 1440p nog steeds de stabiele sweet spot is) met backwards compatibility, modding, emulatie, ander PC software, mogelijkheid voor zoveel HDD's en SSD's als je moederbord maar kan als je games te groot worden, etc. Exclusives zijn ook steeds meer ook naar PC aan het gaan (helaas vaak wel op Epic, niet zo hel epic vindt ik, maarja), maar natuurlijk blijven er nogal wel wat op de consoles...
RTX 3070;2;0.38666900992393494;Dat eerste plaatje in de body op de eerste pagina zorgt voor een optische illusie waar de fans wat draaien als ik de tekst er boven lees
RTX 3070;2;0.3745759427547455;Ik vindt het wel grappig dat bepaalde personen voordat er reviews waren al deden alsof dit een wereldkaart is met geweldige prestaties voor de prijs. Gelukkig dat Tweakers ook al aangeeft dat dit ook bij de vorige generaties gewoon het geval was. Mensen zijn alleen van slag om de RTX2xxx serie. Daarmee probeerde Nvidia te doen alsof dit een wereldschokkende vernieuwing was op gebied van grafische kaarten met ray tracing. En daar vroeg men ook een premium voor. Bij de inleiding zie je een grafiek die het mooi weergeeft wat mij betreft, kijk alleen eens naar waar de RTX2080 Ti prijstechnische gepositioneerd is t.o.v. de GTX 1080TI. Wat ik persoonlijk gewoon belachelijk vond, maar zolang er geen concurrentie is kan men doen wat men wil en blijkbaar werd de kaart ook nog genoeg verkocht om niet meteen prijsverlagingen door te voeren. Ik was van plan om een binnenkort een nieuwe computer aan te schaffen met eventueel deze kaart, maar gezien de leverproblemen van de andere kaarten en de prijzen hoop ik dat AMD het beter voor elkaar heeft en een betere prijs/kwaliteit kan leveren. Wat meer concurrentie is voor de consument alleen maar goed.
RTX 3070;1;0.5862255096435547;En nu wachten we op morgen, dan weten we tegelijk of deze kaarten duurder of goedkoper dan hun adviesprijs verkocht zullen worden. Maar sowieso, geld zal er vloeien richting GPU's dit jaar. Het geheel met het verschijnen van een nieuwe Playstation en Xbox én corona, is een schot in de roos voor beide partijen lijkt mij.
RTX 3070;2;0.4904036223888397;Jammer van het geheugen van 8gb. Ik ben toch blij een 2080ti gescoord te hebben voor hetzelfde bedrag eerlijk gezegd.
RTX 3070;3;0.2542722523212433;Midrange is waar AMD vooral een lans wil breken en ben daarom ook erg benieuwd wat ze tegenover de 3070 gaan zetten.. Naar deze review kijkende wordt dat best interessant denk ik.
RTX 3070;1;0.3748524487018585;@Trygve Ik mis een test met Davinci Resolve en Priemiere Pro. En kunnen board partners geen 3070 of 3060 variant uitbrengen met meer geheugen?
RTX 3070;4;0.3221461772918701;Zoals verwacht ongeveer RTX 2080Ti prestaties dus, inderdaad net zoals destijds bij de GTX 970 en 1070.. Niet verkeerd als je hem dan ook echt voor rond de 500-600 euro kan kopen.. Voor mezelf kijk ik nog steeds naar een RTX 3080 als vervanging voor mijn RTX 2080, hopelijk duurt het niet te lang voordat je die voor de normale adviesprijs kan kopen zonder hele lange levertijden..
RTX 3070;3;0.4809921383857727;En toch.. als ik voor €519 een 3070 of 2080Ti kan kopen.. dan neem ik liever de 2080Ti voor meer vram uiteindelijk. Maar het is goed om te weten dat ze weinig van elkaar verschillen op 4k/1440p. Hopelijk nemen de 2080Ti in de V&A nu wel een snoekduik van rond de 700 naar de 500 euro.
RTX 3070;2;0.39466848969459534;Appart dat de 3070 minder goed presteert met raytracing dan een 2080 ti -.- Dacht dat ze flinke stappen vooruit hadden gemaakt met rtx.
RTX 3070;3;0.6417340636253357;Denk dat het core for core wel beter presteerd, er zitten alleen minder in de 3070 kaart vermoed ik.
RTX 3070;5;0.28326690196990967;De RTX 2070 en RTX 2070 super verslaan de RTX 3080 (met gemak) op 1080p. Apart?
RTX 3070;5;0.4506935775279999;Ik hoop zo erg dat AMD aantrekkelijke kaarten aanbiedt tegen een mooie prijs en daarnaast genoeg stock heeft zodat iedereen NVIDIA laat vallen.
RTX 3070;2;0.3397465646266937;Mainstream, voor 500 euro? Misschien voor hard core gamer, maar niet mainstream voor de gemiddelde Nederlandse PC koper.
RTX 3070;2;0.47923487424850464;Wel duidelijk, over een paar maanden, als de prijzen wat normaliseren koop je dus liever een 3080 voor slechts 150 euro meer. Het verschil in prijs is dan te klein om het niet te doen. Tenzij de 3070 naar 300 euro zakt.
RTX 3070;1;0.5175834894180298;Hoe kom je bij 150€? Het verschil tussen de twee kaarten is 200€.
RTX 3070;5;0.3791808485984802;En de 2080ti is doodverklaard. Ik ben benieuwd wat er nu met de 2dehandse markt gebeurd. Ik zit in ieder geval klaar met een zak popcorn.
RTX 3070;1;0.4357026219367981;Waarom? Heeft dezelfde performance dus mensen gaan de 3070 niet aanschaffen als ze een 2080TI hebben met meer v-ram. Mensen met een 2080TI houden lekker de 2080TI gezien de andere kaarten überhaupt niet leverbaar zijn.
RTX 3070;1;0.6633106470108032;Ik zag 2dagen terug nog een paar prijzen van 650 tot 750 voorbij komen, en nu opeens staat er niks meer op. Hmm, toeval ? Als ik een 2080ti voor 300/350 kan krijgen dan neem ik geen 3070, en ik krijg dan ook nog eens meer vram ook! (1080p120hz en een 4k tv)
RTX 3070;5;0.3521665930747986;Vanaf 1000€ pricewatch: MSI GeForce RTX 2080 Ti Gaming X Trio
RTX 3070;1;0.6849042177200317;Zie dat nog maar eens te verkopen, terwijl een kaart van 500 euro (nieuw) het zelfde doet.
RTX 3070;1;0.5075299739837646;Je gaat voorlopig geen 2080TI krijgen voor €300, misschien pas over een jaar of zelfs langer.
RTX 3070;1;0.4687740206718445;Als mensen 600/700 betalen dan niet nee. Maar die gene moet eens 2x achter de oor gaan krabben, je gooit je eigen ruiten in.
RTX 3070;3;0.35298460721969604;Dit is allemaal leuk, maar er staat op dit moment geen enkele Foudners Edition in de pricewatch...en dat terwijl de kaart vandaag geïntroduceerd wordt.....waar blijven de verkopers?
RTX 3070;1;0.4488063454627991;De NDA op de reviews is vandaag afgelopen, en de kaart is aangekondigd. De verkoop begint pas de 29e... (Dat had best in het artikel gemogen)
RTX 3070;3;0.43355169892311096;en dan mag de prijs toch best vandaag bekend zijn? ik zeg niet dat t vandaag leverbaar zou moeten zijn, maar wel dat webshops ze gaan aanbieden!
RTX 3070;1;0.4428991377353668;Asjeblieft: En afaik zijn er niet veel webshops die FE's verkopen
RTX 3070;3;0.29086312651634216;Op Azerty staan al enkele prijzen. Gaan waarschijnlijk nog wel wijzigen eens de vraag omhoog gaat. Enkele van Asus en MSI: ASUS TUF GAMING GeForce RTX 3070 OC - 659 (non-OC 599) ASUS ROG STRIX GAMING GeForce RTX 3070 8G - 699 MSI GeForce RTX 3070 VENTUS 3X OC - 559 MSI GeForce RTX 3070 GAMING X TRIO - 619
RTX 3070;1;0.3401026129722595;bArAbAtsbB had het specifiek over de Founders Edition, en die komt pas de 29 beschikbaar voor verkoop.
RTX 3070;1;0.40229353308677673;Waarom is eigenlijk de Vega 56 wel opgenomen in de resultaten maar de Vega 64 niet? Dit in verband met de segment plaatsing van de kaart toen deze uit kwam?
RTX 3070;1;0.7302834987640381;Waarom is de Vega überhaupt opgenomen? Die is totaal niet relevant meer.
RTX 3070;3;0.46774056553840637;Omdat veel mensen om de zoveel jaar upgraden is het juist wel fijn om te zien wat je erop vooruit gaat, ik heb zelf een 1080, helaas is die niet getest maar de 1070 wel, bij die scores tel ik een 10-15% op ( verschil 1070 en 1080 ) en dan zie ik makkelijk dat een 3070 me fps zou gaan verdubbelen
RTX 3070;5;0.45366406440734863;2080ti voor 500€ dus. Mooi dan. Nu nog kunnen laten leveren thuis.
RTX 3070;3;0.3409304916858673;Inderdaad, dit is het segment waar ik naar op zoek ben als vervanging van mijn huidige Rx580 Even AMD afwachten morgen en de komende periode wat de beschikbaarheid van de nieuwe kaarten gaat zijn. Mijn voorkeur gaat uit naar AMD maar afgezien van de 3090 zijn de nieuwe Nvidia kaarten ook erg aantrekkelijk. Ik heb geen haast of directe noodzaak om te upgraden (hoewel, CyberPunk .....) edit: hm, er zit maar 8 GB vram op.... hopelijk komt AMD met meer op hun Big Navi's... Ik verwacht dat dit voor nieuwe games een bottleneck gaat zijn omdat de nieuwe consoles ver daaroverheen gaan...
RTX 3070;2;0.35549893975257874;Same situation here. Ik zit alleen in twijfel over de upgrade naar een 1440p scherm danwel een 4K scherm. 4K native is met een 3070/2080Ti net kantjeboord, en jij hebt net zoals mij met een RX 580 nu ook al een tijdje spellen gespeeld met concessies. As je een nieuwe kaart koopt, dan wil je ook niet meer settings hoeven terugschroeven. Gelukkig kan je op een 4K scherm wel weer met DLLS naar 4K schalen. Lastige keus.
RTX 3070;2;0.37863874435424805;Volgens mij veel beter te gaan voor 1440p 144hz.
RTX 3070;3;0.42796388268470764;Het is geen 2080Ti voor minder geld. Het is de opvolger van de 2070/super met betere prestaties voor waarschijnlijk het zelfde geld.
RTX 3070;3;0.370235413312912;Misschien ben ik de enige of lees ik de review niet goed of past het niet binnen de nieuwe doelgroep. Maar in mijn vrijetijd vind ik het leuk om zo nu in dan iets te maken in Premiere Pro en vooral in After effect. Niks bijzonders, puur hobby, maar iedereen die daar dingen in doet weet hoe fijn het is om previews snel te kunnen renderen ect. Als hobbyist moet het betaalbaar zijn, maar waar mogelijk maximaal bijdragen aan een snelle edit. Ik zou het zelf fijn vinden als daar ook een pagina aan besteed zou worden. Volgens mij worden nu niet eens het aantal cuda cores genoemd? Ik kan me voorstellen dat in deze introductie review dit iets minder relevant is dan in een vergelijking met andere kaarten. Maar ik zelf zou daar wel graag zien welke ik het beste kan kopen voor die doeleinden.
RTX 3070;1;0.2904839813709259;Hier staan de reviews voor dat soort zaken. Tweakers zijn doelgroep is veel meer gewoon de gamers onder ons.
RTX 3070;2;0.44136616587638855;Wat ik hier toch wel duidelijk mis, is iets meer uitwijding over geheugengebruik en -limiet. Dat de FPS-nummers etc. er zijn is mooi, maar juist met de discussie over of dit wel genoeg geheugen is had ik echt gehoopt dat er ook werd gekeken naar hoeveel van dat geheugen er dan ook daadwerkelijk gebruikt wordt. In flightsims zoals de nieuwe MS Flight Sim is dit enorm belangrijk en vanwege de enorme hoeveelheid objecten en de render distance kan een sim als die of X-Plane makkelijk een enorme lading VRAM vreten. Datzelfde geldt eigenlijk ook gewoon voor moderne AAA games. De vraag of 8GB wel genoeg is, had wat mij betreft wel iets meer behandeld mogen worden en eventueel zelfs een los stukje verdienen m.i.
RTX 3070;3;0.29113948345184326;als je voluit met MS FS wil spelen ga je toch niet voor de 3070? dan pak je toch altijd de 3080 of de 3090?
RTX 3070;3;0.4816317856311798;Dat hangt af van o.a. resolutie natuurlijk, en ik zeg ook niet dat ik hem voluit wil spelen. Het punt is vooral dat ook met lagere performance je VRAM nog steeds kan vollopen en je daardoor stuttering enzo krijgt. Prestatie van de GPU zelf en VRAM usage zijn tot zekere hoogte niet onlosmakelijk verbonden. Ik hoef hem niet op 4K Ultra met intense FPS te spelen, want ik heb maar een 1440p 60Hz en 1080p 144Hz scherm. Ik wil echter wel dat als ik die spellen voluit kan spelen met een 3070, dat ik niet aanloop tegen 100% VRAM usage wat daarom impact op de performance heeft.
RTX 3070;3;0.6423811912536621;Ik had meer verwacht van de 3070, ~28% meer performance dan zijn voorganger is wat karig. Ben toch wel meer benieuwd naar morgenavond als AMD hun kaarten gaat releasen.
RTX 3070;3;0.43571436405181885;Je moet het zien in verhouding tot de prijs die je ervoor betaald. De 2070 lag bij launch in veel gevallen boven de 600 euro
RTX 3070;1;0.38926294445991516;En deze gaat dat ook zijn. De 3070 heeft dezelfde MSRP als dat de 2070 dat had, beide 499.
RTX 3070;1;0.5993562936782837;Ik heb geen woorden voor de 3000 series, wat een monsters.
RTX 3070;3;0.42762646079063416;Jammer dat de GTX 1660 Super niet in de vergelijking is meegenomen (heb ik momenteel). Kaartje van rond de €250,- en schijnbaar nog steeds erg populair door de goede prijs/prestatie verhouding. Ben wel benieuwd hoe hij zich in verhouding tot de andere in de lijsten genoemde GPU's tot stand houdt. Ach ja, dat wordt dan dus zelf vergelijken her-en-der. Vooralsnog kijk ik eerst de stand van zeken omtrent levering (en wat AMD doet) aan, voordat ik een actie voor een nieuwe kaart uit zet... Ik heb zelf het idee dat niet meegaan in de huidige hype de geduldige gamers zal belonen...
RTX 3070;2;0.35363203287124634;Ongeveer 8% trager dan de GTX 1070. Dat gezegd zijnde ik heb een vier jaar oude 1070 en kocht onlangs een ultrawide 3440x1440 scherm met freesync. Tot mijn grote verbazing draaien The Division 1 en 2 en Project Racing 2 supervlot in combinatie met een oude i5-6600K. Misschien heb ik maar 40fps maar door de freesync voelt het even vlot als 60 op mijn oude 1080p scherm.
RTX 3070;1;0.4482833445072174;Nu nog daadwerkelijk beschikbaar maken van de kaarten, dat blijkt nogal een probleem Ik zou graag een 3080 kopen maarja... Nergens te krijgen.
RTX 3070;4;0.5088347792625427;De 3070 is dus gewoon prima geschikt voor 4K@60Hz. Dat is alles wat ik wilde weten. Als de prijs nu een beetje in de buurt van de € 500 blijft ben ik tevreden.
RTX 3070;3;0.489238977432251;Nice, maar weet iemand waarom de Founders Edition gewoon niet meer te koop is op de Nvidia website? Ik kan me herinneren dat de 2070 Super FE gewoon op de nvidia website te bestellen was in augustus. Maar die optie kan ik nu niet meer vinden voor de 3080 FE. En voor zover ik weet is de Founders Edition ook niet te koop bij de reguliere webshops. Of is dat omdat ze gewoon allemaal op zijn?
RTX 3070;3;0.2641308605670929;Ze zijn inderdaad op, tijdje geleden aankondiging geweest:
RTX 3070;1;0.4492098093032837;Dus het is niet mogelijk om de 3070 founders edition te krijgen in Europa
RTX 3070;1;0.47836267948150635;Volgend jaar september of oktober.. naar alle waarschijnlijkheid een RTX3070 met 12GB GDDR6 Net zoals we een RTX3080 met 12GB of 16GB gaan zien komen maar dan wel met GDDR6X Zie nu trouwens het nut niet om te upgraden als je een RTX 2070 of GTX 1070 in gebruik hebt, opnieuw 8GB GDDR6.
RTX 3070;4;0.34676098823547363;Er zit een wereld van prestatie verschil in de 1070 en 2070 en dus ook met de 3070. Dus dat heeft zeker wel nut. Het draait echt niet alleen maar om de hoeveelheid geheugen.
RTX 3070;2;0.44227081537246704;Tuurlijk is er een verschil, meer cores (rekenkernen) sowieso en vooral al door de TENSOR cores alleen die bij de RTX serie zijn toegevoegd en DXR omhoog pushen en DLSS ondersteunen. Maar ga je dit in combinatie met een hogere resolutie gebruiken, in 4K dan ga je zien dat 8GB GDDR6 niet genoeg meer is en zeker niet voor toekomstige games. Dat zie ik zelf bij de RTX2080 Ti 11GB OC met GDDR6 als ik hiermee in 4K game Dat is tegen de limiet gaan op sommige momenten !! Alleen al in een game als AC: Odyssey (2018) in FullHD met alle details op max gebruikt een GTX1070OC 8GB al meer dan de helft van zijn geheugen, gebruikt op zijn max net rond de 5GB van de 8GB dat ie heeft en dit voor FullHD. Is natuurlijk nog GDDR5, is 8Gbps en heeft 1920 cores De GTX1070 8GB heeft 1920 cores en 8Gbps De RTX2070 8GB heeft al 2304 cores en 14Gbps GDDR6 De RTX2070 8GB Super 2560 cores en 14Gbps GDDR6 De RTX3070 8GB vervolgens 5888 cores en 14Gbps GDDR6 En hoeveel van deze cores zijn TENSOR cores bij de RTX3070 ? Want de RTX2080 Ti 11GB OC gaat met zijn 4352 cores .. de RTX3070 met zijn 5888 cores nog altijd voorbij (of ongeveer net gelijk).. Conclusie, als de RTX3070 8GB GDDR6X had meegekregen of 12GB GDDR6 was dit de perfecte kaart. Maar 8GB GDDR6 doet het hem net niet. Maar die zien we volgend jaar zeker en vast wel verschijnen
RTX 3070;1;0.446768581867218;Ik ben geen expert maar geheugen doet er tegenwoordig niet meer zo toe toch? Vroeger was het een van de belangrijkste specs waar je naar keek, althans ik dan. Tegenwoordig krijg in geen enkele game het geheugen gevuld in ieder geval.
RTX 3070;2;0.44227081537246704;Tuurlijk is er een verschil, meer cores (rekenkernen) sowieso en vooral al door de TENSOR cores alleen die bij de RTX serie zijn toegevoegd en DXR omhoog pushen en DLSS ondersteunen. Maar ga je dit in combinatie met een hogere resolutie gebruiken, in 4K dan ga je zien dat 8GB GDDR6 niet genoeg meer is en zeker niet voor toekomstige games. Dat zie ik zelf bij de RTX2080 Ti 11GB OC met GDDR6 als ik hiermee in 4K game Dat is tegen de limiet gaan op sommige momenten !! Alleen al in een game als AC: Odyssey (2018) in FullHD met alle details op max gebruikt een GTX1070OC 8GB al meer dan de helft van zijn geheugen, gebruikt op zijn max net rond de 5GB van de 8GB dat ie heeft en dit voor FullHD. Is natuurlijk nog GDDR5, is 8Gbps en heeft 1920 cores De GTX1070 8GB heeft 1920 cores en 8Gbps De RTX2070 8GB heeft al 2304 cores en 14Gbps GDDR6 De RTX2070 8GB Super 2560 cores en 14Gbps GDDR6 De RTX3070 8GB vervolgens 5888 cores en 14Gbps GDDR6 En hoeveel van deze cores zijn TENSOR cores bij de RTX3070 ? Want de RTX2080 Ti 11GB OC gaat met zijn 4352 cores .. de RTX3070 met zijn 5888 cores nog altijd voorbij (of ongeveer net gelijk).. Conclusie, als de RTX3070 8GB GDDR6X had meegekregen of 12GB GDDR6 was dit de perfecte kaart. Maar 8GB GDDR6 doet het hem net niet. Maar die zien we volgend jaar zeker en vast wel verschijnen
RTX 3070;5;0.2839955985546112;Ik koop al sinds de 900 serie altijd de xx70 variant. Wat mij betreft zijn dat altijd de kaarten geweest met de beste prijs/prestatie verhouding. Ik heb nu de RTX2070 super dus ik twijfel wel of ik nog even wacht tot de 3070 Super/Ti uit gaat komen.
RTX 3070;3;0.2917315363883972;Het enige wat ik wil weten is deze kaart voldoende om cyberpubk 2077 op 1440p ultra settings te draaien of is een 3080 de minimale vereiste. Ik heb al gekeken naar de minimale specs (1060) maar ik kan verder nog geen duidelijk antwoord hierop vinden.
RTX 3070;1;0.3713834583759308;de 2070 super is in 1080p hoger dan de 3070? en ook boven de 2080super? foutje?
RTX 3070;3;0.4349718689918518;In de 1080p medium prestatiescore leek wat fout te gaan ja, als het goed is klopt het nu wel :-)
RTX 3070;2;0.3577401638031006;Kan iemand mij toevallig eens uitleggen hoe het zit met tflops? De consoles en videokaarten adverteren vaak met een x aantal tflops. Dan kijk in naar een 2080ti 14.2 tflops. Dan naar de 3070 20.4 tflops. Dat is een stuk hoger en toch presteert deze minder dan de 2080ti. Kan iemand dat mij in redelijk begrijpbare taal uitleggen? Alvast bedankt
RTX 3070;2;0.4896148145198822;"Zoals je stiekem zelf ook al had ontdekt; het zegt echt heel weinig. Het is 1 van de meetbare punten uit een kaart, maar zeker niet de enige en ook niet de belangrijkste."
RTX 3070;1;0.5984571576118469;Dat het niks betekend had ik inderdaad al ontdekt. Vind alleen vreemd dat ze juist over de teraflops altijd praten en het niks betekend. Want dat is enige wat ze zeggen onze console/kaart zoveel tflops.
RTX 3070;1;0.3605942130088806;Ja is ook vrij nietszeggend. Uiteindelijk moet je naar benchmarks kijken, dan zie je de cijfers die er toe doen
RTX 3070;1;0.4503251016139984;Ik vrees dat de prijs van de 3070 te dicht komt bij de 3080 , mocht de 3070 500 euro worden koop ik hem .
RTX 3070;1;0.580599844455719;Tegenvaller zo geweldig was de 2080ti nou ook weer niet (quake rtx 35-50 fps 144p), ja zelf een 2080ti gedraaid je hebt dus oud spul met slechts 8GB voor 500+ euro zonder voorraad ,,great news
RTX 3070;3;0.32995638251304626;"Misschien lees ik er overheen, maar waarom zijn er alleen game benchmarks bepaald? Waarom geen video encoding/transcoding, office applications, etc? Moeilijk voor te stellen wellicht, maar er zijn ook mensen die niet de hele dag lopen te gamen ;-)"
RTX 3070;3;0.43952876329421997;Office applications hebben weinig baat bij de gpu toch? Video encoding is best een vak apart, daarvoor kan je beter bij Puget kijken
RTX 3070;3;0.36424046754837036;Lekker stroomverbruik die kaarten.. had de hoop dat er een groene trend ingezet zou zijn.. Helaas trekken ook de chinezen enzo zich daar wederom niks van aan.
RTX 3070;1;0.31663355231285095;ben ik de enige die het gat tussen de 3070 en 3080 aanzienlijk vindt? heb zo'n vermoede dat er een 3070Ti binnenkort aan gaat komen.
RTX 3070;3;0.40381377935409546;zou toch ook graag die fe edities hier zien verschijnen.
RTX 3070;5;0.4620686173439026;Met mijn 1080TI kijk ik nog even de kat uit de boom. Ik zou het graag RTX willen... maar met mijn verouderde systeem moet alles nieuw. Geduld is een schone zaak in deze. Met de 13de maand maar wat leuks kopen in Q1 .
RTX 3070;3;0.3789164423942566;Tja, toch wel leuk hoor, 2080 ti prestaties voor 500-600 euro, plus hij is ook nog een stukkie zuiniger.
RTX 3070;3;0.47232189774513245;Nu zullen de prijzen van de gebruikte 2080Ti's wel flink gaan kelderen. Tenminste, zodra de 3070 goed verkrijgbaar is.
RTX 3070;4;0.3310307562351227;@sdk1985 Hier is gekozen voor een testsysteem met 3900X op 4.2GHz (all-core). Dat is geen lage klok, en kun je niet vergelijken met andere sites die testen met i-9900k of i9-10900k. Tweakers maakt dus een review met gekozen testopstelling, en daar hangen cijfers aan. Voor consument koper, die zelf een Ryzen 3600 heeft, zal dat dus ook afwijken. Daarom is het goed om naar testen te kijken wat gelijk is aan je eigen systeem. oa hier waar ook Ryzen 3600 is gebruik met RTX3080
RTX 3070;3;0.4307200014591217;Historisch gezien wordt bij gpu benchmarks de cpu bottleneck vermeden. Vaak door wat lagere resoluties mee te pakken en de snelste cpu te gebruiken. Hoe iets op je eigen cpu werkt kan ook zeker interessant zijn. Maar dat was hier volgens mij niet de insteek van tweakers. Overigens kun je die 3600 straks mooi upgraden naar ryzen 5xxx .
RTX 3070;1;0.35617130994796753;Interessant hoe iedereen hier zo blijft praten over kaarten die gewoon praktische niet bestaan op een handjevol na . Stop met het feeden/pleasen van Nvidia
RTX 3070;1;0.5549412369728088;En waar koopt men een 3070 founders voor 519 euro in Nederland. De nvidia site laat mij alleen andere retail opties zien.
RTX 3070;1;0.43424177169799805;Ik denk dat 519 euro (dus waarscchijnlijk 600-700 bij retailers) veel te duur gaat zijn Helemaal na morgen. Ik vermoed dat de waarde van de 3080 al flink daalt en dat de prijs van de 3070 wordt aangepast op release. (hijgende en puffende fanboiii downmodders lol, i love it!)
RTX 3070;3;0.34785884618759155;Alleen de FE heeft de afgegeven prijs toch? €519 euro voor een FE, maar een meerprijs voor de AIB's en de vaak betere koeling die ze erop zetten. Al is de Ventus meestal hetzelfde geprijsd. Ben ook wel benieuwd na morgen hoe alles ranked
RTX 3070;1;0.5482519268989563;Het grootse problem is denk ik dat er is nog geen enkele 3070 van Nvidia partners getest zijn door reviewers, wat betekend dat deze bij de launch van de FE waarschijnlijk ook niet beschikbaar zullen zijn. Ik ben benieuwd of de FE ook daadwerkelijk voor €519,- verkocht worden en hoeveel er dan überhaupt beschikbaar zijn. Die twee weken uitstel zal ook geen miljoenen exemplaren opleveren, dus men zit straks in hetzelfde schuitje als de 3080 en 3090. Wanneer de exemplaren eindelijk bij de retailers aan komt is het 2021 en zullen de prijzen de eerste maanden ook veelal hoger liggen. En deze zie ik nog niet zo snel dalen als dat ze gestegen zijn, wanneer de voorraad op pijl is.
RTX 3070;2;0.2650024890899658;Hoezo denk je dat het te duur is? Want mensen betalen meer dan dat momenteel. 519 is de goedkoopste variant. En de tekorten zorgt voor hogere prijzen. Daarnaast krijg je voor de prijs de prestaties van een ~2080Ti die tot voor kort veel meer geld kostte en de snelste kaart was.
RTX 3070;2;0.4382716119289398;"Of de ""waarde"" van de 3080 daalt ligt aan wat AMD gaat aanbieden in de markt. Bijv.; Als AMD met een kaart komt op de zelfde prestaties, de ene game wat meer fps, de andere wat minder, maar met een prijskaart van 100 euro lager dan zal men sneller de AMD kaart nemen. Komt ook bij dat de huidige ""waarde"" ver ver boven de msrp zit. Dit door schaarste. Zelfde met de 3070. De prijs kan op het laatste moment verlaagt worden als AMD een betere kaart heeft tegen de zelfde of lagere prijs. En of de kaarten meer beschikbaarheid krijgen, idk. De een zegt dat er ""plots"" meer zullen zijn wanneer AMD er aan komt, de ander zegt dat de productie van de 3k serie gewoon geflopped is van top to bottom omdat ze met samsung 8nm gingen."
RTX 3070;1;0.35720115900039673;Wanneer komen die AMD kaarten? Ik sta open voor alle opties (zolang het maar snel en goedkoop is!).
RTX 3070;3;0.2865922451019287;Vandaag is de announcement/reveal. Die kan je zien op YouTube , begint om 17:00u. Ik neem aan dat dan ook bekend word wanneer de nieuwe kaarten gereviewed en verkocht gaan worden. edit: tijd aangepast, ik leefde nog een uur later
RTX 3070;1;0.5031001567840576;Er zat een NDA op door Nvidia waardoor iedereen de 3070 reviews pas vanaf vanmiddag 14:00 kon plaatsen. Dit is niet alleen voor tweakers zo, maar voor iedere reviewer die een 3070 in handen heeft. Dit is dus volledig bepaald door Nvidia.
RTX 3070;1;0.7285627126693726;Rustaaagh. Die announcement is morgen pas. Daarnaast is dit gewoon een NDA die afloopt waarna alle tech reviewers hun review mogen publiceren. Dat dit misschien wat strategisch gepland wordt door Nvidia kan Tweakers/HWI ook niks aan doen.
RTX 3060 Ti;1;0.5271264910697937;Waarom staat de 3090 eigenlijk niet in de grafieken?
RTX 3060 Ti;2;0.3717532455921173;Omdat we alle kaarten die in de grafieken staan afgelopen week opnieuw getest hebben met de laatste driver. Daar was een beperkte tijd voor, dus de 3090, en de 2080 Ti zoals @SgtElPotato vermeld, hebben we helaas (nog) niet kunnen testen met deze driver. Om de scores vergelijkbaar te houden hebben we de oude resultaten die we wel hebben van deze kaarten niet in de review opgenomen. Mocht je ze toch willen bekijken kan dat bij hier voor de 3090, en hier voor de 2080 Ti. Let er dan wel op dat voor die resultaten een andere CPU is gebruikt dan voor de resultaten in deze review, dus de resultaten op (vooral) 1080p zijn soms significant anders.
RTX 3060 Ti;2;0.4350762963294983;Ik vermoed omdat deze wat betreft vergelijking niet interessant zijn. Mensen twijfelen niet tussen een 3090 of 3060, maar met een 3070, of een vorige generatie in de zelfde prestatie cq prijsklasse. En hoe een 3070 tov een 3080 en 3090 etc presteert, kun je weer in andere benchmarks zien. Anders moet er veel te veel gebenchmarkt worden. Als je een nieuwe auto koopt wil je ook niet weten hoe een Corsa presteert t.o.v. een Insignia GTi, maar wat deze voor jou in de realiteit betekent. Maar dat is mijn speculatie
RTX 3060 Ti;3;0.36316773295402527;Nee, maar als een 3080 en 6900xt er tussen staan moet die er ook staan om te zien hoe de kaart binnen de huidige generatie/markt presteert. (Waar komt toch die obsessie vandaan om een parralel met auto's te willen trekken?)
RTX 3060 Ti;3;0.2802993655204773;"Waar zie jij die 6900XT staan dan? 🤔 ""(Waar komt toch die obsessie vandaan om een parralel met auto's te willen trekken?)"" Omdat hij dat misschien makkelijker uitleggen vindt? Ik snap alleen niet waarom je Opels als voorbeeld zou nemen, maar goed dat is een ander verhaal 😂"
RTX 3060 Ti;1;0.5111392140388489;"Toch is het vreemd dat alles altijd met auto's vergeleken moet worden. Waarom niet mer boten of vliegtuigen of vakantiehuisjes? Het ergste was de Pentium 4 - tijd, die vrij slechte IPC had vergeleken met AMD. Daar kwamen vergelijkingen als ""intel heeft een hoog toerental, maar AMD heeft meer koppel""... Geen idee wat zo een vergelijking toe moet voegen. Klopt gewoon voor geen meter."
RTX 3060 Ti;3;0.3885274827480316;Die vergelijking klopt anders perfect. Intel gooide meer mhz (toeren) ertegen aan, waar AMD hetzelfde bereikte met een lager toerental maar minder efficientie, dus meer koppel.
RTX 3060 Ti;2;0.5373878479003906;"Ehm nee die vergelijking klopt van geen kant. Minder efficiëntie dus meer koppel: koppel is een maatvoering van kracht. Als de efficiëntie daalt(zie het als slechtere ontbranding) daalt je koppel ook omdat je minder neerwaartse kracht haalt van de arbeid slag in de cilinder. Koppel is in NM dus kracht maal arm Als je kijkt naar de vergelijking ""intel heeft een hoog toerental, maar AMD heeft meer koppel"" schort hier ook het een en ander aan. Hoog toerental is een interessante opmerking aangezien een hoger toerental inhoud dat je meer werk verzet. Toeren per minuut(pk) is dan in zekere mate vergelijkbaar met koppel(nm). De ene verteld je ""hoeveel werk wordt er verzet"" en het andere is ""hoeveel kracht wordt er gebruikt"" Een pk is een lastig ding om uit te leggen maar in theorie is het gewicht over afstand in x tijd De vergelijking met koppel is dus, nou ja, geloof dat het wel duidelijk is zo? Als je praat over een snellere processor heef die in theorie meer pk=hogere ipc+ toerental. Ipc is dan het ""werk"" en de mhz is de ""tijd"". De afstand blijft gelijk. Amd berijkte hetzelfde met minder toeren dus niet minder maar juist veel pk en een veel hogere efficiëntie. Of om het moeilijker te maken. De overbrengingsverhouding van de amd was zo afgesteld dat hij precies aan zijn maximale vermogen zat aan het einde van de weg. De vergelijking ""auto en processor"" is gewoon waardeloos ,mits je de vergelijking van x is sneller dan y gebruikt hebben bijde teveel variabelen om een logische conclusie te trekken. Maar zodra er kracht en vermogen aan te pas komen dan krijg je dit soort verhalen"
RTX 3060 Ti;4;0.39824768900871277;Is geen obsessie, maar in veel gevallen het makkelijkste vergelijk om mee uit te leggen. Een computer en een auto hebben veel parallellen als het gaat om het schrijven van tests en reviews. Bovendien is bijna iedereen bekend met een auto, t.o.v. (@_Pussycat_) boten, vliegtuigen of vakantiehuisjes.
RTX 3060 Ti;2;0.34625163674354553;Of de 2080ti mis ik ook. Ik zie wel de 2080 super, maar die heb ik niet.
RTX 3060 Ti;5;0.37476199865341187;De 2080 Ti kun je gelijk stellen aan de 3070.
RTX 3060 Ti;2;0.44172734022140503;Ander product. Miss gelijkwaardig in prestaties maar kan je dus niet gelijk stellen.
RTX 3060 Ti;3;0.3551560044288635;Precies dat, daarom kun je de RTX 2060 ook niet gelijk stellen aan een GTX 1660 Super terwijl ze qua hardware veel op elkaar lijken en bijna hetzelfde presteren.
RTX 3060 Ti;3;0.6216779351234436;De 2060 is wel echt beter dan de 1660 super, niet gigantisch veel maar wel genoeg om ze niet vergelijkbaar te noemen. De 2080 Ti en de 3070 zitten veel dichter bij elkaar op dat vlak.
RTX 3060 Ti;3;0.4718732535839081;Het enige wat je bij de 2060 vooral meer krijgt is de hardware raytracing, de 1660 gebruikt een aangepaste 2060 core (TU116 tov TU106) waarbij de raytracing er niet meer in zit. Geheugen is wel verschillend. Wat dat betreft is de 1660 meer een uitgeklede 20xx dan een 10xx kaart. Gelijkwaardig zijn ze zeker niet maar als je geen raytracing in games gebruikt komen ze aardig dicht bij elkaar in de buurt qua performance. In theorie is de 2060 wel wat sneller maar als je dat verschil in performance belangrijk vindt, denk ik dat je al eeder aan het kijken bent naar een 2080/3060 of beter in plaats van een instapkaart zoals de 1660.
RTX 3060 Ti;1;0.3142887353897095;Daarom heb ik ook afgelopen de 1660 super gekocht, scheelde toch ook weer 100 euro.
RTX 3060 Ti;5;0.2893049716949463;Op dit moment heb ik een 1660 super en ik ben nu inderdaad ook aan het kijken naar een ~3070 performer met meer dan 12GB, als dit de 3070 (super/ti met 12GB>) gaat worden of een rx 6800 weet ik nog niet.
RTX 3060 Ti;1;0.3652164340019226;ook de 2080 mis ik idd nu ik er op let
RTX 3060 Ti;1;0.39067310094833374;ik vermoed dat dat is omdat ze die niet hebben kunnen testen of nog niet getest hebben
RTX 3060 Ti;3;0.5412463545799255;Is het wellicht mogelijk om bij de stroomverbruik grafiekjes de 1080Ti (en 1070Ti?) mee te nemen. Ik zie wel de 1080FE maar ik vraag me af hoeveel verschil het daar in uitmaakt aangezien dat juist ook kaarten zijn die nog veel mensen waaronder ik in gebruik hebben.
RTX 3060 Ti;3;0.4232327342033386;Helaas kan je die niet in een grafiek met elkaar vergelijken. We hebben deze kaart nog niet getest op ons nieuwe platform met de 5950X. Wel hebben we deze kaart getest in combinatie met de 3900XT @ 4.2GHz. Deze resultaten, inclusief stroomverbruik, kan je hier vinden.
RTX 3060 Ti;5;0.4220733046531677;Is inmiddels toegevoegd aan de grafieken!
RTX 3060 Ti;5;0.29638275504112244;Hulde hiervoor. Dit maakt blijkbaar een flink verschil met de 1080Ti ten opzichte van deze kaart.
RTX 3060 Ti;5;0.43338343501091003;Misschien wel de interessantste kaart van de groene kamp deze gen. Heeft alle toeters en bellen van de 3000 series maar kost je geen halve fortuin. 20-30% goedkoper dan de 3070 voor 85% van de performance. Met DLSS even goed als een 3080 zonder DLSS! Not too shabby.
RTX 3060 Ti;1;0.37146875262260437;In de VS is de adviesprijs slechts $399 of 335 euro, wat een verschil (20% goedkoper). Dat zou pas echt een leuke prijs zijn hier in NL.
RTX 3060 Ti;3;0.321520060300827;Dat is bijna exact de btw! Wat toevallig
RTX 3060 Ti;3;0.5302066802978516;Er zijn 4 staten zonder BTW. Zaken als electronica en kleding zijn daar over het algemeen goedkoper dan hier, ook met belasting. Maar inderdaad wordt het verschil een stuk kleiner als je belasting meerekent.
RTX 3060 Ti;3;0.6433037519454956;Ben ik zeker met je eens hoor dat elektronica daar over het algemeen goedkoper is (grotendeels ook omdat ze minder belasting betalen daar), maar het is eerlijker om de prijzen te vergelijking zonder belasting. Dan zul je zien dat het over het algemeen wel aardig bij elkaar in de buurt zit.
RTX 3060 Ti;2;0.32175490260124207;Alleen word er daar nooit geadverteerd met de belasting included. Het is altijd prijs zonder belasting en zodra je bij de kassa komt word het erbij gerekend.
RTX 3060 Ti;1;0.529478132724762;MSRP in dollars worden altijd zonder BTW geadverteerd in de VS, dollar prijzen van AMD, Nvidia, Intel e.d. zijn dus altijd zonder BTW en kun je dus niet vergelijken met EU prijzen die altijd incl. geadverteerd worden. De prijs is de US is dus vrijwel exact gelijk als je prijs hier wanneer je daar rekening mee houd.
RTX 3060 Ti;3;0.26688259840011597;En een veel kortere garantieperiode
RTX 3060 Ti;1;0.5547473430633545;????? BTW in de VS is per staat verschillend, in New Hampshire betaal je bijvoorbeeld niets. Maar hoezo betalen we 14% te veel? Dat is belasting die de Nederlandse overheid heft, waar de Nederlandse kiezer (min of meer) voor heeft gekozen. Dat geld komt niet bij nVidia terecht maar bij de regering die er (wederom ongeveer) mee doet wat de Nederlandse stemgerechtigden willen.
RTX 3060 Ti;2;0.42517200112342834;Het gaat mij om het feit dat iedereen schreeuwt dat de prijzen 1:1 zijn met america, maar dt is niet zo. Max 7%, soms zelfs 0%...als je geluk hebt en in die staat woont. Je mag gewoon niet zeggen dat de dollar prijs gelijk is aan de euro, dat is liegen.
RTX 3060 Ti;2;0.379014790058136;"Wat iedereen daarmee bedoelt is dat de prijs voor belasting hetzelfde is. Anders dan bv. bij kleding, die in de VS vaak wel echt goedkoper is. Dus begrijp ik je ""we betalen te veel"" niet."
RTX 3060 Ti;1;0.5164384841918945;20% verschil is dus in feite de 21% BTW die je hier extra betaalt (prijzen in de VS zijn ex-belasting omdat die per staat verschillen), dus zijn de adviesprijzen nagenoeg gelijk.. Dat er hier in Nederland zoveel meer belasting bijkomt kan Nvidia niks aan doen, daarvoor moet je bij de overheid zijn..
RTX 3060 Ti;2;0.40455806255340576;In de vs komt er achteraf nog belasting bij. 399 is dus niet de prijs die je betaalt
RTX 3060 Ti;2;0.43007969856262207;Sales tax ( btw ) is veel lager in de VS.
RTX 3060 Ti;3;0.5784002542495728;Klopt, maar hoeveel lager is afhankelijk van de staat.
RTX 3060 Ti;2;0.3883036971092224;Als je dan toch gaat rekenen neem dan ook het besteedbaar inkomen mee wat in NL op $30k ligt en in de VS op $45k, hoewel het inkomensverschil tussen de laagste en hoogste 20% wel 4 keer is in NL en 9 keer in de VS. Maar die iPhones en doktersrekeningen zijn daar natuurlijk ook niet goedkoop. Hoewel de benzine wel weer spotgoedkoop is. Zo kun je wel bezig blijven.
RTX 3060 Ti;3;0.42485108971595764;idd op basis van de resultaten voor 1440p lijkt mij dit ook de ideale kaart. Helaas voor de 2080 gebruikers wil de komst van deze kaart wel zeggen dat de 2dehands waarde een serieuze deuk zal krijgen... nu nog afwachten wat de AMD tegenhanger geeft, het is helaas wachten tot januari maar daar ga ik toch op wachten om te zien wat mijn nieuwe aankoop gaat worden.
RTX 3060 Ti;2;0.4983435571193695;Ik denk dat het weinig uitmaakt aangezien de prijzen in V&A vaak maar enkele tientjes onder nieuwe producten zit.
RTX 3060 Ti;1;0.26732733845710754;Nu nog wel, totdat de markt overspoeld wordt omdat iedereen zijn 2080 gaat upgraden, net zoals dat met de 1080 is gebeurd.
RTX 3060 Ti;2;0.3858241140842438;Dat hangt dus totaal af hoe snel Nvidia haar GPU's kan produceren, gezien de enorme vraag en lage productie gaat dat dus nog een hele lang tijd duren.
RTX 3060 Ti;2;0.3773539066314697;"Aan de andere kant. Als jij een nu een 3080 wil kopen heb je waarschijnlijk ook bij het uitkomen van de 2080 een gekocht. Voor 2 jaar gebruik (waarschijnlijk de hele garantieperiode) en daarna verkoopt mag er van mij ook wel een flinke deuk in de prijs worden geslagen. Dat ze nieuw nog voor een belachelijke prijs worden verkocht is naar mijn idee een los verhaal. Als je de 2080 op een later moment hebt gekocht had je blijkbaar ook niet direct het nieuwste van het nieuwste nodig (een paar uitzonderingen daar gelaten voor een eerste setup etc.) en kan je ook nog wachten voor je een 3080 koopt. Dan heb je het prijsverschil op het moment van switchen er als het goed is wel uit. Videokaarten hebben geen eeuwig leven, het is een verbruiks- i.p.v. gebruiksvoorwerp. Je betaald de prijs voor de tijd dat jij hem ""verbruikt"". Jij is uiteraard bedoeld als ""men""."
RTX 3060 Ti;1;0.6975974440574646;Ik heb zelfs héél even overwogen om m'n 3070 terug te sturen... toch maar niet gedaan. a) ik heb 'm toch al b) die €100 meer of minder voel je over de levensduur van een kaart nauwelijks c) ray tracing doet de 3070 een stuk beter d) dan is m'n gpu geen anagram van m'n cpu meer
RTX 3060 Ti;1;0.5100664496421814;Edit, hoort in andere topic thuis, deze reactie mag worden verwijdert.
RTX 3060 Ti;2;0.3877952992916107;Mwhoa eerder 20% of minder denk ik, de RTX3070 had een adviesprijs van €519 maar de goedkoopste die je wellicht kan bestellen en ergens in 2030 binnen krijgt is 600e volgens de pricewatch. Zou er niet op rekenen dat de RTX3060Ti nou opeens echt 419e gaat kosten en ruim verkrijgbaar gaat zijn.
RTX 3060 Ti;2;0.4544363021850586;ja, vervelend voor degene die hem direct willen hebben maar als hij over een paar maand/half jaar fatsoenlijk verkrijgbaar is voor ca. 400 erg interessant als upgrade van een 1060 bv. Met ruim een verdubbeling van framerate een veel grotere sprong in prestaties dan mijn noodgedwongen upgrade van een 780 naar 1060 voor bijna hetzelfde geld terwijl daar veel meer tijd tussen zat.
RTX 3060 Ti;1;0.4569246172904968;gtx 780 gekocht eind 2013 voor 440 gtx 1060 gekocht midden 2018 voor 360 zaten toch beide rond de 400. 1070 zat toen boven de 500 en 1080 650. Met een budget van rond de 400 viel dat niet meer te verantwoorden.
RTX 3060 Ti;1;0.5180391669273376;Is de FE eigenlijk nog te krijgen in Nederland? Ik weet dat Nvidia hier gestopt is met de verkoop, maar is hij nog via Duitsland o.i.d. te krijgen?
RTX 3060 Ti;1;0.5760746002197266;Vandaag is het embargo op tests van de GeForce RTX 3060 Ti gevallen, de verkoop van de Founders Edition en de eerste custom designs gaan officieel van start op 2 december om 15.00 uur. Alle andere belangrijke data voor de komende week vind je in onderstaande tabel. Dus morgen even de Duitse webwinkels in de gaten houden als je er eentje wil bemachtigen.
RTX 3060 Ti;1;0.4731909930706024;Je hoeft niet eens per se Duitse winkels te kijken. Azerty heeft ze ook al live staan (maar nog niet te bestellen):
RTX 3060 Ti;3;0.4034709334373474;Maar dus geen FE kaarten waar om gevraagd wordt. Dit zijn alleen de aib kaarten en die zijn uiteraard niet alleen bij Azerty te koop binnenkort. Voor FE kaarten zal je dus wel degelijk naar buitenland moeten uitwijken.
RTX 3060 Ti;2;0.4172194004058838;Op het moment zijn de FE kaarten helaas niet verkrijgbaar in Nederland. Mensen hebben geprobeerd via de Duitse Notebooksbilliger.de een kaart te bestellen, maar NVIDIA staat niet toe dat zij FE kaarten naar Nederland verzenden (hoewel ze normaal wel naar Nederland verzenden). Mocht je een vriend hebben in bijv. Duitsland of de VS, dan zou je kunnen vragen of diegene de kaart wil bestellen en doorsturen, maar dat kan wel lastig uitpakken qua garantie.
RTX 3060 Ti;5;0.4930410385131836;Goede tip, ik heb er 2 besteld
RTX 3060 Ti;2;0.3951338827610016;Vindt verder niemand het vreemd dat de 3060 Ti bijna 30% meer verbruikt dan de 2060 FE terwijl het een kleinere chip is en ook nog eens lager geklokt? Dat kan onmogelijk naar die 2GB extra GDDR6 gaan, toch?
RTX 3060 Ti;2;0.27964818477630615;De chip heeft een stuk meer cudacores/compute units. Dat vraagt allemaal stroom. Je kunt twee verschillende chips niet enkel vergelijken op basis van die size en MHz. Uiteindelijk issie ook een heel stuk sneller dan die 2060 FE, en is ook de performance per watt hoger. Het is in lijn met de 3070 qua performance per watt, dus niks geks daar eigenlijk.
RTX 3060 Ti;2;0.5067164301872253;Het oppervlak is doorgaans het enige wat stroom verbruikt. De stroom loopt door de cuda cores heen, die zinkt daar niet in ofzo. Als je meer oppervlak hebt, heb je meer geleidend materiaal en dus meer stroom bij gelijk voltage. Tenminste dat verwacht je door de wet van Ohm. Kleinere chips verbruiken in de praktijk doorgaans minder stroom. Kijk naar wat er met Zen 2 en RDNA 1 gebeurd is: veel lager verbruik door kleinere oppervlakte vergeleken met hun voorgangers. Ik heb het niet over performance per watt, gewoon alleen verbruik.
RTX 3060 Ti;3;0.4993256628513336;Maar is het niet zo dat, puur natuurkundig/elektrisch kijkend, meer kleinere transistoren ook meer kunnen verbruiken dan een kleiner aantal grotere bij gelijkblijvend oppervlak en voltage? Overigens heb je altijd niet geleidende gedeeltes in de chip, om kortsluiting te voorkomen, en je weet natuurlijk ook niet hoe dat zich verhoudt. Wellicht is de 3000 serie dusdanig geoptimaliseerd in dat opzicht. Het is een interessant gedachtenspinsel, maar volgens mij moet je echt veel dieper kijken om echte conclusies te kunnen trekken. Je moet dan eigenlijk de chip gaan doormeten op transistorniveau o.i.d om te kunnen begrijpen waar die verschillen vandaan komen. Maar dat gaat natuurlijk praktisch gezien niet, vanwege de grootte (kleinte) van de componentjes.
RTX 3060 Ti;3;0.5207186937332153;"je hebt volgens mij wel gelijk dat er meer dingen een rol spelen dan alleen maar oppervlak en voltage en dat ohm ' s wet dus niet al het verbruik van een chip kan verklaren. het zou inderdaad kunnen zijn dat er relatief minder ' niet - geleidende delen ' zitten in ampere dan in pascal, en dat daardoor dus meer stroom loopt bij gelijk voltage. in termen van ohm [UNK] s wet wordt de weerstand dan lager ten opzichte van oudere ontwerpen zodat er bij hetzelfde voltage meer stroom loopt door een even grote chip. dit lijkt wel in overeenstemming te zijn met berichten die ik de afgelopen jaren hoor over de ophoping van hitte in chips ; het verbruik blijft al tijden redelijk hetzelfde maar concentreert zich op een steeds kleiner oppervlak. je ziet over de jaren heen wel dat chips van gelijke grootte steeds meer stroom gaan gebruiken. in dat opzicht is de 3060 ti niet opvallend. maar dat steeds stijgende verbruik is historisch ook meestal samengegaan met een toename in kloksnelheid. daardoor is het een beetje appels met peren vergelijken. vergelijk dat met de vorige generatiesprong van amd : het verschil tussen rdna 1 en de rx 500 serie. de rx 5700xt is 8 % groter dan de rx 580, veel hoger geklokt, gemaakt op een kleinere node, en verbruikt ook bijna precies 8 % meer. ofwel toen amd naar een kleinere node ging, gingen de kloksnelheden omhoog terwijl het verbruik per mm identiek bleef. vergelijk dit met ampere, welke bij marginaal lagere kloksnelheid en op een kleinere node 126 % van het verbruik heeft van pascal ( 3060ti vs 2060 fe ), met slechts 88 % van het oppervlak. per mm verbruikt ampere dus 126 / 88 = 143 % van wat pascal verbruikt. ik denk dat jouw verklaring hiervoor een goede is, maar dit onderstreept wel dat waar amd grote sprongen maakt ( je krijgt meer van het goede en minder van het slechte ) nvidia een pas op de plaats doet ( je krijgt meer van alles, ook de slechte dingen )."
RTX 3060 Ti;1;0.663482666015625;En ondertussen is het voor ons Nederlanders nog steeds niet mogelijk om een FE kaart te bestellen omdat Nvidia zogenaamd al 2 maanden opzoek is naar een partner om de kaarten in NL te kunnen verkopen... Yeah right!
RTX 3060 Ti;2;0.4975200593471527;sws onafhankelijk van of ze wel in nl komen of zijn vind ik persoonlijk de FE kaarten bijna nooit interessant zijn als optie om te kopen, meestal ben je beter af met een kaart van een tientje of twee meer met betere koeling stillere fans en iets meer performance, alleen voor die specifieke looks zou je dan nog de FE's kunnen overwegen
RTX 3060 Ti;4;0.3321608006954193;Voor de RTX 3000 serie zijn de FE kaarten echt enorm goed, en ook nog eens de goedkoopste Dus ze zijn echt wel aantrekkelijk hoor Alleen die lelijke power connector...
RTX 3060 Ti;2;0.3724490702152252;ik heb er vooral geen zin in dat mijn CPU recyclede warme / hete lucht krijgt van mijn FE, dat is mijn grootste probleem naast dat je idd die powerconnector hebt heb ik geen ervaring met contact over garantie en dergelijke bij Nvidia en bij MSI, Gigabyte en EVGA heb ik dat wel en over die merken ben ik best tevreden op het punt van service
RTX 3060 Ti;5;0.5320800542831421;Ik zelf ben heel erg geïnteresseerd in de FE kaarten aangezien ze op MSRP zitten en ze 2 slot kaarten zijn. Aangezien ik in een mini ITX case is dat laatste erg belangrijk voor mij. Ook valt de koeling deze generatie heel erg mee, helemaal als je kijkt dat het maar een 2 slot kaart is dat tegen 3 slot kaarten op moet (logisch wie gaat winnen). Ook zien ze er erg goed uit.
RTX 3060 Ti;1;0.5410424470901489;? Winkels zoal Azerty, Alternate of desnoods Bol.com tellen niet (meer) mee?
RTX 3060 Ti;3;0.3208334743976593;Ik heb het hier over de FE kaart van Nvidia zelf. de kaarten van de AIB partners die een paar 100 euro meer kosten zullen over een paar weken wel sporadisch te vinden zijn
RTX 3060 Ti;1;0.7751691937446594;Die verkopen de FE helemaal niet
RTX 3060 Ti;2;0.3906846046447754;Ik zie verdomd veel overeenkomsten met de 3060ti en de 3070. Maarrrrrr.. Will it unlock? 6950 --> 6970 7950 --> 7970 R9 290 --> R9 290X Als ik het me goed herinner, iemand enig idee hoe de potentie bij het groene kamp is?
RTX 3060 Ti;1;0.4878392219543457;Zo'n bios unlock ken ik alleen maar van AMD. Misschien dat Nvidia hardware-matig er voor zorgt dat het niet mogelijk is.
RTX 3060 Ti;3;0.23720930516719818;Het woord in deze 'unlock' moet je toch iets anders verwoorden Het verschil van een BIOS brengen van een ander soort naar een (hoger) ander, of een aangepaste BIOS dat ook de voltages en andere waardes, los kan gooien. Dat laatste kan zelfs een EVGA kaart letterlijk doen 'ontploffen'. Bedoeld voor overklokkers zoals Kingpin of Gamers Nexus en consorten.
RTX 3060 Ti;2;0.3405263423919678;Mwah bedoel inderdaad unlock, als het dan een afgekeurde <hoger gepositioneerde kaart> is met doormiddel van een bios gelockte shaders/cuda cores. Valt er soms met een custom bios, of simpelweg de bios van de <hoger gepositioneerde kaart> nog wel wat winst te behalen. Misschien kan je daardoor een 3060ti niet naar een 3070 tillen, maar ergens tussen die 2 in zou mooi zijn. Zeker voor die 400piek
RTX 3060 Ti;3;0.2734176516532898;Een interessante claim. Ik heb altijd de indruk gehad dat de meeste PC gamers ver onder dat budget zitten voor een video kaart, meer in de richting van €250 en/of veel lager.
RTX 3060 Ti;1;0.42980366945266724;Afgezien dat ik 1x meer heb betaald ik dacht in de 300 euro voor een XFX 6800GT koop ik altijd een kaart van de rond de 250 of zelfs minder (nu een 1050TI van 150euro). Vind de 419 wat vast eerder 460 wordt veel te veel voor soms eens een spelletje spelen.
RTX 3060 Ti;3;0.3370973765850067;Er zal vast gedoeld worden op mensen die niet alleen maar af en toe een spelletje spelen. En als je VR op een budget wil kan ik me voorstellen dat de Quest 2 koppelen met de 3060 een logische keus lijkt. Zelfs als er genoeg voorraad is denk ik niet dat mensen hun 20xx series voor een veel lagere prijs zullen wegdoen dan de 30xx series. En dan voor het significante verschil in prestaties zou ik het wel weten.
RTX 3060 Ti;2;0.5328789949417114;Leuk nieuws, maar ik betwijfel het nut ervan. Prachtige kaarten die 30x serie, maar ze zijn zo slecht verkrijgbaar helaas. Voor mij zelf heb ik die serie nog niet nodig. Ik draai nog lekker met mijn watergekoelde rtx2080 en een rtx3080 zou voor mijn systeem overkill zijn. Dus voor mijn situatie zijn er 2 mogelijkheden: >sneller geheugen kopen ipv 2666 mt >andere kast met nog betere airflow >daar een rtx3080 op prikken of >wachten tot een nieuwe build die ik elke 3 jaar doe >daar een rtx3080 of rtx3090 in prikken Welles of nietes: Dit zal wel voor meer tweakers gelden. Ze zijn moeilijk aan te komen. Nu upgraden is gewoonweg niet mogelijk en als ik even nadenk zou het dan zo'n gewild object zijn? Mijn rtx2080 zit op een G-Sync 1440p scherm en het is helemaal geen straf als je maar 50 fps haalt in zware games want het gaat toch niet haperen. Zelfs 45 fps is nog heel goed te doen en natuurlijk is de volle 144 fps wel wat beter, maar dan moet ik wel aan zo'n kaart komen en de tweede vraag is of ik het 'nu' nodig heb. Wat doet het met je als je nog een dik jaar zou wachten? Mijn systeem moet nog 2 jaar mee en misschien kies ik straks voor optie 1 om het ram om te ruilen voor sneller 3600 mt ram. Dan kan ik een totale vervanging nog 1 of 2 jaar uitstellen. Zo bang ben ik niet voor nieuwe games. Met DLSS aan gaat RTR ook een stuk beter Mogelijkheden zat. Voorlopig ben ik content met mijn systeem en ik ben blij dat de rtx30x serie op de markt is, maar ik wacht rustig af omdat ik nog prima vooruit kan met mijn systeem.
RTX 3060 Ti;1;0.6760339140892029;"Waarom wordt er alleen gefocused op games en niet het renderen van 4k/6k/8k (evt RAW) video footage. Dat is juist wat er ontzettend vaak gemist wordt. Er wordt alleen maar gekeken naar games en wat dat betreft is er dan ook geen onderscheid met andere ""testen"". Voor het bewerken van videos in Davinci zijn bijv de Cuda cores van Nvidia beter dan de grafische kaarten van AMD (OpenCL). Wat is de reden dat dit soort zaken niet meegenomen worden? Echt een gemis!"
RTX 3060 Ti;1;0.32782912254333496;De reden dat dit soort zaken bij déze launch niet zijn meegenomen is simpel : tijdgebrek. Op ons nieuwe platform met 5950X hebben we dit keer gefocust op het testen van zoveel mogelijk relevante videokaarten. We hebben de vraag naar andere benchmarks dan games op dit soort kaarten meer gehoord de afgelopen tijd, dit is zeker iets waar we naar gaan kijken voor volgende reviews. Vooralsnog zijn we vooral naar rendering applicaties aan het kijken, mocht je nog andere suggesties hebben : shoot!
RTX 3060 Ti;3;0.27169278264045715;Dank voor de response. Denk dat renders in Premiere Pro, Davinci Resolve (en dan free en studio apart) en Blender wel grotendeels de lading dekt samen met het encoden van H.265 naar H.264 in Handbreak. Wellicht in Premiere Pro en Davinci ook een test doen met het encoden van video naar bijv. Prores voor bewerken etc.
RTX 3060 Ti;1;0.6139842867851257;Prijzen van nieuwe hardware zijn idd doorgeslagen. Maar ligt dat niet aan ons zelf ? Als je genoeg gekken vind die er nog eens 250€ extra voor betalen om de kaart onmiddelijk te hebben tja. Je ziet mensen een 1850€ kostende GPU kopen om een spelletje te spelen.(meeste gekocht voor mining) 15 jaar geleden had je een high end pc voor 2000€ , nu betaal je dit enkel voor de videokaart. Ons loon stijgt niet mee met de prijzen van alle producten tegenwoordig ! Als je 15 jaar geleden een loon had van 1500 netto , dan zal dit nu hooguit 100€ meer zijn. Waar ik ook een probleem mee heb , is dat een GAMING kaart uitgebracht word. En dat de kaart niet beschikbaar is juist voor de groep GAMERS ! Omdat de cryptominers ze in grote getallen opkopen en kunnen opkopen. Dan nog te zwijgen van de groep die ze opkopen om schaarste te veroorzaken ! En er bakken geld op verdienen zonder te hoeven werken ! ( hebben niet eens een videokaart nodig)
RTX 3060 Ti;3;0.4294971823692322;Voor 2000 euro heb je nog steeds een prima high-end systeem hoor. Als je persé het snelste van het snelste wilt betaal je dik inderdaad, maar ben je bereid daar iets onder te gaan zitten dan is nog prima te doen.
RTX 3060 Ti;1;0.5415512323379517;Gewoon wachten tot na nieuwjaar en hopen dat die corona verdwijnt ! Ga je prijzen zien droppen niet normaal Nu zijn de prijzen mega hoog omdat : 1 ) Sinterklaas 2) Kerst 3) Nieuwjaar 4) Lockdown horeca dicht ( iedereen zit nu binnen ) 5) Nieuwe hardware waar we lang op gewacht hebben bijna niet leverbaar ( alles op 7nm ) 6) Vele mensen hebben nu centen over door thuis te zitten Eens het met die corona de goede weg opgaat , zie ik alles de dieperik ingaan Gewoon omdat iedereen dan terug naar de kroeg kan en op restaurant. Terug winkelen op normale manier. Terug familie bezoeken. En iedereen die verslaafd is komt er ook weer makkelijker aan enzo:) Dit zal ook zo zijn met aandelen ! De echte schade moet nog steeds komen, maar die manipuleren ze door geld bij te drukken gratis. ( niet gedekt door een tegenwaarde als goud ) Nog effe tanden bijten is de boodschap.
RTX 3060 Ti;1;0.35724037885665894;Ben ik de enige die het nogal overbodig vind dat de actie bij de 3060TI is dat je er een jaar lang Geforce NOW bij krijgt? Ik heb net een vrij dure videokaart gekocht, ik was niet per se van plan om nu al m'n spellen te streamen om die nieuwe videokaart te sparen of zo... Doe er dan een spel bij of zo.
RTX 3060 Ti;2;0.40264031291007996;Wait what? Geen 1080ti of 2080ti in de grafieken? En de reden, omdat die nog niet getest is met nieuwe drivers? En je hebt de grafieken zelf wel bekeken? Of ben ik nu gek en komt het nogal apart over om zoiets aan te geven terwijl de 1070 en 1080 er wel in staan? Zelfs de rx580 wordt meegenomen....really? no serious really? Sinds wanneer doen high end videokaarten, die ook nog erg veel geld hebben gekost, niet meer mee? Ik heb gezien dat de 3060ti iets sneller is dan de 1080ti/ 2080 super(praktisch zelfde kaart behalve geheugen en rtx), dus waarom die niet vergeleken wordt? Omdat de 1080ti en 2080ti nog goede kaarten zijn of voor een erg mooie prijs verkocht worden? Meerdere mensen geven aan dat het de tijd voor upgraden is....again really? Ik hoop echt dat ik niet de enige ben die een trend ziet.
RTX 3060 Ti;3;0.3408478796482086;Gemiddelde fps op 1440p: 1080ti 91.2 3060 ti 110.6 2080ti 124.1 Denk dat beide bezitters niet warm lopen voor deze kaart. Voor 2080 Ti bezitters is de 3080 een upgrade.
RTX 3060 Ti;2;0.26825010776519775;Zoals ik in andere comments ook al duidelijk heb gemaakt, zijn er gewoon resultaten op de site beschikbaar van deze twee kaarten. Waarom ze niet in de grafieken zijn meegenomen is ook al meerdere keren uitgelegd : omdat ze niet 100% vergelijkbaar zijn met de resultaten die in deze review staan. Of de prioritering voor het testen van de kaarten anders had moeten liggen kunnen we over discussiëren. Feit is dat er 17 (!) kaarten zijn getest in een week tijd. Dat niet alle kaarten van 2 generaties terug zijn getest, kan natuurlijk voorkomen. Overigens zijn de kaarten vandaag alsnog getest, die zullen we morgen nog even toevoegen aan deze tabellen zodat je ze ook in deze review kan vergelijken.
RTX 3060 Ti;5;0.46273306012153625;Zijn inmiddels toegevoegd aan de grafieken!
RTX 3060 Ti;2;0.32472172379493713;Vraagje. Als ik dit in tandem zou zetten met een AMD Ryzen 5 1600, en ik doe de games op 1080p... zou ik dan last hebben van CPU bottleneck? En, als dat zo zou zijn, zou het helpen om een snellere CPU in te prikken (bv 3600X). En/of frames cappen op 120fps? (daarvoor bestaat een setting in NVidia Control Panel)
RTX 3060 Ti;2;0.5571079850196838;Ja, die 1600 gaat een serieuze beperking vormen. Een 3600(x) is al een mooie stap, maar de huidige prijzen zijn eigenlijk te hoog. Frames cappen op de maximale refresh rate van je monitor is sowieso een goed idee (tenzij je extreem competitief speelt en geen problemen hebt met tearing). Maar jouw 1600 gaat in veel moderne games al moeite hebben om je gpu van 120 fps te voorzien.
RTX 3060 Ti;3;0.5777496099472046;In de meeste games is de CPU toch niet echt de bottleneck. Zie bijvoorbeeld Linus tech tips: En de single core prestaties van een 3600 zijn maar ~30% beter dan die van de 1600. bron: Dus waarschijnlijk ga je heel veel games goed op 120 fps kunnen spelen met een 3060ti. Disclaimer: het is natuurlijk wel afhankelijk per situatie.
RTX 3060 Ti;3;0.4371117055416107;Mijn 3700x vormt in shadow of the tomb raider op wqhd ultra al een bottleneck icm een rtx 3070. 30% verschil singlethreaded is wel echt enorm in mijn ogen trouwens. Maar je hebt gelijk dat de mate waarin de 1600 daadwerkelijk een bottleneck vormt afhangt van de game en gebruikte api.
RTX 3060 Ti;4;0.5215270519256592;Dit is een goede video van betrouwbare reviewers: Je 1600 zal iets minder dan een 1700 presteren uit de charts en de 3060ti is iets sneller dan een 2080(s). Bepaal zelf of je dit nog goed vindt of dat je op je platform overstapt op een recentere zen CPU.
RTX 3060 Ti;3;0.30147820711135864;"nu is het wachten en hopen dat er dit keer wel wat kaarten beschikbaar zullen zijn voor de ""normale"" mensen"
RTX 3060 Ti;3;0.3470824658870697;De RTX 3060 Ti schijnt het goed te doen in ethereum. Ik krijg het idee dat miners duur hebben betaald voor de eerdere RTX 3000 kaarten, terwijl deze RTX 3060 Ti gelijkwaardig kan presteren als RTX 3080. Misschien dat miners hun RTX 3080's dumpen als ze de RTX 3060 Ti kunnen bemachtigen.
RTX 3060 Ti;3;0.3248392939567566;ik hoop het, misschien dat ik dan ergens mn handjes op kan krijgen
RTX 3060 Ti;1;0.2859805226325989;Net als de RX 480 kaarten die de tweede hands markt zouden verpesten wanneer de miners zouden stoppen?
RTX 3060 Ti;1;0.4650219976902008;Doet Nvidia dit vaker? Gewoon uit het niets een kaart droppen?
RTX 3060 Ti;1;0.4719390571117401;Dus al die 100 berichten over een 3060Ti release heb je gemist?
RTX 3060 Ti;5;0.3792000114917755;Officieel van Nvidia hé...
RTX 3060 Ti;2;0.38053345680236816;"ik weet niet zeker of dit uit het niets was, zoek maar op het internet of tweakers zelf en kijk maar hoeveel ""leaks"" en vergelijkend materiaal er is waaruit je dit zou op kunnen maken (dat de kaart kwam dan, niet de performance doel ik dan op) nu is dit laatste artikel niet geheel juis maar het laat wel zien dat er wat aan zat te komen"
RTX 3060 Ti;1;0.39215409755706787;Droppen die kaarten? Toch nog niet in mijn bus... Veel statistiek en geblaas, dat wel...
RTX 3060 Ti;1;0.32169753313064575;Huh dit is al maanden bekend eerst high end **80 dan volgenden de **70 en daarna de **60 reeks
RTX 3060 Ti;1;0.7734751105308533;om even na 15u uur zag ik ze hier en daar te koop staan, duurde een uurtje voordat ze allemaal weg waren. Nu alleen hier en daar nog een 600euro exemplaar.
RTX 3060 Ti;1;0.7155606746673584;Om 15u30 stond er al 1 te koop op V&A voor 560€ Hij had er 2 besteld en dus 1 over. Vermelde erbij ik vraag niet de hoofdprijs hehehe
RTX 3060 Ti;1;0.3831692337989807;Bij Azerty staat er nog voorraad Zotacs 3060 ti zag ik net .. worden morgen pakjesavond geleverd
RTX 3060 Ti;2;0.45809274911880493;Is er ergens een goede vergelijking met alle laatste grafische kaarten waarbij Ray tracing uitstaat? Liefst op 3440x1440, maar anders gewoon 1440p? De performance drop vind ik nog veel te groot en heb liever degelijke performance met 100-120fps zonder Ray tracing. En weet niet zo goed of je dan bij de 3070, 6800 of 3060ti uitkomt. Ben gewoon een casual gamer en zoek de beste prijs/kwaliteit, maar ook waar ik de komende 3 jaar op deze UWQHD resolutie op vooruit kan.
RTX 3060 Ti;1;0.5618079304695129;Bij alle benchmarks in deze review staat raytracing uitgeschakeld, behalve bij de benchmarks op de 'Raytracing' pagina uiteraard.
RTX 3060 Ti;3;0.3780367076396942;"Leuk om te zien hoe, in Exodus, de fps nogal dichtbij elkaar ligt bij 4K medium en 1440p Ultra... Vraag me af wat er beter uit zou zien op een 32"" 4K scherm!"
RTX 3060 Ti;3;0.4911847412586212;zou ik toch in eerste instantie voor de 4k gaan, maar hangt van de situatie af, bijvoorbeeld hoe ver je van je beeldscherm af zit en dus hoe belangrijk meer pixels op die afstand zijn
RTX 3060 Ti;3;0.5641966462135315;"Ik denk dat de PPI een betere vergelijking is dan het aantal inches van het scherm. 32"" 4K = ~140 PPI En dan ligt het er ook nog aan hoe ver je van het scherm af zit."
RTX 3060 Ti;3;0.389862060546875;Is zo'n kaart nou geschikt voor machine learning toepassingen, zoals trainen met BERT voor QuestionAnswering? Of komt er dan nog een geheugen probleem bij?
RTX 3060 Ti;4;0.31387194991111755;BERT wordt al krap op 24GB (trainen / fine tuning). Veel andere toepassingen kunnen prima op een 1070 ~ 1080TI
RTX 3060 Ti;3;0.5303019285202026;Fijn dat NVIDIA deze videokaart snel op de markt brengt. De aantallen zullen wel meer zijn dan de duurdere modellen, maar ik verwacht ook dat veel gamers genoegen zullen nemen met minder prestaties. Gevolg is dat deze videokaart dus ook niet goed leverbaar zal zijn. Het zal ook niet lang duren voordat AMD met een 6700XT met een adviesprijs van 349 euro zal komen
RTX 3060 Ti;1;0.5794948935508728;En ik zit hier nog met een 5Ghz CPU en onboard Intel H630 waar je 10 fps scoort in alle games op 1440p.. Bedankt Nvidia en AMD. WOOW such leverbaarheid SO nice.. Nee, ik ga zeker geen scalpers geld geven. Wat een losers die gasten die nog even wekenlang 200-500 euro winst proberen te maken over de rug van de echte tweaker. Om je kapot te schamen mensen. Draai dan je 2e shift bij een friettent in je dorpje, ga daar die paar euro's bijverdienen laat ze maar lekker stikken met die serie 3080 en 3070 stapels die ze hebben liggen
RTX 3060 Ti;2;0.4770732522010803;Ze zijn zeker slecht leverbaar, maar als je er een beetje op let dan had je al een 3080 kunnen hebben, en zeker een 3070. Ik heb zelf al 2x een 3080 gehad, en de 3070 is vaak genoeg gewoon op voorraad.
RTX 3060 Ti;1;0.29459044337272644;Tip: Koop gewoon een GTX1070 8 GB (€ 175) of GTX 1060 6GB (€ 125) in V&A om de tijd te overbruggen. Zodra de gewenste kaart tegen een voor jou acceptabele prijs leverbaar is, kun je van de tweedehands een derdehands kaart maken. Dan zal het je ca € 25,- gekost hebben om gedurende de overbruggingstijd op een discrete GPU ipv APU gespeeld te hebben en steek je een dikke middelvinger op naar de scalpers.
RTX 3060 Ti;3;0.3314058184623718;"Er bestaat een soort Tweakers benchmark database toch? Kan er worden toegelicht hoe een bezoeker deze kan gebruiken? Sommige features op Tweakers zijn enorme USP's zoals de Pricewatch en V&A - maar ik ga nog steeds naar Anandtech of UserBenchMark voor hun ""compare"" functionaliteit. Ik heb bijvoorbeeld een 1080 Ti die ik graag zou vergelijken !"
RTX 3060 Ti;1;0.39606112241744995;Er bestaat een benchmark database? Eerste keer dat ik hiervan hoor
RTX 3060 Ti;3;0.26005974411964417;plan: Afronding BenchDB en - Development-iteratie #87 Ik heb daar ooit met mijn tweakers vingers in kunnen queryen / vergelijken / samenstellen 100%, maar lees nu alleen nog over de BenchDB als interne tool. Jammer die data nog kunnen vergelijken, ook al is het op historische platformen, is episch! Edit zie ook: pricewatch: Nvidia GeForce GTX 1080 Ti Founders Edition onderaan - de data is er! Nog een QL en GUI erop en supertoffe vergelijkingen maken !
RTX 3060 Ti;5;0.6329047083854675;De 1080 Ti hebben we alsnog getest op ons nieuwe platform en toegevoegd aan de grafieken!
RTX 3060 Ti;1;0.4093721807003021;Sweeet
RTX 3060 Ti;2;0.3557957410812378;Hmm, kan het zijn dat ik ze niet zie (ook niet na hard refresh) ? Thnx for the effort either way!
RTX 3060 Ti;2;0.4957691431045532;Er zit, helemaal als de review al wat ouder is en de grafieken dus minder opgevraagd worden, behoorlijk wat caching op de grafieken. Zou er inmiddels allemaal wel in moeten staan
RTX 3060 Ti;5;0.5864530205726624;Allemaal present - dank
RTX 3060 Ti;3;0.4894300103187561;"Ik zelf heb een 1080TI; en met mij denk ik velen anderen. Voor die groep is upgraden momenteel interessant, alleen kan ik hem nergens terugvinden in de grafiek. is daar een reden voor? en met welke kaart kan ik het het beste vergelijken?"
RTX 3060 Ti;3;0.27396446466445923;Met de 2080 (op ray tracing / dlss prestaties na, uiteraard). De 2080 zit zo'n 5% onder de 2080 Super.
RTX 3060 Ti;3;0.3652045428752899;Die reden is er zeker : we hebben deze kaart niet op dit systeem met deze driver getest. Wel kan je de resultaten behaald op ons testsysteem met een 3900XT op 4.2GHz bekijken in de prijsvergelijker. Een makkelijke manier om die in een grafiek te zetten met de resultaten die je hier ziet is er op dit moment helaas niet. De resultaten uit de link hier boven zijn op 1080p overigens vaak niet te vergelijken met de scores die hier gehaald zijn, vanwege mogelijke CPU-bottlenecks. Wel kan je de 4K resultaten aardig vergelijken. @bloedsmoel ook meteen antwoord op jouw vraag van hier boven
RTX 3060 Ti;5;0.24913711845874786;1080 Ti is inmiddels toegevoegd aan de grafieken
RTX 3060 Ti;5;0.6917091012001038;Top! ik ga de review nog eens rustig bekijken
RTX 3060 Ti;1;0.3372056782245636;Hoe kunnen mensen dan toch al de 3060Ti in huis hebben? Zijn dit eigenaar van shops enzo? Nog tips hoe wij hier dan toch aan kunnen komen ?
RTX 3060 Ti;1;0.32046857476234436;Ik heb liever wat links waar ik deze kan kopen ipv tekst.. 🤣
RTX 3060 Ti;1;0.4081689417362213;Woensdag 15:00 en F5'en maar.
RTX 3060 Ti;1;0.48110049962997437;"Idd, zie hem nu trouwens op de Nvidia website zelf staan vanaf; 419€"
RTX 3060 Ti;2;0.3611966073513031;"Naar vele ""geruchten"" is de kaart er eindelijk. My opinion blijft het zonde dat nvidia de 3060 & 3070 met slechts 8 GB uitrust. De 3060ti in een kast hangen met 450watt voeding moet wel lukken qua stroomverbruik? (asus H370 mobo + i5-8400 + 2x8 GB ram + 1x m2 SSD). Silverstone SFX 450watt voeding? Mijn verwachting is dat de kaart ook snel uitverkocht zal zijn, de niet beschikbaarheid van de andere kaarten en het feit dat mensen in deze pandemie toch nog graag even willen profiteren van extra prestaties, voordat de PC wellicht weer in mindere mate gebruikt gaat worden, maakt het denk ik dat veel mensen dit zullen zien als een ""bang for buck"" kaart. Al vind ik de prijs aan de stevige kant (zal waarschijnlijk nog wel oplopen). De enige concurrentie die Nvidia hier heeft is de 6700 die nog moet uitkomen (krijgt wel 12 GB) of eerdere generaties van beide merken. Ik wacht rustig af tot de markt gesettled is en duidelijker is welke van de 2 de meest interessant is (prestaties / wattage benodigd)."
RTX 3060 Ti;3;0.46816229820251465;Moet lukken. Mijn Fractal SFX-L ion 500W heeft geen enkele moeite met een 3700x + 3070. De fan springt niet eens aan tijdens het gamen. Ik heb wel beide een undervolt gegeven. Maar dat is bij SFF sowieso een goed idee.
RTX 3060 Ti;2;0.36800482869148254;Verbruik van een systeem met i5-8400 ligt onder de 150 Watt. Aangezien de SFX 450 een efficiëntie van ca 87% heeft heb je meer dan 235 Watt over om de kaart te voeden. Ingame wordt in deze review een verbruik gegeven van 105W, gesteld dat de pieken het dubbele zijn, moet het dus wel lukken, zeker wanneer je net als @Seb83 al aangeeft een undervolt toepast.
RTX 3060 Ti;2;0.4693913459777832;Efficientieverlies gaat niet ten koste van het vermogen van je voeding, het werkt juist andersom. Een 450W voeding die bij 200W 87% efficient is trekt zodoende aan het stopcontact 230W. Maar dat even terzijde
RTX 3060 Ti;2;0.3979558050632477;Ja, je hebt volledig gelijk, maar ik reken liever toch op de verkeerde wijze om een ruime marge te houden. In het (verre) verleden waren er (el cheapo) voedingsfabrikanten die juist het maximale vermogen aan het stopcontact aanhielden. Daardoor had je maar een deel van het beloofde vermogen beschikbaar voor je systeem. Zelf ontweek ik altijd goedkope voedingen van exotische makelij.
RTX 3060 Ti;1;0.47832611203193665;Verstandig. Van die 600W voedingen voor €35, die de helft van het vermogen op de 3,3 en 5V rails leveren... Huilen.
RTX 3060 Ti;3;0.26731163263320923;...en dan ook nog eens als een stofzuiger klinken en functioneren Kom ze wel eens tegen in oude PC's van kennissen, een zogenaamde gold voeding van 450Watt, het enige gold dat erop zit is het kleurtje....
RTX 3060 Ti;3;0.442175030708313;Ik had hiervoor een gtx 1080 ti / 2080 super niveau. Wanneer de 3060 ti net zo goed presteert.. waarom zou ik dan in ellenlange rijen gaan staan voor een 3070/3080 of wachten op een echte release van de 6800? Ik game nu nog op 1080p, maar overweeg of 1440p of 4k60fps. Dat wil zelfs met deze kaart wel aardig als je bepaalde instellingen tweakt. Nu nog er een te pakken krijgen, maar dit lijkt mij inderdaad de betere subtopper om te halen voor een betaalbare prijs. (in theorie) Zal morgen wel een ddos/f5 fest gaan worden.
RTX 3060 Ti;3;0.4737997353076935;1440p gaat prima met een GTX1080 (zelfs een 1070 trekt het aardig).
RTX 3060 Ti;1;0.2919797897338867;Welke websites moet ik in de gaten houden als ik deze wil bemachtigen?
RTX 3060 Ti;3;0.22945739328861237;kun je zoiets nou ook via eGPU aan een laptop hangen? (Als je er de juiste case + voeding bij koopt)
RTX 3060 Ti;3;0.35818153619766235;Dit zal wel een behoorlijk off-topic vraag zijn: Hoe presteert de kaart met 4K video-editing?
RTX 3060 Ti;5;0.599202036857605;Hier ben ik ook benieuwd naar!
RTX 3060 Ti;3;0.3518024981021881;Gezien Microsoft Flight Sim met name de CPU zwaar lijkt te belasten ben ik benieuwd of een 3070 t.o.v. deze 3060 Ti nog voordelen biedt, of dat deze kaart 'voldoende' is.
RTX 3060 Ti;3;0.654634952545166;Ok mis idd ook de flight simulator test, ik had eigenlijk gehoopt dat deze standaard zou gaan worden.
RTX 3060 Ti;1;0.6993954181671143;Het maakt niet uit welke versie het is en hoe snel, als AMD of NVIDIA maar één van deze nieuwe generatie kaarten kan leveren gaan ze als warme broodjes. Het probleem is alleen, geen van beiden kunnen leveren. Beiden kampen missen enorme potentie om nu flinke klappen uit te delen aan de concurrent. Het enige dat het feestje van AMD compleet had kunnen maken was het kunnen leveren, erg zonde
RTX 3060 Ti;2;0.3875759541988373;"Ik vind het vreemd dat er niet eerst een ""RTX 3060"" is gelanceerd. En dan ook nog de prijs... 420 euro (als het al klopt en niet meer is). De GTX1060 (6GB) kon je bij lancering voor 290 a 300 euro bemachtigen. Deze ging pas naar de 400 en zelfs 500 euro toen de cryptomining meute deze kaarten in beeld kregen. Dacht ik dat Nvidia eindelijk hun naming 'scheme' in combinatie met low-mid-high range ging standaardiseren, doen ze dit."
RTX 3060 Ti;2;0.5618264079093933;Ik denk dat ze overwacht onvoldoende chips hadden die goed genoeg waren voor een 3070 en om ze toch zo duur mogelijk te verkopen ze die als 3060 ti gebind hebben. Wat cores uitgeschakeld en de overblijvende zijn misschien ook slechter, overklok en over/undervolt zal dan ook slechter uitvallen. Kan ook zijn dat na uitschakelen de overgebleven cores beter zijn, maar verwacht eigenlijk meestal niet. Ben aan het twijfelen of ik op mijn 1070 blijf zitten of toch probeer een 3060 ti te scoren. Kan ook zomaar zijn dat ze zoveel over hebben van tegenvallende 3070 yields dat deze kaart wel wat beter leverbaar is. Ik vind het knap dat er zo weinig lekt en wij dus in het duister tasten. performance per wat is hoog en per euro ook, dus ethereum miners zullen er ook wel op azen.... Jammer voor ons gamers en neural network hobbieisten.
RTX 3060 Ti;3;0.4952002465724945;Wat zou de prijs kwaliteit doen ten opzichte van een 5700XT? Qua prijs zal deze een 15-20% hoger liggen denk ik, maar is dat het waard?
RTX 3060 Ti;2;0.2940879762172699;Zo he, hadden ze niet een nog onhandigere plek kunnen bedenken voor de voeding van die kaart? Jarenlang zit ik al te wachten tot die aansluiting eens gewoon aan het uiteinde van de kaart zit, mooi uit het zicht (beetje tegen het moederbord aan zeg maar), naar beneden wijzend. En dan maken ze 'm daar... precies in het midden
RTX 3060 Ti;1;0.5250722169876099;Ik irriteer met hier ook aan, ik heb alles netjes weg-gewerkt in mijn kast, deze kabel is echt storend en zit bijna tegen het glas aan. Heb al even gekeken of er ook een kabel is met een hoek zodat het niet zo uitsteekt... Ik kijk niet de hele dag naar mijn hardware maar toch is het irritant.
RTX 3060 Ti;3;0.40985262393951416;Prijzen zijn toch aardig doorgeslagen in GPU land, 400 euro voor een midrange kaartje. Dan wacht ik nog wel even op de 6700XT (die ook niet te leveren wordt haha).
RTX 3060 Ti;3;0.5217458009719849;Ik wacht ook even op die AMD, maar dan de 6700. Voldoende power voor mij voor de aankomende jaren en een mooie update van mijn RX 580. Maar dat wachten is wel een dingetje, komen in Januari, maar ben bang dat het wel Mei kan worden voor ik 'm heb. Ga niet een premium op al een behoorlijk bedrag betalen om 'm gelijk te hebben.
RTX 3060 Ti;1;0.3991550803184509;Vraag mij toch af hoe jullie gemiddeld meer dan 70fps hitten met een 3060Ti in Red Dead Redemption 2. Ik speel op een 3080 met een 5600X en ik krijg gemiddeld 70-80 fps op RDR 2 op 1440P met een 3080 op ultra settings.
RTX 3060 Ti;2;0.4576321542263031;In de praktijk dus meer rond de 500 euro, nog steeds niet echt goedkoop te noemen.. Daarvoor moet je denk ik op de gewone RTX 3060 wachten.. Dan is de crisis hopelijk ook al voorbij en de grotere vraag en zullen de prijzen ook meer richting de adviesprijzen gaan.. Dan heb je straks een RTX 3060 voor rond de 300-350 euro ofzo die ook nog snel genoeg is voor 1440p ultra 60+fps gaming..
RTX 3060 Ti;3;0.4826878607273102;500 is nog vrij optimistisch, je ziet toch wel minimaal +100 tot +250 voor de meeste kaarten momenteel.
RTX 3060 Ti;1;0.48309803009033203;Weet iemand op welke websites ik de FE kan bestellen? Via Duitsland?
RTX 3060 Ti;5;0.3058238923549652;Ik woon zelf in Praag, dus dat zit wel snow. Dankje
RTX 3060 Ti;1;0.24914264678955078;Dus voor het eerst in geschiedenis gaat mid range ook out of stock xD
RTX 3060 Ti;3;0.4801506996154785;Mid range, voldoende, welk model je ook neemt ... op 1080 p en 1440 p . Duurst of goedkoopst model ... denk niet dat het echt het verschil maakt. Of heb ik het verkeerd ??? ( of zal wel zijn dat mensjes vergeten dat ze een nieuwe voeding moeten kopen ... wegens net tekort aan juice)
RTX 3060 Ti;1;0.4770753085613251;Nvidia... you're drunk- go home..
RTX 3060 Ti;1;0.2637641131877899;Weet iemand waar ik deze kaart kan kopen? De RTX 3060Ti Founder Edition.
RTX 3060 Ti;5;0.4449310898780823;Dream on
RTX 3060 Ti;3;0.2576931416988373;Ik heb de mijne vandaag aan de haak geslagen. Gainward 3060 TI Ghost voor 430EU. Nu moet ik wel zeggen dat ik heb in Tsjechie heb gekocht
RTX 3060 Ti;5;0.334682434797287;ik hoop voor je dat het een betrouwbare winkel is. Maar veel plezier ermee als je hem krijgt
RTX 3060;2;0.31253349781036377;Ik denk dat de belangrijkste vraag zal zijn: Zijn er genoeg leverbaar? De policy van Nvidia wordt door Linus grondig becritiseerd:
RTX 3060;3;0.29312366247177124;Voornamelijk in deze video komt goed naar voren hoe krom de praktijken van Nvidia zijn.
RTX 3060;1;0.6211053133010864;Heb je de video überhaupt gekeken? Nvidia maakt zich schuldig aan anti-consumer (gamer) gedrag terwijl ze zich profileren als de 'good guys' die 'pro gaming' zijn door de huidige koers die ze varen waarin ze temidden van wereldwijde tekorten aan chips een deel van hun productiecapaciteit van GPUs exclusief reserveren voor de dik betalende miners. Het is op zijn zachtst gezegd dubieus. Door deze exclusiviteit blijft het aanbod van (gaming) GPUs laag en de prijzen hoog. Bovendien zijn de dedicated mining GPUs waardeloos zodra ze hun mining-taak voldaan hebben en komen ze terecht op de immer uitdijende berg e-waste. Dit in tegenstelling tot wanneer nVidia deze chips had ingezet als gaming GPUs waardoor ze een veel langer leven krijgen op de 2ehands markt. Maar nVidia boeit de 2ehands markt niet, en e-waste ook niet. Sterker nog, als ze een mogelijkheid hebben om de 2ehands markt kunstmatig krap te houden doen ze dat, want dat betekent op de middellange termijn weer meer verkopen en een blijvend hoge vraag. En die manier hebben ze nu gevonden met een zogenaamd positief verhaal.
RTX 3060;2;0.5850846767425537;"Natuurlijk boeit het ze niet. En natuurlijk proberen ze een mooi verhaal op te hangen. Echt niet spannend. En ja, Linus interesseert zich ook niet voor de ""community"" maar ook voor zichzelf, en moet dus elke dag zogenaamd spannende dingen onthullen. Zelfs als die spannende onthulling is dat firma's graag geld verdienen, en daar graag een mooi verhaaltje bij verzinnen."
RTX 3060;5;0.2855775058269501;Hahaha ja, die gast zit zijn zakken goed te vullen met zn sensatie videos.
RTX 3060;1;0.6842629909515381;Ik heb je geplussed niet vanwege Linus, dat kost je bijna een minnetje , maar wel over de levering. Echt, ik vind dit zo een ondergewaardeerd punt: ze blijven nieuwe producten/reeks uitbrengen, terwijl mensen sinds nov/dec nog op de vorige reeks zitten te wachten. Ik sta al 3 (!!!) maanden in de wacht en ik heb sindsdien een paar leuke teksten voorbij zien komen over de nieuwe 30xx die uit gebracht wordt, terwijl de vorige nog niet leverbaar is. Azerty is helemaal lekker bezig, die zijn nu 3070 kaarten voor 3080 prijzen aan het verkopen, en zo nog meer. Het is echt een smerig spelletje en Nvidia heeft hier 100% aan bijgedragen...
RTX 3060;2;0.5536765456199646;Tja het is in ieder geval zeker niet zo dat er sprake is van weinig productie. Pak hem beet 10 jaar geleden deed Nvidia 3.32 miljard omzet, momenteel draaien ze 14.777 miljard omzet. Ze zijn dus 445% meer omzet gaan draaien. Ook ten oprichtte van begin 2020 zijn ze flink meer gaan leveren (10.9 miljard). Tevens zitten ze nu al 2 miljard boven de vorige crypto hype. De vraag is momenteel té groot om aan te voldoen. Zaken als gelijktijdige launches ipv geografisch verspreid en mining hypes helpen daar niet bij. Als consument niet leuk maar wel de realiteit van de globalisatie.
RTX 3060;1;0.5717966556549072;"Hoewel je helemaal gelijk hebt met de constatering dat Nvidia de geëxplodeerde vraag naar videokaarten niet kan bijhouden, baseer je die conclusie op de verkeerde getallen. De jaaromzet over 2019 was US$ 10,9 mld en over 2020 US$ 14,8 miljard. Van die enorme omzetstijging is $ 2,2 miljard gerealiseerd door de gamingdivisie, waar de videokaarten onder vallen. De veel kleinere data center-divisie is nog veel harder gegroeid: daar is de omzet in 2020 verdubbeld, resulterend 1,7 miljard omzetstijging. Tezamen maakt dat een omzetstijging van $ 3,9 mld over 2020. Tien jaar geleden bestond die divisie nog niet; hij omvat onder andere het voormalige Mellanox, dat Nvidia in 2019 overnam voor een bedrag van $ 6,9 mld. Een fors deel van de groei van Nvidia in de afgelopen 10 jaar is dus het gevolg van een grote overname. Dat doet niet af aan het feit dat Nivida over 2020 een enorme omzetstijging met de verkoop van videokaarten heeft gerealiseerd, maar zonder deze toelichting zou je kunnen denken dat het succes alleen aan de videokaarten te danken was."
RTX 3060;3;0.5101589560508728;Beetje kip ei probleem wellicht? Nvidia is natuurlijk meer gaan doen met hun GPU tech door ook in andere markten te starten. Er is een tijd geweest dat er op gaming eigenlijk geen winst werd gemaakt en alles van Quadro kwam. Proefballonnetjes als automotive en tegra waren niet echt winstgevend. Vanaf Geforce 6xx begonnen de gaming producten winst te maken en dat heeft verdere groei en investeringen (in R&D) mogelijk gemaakt. Maar goed verder wel een prima punt dat je maakt. Als ik Nvidia mag geloven dan hebben ze het in 2011 over 2.52 miljard omzet met de GPU business. In het meest recente verslag was dit 5.52 miljard. Dat is iets meer dan een verdubbeling. Ik spiek even snel bij AMD om te kijken wat die nog zouden kunnen. In 2011 rapporteerde AMD 4.82 voor Computing Solutions en 1.66 voor de Graphics. AMD is helaas gestopt met het publiceren van hun GPU omzet. Momenteel doen Computing and Graphics samen 6.64. In 2019 was dat nog 4.7. Dus AMD is ook flink meer gaan omzetten, maar zijn nog niet verdubbeld.
RTX 3060;2;0.48338666558265686;De omzet is niet direct te vergelijken zonder de prijsstijgingen en afname in acht te nemen. Als 10% in het hogere segment de afzet gekocht heeft voor 400% meer dan 10 jaar geleden, verklaar je al een deel van het omzetverschil. Vervolgens 50% van de afzet 140% etc. etc.
RTX 3060;1;0.5789207220077515;Megekko heeft twee MSI 3070s te koop staan voor 1349 en 1399 euro.
RTX 3060;1;0.68293696641922;Megekko zijn schaamteloze scalpers en heb ik dan ook per direct uit mijn favorieten verwijderd. Doen alsof ze geen voorraad hebben en ondertussen dit soort prijzen vragen én stiekem uitleveren
RTX 3060;1;0.5288227200508118;Ik heb m'n 3070 nog gewoon voor 590 geleverd gekregen. Wel 3 maanden op moeten wachten. Dat ze dat nu niet meer doen is niet zo vreemd. Waarom zou je ze voor 10% winst verkopen als je er ook 80% winst op kan maken? Wanneer zij het niet doen, dan krijg je opkopers die het voor ze doen.
RTX 3060;1;0.5779811143875122;Jij denkt echt dat de winkelier dat geld opstrijkt? Ik denk het niet. nVidia en AMD zullen ook ongetwijfeld hun prijzen hebben bijgesteld. En dan is er nog wat tussenhandel. En uiteraard gaat er dan ook nog een deel naar de staat vanwege de BTW (een kaart van €500 netto krijgt er nog eens €95 extra bij, dus een kaart van €750 netto gaat €142.50 opleveren).
RTX 3060;1;0.4578779637813568;Azerty vraagt anders ook die prijs. En Anternate heeft ook een rtx 3070 voor 1k staan. Haal je die dan ook even weg? Want ik denk dat je dan wel elke shop uit je favorieten halen.
RTX 3060;5;0.36156144738197327;Inderdaad en ze zijn er ook allemaal uit: wie gaat in deze tijd nou in een videokaart investeren
RTX 3060;1;0.43525996804237366;Alternate vraagt nu 650 en bijna 700 voor de RTX3060. Te grof woorden, dus ik zeg maar niets Grrrrr...
RTX 3060;2;0.34718430042266846;Tja, wie is er gek: de winkel de meer vraag of de consument die meer geeft? Mooiere graphics zijn leuk, maar als je een GTX1xxx hebt, kan je nog wel een paar maand verder tot de prijzen genormaliseerd zijn. Maar ik zie ook bij andere dingen dat Corona rare dingen doet bij mensen.
RTX 3060;5;0.5219637155532837;Ik zit met een rx 580 4gb nog prima. Was van plan de upgraden naar een 3060/3060ti dan wel amd variant. Maar dat stel ik rustig uit. Wachten tot de mining bubble instort en dan een mooie ex mining kaart kopen. Heerlijk als je kan wachten.
RTX 3060;5;0.551875114440918;De RX580, maar dan wel de 8gb variant, is anders een van de beste mining kaarten die je kunt kopen. Die heb je na 3 maanden minen al terugverdiend 😅
RTX 3060;1;0.3893287479877472;Is dat zelfs het geval in NL met de electriciteitskosten die we hebben? Ik dacht dat mining alleen maar interessant was voor mensen in Azië enzo?
RTX 3060;3;0.3292534351348877;Als ik kijk naar mijn energierekening is het wel degelijk ook hier interessant. Levert een paar euro per dag op. Op jaarbasis zal de netto winst tussen de 500 en 1.000 euro zijn afhankelijk van de koers. Mijn pc staat toch 60 procent van de tijd aan, dus qua verbruik maakt het schat ik permanent zo'n 100 watt verschil. En ik hoef mijn werkkamer niet meer te verwarmen dus het scheelt gas
RTX 3060;1;0.2415454238653183;Bedankt voor de reactie! Uit nieuwsgierigheid, mine je dan verschillende (niche) coins of een specifieke (mainstream) coin zoals Ethereum/Bitcoin?
RTX 3060;5;0.3145354092121124;Je kunt voor je eigen videokaart uitrekenen wat de meest gunstige coin is om te minen. Ethereum staat vaak hoog. EDIT: Ik doe het met mijn RTX 2070 super nu ook als ik niet aan het gamen ben.
RTX 3060;5;0.6662000417709351;Ik mine Ethereum. Dat is op dit moment ook de meest interessante.
RTX 3060;1;0.3965185582637787;Ik heb hem van een ex miner. Gekocht toen de boel weer instorten. Nu weer wachten😀
RTX 3060;3;0.34936660528182983;Ik zit met m'n GTX 1060 ook te wachten, maar heb me er al op ingesteld dat het volgend jaar wordt voordat het een 3060 of 6700 wordt. Een ex miningkaart zou ik persoonlijk nooit voor gaan, je weet zeker dat die heel lang heel hard gewerkt heeft, dan koop ik liever nieuw (al moet ik toegeven dat dit op basis van gevoel is, niet op feiten) Aan de andere kant, misschien gaan de zaken na de zomer/tegen het einde van het jaar wel zo goed dat ik mezelf voor kerst een 3070 of 6800 (XT) kan geven?
RTX 3060;5;0.5750393271446228;Ik heb nu mijn 3de ex mine kaart. Nog nooit problemen mee gehad. En ja ze werken hard. Maar meestal met een undervolt. En gaan 1 keer aan. En daarna niet meer uit. Wat ook weer gunstig is. Tevens staan ze meestal in een Stofvrije en koele omgeven. Wat veel beter is als dat ze aan het zweten zijn op iemands muffe zolder. Maar er zijn groepen die er direct 1 kopen. En die toch liever nieuw gaan. Maar gelukkig gaat de nieuwe prijs dan meestel ook mee.
RTX 3060;3;0.27930447459220886;Wat voor kaarten heb/had je dan en hoeveel scheelt zoiets in prijs als ik vragen mag?
RTX 3060;2;0.46847689151763916;Een constante belasting waarbij de tempratuur gelijk blijft is denk ik beter voor de levensduur dan een kaart die steeds opwarmt en afkoelt
RTX 3060;2;0.42151522636413574;tsja hoe meer ik er nu over nadenk... had het beeld in m'n hoofd dat veel mensen zo'n pc ergens ver uit het zicht in een stoffig hoekje zouden zetten
RTX 3060;3;0.451137900352478;Alleen wordt het videogeheugen vaak veeeeel warmer dan tijdens bijvoorbeeld gaming.
RTX 3060;1;0.726477324962616;Ten zei je GTX1xxx kaart stuk gaat. Dan heb je een probleem.
RTX 3060;3;0.4121546745300293;Ik heb een GTX 1070, maar wil (met nadruk op wil, dus niet moet) een RTX 3080 om mijn Valve Index beter tot z'n recht te laten komen. In de tussentijd vermaak ik me inderdaad prima met 1080p gamen op een plat scherm en een iets inferieure VR-ervaring. In mijn mening houden de winkels en de gekken die 250%+ MSRP betalen elkaar in stand. De normale markt is op het moment gewoon even ver te zoeken.
RTX 3060;1;0.5318252444267273;Het probleem is dat de meeste consumenten niet zo een dure grafische kaart kopen, maar juist de mensen die cryptocurrency mining doen, en een berg grafische kaarten kopen, want wat maakt het hun uit dat de kaarten 2x zo duur zijn als ze het toch weer met +/- een half jaar het er al uit hebben, en Nvidia geeft geen bal om gamers, het is eerlijk gezegd een vies bedrijf geworden, die zeggen dat ze geven om gamers, maar daar glashard over liegen. Linus heeft er een goede video over gemaakt.
RTX 3060;2;0.30299103260040283;Ik zag ze ook.... en die blijven wat mij betreft ook te koop Het is echt van de zotte tegenwoordig. Maar ook Ryzen 5000 cpu's stijgen in prijs. Ik dacht duur uit te zijn laatst, maar hij stijgt alleen maar door, dus valt weer mee?
RTX 3060;1;0.699120044708252;De AMD Ryzen 7 5800X zijn juist het zelfde gebleven sinds 13 januari, en nog steeds rond de €480, maar de AMD Ryzen 5 5600X is weer rond de €370 gegaan, en de AMD Ryzen 9 5900X is op tweakers nergens meer op voorraad, en de AMD Ryzen 9 5950X is ook nergens meer op voorraad, ben blij dat ik de AMD Ryzen 5 5600X in december gekocht heb voor maar €319,- Triest dat het zo slecht gaat met CPU's en GPU's, 2021 is een waardeloze tijd voor gamer en video editors en zo.
RTX 3060;1;0.31367242336273193;Bizar voor een kaart die zo rond de 550 euro moet kosten.
RTX 3060;1;0.4911084771156311;Hah, ik denk dat ik mijn 3080 te koop ga zetten voor dat geld, ik heb mijn kaart uiteindelijk letterlijk voor de helft gekocht. Man man man, wat een ellende Gelukkig lijken een aantal AMD's 5000 CPUs inmiddels wat minder slecht leverbaar te zijn..
RTX 3060;1;0.5365604162216187;Wat dacht je van Max ICT die iets van 100 stuks 5700 XT op voorraad hebben voor 1800 euro per stuk
RTX 3060;1;0.5848230123519897;Ja, dat is ook absoluut verdorven. Oude kaarten zijn nu ook gigantisch in vraag gestegen, waardoor de price/performance ratio van die kaarten ook steeds slechter wordt. Mijn MSI GTX 1070, die ik bijna vijf jaar geleden voor ~€500 kocht, zie ik nu geregeld in V&A staan voor ~€250. Als je dan zo'n €80 bij zou lappen, zou je een gloednieuwe RTX 3060 voor MSRP kunnen kopen. Jammer genoeg is de markt voor nieuwe kaarten gewoon een fabeltje. Als de supplies toereikend waren, zou ik misschien hooguit €100 voor die GTX 1070 vangen.
RTX 3060;2;0.34968647360801697;Ik denk niet dat je (alleen) Nvidia verantwoordelijk kan houden voor de wereldwijde tekorten van chips...
RTX 3060;2;0.5426592230796814;En aan de andere kant denk ik dan ook: de GPU is kleiner en mag net iets meer defecte cores hebben. Is met andere woorden eenvoudiger te produceren en mogelijks voor vele mensen die vandaag in de wachtrij staan ook een waardig alternatief waardoor die mogelijks alsnog voor deze kaart kiezen en de wachtrijden daarmee korter worden. Uiteindelijk zal dit op de totale vraag naar RTX30 kaarten weinig invloed hebben en zorgt de introductie dus niet noodzakelijk voor een negatieve impact en mogelijks zelfs een positieve.
RTX 3060;4;0.2722783386707306;En tel daarbij op dat de kaart ondanks de maatregelen van nVidia toch winstgevend is voor mining. Tomshardware - RTX 3060 Still profitable mining
RTX 3060;1;0.4208993911743164;Tij om op een ander algoritme te minen KaWPow is opzich ook profitable en let maar op, waar geld te verdienen valt, verhuizen de miners ook heen...Linus geeft inderdaad goeie kritiek, dit is een lachtertje en ik durf te wedden dat er dit jaar nog wel een goeie driver wordt uitgegracht voor miners door een derde partij...met of zonder malware
RTX 3060;5;0.693615734577179;Ze kunnen nauwelijks leveren in theorie, maar ze hebben het beste jaar ooit acher de rug. Ja Ja.. ze leveren gewoon enorme hoeveelheden van de 30x via hun partners naar de mining industrie. EN ze kunnen door de schaarste bijna iedere prijs vragen die ze willen voor hun Chips. Waar de miningboeren nu zelfs de laptops hebben ontdekt, bij gebrek aan normale GPU's. Hallen vol laptops met 3060's worden tegenwoordig al ingezet, ook handig als je powergrid niet zo stabiel is.
RTX 3060;1;0.3679267466068268;De koptekst van de 2e alinea zegt het al: Ampere voor iedereen
RTX 3060;3;0.3757030963897705;Waarvoor zou je in 2021 immers nog meer een GeForce-videokaart gebruiken dan alleen voor gaming? Ik gebruik in een typische werkdag nog best vaak mijn gpu. Een paar voorbeelden: - Videobewerking Davinci Resolve maakt veel gebruik van de GPU, en het werkt stukken lekkerder met gpu-acceleratie. OBS studio gebruikt de gpu ook. - Fotobewerking Photoshop en Lightroom werken gebruiken de GPU ook, maar het verschil is niet zo groot als bij resolve. - Photogrammetry Ik maak regelmatig 3D modellen op basis van foto's met Agisoft Metashape. Metashape en soortgelijke software heeft erg veel baat bij de aanwezigheid van een of meer gpus. - Scientific computing (MATLAB / Python) MATLAB maakt het gebruik van de GPU in code erg makkelijk, althans voor grote matrix berekeningen. Voor de juiste werklast, kan het zo een factor 10-100 schelen. In Python zijn ook tools beschikbaar om cuda gpus te gerbuiken. - Machine learning De meeste frameworks hebben een GPU nodig. Dit is wel een toepassing waar je makkelijk een cloud GPU in zou kunnen zetten ipv eentje in je eigen machine. In een echt workstation zal waarschijnlijk een Quadro kaart zitten ipv een Geforce, maar nu we allemaal thuis werken moet dit vaak op de PC gedaan worden en voor veel van deze taken werkt een Geforce ook prima.
RTX 3060;3;0.3918382525444031;Ik denk dat je alleen wel in de 1% groep zit met zulk gebruik De meesten zullen voornamelijk gamen......
RTX 3060;1;0.3002350330352783;Minen bedoel je?
RTX 3060;1;0.7555601596832275;Owja, de gamers kunnen er geen kopen.....
RTX 3060;1;0.5195477604866028;Dat zegt ie niet.
RTX 3060;1;0.3572920262813568;Waarom niet beide ?
RTX 3060;3;0.4538942873477936;+1% (jaja, ik weet dat dat zo niet werkt) voor mij dan, iig. die 1e 3 punten
RTX 3060;4;0.3519550561904907;Het hier bedoelde doeleinde is vanzelfsprekend mining
RTX 3060;3;0.3200727105140686;Beetje offtopic, maar het valt me erg mee hoe goed een 1080 ti presteert in het gehele lijstje. Degenen die destijds deze kaart hebben gekocht hebben toen een achteraf een mega deal gedaan.
RTX 3060;4;0.43226736783981323;"Blijf ook lekker op mijn 1080ti zitten.. dingetje doet alles nog prima hier op 1440p. Dus ik wacht nog wel een jaar of 2 voor ik aan upgraden ga denken ;-)"
RTX 3060;5;0.584380030632019;Zo zit ik er ook in. Ik heb nog een tragere kaart dan die 1080 volgens mij en zelfs ik ben nog wel even tevreden zo. Ik heb een RX570, o.a. DCS loopt als een trein op 2560x1440 dus ik houd de hand op de knip voorlopig.
RTX 3060;2;0.44751185178756714;Kostte toentertijd ook een dikke 800 euro. Megadeal is dus wat optimistisch. Ik blijf erbij dat kaarten rond de 400-500 euro op de sweetspot blijven zitten. Alles erboven en je betaalt significant meer voor marginaal betere prestaties.
RTX 3060;2;0.4404572546482086;Wordt het zo langzamerhand geen tijd om ook VR performance te testen op bv een Oculus 2, Valve Index en HP Reverb G2? Lijkt me dat voor VR framerate en framedrops nog veel meer van belang zijn dan bij games op je scherm, aangezien je hier flink misselijk kan worden van mindere performance. Ik kan er ook naast zitten, uiteraard. Schaalt dit lineair en kan ik gewoon de 4k resultaten als referentie of zit er toch een verschil in het renderen van 2 beelden tegelijk ipv na elkaar? Heb helaas nog niet gigantisch veel ervaring met VR. Was hiervoor eigenlijk aan het wachten op de 30x reeks maar gezien de huidige woekerprijzen voor deze kaarten zal ik blijkbaar moeten wachten tot de vraag afneemt of productie toeneemt.
RTX 3060;2;0.40475791692733765;Eens, zit op een 1080GTX en geen enkele behoefte te upgraden... Behalve voor VR waar ik nu over na aan het denken ben. Alles presteert nog op high met 1440p. Denk dat Nvidia zich in zijn voet aan het schieten is momenteel. Cloud gaming is in opmars, en cloudgamers hebben geen 3080 nodig. Voor gamers die minder vaak gamen is stadia of andere gaming platformen een optie, en partijen als Google staan er om bekend dat ze zelf hardware kunnen ontwikkelen. Ze hebben de schaal, ze doen het al met opencompute hardware. Door de slechte levering van hardware en de extreme kosten is de kans groot dat het over een paar jaar EOL is voor Nvidia. Je consumenten lopen over naar cloud en cloud wil kosten en performance optimaliseren.
RTX 3060;2;0.47975099086761475;De grotere hoeveelheid VRAM gaat pas verschil maken op resoluties en instellingen die sowieso onspeelbaar zijn, simpelweg omdat de GPU daarvoor te traag is.. Het is dus een niet relevant iets voor de gemiddelde gamer en ik zou nooit een 3060 kiezen boven een 3060Ti, laat staan 3070, alleen omdat er meer VRAM op zit.. Hier een video die dat nog even goed laat zien: 2GB vs 4GB, maar zelfde principe nu in feite met 6GB vs 12GB..
RTX 3060;3;0.4389626681804657;Naar welke resolutie en instelling refereer je dan? Want de kaart presteert op 1440p nog prima en ook op die resolutie wil je echt wel wat VRAM hebben. In nieuwere titels is 6GB al te krap en wil je echt wel minimaal 8GB hebben, blijkt uit tests van ComputerBase.
RTX 3060;2;0.3699016869068146;> Voor gamers zelf is de komst van de RTX 3060 in ieder geval niet spannend. Nou... Dit is toch eindelijk een leuke opvolger voor m'n gtx1060, en ik denk dat een hoop andere gtx1060 eigenaren hetzelfde denken. Het is alleen 2 jaar (of langer) wachten totdat ie betaalbaar is .
RTX 3060;1;0.33104145526885986;Inderdaad! Dit zou de perfecte vervanging van een 1060 zijn..... als de adviesprijs van € 350 klopt.
RTX 3060;5;0.6092798709869385;Mijn GT 440 houdt het nog steeds vol XD
RTX 3060;3;0.35391342639923096;Had ook even 2080's meegenomen. Was wel benieuwd hoe zo'n 3060 het tegen een 2080 Super had gedaan. Of een Ti in de lijst voor compleetheid.
RTX 3060;2;0.4818650186061859;Helaas hadden we geen tijd meer om deze kaarten te testen met de nieuwste driver, vandaar dat de 2080's niet in deze review zijn opgenomen. Je kan wel zelf deze resultaten vergelijken, met dus de kanttekening dat het niet helemaal een eerlijke vergelijking is :
RTX 3060;3;0.4304206371307373;Ik was vergeten dat benchmark resultaten nu in eigen vergelijkingen mee worden genomen, wat handig is. Bedankt voor de herinnering.
RTX 3060;2;0.5381101965904236;Teleurstellende price/performance aan MSRP, zeker voor een kaart die richting het lower-end segment gaat.
RTX 3060;2;0.330641508102417;En dan nog 170W verstoken. Ik dacht eerder aan rond de 120-130W gezien zijn voorgangers *muv de RTX2060
RTX 3060;5;0.42832285165786743;De performance per watt is vergelijkbaar met andere Ampere kaarten zoals de 3070. Alle efficiëntie is performance die op de tafel is blijven liggen, dus pushen ze de chips harder indien mogelijk.
RTX 3060;2;0.3641515374183655;Dit had ik ook verwacht. Zeker voor een non-Ti -60 series kaart had ik anno 2021 wel verwacht dat we onder de 150W zouden zitten. Waarschijnlijk hebben ze de TDP hoger gelaten omwille van de performance, ze moeten iets doen om de MSRP te rechtvaardigen. (Niet dat dit echt uitmaakt, het is sowieso allemaal uitverkocht.) Edit: en we moeten natuurlijk het raytracing paradepaardje niet vergeten
RTX 3060;2;0.5261857509613037;Als ik de prestatie per watt bekijk met men oude 1070, valt het enorm op Ampere gewoon een Pascal kaart is met een hogere stroom verbruik ( Als we de Tensor cores negeren en puur focusen op game FPS, niet tracing etc ). Feit dat de 1070 een 16nm kaart is en de 3060TI een 8nm, dan zou je gewoon een verdubbeling van FPS verwachten voor dezelfde TDP. Niet 40% meer stroom verbruik voor 45% meer FPS. Je ziet hetzelfde weer met de niet TI 3060 waar er een enorme link zit met stroom vs FPS. Is nu al 2 generaties en zie nog altijd geen nut om te upgraden. Alles draait op de 1070 en de nieuw generaties worden duurder en duurder, hogere TDP ( aka sterkere PSU nodig ) en bieden eigenlijk weinig nieuws. Ray tracing zelf zonder performance hit is niet zo spectaculair om voor te upgraden, laat staan de massieve performance hit dat het veroorzaakt.
RTX 3060;2;0.5459815263748169;Als ik het goed begrijp is het zo dat maar een (klein) gedeelte van de chip daadwerkelijk 8nm is, bepaalde onderdelen van de chip zitten op bv. 16 of 12nm. Een halvering van stroom gebruik zou je dan ook nooit moeten verwachten wanneer van van 16 naar 8nm gegaan wordt, zo ver ik weet. Daarnaast zijn deze kaarten gewoon harder gepusht om een betere performance te leveren, ten koste van hun efficiëntie.
RTX 3060;2;0.4804978370666504;"> Een halvering van stroom gebruik zou je dan ook nooit moeten verwachten wanneer van van 16 naar 8nm gegaan wordt, zo ver ik weet. Als ik eventje vergelijk met een Ryzen 1800x @ 14nm en een 3900X @ 7nm, zie je dat men de de performance bijna verdubbeld heeft met bijna dezelfde stroom verbruik. 50% meer cores + 30% meer IPC gain. En dit is een onjuiste vergelijking want het AMD voorbeeld houd geen rekening dat de CPU cores eigenlijk ""groter"" geworden zijn, ondanks de shrinking, met een hoop meer cache. Als je een pascal 16nm neemt, smijt er 50% meer cores op in 8nm, zou je verwachten om ongeveer dezelfde power gebruik te hebben met een verhoging van 50% meer cores. En dat met grotere ""cores"" als we het AMD voorbeeld vergelijken. Het probleem is dat die Tensor cores ook volop stroom zuipen wanneer je ze niet gebruikt omdat ze integraal deels zijn van de 16/32 bit cores. En ik denk dat dit een deel van het probleem is. Een 1660 TI heeft 20% minder cores dan een 2060 maar met een 20% hogere clock snelheid ( wat eigenlijk de stroom verbruik hoger kan maken als het buiten de optimale zone is maar voor eenvoud laten we zeggen dat die gelijkaardig word ). Maar een groter verschil is een 40W drop ( 120 vs 160W ) stroom verschil zonder die tensor cores, een deel minder cores maar een hogere clock snelheid. Op hetzelfde proces! Er is een reden dat veel laptops ervoor kozen om 1660TI ipv de 2060. Volgens mij zijn die tensor cores een deel van het probleem ook bij de 3000 series want eigenlijk is het ontwerp niet veel veranderd tussen de 2000 en 3000 serie."
RTX 3060;3;0.23749294877052307;"Ik had het natuurlijk over videokaarten, niet CPUs, dus ik denk dat het beeld dat je tussen een 1800X en een 3900X schetst niet helemaal doorgetrokken kan worden naar videokaarten. Series in Nanometers: 700 series | 40nm | TSMC 28nm 900 series | 27nm | TSMC 28nm 1000 series | 18nm* | TSMC 16nm / Samsung 14nm 2000 series | 14nm* | TSMC 12nm 3000 series | 10nm* | Samsung 8nm Wattage: Nvidia GeForce GTX 770** | 230W | Nvidia GeForce GTX 970 | 145W | -59% gebruik in watt t.o.v. voorganger Nvidia GeForce GTX 1070 | 150W | +3% gebruik in watt t.o.v. voorganger Nvidia GeForce GTX 2070 | 175W | +17% gebruik in watt t.o.v. voorganger Nvidia GeForce GTX 2070 | 220W | +26% gebruik in watt t.o.v. voorganger Performance: Nvidia GeForce GTX 770** | 5.886 punten | Nvidia GeForce GTX 970 | 9.687 punten | +64% performance t.o.v. voorganger Nvidia GeForce GTX 1070 | 13.357 punten | +38% performance t.o.v. voorganger Nvidia GeForce GTX 2070 | 16.100 punten | +20% performance t.o.v. voorganger Nvidia GeForce GTX 2070 | 21.660 punten | +34% performance t.o.v. voorganger Kijkend naar die nummers zie ik in ieder geval niet terug wat jij tussen die 1800X en 3900X aangeeft. * 800 serie ""bestond niet"", dus vandaar de 700 serie. **nanometers is meer een marketing verhaal dan de daadwerkelijke nanometers: Rekenhulpje van ASML - pagina 4 Uitgebreid Tweakers artikel"
RTX 3060;3;0.4224683344364166;Mag ik vragen waarom de RTX3070 niet in de gaming-performance vergelijking staat en wat het geheugengebruik is ten opzichte van een 3060Ti (heeft de consument nu al iets aan de 12GB op de 3060?)? Het lijkt mij dat de meeste lezers hier wel een antwoord op zouden willen. Interessant genoeg lijkt de 3060Ti ook een stuk beter te zijn qua fps/per euro. Dit geeft AMD volgende week met de lancering van de RX6700(XT) best een mogelijkheid om goed voor de dag te komen.
RTX 3060;2;0.4740133583545685;Met een harde refresh zou het cache moeten verversen en ook de RTX 3070 moeten verschijnen in de grafieken. Wat betreft het geheugengebruik is er een verschil tussen de allocatie en hoeveel er daadwerkelijk nodig is om een spel vloeiend te draaien. Het geheugengebruik (vram) dat af te lezen is met verschillende tools is daarom niet geschikt om uitspraken te doen over wat nodig of nuttig is.
RTX 3060;2;0.44573974609375;"Geheugen is meer dan alleen d eheoveelheid en veel vergeten dit in deze vergelijkingen. Hoe hoe snel is het geheugen, hoe is de bandwidth etc. De 3080RTX heeft dan bijvoorbeeld ""maar"" 10GB maar is wel meteen het snelste geheugen deze generatie. Je kan wel bijvoorbeeld 16GB vram hebben, maar als het niet snel is houdt het nogsteeds de prestaties omlaag."
RTX 3060;2;0.41222232580184937;Het argument dat Nvidia begaan is met het lot van de gamers die nu geen videokaarten kunnen krijgen door COVID19/Cryptocoins is belachelijk: als Nvidia nou serieus niet wilde dat zijn kaarten voor mining gebruikt werden, dan hadden ze dat vanaf dag 1 voor alle nieuwe kaarten kunnen dichttimmeren, al is het dan nog maar de vraag of die lui dat probleem ook niet hadden kunnen omzeilen... het is niet zo dat gebruik van videokaarten voor crytomining nu volkomen nieuw is...
RTX 3060;2;0.3597593903541565;Dat valt allemaal wel mee, die 3xxx serie kwam uit voordat bitcoin/ethereum weer echt als een speer omhoog ging, op dat moment was er helemaal geen sprake dat miners die dingen massaal gingen opkopen, dat is na de launch pas gekomen. Tuurlijk willen ze het niet onmogelijk maken als het niet hoeft, want miners is ook gewoon een afzetmarkt en die willen ze heus wel bedienen maar misschien niet ten koste van hun (steady?) gamer-markt... Dus ik snap best dat ze daar niks tegen wilden doen op het moment dat het nog helemaal geen echt probleem was.
RTX 3060;1;0.5757655501365662;"Waarom denk je dat het zo simpel is? Vanaf dag 1? Vergeet niet, als de ""mining detectie"" per ongeluk aanslaat op een game, dan halveert dus ook je FPS. Weet je hoe dat zou vallen bij gamers?"
RTX 3060;5;0.4594486653804779;More like Amper voor iedereen
RTX 3060;2;0.38126951456069946;Ik heb niet elke benchmark pagina gekeken, enkel de eerste en laatste paar pagina's. Maar het is opvallend dat er helemaal niets van prijzen genoemd wordt. Wat is de adviesprijs en wat zijn nu gangbare prijzen om er een te krijgen op de markt? Dat is toch wel belangrijk om hem te kunnen vergelijken met andere kaarten op dit moment en of het wel/niet een goede upgrade is. De pricewatch heeft nog geen prijzen. Het lijkt zo meer een preview
RTX 3060;2;0.47594669461250305;"De adviesprijs is vrij zinloos voor de 30 serie; daar is het aanbod te klein voor. En er is inderdaad nog geen marktprijs, de PriceWatch is extreem actueel. Dus die kan ook niet in het artikel."
RTX 3060;1;0.35551419854164124;Nvidia zei iets van € 350. Dus totdat de prijzen onder de € 400 komen, zou ik gewoon je huidige kaart blijven gebruiken. Want de € 600 die de meesten er nu voor vragen is echt volledig over de top.
RTX 3060;3;0.611957311630249;Ik ben niet echt onder de indruk over de performance winst ten opzichte van de RTX 2060, en helemaal niet vergeleken met de Super variant. Je gaat er op prijs/performance nauwelijks iets op vooruit. Alleen op RT zie je een wat duidelijker sprong.
RTX 3060;5;0.5494077801704407;Als de adviesprijs van € 350 klopt dan is dit een hele goeie upgrade voor mensen die nog een 1060 of lager hebben.
RTX 3060;1;0.48316386342048645;Die mensen gaven destijds 160-250 euro uit voor een GTX 1060. Dit ziet er niet uit als een goede upgrade. Tenzij ze graag 100-200 euro extra willen uitgeven en met deze prijzen nog veel meer. Bekijk maar eens de prijzen van de kaarten: Mijn edit kan je negeren want de MSRP werd niet geplaatst. Die mensen hadden uiteindelijk dus een 5700, of 2060/2060 Super een hele tijd eerder kunnen hebben voor een vergelijkbare prijs.
RTX 3060;2;0.4741203188896179;Oh, ik wist niet dat de 1060 ooit zo goedkoop was. Ik had alleen even gekeken naar de prijzen van de kaart die ik heb en die begon in 2016 op ongeveer € 340, dus ik dacht dat geld wel voor de meeste kaarten. Maar als de meeste 1060's een stuk goedkoper waren en de adviesprijs niet zal worden gehaald, wordt het inderdaad wel een stuk slechtere upgrade.
RTX 3060;2;0.5336675643920898;Het valt mij op dat de GTX1080 niet is meegenomen in de benchmarks. Dat is best jammer aangezien het verschil tussen de GTX1070 en GTX1080 best 'groot' was en tussen de GTX1080 en de GTX1080ti ook. Ik kan me ook herinneren dat de GTX1080 vaak tussen de RTX2060 en RTX2070 in zat qua prestaties afhankelijk van de game/benchmark. Laat dat nu ook het punt zijn waar de RTX3060 terecht komt. Als ik voor mezelf spreek zou ik net als iedereen graag willen upgraden maar ik zie voor mezelf in de RTX3060 nou niet echt een vooruitgang t.o.v. mijn GTX1080. Dus de slogan 'Ampere voor iedereen' is in mijn ogen een magere spreuk. Tel daarbij op het 'hoge' stroomverbruik van de kaart en ik heb nog minder reden om deze kaart te kopen. Zelfs als de RTX3060 ook daadwerkelijk te koop komt voor rond de adviesprijs. Voor de high-end Pascal kaart bezitters is er eigenlijk op dit moment geen upgrade mogelijk tenzij het geld uit de kraan stroomt en je een paar weken of maanden tijd hebt om te wachten. Het is gewoon triest.
RTX 3060;3;0.4247933328151703;Die hebben we inderdaad niet op dit moment met deze driver getest. Wel kan je de resultaten die we in het verleden hebben behaald, met dus een andere driver, zelf vergelijken. De verschillen in prestaties zijn over het algemeen hoogstens een paar procent, dus de vergelijking zegt genoeg :
RTX 3060;1;0.2804913818836212;Zeer opvallend hoe groot het verschil is tussen de 2070 (Super) en 3070, en de 2080 (Super) en 3080, en hoe klein (non-existent) het verschil is tussen de 2060 (Super) en de 3060 De 3060 lijkt misschien wel de slechtste koop van de hele Ampère line-up
RTX 3060;1;0.4346649646759033;En wanneer zijn de RTX3060 kaarten beschikbaar? Ik kan dit nergens vinden. Wellicht als ik er vroeg bij ben dat ik er nog één kan krijgen, ik zit tevens nog met een GTX 960 .
RTX 3060;1;0.30690550804138184;Om 18.00 uur
RTX 3060;1;0.3188806474208832;Waar?
RTX 3060;2;0.39692220091819763;Ik geef je de meeste kans bij de officiële Nvidia partners: Alternate, Azerty, Megekko/Cdromland, Paradigit en Informatique. Maar reken op hooguit enkele honderden stuks voor iets dat in de buurt komt bij MSRP.
RTX 3060;1;0.5283951163291931;Bedankt, ik zal m'n ogen open houden!
RTX 3060;3;0.3249400854110718;Alternate heeft er nog
RTX 3060;1;0.7938240170478821;ik ben nu vanaf 18:00 al aan t f5'en op azerty en alternate, nogsteeds maar 4 kaarten op azerty en op alternate helemaal niks, iemand een idee waarom de kaarten langzaam online komen en niet in een keer?
RTX 3060;1;0.3589828908443451;welgeteld 10 seconden beschikbaar op alternate.be voor 339, nu weer overal 500+, dáááááág
RTX 3060;2;0.41091984510421753;Op Alternate 650 en bijna 700. Zijn nu zelf aan het scalpen. De kaarten zouden iets over de 300,- gaan kosten.
RTX 3060;5;0.4108411371707916;Op Alternate eindelijk beschikbaar! Ik heb geen idee, was volgensmij ook zo in de VS.
RTX 3060;1;0.6747509241104126;ik zie op alternate nogsteeds niks, kan aan mij liggen.
RTX 3060;1;0.3551884591579437;grafische kaarten, dan 3060 en dan kom je bij de page uit
RTX 3060;5;0.39676234126091003;Voor de Inno3d en Gigabyte kaart RTX 3060
RTX 3060;2;0.4304639995098114;Ah jammer, het spijt me. De goedkopere waren uitverkocht. Ik hoop dat ze snel terug in stock komen! :
RTX 3060;2;0.3538437783718109;Alleen de gigabyte is nog verkrijgbaar. Moet bijna 700 kosten. terwijl de adviesprijs iets over de 300,- is.
RTX 3060;1;0.4245883524417877;Ja dat was voor mij ook een teleurstelling dat ik voor meer als 500 een kaart kon kopen en niet 329...
RTX 3060;5;0.5862504243850708;Ik hoop dat je snel een kaart vind voor een bijpassende prijs. De prijs/kaart verhouding is helemaal uit balans. Succes!
RTX 3060;1;0.5927268862724304;Helaas is het enkel ethereum te detecteren niet genoeg gebleken. Met andere coins (octopus en cortex) is er alsnog een profit te halen van 7$ per dag. Bron: techlinked van deze ochtend
RTX 3060;2;0.44539496302604675;En ik geef je op een briefje dat die ethereum block ook wel overwonnen wordt. Het irritante van het verhaal is dat we van AMD ook niks hoeven te verwachten, ook als ze het in hun windows drivers blokkeren, zijn de linux drivers open source en daarmee dus prima om te katten als er een mining blokkade in zit. Ik ben in ieder geval blij dat ik een RX570 heb, daar is nog op te gamen, maar het nieuwe beeldscherm vorige week is toch maar gewoon 1080p geworden, gokken dat ik binnenkort een 1440p capable kaart kan krijgen leek me niet verstandig
RTX 3060;3;0.40717726945877075;Het verschil is wel dat de amd kaarten in de basis al minder geschikt zijn voor mining. Quote van pcgamer.com: In fact, were one setting out to design a GPU architecture that was fast for gaming but not so much for mining, it would look an awful lot like RDNA 2 with its modest 256-bit plus Infinity cache technology and conventional ratios between compute units, texture processing and pixel output. Dit vanwege de bandbreedte van het geheugen als ik het goed begrijp. Het is nu een race naar marktsaturatie dus ben benieuwd hoeveel 6700 kaarten amd kan maken.
RTX 3060;2;0.34012824296951294;"Klopt het dat ik geen temperatuur-kopje heb? Onder het hoofdstukje ""stroomverbruik en temperatuur"" is alleen het stroomverbruik te zien volgens mij. Of kijk ik ergens grandioos overheen?"
RTX 3060;1;0.7592613697052002;Die pagina was verkeerd genoemd, hoorde alleen over stroomverbruik te gaan omdat er zowel custom kaarten als referentiekaarten zijn getest.
RTX 3060;1;0.40615108609199524;Hebben jullie met of zonder ReBar getest? Ik zie op Eurogamer dat het best wat kan schelen: En ik ga ervanuit dat alle testen zonder DLSS zijn gedraaid?
RTX 3060;5;0.320531964302063;ReBAR support staat standaard aan op ons testsysteem, heb het even toegevoegd aan pagina 2. En alle tests zijn inderdaad standaard zonder DLSS gedraaid, tenzij anders aangegeven.
RTX 3060;1;0.6002867817878723;Ze gaan al voor 1000 Dollar over de toonbank. Miners.
RTX 3060;1;0.3027145564556122;Scalpers.
RTX 3060;1;0.42753127217292786;In de diverse mining Discord groepen zag ik al veel aanbiedingen voorbij komen en ook al out of stock binnen enkele minuten.
RTX 3060;1;0.7857146859169006;Als we nou met z'n allen geen prijzen boven MSRP zouden betalen dan zijn alle kaarten wellicht binnenkort weer voor normale prijzen te koop. Leuk dat er nieuwe kaarten geintroduceerd worden maar heel de markt, nieuw en tweedehands, is inmiddels leeg gekocht tot 3 generaties geleden. Los dat op voor je nieuwe dingen lanceert die weer niemand kan kopen. Een mining blokkade dmv een driver is een druppel op een gloeiende plaat. Niet alleen de miners zijn schuldig aan de markt van nu.
RTX 3060;1;0.608063817024231;Heb mijn 5700XT eind 2020 verkocht (voor 330 euro) in de hoop een sterkere rtx 3000 series te kopen. Domme fout 5700xt wint zelfs van de 3060
RTX 3060;2;0.492196649312973;Het komt niet zo vaak voor dat je met een veel goedkopere kaart veel meer fps gaat halen. Je had de 3060 Ti kunnen aanschaffen voor 450 euro. Dat is zo ver ik mij herriner ook de prijs waar de 5700XT voor te koop was. Dan had je er nog wel wat aan je upgrade gehad. Maar ook dat is beperkt tot pak hem beet 27% op 1440p (techpowerup). De 5700XT had ook best goede prijs/prestatie dat helpt niet mee .
RTX 3060;2;0.49436333775520325;De radeon 5700 serie houdt zich redelijk sterk tegen deze nieuwe kaart
RTX 3060;1;0.6510383486747742;"Weten we nog dat iedereen zei ""Al die 2080 TI's zijn niets meer waard nu de 3080 er is"". Dat waren tijden."
RTX 3060;3;0.3308635354042053;Is dit een vooruitgang t.o.v. de GTX980? Draai PUBG nu nog op 90 - 110fps op een 4/5 jaar oude kaart.
RTX 3060;2;0.29428985714912415;Ik wacht nog wel even totdat de 3080 weer gewoon verkrijgbaar is voor een normaal bedrag. Heb nu een 2080 Super dus kan nog wel even wachten. Ik hoop ergens dat NVidia toekomstige 3080's ook ongeschikt maakt voor mining ... eventueel door een minimale wijziging door te voeren waardoor een nieuwe driver en nieuwe firmware noodzakelijk zijn.
RTX 3060;5;0.6079421043395996;2080 super is nog meer dan voldoende.... zou lekker je geld in je zak houden voor de 5000 serie
RTX 3060;3;0.29638081789016724;Niet een hele heftige performance maar daar zou normaal gesproken de prijs ook naar zijn +-€ 350,-. Maar ze worden nu al bij alternate en azerty tussen de 529 en 700 eu verkocht wat een waanzin....
RTX 3060;1;0.5518128871917725;Nonderju... Ben een tijdje uit de pc hardware wereld geweest, omdat de prioriteiten ergens anders lagen, en m'n huidige pc nog wel prima beviel.. Dan krijg je toch z langzaam maar zeker de neiging om iig de grafische kaart eens te upgraden,.. erm, nou, dat gaat hem niet worden. Ik kreeg een mailtje van de nvidia newsletter dat de 3060 uit zou zijn gekomen. Gezien ik toch maar een 6600k processor gebruik, leek me een te zware grafische kaart geen meerwaarde meer te hebben. Dus de 3060 leek me een mooie keus dan... tot ik de prijzen zag. Ik denk dat m'n 2 970 GTX 's nog maar een jaartje door moeten. Maar zoveel geld voor een midrange kaart,.. echt niet. Zo nodig is het gelukkig ook weer niet. Had graag geüpgrade.. maar genaaid worden word ik liever met m'n broek uit... edit: De negatieve votes zijn geheid van mensen die nu een oudere grafische kaart willen verkopen, en er hetzelfde voor kunnen vangen als ze er zelf 2 jaar geleden voor hebben betaald. xD
RTX 3060;1;0.28922149538993835;Nog ff en je koop een 3060 voor wat een 2080 destijds kostte.
RTX 3060;1;0.6197943687438965;Ik begrijp niet waarom jullie zeggen dat het een acceptabele generatiesprong is, terwijl de 2060 helemaal niet meer de voorganger is maar de 2060 Super, en die sprong is gewoon bijna niet bestaand qua vooruitgang. Dit is gewoon een kansloze kaart.
RTX 3060;1;0.7971156239509583;de advies prijzen is gewoon clowning! hebben we dan geen enkel recht ? het is gewoon net als met huizen en voedingsmiddelen alles is onderhevig aan vraag en aanbod . Alles word vast gehouden om de prijs hoog te houden . de reinste zwendel praktijken !
RTX 3060;2;0.44993510842323303;Ik heb voor mijn GTX 2070 super 550 euro betaald, absoluut te veel in juni 2020. Maar ik wilde voor mijn zeer oude upgrade pc wel een goede kaart. Telkens als ik de nieuwe kaarten zie, denk ik, had ik maar gewacht. Maar als ik het zo lees, denk ik een prima kaartje te hebben gekocht waar ik nog wel minimaal 2 jaar mee vooruit kan en tegen die tijd zien we wel weer hoe de vork in de steel steekt.
RTX 3060;2;0.544634222984314;Ik vind de prestaties karig ten opzichte van de adviesprijs van 349 dollar en Megekko heeft de Inno3D nu op voorraad voor jawel €829 euro!!! Ik ben benieuwd hoe lang het duurt voordat alles een beetje normaliseert. Ik vrees kerst en misschien wel langer... voorlopig moet ik het maar doen met mijn RX570.
RTX 3060;3;0.33411893248558044;|Daarom, had een 2e hands 1080 gehaald vorige maand .. voor 300 gelukkig is dat nog een redelijke 1440p kaart
RTX 3050;3;0.3362053632736206;"het verbaast me dus enigszins dat ga106 - gpu in de xx50 serie videokaarten gebruiken. het is een flinke chip van 276 mm2 ( samsung 8nm, 12 miljard transistors ). als we kijken naar de chips die historisch voor xx50 kaarten zijn gebruikt : gtx 650 ( kepler ) : gk107, 118 mm2 ( 28nm ) gtx 750 ( maxwell 1 ) : gm107, 148 mm2 ( 28nm ) gtx 950 ( maxwell 2 ) : gm206, 228 mm2 ( 28nm ) gtx 1050 ( pascal ) : gp107, 132 mm2 ( 14nm ) gtx 1650 ( turing ) : tu117, 200 mm2 ( 12nm ) dit is dus de grootste chip voor een xx50 videokaart in de recente geschiedenis. het interessante is : er is een kleinere ga107 gpu beschikbaar dan de ga106 die ze nu gebruiken. die ga107 wordt al gebruikt in de rtx 3050 mobile, rtx 3050 ti mobile, nvidia a2 en nvidia a16. 0 x8 bus aanwezig, en dus niet een kleinere x4 bus zoals op navi24 van amd. sterker nog, ze hadden dus makkelijk een rtx 3050 ti kunnen maken die de volledige configuratie met 3072 shaders gebruikte, en eentje met 1 / 6e van de gpu uitgezet, en dus 2560 shaders, wat de rtx 3050 nu ook heeft. maar blijkbaar heeft nvidia dus veel ga106 chips die dusdanig stuk zijn dat ze niet meer een rtx 3060 of rtx a2000 gestopt kunnen worden. terwijl samsungs 8nm gebaseerd is op hun 10nm en een heel stabiel duv ( voor euv ) process. dus dat, of de productie / orders voor ga107 waren te laat en te weinig. maar je verwacht dat nvidia gewoon wafercapaciteit heeft gekocht en bij samsung nog wel de verhouding tussen ga107 en ga106 wafers bij kan stellen een paar maanden van te voren. tl ; dr : nvidia moet er een goede reden voor hebben, maar het valt wel op dat er zo ' n grote gpu met 1 / 3e van de chip uitgeschakeld in deze rtx 3050 gaat, terwijl er ook een kleinere, geschiktere chip beschikbaar was."
RTX 3050;1;0.5166171193122864;Het was leuk geweest als AMD een beetje die kant op was gegaan met de 6500 XT. Die heeft een diesize van 107 mm2 op TSMC 7nm. Nu is het gewoon een kaart die droevig slecht is voor zijn prijs vergeleken met wat we er vroeger voor kregen. Niet dat de 3050 een veel betere prijs/prestatie heeft.....
RTX 3050;3;0.5333765745162964;Vind het eigenlijk wel grappig dat de 6500xt bijzonder dicht bij de 1070 zit, misschien ligt het aan de beschikbaarheid van kaarten na de 10xx serie waardoor ik dit denk. Maar dat klinkt zo gek nog niet voor zo'n kleine chip een paar jaar later.
RTX 3050;3;0.42223310470581055;Ja precies wat ik ook dacht, als amd in de 6500 XT geen 4 gb maar 8 gb gezet had en mischien nog een paar andere verbeteringen dan was de prestatie dicht bij die van deze GTX 3050 geweest met een lager stroom verbruik. Mischien gaan ze nog iets concurerends uitgeven als ze wakker worden bij amd ik heb op mijn huidige 1650 4 gb zitten maar dat is nu al te weining voor sommige games en deze nieuwe GTX 3050 is ook bijna 2x zo snel. Deze nieuwe GTX 3050 heeft wel een nadeel, volgens toms hardware worden bij de productie ook chips gebruikt die in duurdere versies op de kaart zitten en waar van er een tekort van is, producenten zullen dan eerder een duurdere kaart maken waarmee ze meer verdienen in een markt waar de vraag hoger is dan het aanbod. De vraag is dan ook of er echt veel van geproduceerd gaan worden. We gaan het zien.
RTX 3050;2;0.23890553414821625;Als ze bij amd eindelijk eens wakker worden lanceren ze een kaart die 550 kost in het echt voor een msrp van 285. Helaas hebben ze een kaart van 210 gelanceerd die voor 300 te koop is. Dus als de reviews daar rekening mee hadden gehouden hadden ze overal moeten schrijven en vertellen hoe veel zwakker de 3050 is ten opzichte van de gelijk geprijsde 6600. Maar zoals je hierboven kan lezen wordt deze kaart vergeleken met de 1660 super en de 6500xt en is ook Tweakers in deze marketingstunt getuind.
RTX 3050;2;0.4126996695995331;Wat ik nog vooral mis zijn DLSS benchmark resultaten. Het mooie aan deze kaart leek mij vooral dat het DLSS mogelijkheden heeft met zijn Tensor cores en juist daarom voor die prijs best interessant zou kunnen zijn. Misschien mogelijkheden tot 1440p gaming? of Ultra gaming met 60 fps in cyberpunk? Zou iig nog een mooie aanvulling zijn, aangezien DLSS toch redelijk volwassen begint te worden.
RTX 3050;1;0.2959897220134735;Dat ben ik met je eens. We hebben de kaart 24 uur geleden eindelijk ontvangen en hebben daarom wat keuzes moeten maken voor het embargo, vandaar dat er geen DLSS-resultaten in de daarvoor geschikte games staan.
RTX 3050;3;0.3154841363430023;Had al zo'n vermoeden inderdaad. Zou alsnog een mooie additie zijn aan de review als jullie daar de tijd voor kunnen vinden!
RTX 3050;3;0.32121148705482483;De review is inmiddels bijgewerkt met enkele DLSS-resultaten, zie hiervoor pagina 15.
RTX 3050;3;0.4593023359775543;Dus met DLSS aan, op 1080p is het geen verkeerde kaart (ook niet geweldig, maar toch) Nu nog zien wat de prijs wordt
RTX 3050;5;0.7803058624267578;Grote klasse, dank!
RTX 3050;3;0.41556528210639954;Keuzes zijn zeer moeilijk, en ik begrijp het heel goed, alleen een beetje jammer dat jullie vrijwel alle testresultaten en dit geval gepubliceerd hebben die op menig ander techsite of kanaal te vinden zijn. Misschien kan de review nog aangevuld worden als jullie de gpu nog langer tot beschikking hebben. En misschien is het wel handig om binnenkort eens (youtube) livestream te starten met alle vragen die wij tweakers hebben mbt gpu's en welke keuzes je het op dit moment beste kunt maken. Ik (en ik denk veel andere tweakers) kennen nogal wat techkanalen, maar heb tot nu toe nooit gehoord wat je nu het beste kunt aanschaffen voor het geld dat je nu op dit moment te besteden hebt. Advies en discussies (onder experts op dit gebied) is natuurlijk nooit weg.
RTX 3050;2;0.3264826536178589;heb je nog meer wensen? : - ) er zijn wel zat van dat soort kanalen online hoor. ik betwijfel of tweakers hier veel resources aan wil / moet besteden als dit grotendeels een kopie is van een hoop pure youtube kanalen die eigenlijk hier beter geschikt voor zijn ( ze hebben dat meer als core business / journalistiek ). zoek je wel goed? het internet en youtube staat vol met adviezen welke gpu je kan kopen. ik vraag me serieus af waar mensen nog meer naar op zoek zijn. van alle producten op de wereld zijn gpu en cpu reviews het meest inzichtelijk en kunnen deze het meest objectief worden benaderd. je kan bijv. exact de fps per euro uitrekenen. met welke producten kun je de waarde nog zo exact bepalen? veelal kan je iets vergelijkbaars met andere hardware zoals ssd ' s, maar imo niet zo goed als met gpu ' s en cpu ' s. prijscategorie 3 eindigt bij 600 en prijscategorie 4 start bij 600, en de aantrekkelijke videokaarten respectievelijk 590 en 610 euro zijn, zijn die videokaarten vooral concurrenten t. o. v. elkaar i. p. v. concurrenten binnen die prijscategorie. los daarvan is het grote probleem met deze guides dat ze heel erg afhankelijk zijn van de momentopname : op welk moment van schrijven keek de auteur naar welke pricewatch om de prijzen te achterhalen van deze videokaarten. zeker de laatste 1, 5 jaar is dat nogal een grote factor. prijzen schommelen flink. een hoop kaarten staan niet pricewatches vanwege bijv. slechte verkrijgbaarheid. wat dat betreft kan je het beste zelf even goed kijken aan welke videokaarten je kan komen voor welke bedragen. videokaarten op een v & a kunnen bijv. erg aantrekkelijk zijn. met alle scalping tegenwoordig staan daar ook veel nieuwe kaarten tussen. eigen keuze natuurlijk of je dat wil maar het kan honderden euro ' s schelen. en dan zou je ik je toch weer adviseren om daarbij zelf goed de benchmarkresultaten te bekijken. daarmee maak je uiteindelijk gewoon de beste keuze.
RTX 3050;2;0.46283072233200073;Beetje off-topic, maar ik vind het triest dat fabrikanten embargo’s kunnen opleggen en dat die embargo’s vaak review kwaliteit beïnvloeden. Als je het vergelijkt met 10-20 jaar geleden zie je zo’n groot contrast. Ik weet dat het niet aan jullie ligt, aangezien tweakers nog een van de weinige kwalitatieve tech mediums is.
RTX 3050;1;0.4741831421852112;Embargo's verbeteren juist de reviews, want het voorkomt dat reviewers moeten haasten om niet gescooped te worden door iemand anders die verregaande conclusies trekt op basis van 10 minuutjes benchmarken. En ook dat een reviewer die de pech heeft om zijn kaart later te krijgen of die een defecte kaart krijgt, pas een review kan publiceren als daar weinig interesse meer in is.
RTX 3050;2;0.49947312474250793;Persoonlijk ben ik enorm teleurgesteld in DLSS. Zelfs op de quality preset zien fijne, scherpe dingen zoals haar er echt verschrikkelijk uit in CyberPunk 2077 (ik draai een 2070 Super op 1440p). Doe ik iets fout? Want het ziet er voor mij echt uit als simpelweg een lagere resulutie met wat sharpening.
RTX 3050;3;0.36454111337661743;DLSS is op zijn best in de latere versies en op hogere resoluties. Daarnaast zijn er objecten welke beter er uit zien met DLSS enabled dan standaard temporal based anti aliasing wat alles waziger maakt. Beetje sharpening toevoegen en je hebt gewoon een betere framerate en vrijwel identieke en in sommige gevallen betere resultaten. Maar niet elke DLSS versie is hetzelfde. DLSS nu is veel beter dan 2 jaar terug bijvoorbeeld.
RTX 3050;2;0.3326265811920166;Is 1440p geen hoge resolutie?
RTX 3050;2;0.4386306703090668;Is gemmideld nu. Veel minder qua pixelcount dan 4K bijvoorbeeld.
RTX 3050;2;0.3660159707069397;"Wat vind je ""gemiddeld""? 67% draait nog steeds 1080p."
RTX 3050;5;0.3080718219280243;Die stats worden voor een groot deel ook bepaald door alle internet cafe s en low spec machines in Azie en Zuid Amerika.
RTX 3050;5;0.5191892385482788;Je doet zeker niks fout, bij ons kwam het er ook zo uit. Wij testen het in de nacht waardoor dit zeker opvalt.
RTX 3050;2;0.42774292826652527;Ligt heel erg aan de implementatie van de game. Bij Icarus, Rust, Cyberpunk en nog wel een hand vol games is het echt verschrikkelijk. Maar bij BF2042 gaat het weer prima.
RTX 3050;3;0.35896649956703186;DLSS lijkt me inderdaad een van de meer waardevolle features voor de speelbaarheid van games met deze kaart
RTX 3050;3;0.47009676694869995;Hier een channel gevonden die aardig wat spellen test met DLSS enabled:
RTX 3050;1;0.6606170535087585;Exact dit. Ik was aan het zoeken naar een tabelletje of tabje met de benchmarkresultaten met DLSS enabled, maar in de hele review komt er geen enkele vermelding van DLSS voor en dus ga ik er ook even vanuit dat er totaal niet mee gebenchmarkt is. Begrijp ik eigenlijk niet zo goed. Zelfs met m'n 3080 ga ik DLSS aanleggen in Cyberpunk, met een 3050 ga je dat absoluut ook doen. Ik snap niet waarom dit dan niet getest is.
RTX 3050;3;0.4031194746494293;Misschien gebrek aan tijd, hoop daarom dat het alsnog later toegevoegd wordt. Op youtube staan momenteel al wel wat informatie erover: Lijkt erop dat vooral de softening op lagere resoluties soms een iets minder mooi beeld geven, maar de resultaten mogen er op het eerste gezicht zeker wezen. Edit: in cyberpunk (hoewel niet werd rondgereden/gelopen) was het verschil op 1080p high: 55 fps voor native en 91 fps voor DLSS balanced
RTX 3050;1;0.92243891954422;is gewoon slechte kaart die 3050 { 250dollar} en dan hier 2 x zoveel? Ze staan nu sterk in de markt vanwege schaarste....Waarom zou je deze kaart kopen? Je moet wel heel erg wanhopig zijn dan! En dan de prijs die ze durven te vragen!!! 550 euro's? wtf echt links laten liggen. De gene die dit koopt moet echt naar de huisarts.
RTX 3050;3;0.326806902885437;hmm hoop dat er ook een low profile versie van de 3050 komt, op het moment de 1650 in bezit maar zal op full hd een mooie toevoeging zijn
RTX 3050;1;0.4312533438205719;Is dit een interessante kaart voor miners? Zo niet dan hoop ik er 1 te kunnen bemachtigen om mijn budget pc build eindelijk af te maken.
RTX 3050;2;0.40092164278030396;Vind niet dat je nog echt van een budget kaart kunt spreken als die meer dan 200 euro is, waarbij deze sowieso dus waarschijnlijk zo'n 600 euro gaat kosten in de webshops.
RTX 3050;3;0.5018733143806458;afhankelijk van de prijs wellicht.. eerst maar eens zien of ze te leveren zijn. Ik was zelf wel benieuwd of het een vervanger zou kunnen zijn voor mijn 1660 super, maar helaas weinig verschil.
RTX 3050;1;0.47467437386512756;"op een sticker op de antistatische zak van onze testkaart staat ""This product is not designed for Crypto Mining. We reserve the right to void the warrenty if there is any damage associated with this application."" dus dat zit goed dichtgertimmerd"
RTX 3050;3;0.4111679494380951;In de huidige markt is de prijs/prestatie-verhouding gewoon niet meer vergelijkbaar met voor de crisis, toen ik mijn 1660 S heb gekocht. Dus ik verwacht geen redelijke vervanger voor mijn kaart te zien totdat de crisis eindigt. Dan zitten we minimaal bij de 40x0 kaarten.
RTX 3050;3;0.3482006788253784;Waarschijnlijk wel gezien de 8GB ram.
RTX 3050;4;0.383727103471756;Zoals in het artikel staat, met 8GB VRAM interessanter dan bijvoorbeeld de 6500XT, maar wel allemaal met LHR. Afhankelijk van de echte prijs denk ik dat een miner efficiënter kan mijnen met een 3060 (Ti).
RTX 3050;3;0.43155866861343384;Volgens videocardz zou de 3050 inderdaad te lange ROI hebben, mede veroorzaakt door LHR. Op zich natuurlijk prima.
RTX 3050;2;0.5093953609466553;Zo niet, dan wil dat niet meteen zeggen dat de prijs goed wordt. Mining maakt niet alleen kaarten duurder die geschikt zijn voor mining, maar veroorzaakt ook een waterbed-effect: er zit nu een heel leger gamers al twee jaar te wachten op een beetje redelijk geprijsde videokaart, dus zodra die er is is de vraag alsnog enorm, ook al kopen miners hem niet. Dat de koersen van crypto-valuta nu hard aan het dalen zijn zal meer effect hebben op wat je voor een redelijke prijs kan krijgen.
RTX 3050;1;0.3262614607810974;Ach als deze massaal door miners opgekocht worden kan je misschien voor een 3040 potato edition gaan . Echt zal blij zijn als die crypto hype eens over gaat.
RTX 3050;2;0.41779381036758423;De RTX3050 is niet interessant voor mining, maar doordat niemand voor een normale prijs een RTX3070/3080 kan kopen zullen ze op de RTX3050 storten en zodoende veel vraag naar zijn.
RTX 3050;1;0.5979355573654175;NVIDIA(en ook AMD) lacht zich dood, gamers kopen hun low end 3050 wel om uiteindelijk een (hopelijk betaalbare) 4000 serie te kopen omdat die 3050 nu al op zijn teentjes loopt.
RTX 3050;3;0.5016754865646362;Ik ben nog niet echt thuis in gpu mining, maar dit lijkt mij niet een interessante kaart voor minen. winst erop maak je wel maar is mimimaal en zeker dankzei de huidige nieuwe stroom tarieven... de zon schijnt nog te weinig...
RTX 3050;3;0.5865820646286011;Stroomverbruik is minder dan een 1660s/ti. Dus als je met de 3050 ongeveer 30 MH/s haalt is qua verbruik prima. Inkoop moet wel rond de 350-400 zijn. Dat kosten de 2e hands 1660s/ti ook.
RTX 3050;1;0.5255799889564514;Dat haal je dus niet, zie de link hierboven. Dit wordt geen succes voor miners tenzij men in de glazen bol kan zien dat ethereum nog flink mooned voor het overgaat naar POS. De vraag is ook maar wat er gaat gebeuren qua waarde/hashrates als eth naar POS gaat en iedereen altcoins/shitcoins gaat minen.
RTX 3050;1;0.4111880958080292;"Zon maakt niks uit met de huidige saldering regeling, je kan in de winter je ""voorraad"" opmaken. Daarnaast heeft de overheid juist de energie belasting verlaagt om warmtepompen te promoten."
RTX 3050;1;0.5337512493133545;"Volgens mij heb ik vier jaar terug ongeveer 400 euro betaald voor mijn 1070; die presteert maar marginaal slechter dan deze kaart die slechts 100 euro goedkoper is terwijl we vier jaar later zijn - vroeger een eeuwigheid in computer hardwareland. M.a.w. relatief weinig performancewinst in vier jaar tijd als je het per euro afzet. Met mijn HD4870 die ik hiervoor had heb ik ook 8 jaar gedaan, ik verwacht dat mijn 1070 dat nog wel gaat overtreffen."
RTX 3050;2;0.44201934337615967;Dit is eigenlijk nog een stuk erger. De verwachting is dat deze kaart qua prijs tussen de 1660 Super en 3060 komt. Dan heb je het over een budget kaart voor zo'n misschien wel 600 piek.
RTX 3050;5;0.3099854588508606;Als je het opzoekt was die kaart volgens mij wel een stukje duurder dan dat, of je moet echt gemazzeld hebben met een deal. Ik heb me destijds bij de aanschaf ook nog even op het hoofd moeten krabben omdat 500 piek toch wel meer was dan ik gewoon was te betalen, maar de 1060 was net niet sterk genoeg voor wat ik wilde spelen. Inmiddels heb ik een hele innige relatie met mijn 1070 en fluister ik hem elke dag lieve woordjes in zodat hij toch maar vooral nog een tijdje mag meegaan...
RTX 3050;5;0.47674253582954407;ik hoop het voor je met je 1070, ook gehad fijne kaart
RTX 3050;5;0.387296199798584;Ik streel mijn GTX 980 en bedank hem na elke game sessie. Wat een eindbaas.
RTX 3050;3;0.2932983338832855;Mijn GTX 1070 stamt uit 2016 inmiddels, voor die enkele games waar hij tekortkomt op 1440p/60fps (mits enige concessies) is er gelukkig nog Geforce Now.
RTX 3050;3;0.46302688121795654;Het is niet spectaculair, maar volgens mij trek je de vergelijking wel wat scheef: 400 voor een 1070 klinkt vrij goedkoop, dat betaalde ik voor m'n 1060 (ok, ik woonde toen in Malta waar electronica duur is), maar een een xx50 kaart presteert nu beter dan een xx70 model van toen en als we er nog wat inflatie rekenen + het feit dat electronica sowieso al duurder is geworden.. Ik zou zeggen dat je zo'n >50% meer prestatie per € krijgt? Hoedanook, zelfs voor de adviesprijs ben ik hier niet in geinteresseerd, de prestaties liggen gewoonweg te dicht bij mijn 1060, ik wacht nog wel een jaar op de RTX 4050 Ti ofzo.
RTX 3050;1;0.5226427316665649;8 jaar met een videokaart doen is extreem lang, dan stel je ook zeer lage eisen aan gamen of het is 1 blokkendoos kwa beeldkwaliteit, voor mij is lager dan 60 fps sowieso onacceptabel op 1440p.
RTX 3050;3;0.32365331053733826;Door de tijd heen: GTX1060 - betaald voor 190 Euro via Marktplaats (circa 3 jaar geleden) GTX1070 - betaald voor 150 Euro via vriend (bijna 2 jaar geleden) Nog steeds GTX1070, loopt goed in veel VR games en kan nog meelopen met VR + Quest 2. GTX1060 in 2e game pc. Als je nu kijkt naar de prijzen. Alsjemenou, wat zijn de prijzen behoorlijk gestegen. De resultaten geven aan dat RTX3050 danst rond GTX1660 / GTX1070. Dus als je deze kaarten al hebt, niet upgraden. De RTX effecten zijn leuk, maar je kan nog steeds prima zonder leven, want de games kunnen met tricks mooiere beelden maken, al jaren voordat RTX op markt komt. Mijn volgende upgrade wordt RTX3060 of RTX3060Ti. Omdat het veel zinvoller is. Je moet grote stappen maken in GPU aankopen, niet kleine stappen. Zeker niet met hoge GPU prijzen tegenwoordig. Wat je dus al hebt, maak niet te kleine stap met RTX3050, ook al wil je even RTX zien. Dat is niet de moeite waard en ben je iets minder future voorbereid.
RTX 3050;5;0.4720205068588257;128 watt op een 50-series kaart
RTX 3050;3;0.4429829716682434;Lijkt me toch niet zo gek als je kijkt naar hoe de kaart zich plaats tussen de rest op vlak van prestaties?
RTX 3050;3;0.43957823514938354;je zou verwachten dat het generatie op generatie efficiënter zou worden, wat ook gebeurde vanaf de 600 series tot en met de 1000 series, sindsdien neemt het enkel toe.
RTX 3050;2;0.4749443531036377;Ah die trend kende ik niet . Misschien zijn ze op een limiet gekomen of vinden ze energie-efficiëntie (helaas dan) geen prioriteit.
RTX 3050;5;0.23634028434753418;Samsung's 8nm is qua efficiëntie gewoon een ramp, daarnaast is dit ook wat competitie doet met een markt, Nvidia moet z'n kaarten nu veel verder pushen om met AMD te concurreren
RTX 3050;1;0.533802330493927;Dat Samsung zijn 8nm procede een ramp is, is echt een hele grote aanname welke nooit ergens bevestigd is. Ze verbruiken maar mild meer dan de Radeon tegenhanger en dat vaker met betere resultaten op hogere resoluties en veruit betere resultaten met raytracing enabled.
RTX 3050;3;0.2651253640651703;Deze video gaat er op in Dit artikel laat ook de problemen met Samsung nodes zien. Wat ook blijkt aan het zwaar oververhitten van de Xiaomi 12, die de Samsung-procede variant van de Qualcomm 8 gen 1 heeft.
RTX 3050;1;0.6346815824508667;Adored is niks meer dan assumption TV x rumors aka Moore's law is dead. Andere chips, andere procede. UIteindelijk nu in de praktijk verbruikt een Geforce 3080RTX bijvoorbeeld maar rond de 20 watt meer dan de Radeon 6800XT en de prestaties liggen super dicht bij elkaar, op lage resoluties voordeel van de Radeon, hgoere in het voordeel van de 3080RTX. Naast de 3080RTX heeft dan al die RTX cores extra etc. Is het dan echt zo inefficient. De praktijk zegt van niet.
RTX 3050;3;0.29290685057640076;Bijna elk gangbaar product heeft energie labels in Europa. Het wordt tijd dat ze die ook op video kaarten gaan plakken. Naar mijn mening is 300 watt verbruik voor een videokaart niet gering. Deze kaart dus niet zoveel, maar je begrijpt wat ik bedoel.
RTX 3050;3;0.3819458782672882;Ze verbruiken 2x zo veel als een 1050ti ja. Echter ontvang je hiervoor wel zeker 3x het aantal frames van de 1050ti op 1080p. Wat mij betreft is dit DE kaart voor een budget systeem deze tijd (mits je een nieuwe kaart wil natuurlijk en voor zover budget nog iets is in deze tijden.) Meeste games haal je prima fps op 1080, en in dit artikel is er niks over DLSS gezegd, wat alleen maar voor nog meer frames zorgt.
RTX 3050;2;0.4030552804470062;Door dat vermogen zal het niet in een single slot format passen, de 1050 Ti en ik meen de 1650 zijn wel single slot uitgebracht. Misschien een niche markt maar het is voor VM of decoder/encoder toepassingen handig in een NAS en daar past bij mij alleen een single slot en beperkte lengte in.
RTX 3050;3;0.583044707775116;Dat is dan wel weer waar inderdaad, daar had ik niet aan gedacht. Anderzijds, die performance in een nas is misschien ook wel wat overkill
RTX 3050;2;0.40572357177734375;Wellicht overkill inderdaad, maar het kan dus dan moet ik het in iedergeval proberen maar niet voor de huidige prijzen, de 1050 ti single slot die ik nu heb koste me iets 100 euro nieuw in doos (wel op marktplaats), dat gaat Me never nooit meer lukken met iets nieuwers Een nieuwe NAS zou ik nu ook niet moeten kopen, die zijn ook 1,5 tot 2x zo duur geworden.. Ik heb ook een 10gbit ethernet kaart in de NAS zitten, heb ik het nodig? Nee, maar de backup naar de server is wel lekker snel klaar
RTX 3050;3;0.5725820064544678;Voor de hobby is het inderdaad wel leuk spul. Zeker als je er regelmatig vm's op draait met een gui. Die kan je dan net wat meer gpu power toewijzen.
RTX 3050;2;0.35871556401252747;Ja, ik had ook gehoopt op een vervanging van mijn GTX 1050ti zonder extra PCIe power connector. Bij de eerder uitgelekte specs had ik al mijn hoop opgegeven maar helaas nu helemaal bevestigd.
RTX 3050;2;0.5769302845001221;"Het probleem is dat APU's langzaam maar zeker de markt voor budget videokaarten aan het inhalen is. Er is al jaren geen GT kaarten uitgekomen, GT1030 was de laatste low-end videokaarten series, en ""alsermaarbeelduitkomt""-end waren het de GT710 en GT720 van 10 jaar geleden Ik veel benchmarks staat de Ryzen 5700G staat al redelijk gelijk met de 1050 non-TI. Ik denk dat het voor Nvidia niet meer intressant is om nog een GT3030 uit te brengen. Wat heel jammer is gezien er nog zat mensen zijn die op oudere hardware spelen/iets beters willen dan de 5700G/met een OEM PSU opgescheept zitten."
RTX 3050;4;0.35496821999549866;En de evolutie met de 6000 series monolith laptop's CPU's met RDNA2, zal dat nog meer pushen. Twee maal betere performance, en je zit al in lage 1060 gebied. Note: De 5600G /5700G zijn de 5000 series monolith laptop CPU's met een hogere clock / stroomverbuik. Voeg er nog een bij, dat AMD ook iGPU's zal hebben in de Zen 4 CPu's ( waarschijnlijk, net zoals Intel deed met 32 core vs 96 in de laptop, zal de Zen 4 mogelijk 4? core RDNA 2 zijn, ipv 12 in de laptop ). Gevolg is dat er nog minder vraag is voor lagere modelen van GPU's.
RTX 3050;1;0.45334333181381226;Ik hoop zo erg dat APUs sterk genoeg worden dat je geen GPU meer nodig hebt voor 1080p gaming. Het is echt te idioot wat je voor een 1080p pc betaalt om te bouwen zonder het minste van het minste te nemen. APUs worden steeds sterker, maar dit niveau hebben ze nog niet bereikt naar mijn idee.
RTX 3050;3;0.4059183895587921;Daarom is het jammer dat bv de Xbox Series X (of zelfs de S) niet ook normaal windows kan draaien. Wel vraag je je dan af waarom het niet mogelijk is om kleine PC's te maken die net zo krachtig zijn voor maar zo'n 200 euro meer.
RTX 3050;2;0.43408191204071045;Het punt is dat ik veel voor cuda programmeer en ik vaak alleen maar een paar simpele kaart(en) nodig heb om te testen binnen dezelfde gpu generatie. Oudere kaarten zijn ook haast niet meer te krijgen.
RTX 3050;3;0.5802059173583984;Aan de andere kant: dat is slechts 8W meer dan de RX6500XT, terwijl de RTX3050 op een beetje leuke instellingen zomaar dubbel zo snel kan zijn. Dit ligt natuurlijk vooral aan dat de RX6500XT zo'n ****-kaart is, maar toch fijn dat er ook een capabele kaart is.
RTX 3050;3;0.5704939961433411;Inderdaad niet super efficiënt, voordeel van hogere power budget is wel meer ruimte voor overclock. Plus je kunt altijd undervolten natuurlijk. Maar dat neemt de power connector idd niet weg.
RTX 3050;4;0.428629994392395;Het is toch alleszins een betere keuze dan de RX6500XT die vorige week door het ander kamp is gelanceerd. Voor de gamer die op 1080p ultra niet meer dan 30fps verwacht is dit blijkbaar de juiste kaart.
RTX 3050;4;0.365375280380249;Dat is geen hele hoge drempel om over te springen -- bijna alles is beter dan een RX 6500 XT, op een iGPU na dan. De enige reden dat AMD een laptopchip heeft opgepoetst en als standalone kaart in de markt gezet heeft is omdat het in het huidige klimaat toch wel verkoopt voor een prijs waar ze toch nog wat winst op maken.
RTX 3050;2;0.4074885845184326;AMD had dezelfde componenten in een duurdere videokaart kunnen stoppen en er meer winst mee kunnen maken. De reden dat AMD deze kaart waarschijnlijk op de markt heeft gebracht is om het marktaandeel te laten groeien en pre built klanten te kunnen bevoorraden. Verder is de prijs bij Nvidia tussen de 25% en 50% virtueler dan bij AMD. Zoals in het artikel al wordt gesteld zal deze waarschijnlijk boven de 1660 super van 500 euro uitkomen. Waar de 6500xt voor 370 wordt aangeboden. Deze kaart kan je dus waarschijnlijk beter met de 6600 vergelijken.
RTX 3050;2;0.5231490731239319;En daar legt de 3050 het dan tegen af qua prestaties.
RTX 3050;3;0.48587411642074585;Sowieso, maar het zijn geen kaarten in hetzelfde segment. Het is een trieste conclusie van de huidige markt, maar de prijs zal alles bepalen
RTX 3050;4;0.445976585149765;Bijzonder dat er niet ook getest is met DLSS? Dat is juist een mooie feature voor deze kaart vooral vergeleken met bijvoorbeeld 1660 ti. Daardoor kun je bij ondersteunende games toch met wat hogere settings/resolutie spelen en een redelijke framerate houden en in sommige gevallen raytracing zelfs speelbaar wordt.
RTX 3050;5;0.28399980068206787;Als het niet getest is met DLSS (heb het artikel alleen nog maar even snel gescand) dan heb je een goed punt. DLSS is juist één van de product sellers van deze 3k serie, dit zou een 3050 veel interessanter maken op papier dan bijvoorbeeld een 2k serie kaart.
RTX 3050;3;0.3645796477794647;DLSS werkt toch ook op de RTX 2000 serie? Het is dan wel weer een product seller vergeleken met AMD.
RTX 3050;1;0.36754241585731506;de 2k serie heeft ook DLSS. De 1600 series heeft het niet.
RTX 3050;3;0.3763413429260254;ik ben blij met mijn GTX 1660 ti 6Gb die 4% beter presteert dan de 1660 super en 4% minder dan gtx 1070, en raytracing vind ik persoonlijk niet zo belangrijk, en in mijn mac pro 5,1 zit een RX 580 8gb special edition van Sapphire, jazeker de blauwe. De huidige prijzen zijn buiten proportie en zolang dat zo is koop ik geen nieuwe laat staan 2e hands videokaart. De 3050 met raytracing haalt geen framerate waar mijn hart sneller van gaat kloppen, lijkt wel meer een marketing verkoop verhaaltje, als je daar geld in wilt steken dan wil je denk ik ook playable framerate, dus minimaal een 3070 of hoger.
RTX 3050;5;0.4872416853904724;Het ligt er inderdaad helemaal aan hoe je het bekijkt. Toen ik een GTX1660Ti kocht was ik verontwaardigd dat ik meer dan 250 euro kwijt was voor een x60 kaartje. Inmiddels zijn we drie jaar verder, en released Nvidia een kaart die: - 1 á 2 fps meer haalt in de prestatiescore over alle games heen - 4 á 5 watt minder verbruikt - een heel stuk meer gaat kosten Top, die vooruitgang
RTX 3050;3;0.40875208377838135;Prijzen zullen bepalen of het een aankoop gaat worden hier. De 1050ti in mijn gaming VM begint net wat te kort te komen maar met de huidige prijzen was ik niet van plan deze te vervangen. Voor een dikke 300 zou dit een optie zijn maar ik vermoed dat de prijzen rond de 500 worden
RTX 3050;3;0.2647920846939087;Ik ga voor 550
RTX 3050;2;0.40704235434532166;Wellicht, daar kocht ik bijna mijn 2070 Super voor (589) 2 jaar geleden. Dus mooi niet dat ik dat ooit ga betalen voor zo'n magere kaart. Dan maar een tijd wachten. Kopers zijn er altijd wel helaas dus de prijzen zullen niet op normaal niveau liggen voorlopig.
RTX 3050;5;0.5924840569496155;Dat is ook nog een prima kaart, de 2060 Super die ik had presteerde al bijna zo goed als de 3060. De 3060 Ti die ik nu heb bijna zo goed als de 3070 die ik had, vanaf de Ti en 3080 ga je pas echt knallen.
RTX 3050;2;0.5143179893493652;Mijn hoofd PC moet ook naar een 3080ti (van de 2070S nu) maar de gezin pc met de 1050ti doet wat minder pijn om te upgraden. Ideaal gezien zou ik een 3080ti kopen en de 2070S doorschuiven, maar die prijzen zijn belachelijk nu.
RTX 3050;1;0.4879287779331207;Adviesprijs van de 3080 Ti FE is net verhoogd naar 1269 euro, dus lager dan 1300 worden de AIB's niet, of alles moet instorten. Bol heeft wel een 3070 Ti voor 1090
RTX 3050;5;0.36199328303337097;Laten we het hopen
RTX 3050;3;0.42844077944755554;Dit is toch wel een deftige 1080p kaart, ALS de prijs niet hoger is dan 500€. Ander kan je al overstapppen naar een RX 6600 (XT) want die zitten rond de 600
RTX 3050;2;0.33862796425819397;Sowieso vind ik 500 een idiote prijs voor een kaart die wordt vergeleken met de 1660ti. Die kostte in 2019 250 euro. Maar goed, tegenwoordig kost een tweedehandse ook 400-500 euro, en dat is nog goedkoop vergeleken met de afgelopen maanden. De 3050 kan zomaar 500-600 worden.
RTX 3050;1;0.5689395666122437;Welke 1660ti kostte er minder dan €300 destijds? Alle prijzen die in de pricewatch zie zitten minimaal rond de 300 of 325
RTX 3050;3;0.3175986111164093;In de Pricewatch zitten ook bijna geen 1660ti kaarten meer. Deze kaart is wel 275 geweest in 2019, iets duurder dan ik zei: pricewatch: Palit GeForce GTX 1660 Ti StormX
RTX 3050;5;0.34941938519477844;Rx 6600: 550. Zelfs voor 500 te krijgen bij de oosterburen. Stuk sneller ook.
RTX 3050;2;0.46416449546813965;Zo jammer, 2 jaar geleden een nieuwe pc gebouwd. Behalve de videokaart een 1050 ti, want ik wou de opvolger kopen van de amd 5700xt op dat moment. Die kon je toen krijgen voor 400 euro nieuwe.. en het geld om een voor een videokaart + 600 euro te betalen vind ik toch te duur. Ik hoop echt dat er binnenkort kaarten komen voor 1440p gaming die rond de 400 euro kosten.. want die 1050 ti is leuk tot dat je een nieuwe generatie spel wil spelen op normale settings..
RTX 3050;4;0.3714901804924011;Zelfde verhaal, ik dacht eerst een nieuwe PC videokaart komt wel als de 3000 serie uitkomt Hoewel ik RDR2 en Horizon Zero Dawn er nog mooi uit vond zien en speelbaar waren begin ik nu bij Cyberpunk wel heel erg de leeftijd te merken... (zit met een 1060) Mijn hoop is weer op het einde van het jaar gevestigd, als alles een beetje meezit ga ik dan wel gewoon direct voor de 4070.
RTX 3050;2;0.3429311513900757;Hier precies andersom, oude game PC nog even intact gehouden en er een 5600xt bij geprikt voor €300 om later de rest te upgraden. Als ik zie dat een minder presterende kaart nu anderhalf jaar later voor anderhalf keer de prijs uitkomt... tja.
RTX 3050;2;0.5367448329925537;Jammer van het hoge idle gebruik. Zou anders opzich prima kaart zijn voor een home-server die soms moet transcoderen en soms een game moet streamen maar vaak ook gewoon idle is.
RTX 3050;5;0.3290626108646393;Ik ben benieuwd wanneer deze woekerprijzen over zijn, maar dat is heel moeilijk te zeggen. Linus Sebastian heeft kort geleden dit helemaal uitgezocht en kwam tot de conclusie dat dit misschien nog wel jaren kan duren omdat het meerdere oorzaken heeft. (chiptekort, mining, hoarding, etc.) Ik verkoop momenteel veel RX 550 4GB videokaarten aan jeugdige klanten met een krappe beurs en ook aan weinig veeleisende gamers. Deze kaarten zijn rond de 145 euro te koop via o.a. ali-express en Kllisre is een goed merk voor wie interesse heeft en hebben 10 daagse levering. Ze spelen vrijwel alle huidige games nog op lage settings en voor specifieke gameplay en fps is youtube je beste vriend.
RTX 3050;3;0.44050338864326477;Heeft deze wel video encoding? in tegenstelling tot RX 6500 XT die dat totaal ontbreekt toch wel pluspunt voor meesten gezien Twitch/Youtube
RTX 3050;2;0.4308209717273712;Hoezo brengt Nvidia theoretisch een betaalbare videokaart uit met deze RTX3050? Alle kaarten zijn theoretisch betaalbaar, maar door de tekorten zijn de prijzen zo hoog. Nvidia heeft zelf trouwens ook al weer de prijs van de FE kaarten verhoogt. Gelukkig zie je de laatste maand dat de prijzen weer wat dalen en de voorraden lopen gestaagd op, maar de webwinkels willen de prijzen nog niet verlagen. Hetzelfde geldt voor de scalpers en aanbieders hier op Tweakers, iedereen wil nog steeds graag de hoofprijs voor een GPU. De nieuwe serie van Nvidia is ook hier op Tweakers goed te verkrijgen via het vraag/aanbod, in totaal van de 30xx serie staan er al meer dan 100 advertenties alleen hier op Tweakers. Waarvan het aanbod 3060 Ti of lager maar rond de 20 is. Ook al blijft het bij mij jeuken om een nieuwe kaart aan te schaffen ik ga niet 2x of zelfs meer de adviesprijs betalen voor een kaart. En al helemaal niet €400,- of meer voor een kaart die eigenlijk minder dan €200,- moet kosten. Wat de 3050 kaart alleen maar interessant maakt is de DLSS om het nog een beetje toekomstbestendig te maken. Maar men had op dit prestatieniveau raytracing net zo goed weg kunnen laten ook i.c.m. DLSS. Edit: Waar is de conclusie dat we twee jaar verder zijn en dat de adviesprijs waarschijnlijk net zo hoog of hoger ligt dan die van een RX 5600 XT met 10% mindere prestatievermogen. Ik vind de review veel te positief, met alleen de kanttekening dat de het afwachten is wat de prijs gaat doen.
RTX 3050;1;0.37693026661872864;Je ziet dat veel tweakers zo'n kaart aankopen voor 800 of 900 euro en hem dan gelijk door zien te verkopen voor 13 1400 euro....geldwolven. En daarnaast wordt je energieleverancier erg blij van deze kaarten.
RTX 3050;1;0.8605445623397827;Holy crap. Deze kaart scoort ongeveer hetzelfde als een 980ti van 7 jaar oud terwijl de prijs ongeveer hetzelfde is als die 980Ti 6 jaar geleden. Het begint toch echt op een scam te lijken ondertussen.
RTX 3050;3;0.332418292760849;It's only a scam when you fall for it
RTX 3050;2;0.4265417158603668;De prijzen zullen nog wel flink omhoog schieten. De 1660ti waarmee de 3050 veelal wordt vergeleken kost momenteel 400-500 euro tweedehands, en dat is dan nog lager dan die in maanden is geweest. Ik reken op 500-600 euro op de 3050.
RTX 3050;5;0.2540614902973175;Dat denk ik ook.
RTX 3050;2;0.5717432498931885;Mweh valt best tegen eigenlijk, presteert nauwelijks beter/soms slechter dan een rtx2060 die inmiddels al 3 jaar oud is nu, alleen interessant als de prijs onder de €250,- is eigenlijk .... Gelukkig is het niet zo dramatisch als de 6500xt.
RTX 3050;3;0.3582082986831665;Mwah, ik hou het voorlopig maar bij mijn RTX 2060.
RTX 3050;3;0.2831740379333496;Instap kaart met mid-range prijs... fml.
RTX 3050;1;0.6777408123016357;128bit? nee thnx, en dan ook vast nog voor belachelijke prijzen zometeen..
RTX 3050;1;0.44485390186309814;Voor mensen die RDR2 willen spelen let op! De AA werkt eigenlijk niet op 1080. Resolution scala lijkt enige oplossing helaas
RTX 3050;3;0.7173106670379639;Is niet echt de upgrade van mijn 1080 kaar waard. Beter komt de 4000 serie uit dan wordt de 3000 serie ook minder aantrekkelijk.
RTX 3050;1;0.3747427761554718;Hopelijk is een 4060 net zo snel als een 3070.
RTX 3050;3;0.5089550018310547;"""Instapkaart met raytracing"" 'Instapkaart met DLSS', zou het moeten zijn. De kaart is te langzaam voor raytracing, maar kan dankbaar van DLSS gebruik maken om mee te komen."
RTX 3050;1;0.3646481931209564;Spijtig dat er geen stream test gedaan is. Ik had wel willen zien of deze (met NVENC) een fatsoenlijke stream kan verzorgen op 1080p.
RTX 3050;2;0.4116649031639099;Zou een goede upgrade zijn voor mijn 1070 ^^ Maar waarschijnlijk niet aan te komen op een normale manier
RTX 3050;1;0.6286503076553345;Waarom komen ze nog met nieuws naar buiten over kaarten naar buiten als er toch niks leverbaar is? echt.. kap er gewoon eens mee.. NVIDIA, SHUT UP WITH YOUR NEWS AND START SELLING DIRECTLY TO CONSUMERS (1 PER ID CARD PER 2 YEARS FOR ALL I CARE!)
RTX 3050;1;0.4988134205341339;Die 1660 Super blijft hier zo nog een heeeele tijd. Alleen maar prestatie verbetering bij RT en DLSS is natuurlijk geen reden om opnieuw geld te betalen voor een videokaart. En ik ga al helemaal niet MEER betalen dan 3 jaar geleden!
RTX 3050;1;0.5896144509315491;Het was een leuk idee 'instapkaart'. Hoe is dit nu een 'instapkaart' als je meer dan €500,- moet aftikken?? Ze zijn gek geworden! Wie krijgt hier nu eigenlijk de meeste marge? Intel? Gigabyte? Of het verkooppunt??
RTX 3050;3;0.4784460961818695;Ok...ook ik ben 1 van de velen die naar een GPU uit zit te kijken voor mijn pc in mijn studio. Heb destijds een simpel systeempje in elkaar geknutseld aangezien ik een iGPU over had maar door omstandigheden heb ik die moeten vervangen aangezien hij om onduidelijke redenen kapot was gegaan..😭😭 Nu had ik mijn hoop gevestigd op de 6500 XT...keek voorheen ook uit naar een 5500 XT maar vrees dat ik die never nooit meer ga kunnen krijgen vooral omdat ik een kaart met 8 Gb Vram wilde hebben. Ik heb niet veel eisen...speel geen loodzware games....enige wat ik wellicht ga willen spelen is Tekken 7 of Apex Legends op 1080p. Mijn systeem - B450 gaming max pro moederbord van Asus. - Voorheen 3400g cpu maar die is 1.5 maand geleden kapot gegaan en heb er nu een 5600g cpu in zitten. - 32 gig ram geheugen. AMD genoot mijn voorkeur vanwege gunstige prijzen maar ja...als je iedereen zijn mening leest over de 6500 XT is het een waardeloze kaart...maar denk dat dat ligt aan wat je allemaal van plan bent om ermee te doen. Wat is nu dus slim om re kiezen..?? 🤔🤔
RTX 3050;1;0.5151334404945374;Vandaag de eerste prijzen gezien. Mijn hemel, wanneer houdt dit een keer op. Advies prijs tussen 260 en 300 euro. Verkoop prijzen tussen 550 en 600 euro. Echt
RTX 3050;1;0.3411203622817993;549 euro maar liefst!! pricewatch: Gigabyte GeForce RTX 3050 GAMING OC 8G Daar word ik niet vrolijk van.
RTX 2080 Ti;2;0.4962785243988037;Ik moet toegeven dat vooral de RTX 2080 sneller is dan ik had verwacht. Ook de 2080 Ti doet het beter dan ik had verwacht. Maar de prijs prestatie verhouding is nog steeds ronduit slecht. Dus in mijn ogen kan je nog steeds beter voor een 1080 Ti gaan of een goedkope RX Vega als je die ergens kan krijgen. 1080 kan ook mits je niet van plan bent met HDR aan de slag te gaan want bij de oudere kaarten werkt volgens mij bepaalde memory compressie niet als er 10bit kleur gebruikt wordt en zakken de prestaties flink in. De 1080 Ti vangt dat deels nog op door zijn 384 bit bus. Tweakers heeft waarschijnlijk getest met een 1080 Ti FE welke toch snel 5-9% trager is als de non refferenced kaarten die de meeste mensen kopen en welke vaak ook nog goedkoper zijn. Als je dan de RTX 2080 er naast zet is de snelheid winst wel heel minimaal. Ik vraag mij ook af hoeveel sneller non refferenced RTX kaarten worden nu nVidia een nieuwe dual fan cooler heeft. Ik denk dat dat verschil aanzienlijk kleiner is. Als je hier kijkt naar de strix OC is dat maar 2%: Dus in mijn ogen moet je als je de review volledig wil maken ook even een aftermarket 1080 Ti kaart mee nemen. En ik wil nog wel even aanhalen dat de claim van 50% winst dus lang niet gehaald wordt. Dat is alleen maar in specifieke cherry picked gevallen in het geval van de 1080 dus 4K (waar die kaart niet voor gemaakt is) HDR (waardoor je een extra zware penalty krijgt door de beperkte geheugen bandbreedte op die kaart) Het is wel mooie hardware maar veel te duur en dus alleen maar nuttig als je perse koste wat het kost meer prestaties wil hebben dan een 1080 Ti.
RTX 2080 Ti;3;0.4414820671081543;"GamersNexus heeft een uitgebreide review waarbij ze de RTX 2080 FE vergelijken met o.a. een 1080 Ti SC van EVGA. Performance is inderdaad gelijk / verschillen zijn verwaarloosbaar tussen te 2 kaarten; Overigens wordt na het lezen van enkele reviews al snel duidelijk dat de 2080 FE eigenlijk eigenlijk een ""custom model"" is. Prima koeler, prima clocks, deze prestaties liggen dan ook veel dichter in de buurt van de custom 2080's, zoals je al aangaf is een Strix 2080 slechts 2% sneller (wel wat koeler en stiller)."
RTX 2080 Ti;3;0.4491429030895233;Inderdaad en dan betaal je dus 200 euro meer voor DLSS en RTX wat je niet altijd kan gebruiken. Niet echt de moeite als je het mij vraagt. Over 2 generaties wordt dat wel interessanter (vooral RTX, DLSS is nu opzich al nuttig maar niet nuttig genoeg om die prijs te rechtvaardigen)
RTX 2080 Ti;2;0.5447322130203247;Lijkt meer aan de manier van testen te liggen, cpu bottleneck. Zie bijvoorbeeld techpowerup bij een game waarbij de gpu het niet trok: Daar zie je 60.2 fps voor de 1080 en 79.2 fps voor de 2080. De 1080 ti zit daar op 70.7 fps en de vega64 haalt slechte 57 fps. De 4k resultaten wijzen ook op grote verschillen in ruwe prestaties. Maar op het moment dat je baseline al 150 fps is (Battlefield 1) komt er een moment dat je tegen de limieten van de cpu dan wel de engine loopt.
RTX 2080 Ti;3;0.4441544711589813;Ik kijk liever naar 1440p of 4K waar je minder last van de CPU hebt. Het verschil is daar +- 5-8% tussen de 1080 Ti en de 2080 maar Techpowerup heeft een 1080 Ti FE!! en geen aftermarket wat een eerlijkere vergelijking is dan kan je er zo 5-9% bij op tellen voor de 1080 Ti en is het gat met de 2080 dicht geslagen.
RTX 2080 Ti;3;0.3097013831138611;Waarom zou je een 1080 Ti custom met een 2080 FE gaan vergelijken? Dat is toch juist appels met peren vergelijken.
RTX 2080 Ti;3;0.46808215975761414;Omdat de meeste mensen een non refferenced gekocht vinden omdat ze die blowers niet goed genoeg vinden. De nieuwe FE hebben wel dual fan's en geen blowers meer. Daarnaast mogen ze hem van mij ook met een non refferenced 2080 vergelijken. Maar heel weinig mensen hebben een 1080 Ti FE die non ref waren een stuk beter. Dus vandaar dat dat een betere vergelijking is. De 2080 FE en 2080 Ti FE zijn in dat opzich wel beter dan de voorgaande versies. Denk dat minder mensen het nu nodig vinden een non refferenced versie te kopen.
RTX 2080 Ti;2;0.33620402216911316;Ja maar daarom testen ze FE tegen FE, dan zou het dus betekenen dat de non reference 2080(ti)'s ook sneller zijn dan de reference (en goedkoper) en dan moeten ze de non reference 2080(ti)'s testen tegen non Ref.... Wat dus per saldo het zelfde oplevert.....
RTX 2080 Ti;2;0.535461962223053;je begrijpt me niet goed. De FE kaarten van de 20x0 generatie hebben een beter PCB, betere cooling en hogere clocks dan de normale 2080 en 2080 Ti kaarten. Dus deze FE kaarten zitten dicht op de 2080 Ti OC kaarten aan qua specs waar die eerst lager zaten zitten de FE nu hoger dan een standaard 2080 / 2080 Ti. Die standaard kaarten hebben een lager TDP en lagere clocks. Voorheen hadden FE kaarten en standaard pascal 1080 Ti kaarten dezelfde clocks en de factory oc kaarten waren sneller deze generatie is dat dus anders en is het gat kleiner. Er komen straks dus stock kaarten van Asus, MSI en GA die langzamer zijn dan de FE kaarten en er komen factory OC kaarten die vergelijkbaar zijn en wellicht ook iets sneller.
RTX 2080 Ti;3;0.4402780830860138;Er komen kaarten die langzamer zijn en kaarten die sneller zijn, er zullen fabrikanten komen die een veel snellere kaart gaan leveren. % sneller dan 20x0 FE zal min of meer gelijkwaardig zijn aan %sneller dan FE 10x0. Het punt is dat een FE - FE vergelijking het beste is zowel in prijs als in features aangezien het de visie van de fabrikant was toen en nu. Een FE van 20x0 testen tegenover een kaart van no FE slaat nergens op. Het is duidelijk dat de kaarten meer cuda cores hebben, duurder zijn maar voor het aantal cuda cores- prestatie verhouding niet heel veel sneller zijn dan pascal. Maar de benchmarks laten zien dat de kaarten wel degelijk aanzienlijk sneller zijn en die 5% meer tav third party1080ti's niks doet, straks blijkt dat de 2080 een prima OC'er is en komen er bedrijven met standaard 10% meer . Verder hebben de 20x0 kaarten technieken die waarschijnlijk door grote bedrijven wel gebruikt gaan worden en dat kaarten nog sneller zullen maken. Met een 1080ti weet je zeker dat dit je max is en je geen raytray zal hebben. RT in BFV is momenteel beperkt tot 1080p maar het is de bedoeling dat de game uiteindelijk gerenderd wordt op 4k maar de raytrace effecten op 1080p of 720p wat mij een prima compromis lijkt. Aangezien de schaduwen en reflecties best iets lagere res mag zijn om nog steeds mooi over te komen. Maar de gameplay footage van RTX on/off (van anderen dan nV) ziet er gewoon indrukwekkend uit. De kaarten hebben simpelweg veel meer rekenkracht dan de specs en benchmarks nu laten vermoeden. Alleen komt dit niet tot zijn recht in simpele fps. Voor mij is dit een prima ontwikkeling, ik hoef geen 200fps een constante fps maar met betere schaduwen en licht effecten zijn voor mij belangrijker en dit is een goede eerste stap van nV. Ze hadden ook gewoon simpwel weg een nieuwe kaart kunnen introduceren met hogere klok minder extra's, je kan het al grootste gewoon ook nooit goed genoeg doen voor sommige.
RTX 2080 Ti;1;0.3870369493961334;"De FE's worden nu gemarket als 'de' kaart die het maximale uit Turing haalt; ze krijgen zelfs een hoger TDP mee. Dus dit is het omgekeerde van wat de voorgaande FE's waren. Er zit nu een fatsoenlijke koeler op en ze hebben meer ruimte voor GPU Boost. De FE/FE vergelijking gaat dus totaal niet op, integendeel, dan ben je jezelf voor de gek aan het houden."
RTX 2080 Ti;2;0.33924224972724915;een discussie is vrij moeizaam als er niet gelezen wordt. precies, nv heeft dit jaar een andere strategie dan bij de 10x0, wie is de fabrikant van deze gpu ' s... nv. dus wiens product en visie wil je bekijken die van de fabrikant, niet die van asus, niet die van gigabyte en niet van nog een andere. nv levert dit jaar kaarten die het maximale uit turing halen, ze nemen simpel weg de best gebinde gpu ' s, maar oc ' en de kaarten niet tot max. dat gaan 3den wel doen en kan je zelf ook doen met de fe ' s ( want je weet dat je voor die prijs de besten krijgt ) dit jaar kan je dus het beste een fe halen als je zeker wilt zijn van de beste ( binned ) gpu. ( nv levert ook de software : api / dll ' s voor third parties om oc utilities te maken ) de 3th party 1080 ' s zijn niks anders dan factory oc ' d kaarten met hogere tdp ' s en betere coolers. ik ben geen fanboy maar ik hou van techniek en deze kaart heeft nieuwe features die echt niet alleen maar marketing gebral is. ( ik hoop ook dat de modding community gebruik gaat maken van de nieuwe features - > civ 6 - > tensor cores : met echte intelligentie vs nu verder is misschien ook nog wel handig om te melden dat dice letterlijk een ' aantal ' weken ( weet niet exact hoeveel, maar kort ) de tijd heeft gehad om rtx toe te passen. dus je kan je voorstellen dat een ontwikkelaar met meer tijd iets leuks kan bedenken. zeggen dat rtx / tensor nu niks toevoegt is is correct maar wel een zeer beperkte visie, want het potentieel is er. naast het feit dat de kaart gewoon snel is. en een enkele blik op steam laat zien dat nv heer en meester is in gaming kaarten iets van niet onopgemerkt is bij ontwikkelaars. interessant is ook wat amd gaat doen aangezien zij ( als het goed is ) al ver zijn met navi en waarschijnlijk niet snel antwoord gaan hebben op rtx / tensor of de toch al hoge game prestaties van nv.
RTX 2080 Ti;1;0.8552237749099731;"een heel verhaal dat nul komma nul toevoegt aan die ' discussie ' die volgens jou niet gelezen wordt. je noemt niets dat ik niet al weet of dat er nog niet stond. de discussie is niet ingewikkeld en geklets over ' de visie ' van nvidia... alsjeblieft zeg. turing is een belachelijk slecht geprijsde generatie waarin er qua perf / dollar niets verandert en de fe is slechts een van de vele aspecten waarmee nvidia probeert om dat beeld iets bij te draaien ( met geklets over enorme oc ruimte, terwijl dat vantevoren al vast ligt. pascal overklokt vrij weinig buiten het eigen gpu boost en turing gaat nog een stap verder. de fe is nu gebouwd om out of the box al te doen wat ie kan ). dat lukt ze niet en rtx is tot zover een technologie die op geen enkele van de drie turing releases tot echte meerwaarde leidt. het is te zwaar voor fatsoenlijk 60 fps of het is te licht waardoor je beter gewoon op de oude manier kunt werken omdat er anders van accurate reflecties nog minder overblijft. je zegt ik ben geen fanboy, dan zou het je sieren om een beetje nuchter te blijven en niet zo mee te gaan in het marketing gelul van de fabrikant. want meer dan dat breng je eigenljk niet in. je hebt geen enkele bron aangevoerd die mij overtuigt dat rtx tot iets substantieels gaat leiden. want die bronnen bestaan simpelweg niet... overigens nog zoiets waaruit blijkt dat je er weinig van snapt : denken dat de fe "" s beter gebinned zijn. als dat het geval was, dan kregen ze geen hoger tdp budget mee. binning betekent namelijk dat je met hetzelfde tdp hogere clocks haalt. pascal toonde al aan dat binning vrijwel niets meer oplevert dankzij gpu boost 3. 0, en turing verandert daar niets aan. verder is bij pascal geen enkele aib kaart gezegend met een hoger tdp budget - de bios is gewoon locked en overal hetzelfde. ook dat schijn je nog niet te hebben ontdekt. kortom : tijd om je wat beter te gaan informeren ipv louter ' enthousiast ' te zijn over nieuwe technologie."
RTX 2080 Ti;2;0.40891319513320923;"waar precies ben ik niet nuchter? jij leest alleen wat je wilt lezen en je stuk hierboven gaat niet zozeer over wat ik zeg maar voornamelijk wat jij over nv denkt. ( oooi je kon niet wachten om dat fanboy stukje er tussen te gooien, daar heb je mij te pakken!! ) de rek is uit de gpu scaling ( zie amd ervoor ) ze gaan overstappen naar multi - chip ontwerpen ik verwacht dat dit de laatste van de echte single die gpu ' s zal zijn ( van nv ) dat zie je ook wel en amd zal wel al de aanstaande gen komen met multi chip en interconnect ontwerp ( navi ). om terug te komen op nv, dit zie je ook aan het feit dat ze weer volop inzetten op sli ( nvlink ) functionaliteit. of de benchmarks van gamers nexus. ik zeg dus nergens dat turing zo snel is, ik zeg dat hij 20 % sneller is dan de xp en heeft de extra goodies. voor de zelfde prijs. die goodies vertalen nu nog niet naar fps. fps is het enige wat telt!!! deze kaart heeft een factor 10x meer raytracing kracht dan een 1080ti. "" maar je hebt er nog niks aan "" er zijn meer mensen enthousiast. ( nog zo ' n fanboy ) - - > lees ook de conclusie. grootste fout die nv heeft gemaakt is dat er geen software voor is, maar dit begint het kip - ei verhaal te worden. is dit nog steeds de ' vega ' frustraties? je toont alleen maar aan niks anders te zien dan fps, niks te begrijpen van wat de toekomst gaat brengen ( want nv is marktleider ), niet snapt wat rtx is en hoe belangrijk het is ( ook al is het nog maar een pril begin ). rtx gaat meer toevoegen dan fps of resolutie ooit heeft gedaan en rtx is het keerpunt van fps naar foto realisme in gaming, want zonder raytracing is dat namelijk niet mogelijk. rtx memes heb ik ook goed om kunnen lachen, ik laat het hierbij. blijf lekker de frustraties van je afschrijven, dat houdt je van de straat."
RTX 2080 Ti;2;0.46768325567245483;je doet nog steeds net alsof ray tracing iets nieuws is. het enige verschil is dat het realtime gebeurt. dat is voor content creation een prachtig gegeven en daar ben ik het helemaal mee eens. de nieuwe quadros - prima producten. rt cores in een gaming gpu? complete waanzin - er is nog altijd een goede reden dat we dat lekker vooraf deden en met wat trucjes om het efficient te houden. daar veranderen 10 giga rays nog niets aan, en als je nu al kijkt naar wat voor die space er nodig is om dat te produceren... maar goed we vallen in herhaling - agree to disagree. vega frustraties zijn mij vreemd en fps ( raw performance ) is inderdaad het enige dat telt - dat klopt. de markt bepaalt inderdaad, en tot zover is rtx op een geforce gpu zwaar ondervertegenwoordigd en gezien het prijspunt gaat dat voorlopig niet veranderen. ondertussen heeft amd de consoles in handen en nul komma nul hardwarematige rtrt ondersteuning. en wat is ook alweer de echte drijvende kracht achter vooruitgang in games tegenwoordig? oh ja. consoles want : aantallen, massa, en laagdrempelig. rtx : niche, duur, hoge drempel, geen massa. vergelijk het met vr : dat komt als niche ook zo lekker van de grond. en daarover riep men hier precies hetzelfde : ' dit is de heilige graal van gaming '. nee. echt niet. de heilige graal van gaming is content, inhoud, goede gameplay. de sterkste games aller tijden zijn nog altijd degenen met hoge replay value, met sterke mechanics. dat zijn de system sellers, en dat zijn de zaken die mensen overhalen om ergens in te stappen. en dat produceer je alleen maar met talent - niet met een slim algoritme om lichtstralen te berekenen. dromen mag, dus droom lekker verder maak alleen niet de fout om een kritische blik te verwarren met ' frustraties ', want dan houd je alleen jezelf voor de gek.
RTX 2080 Ti;2;0.4390271306037903;Ben ik het niet mee eens was ik het ook niet met de vergelijking 980 Ti vs 1080. En nu met de nieuwe FE helemaal niet. De meeste mensen gebruiken non refferenced. Dus door die mee te nemen krijg je een realistischer beeld over wat de nieuwe kaarten bieden tov wat mensen hebben. Allemaal leuk en aardig die extra features die nVidia ons wil aansmeren maar het gaat er om wat we er aan hebben. DLSS zie ik wel als iets nuttigs RTX simpelweg nog niet tijd zal het leren. Misschien hebben we nog 1-3 generaties nodig. Dat is opzich niet erg. Maar als je dan kijkt naar de prijs zeg ik nee. En dan zeg ik dan liever een 1080 Ti of goedkope Vega kaart en dan kijken we de volgende generatie wel weer. 980 FE > 1080 FE was 71% verschil 1080 > 2080 is slechts 40% in het geval van de FE en stock versie 35%. Maar ze zijn wel een stuk duurder Ik vind de RTX 20x0 kaarten slechte value for money geven. Als jij dat niet vind prima ik houd je niet tegen om zo'n kaart te kopen.
RTX 2080 Ti;1;0.3102377951145172;Waarschijnlijk doet @Astennu dat omdat de nieuwe reference koeler dual fan heeft en die van de 1080 Ti maar een single fan.
RTX 2080 Ti;4;0.37339863181114197;Ook bij DLSS is het maar de vraag of het van toegevoegde waarde is. Op hogere ppi monitoren is aa voornamelijk bedoeld om 'pixel-crawling' tegen te gaan. Over het algemeen slagen standaard aa technieken daar prima in, vaak volstaat de laagste instelling prima bij hardware matige aa. Ray-tracing is leuk voor professionals. Mooie maar vooral vlotte renders en geloofwaardige animatie/cgi/games. In-game gaat niemand met 40-50fps gamen op 1080p met een peperdure gpu. Van den zotte. Eigenaardige keuze's van Nvidia, zowel qua productie als marketing.
RTX 2080 Ti;3;0.47150564193725586;"Op hoge PPI monitoren is het idd niet nodig. Dit is meer voor 1080p en 1440p maar 27""4K heeft niet echt AA nodig."
RTX 2080 Ti;2;0.2969549894332886;Het rtx deel van de 2080ti komt bij rendersoftware veel beter tot zijn recht. Games profiteren nauwelijks. Unicorn profiteerde optimaal van de nieuwe kaarten. Rendersoftware speelt sneller in op de nieuwe mogelijkheden. Het is een onderdeel dat ondergeschikt is aan de game industrie, maar toch een belangrijk onderdeel
RTX 2080 Ti;3;0.6547603011131287;Maar dat kon opzich ook al redelijk snel op de cuda cores of op de shaders van AMD kaarten. Als je die RT cores en Cuda cores tegelijk kan gebruiken om te RT te renderen is het wel een voordeel bij die software.
RTX 2080 Ti;2;0.600323498249054;Mij valt het tegen. Ik rekende op 35% bij techpowerup en dat is toch aanzienlijk minder. Op 1080p is het te verklaren door cpu bottleneck en de hoge framerates die bij sommige games worden behaald. Maar ook op 4K wordt de 35% relatief niet gehaald (50% onderling). De 2080 FE blijft steken op 31%:
RTX 2080 Ti;3;0.3560175895690918;Je gaat hier wel compleet voorbij aan het Raytracing gebeuren, wat eigenlijk de echte seller feature van deze kaart is. Iets wat geen enkele andere kaart nog kan. Wat dat uiteindelijk gaat doen weten we nog niet. Wat mij betreft is deze review ook gewoon nog niet af. Ik wacht rustig af tot we kunnen zien wat raytracing echt gaat doen.
RTX 2080 Ti;3;0.42620500922203064;Ray-tracing is al uitgebreid gedemonstreerd in bf5. Prachtige reflecties in waterplassen, diffuse reflecties op auto's en vieze ramen. Maar op 1080p met matige framerate. Een paar stappen vooruit, en nog meer stappen terug.
RTX 2080 Ti;3;0.4692477583885193;"Daar heb ik al genoeg over gezegd in het verleden. RTX is nu een gemmick niet meer niet minder. Het is er voor de game Dev's dat die aan de slag kunnen maar de performance hit is nog veel te groot. BF5 draaide 1080p met minder dan 60FPS. Er gaan nu ook al geruchten dat de effencten in de final release flink terug geschroefd worden om het speelbaarder te maken maar daardoor dus ook minder ""mooi"" daarnaast vond ik het ook niet eens altijd mooi. Soms was het echt to much en niet realistisch hoe het ea reflecteerde. Dan vond ik de oude manieren die gebruikt werden voor reflecties mooier. Dus ja ik ga daar met opzet aan voorbij alleen DLSS vind ik echt interessant. RTX zie ik pas over 2-3 generaties nuttig worden."
RTX 2080 Ti;2;0.4611138105392456;"De rant van Linus was wel 'onpoint' over het hele release/RTX debacle; Ik persoonlijk vind de prijzen belachelijk."
RTX 2080 Ti;1;0.4629884362220764;De prijzen zijn inderdaad belachelijk. Als je ziet dat je voor 700 euro vandaag een 1080 Ti kan kopen die 0 a 5% langzaamer is dan een 2080 maar 200 euro minder kost dan is de keuze snel gemaakt... Bij elke vorige generatie kaarten kreeg je bij een nieuwe generatie voor de zelfde aantal euros (veel) meer performance. Nu krijg je ongeveer de zelfde performance voor (veel) meer geld, wtf !?!?!
RTX 2080 Ti;2;0.3374275267124176;Ik vind het nog te vroeg om te oordelen zolang er geen game benchmarks met raytracing en dlss zijn. Wie weet haal je dan een veel hogere fps en/of een veel mooier beeld. En in het verleden viel er ook nog heel veel winst te halen met driver en game optimalisatie. Het is ook maar de vraag of, en hoe lang de kaart de adviesprijs kost.
RTX 2080 Ti;2;0.47733137011528015;Dat is een amd kaart kopen in 2012-2014 omdat je dacht dat dx12/mantle/vulkan de toekomst was. Uiteindelijk maar mondjesmaat games uitgekomen die het werkelijk gebruiken. Raytracing is er nu niet, je beoordeelt een kaart voor wat hij nu kan, niet in de toekomst. Je kan het wel aangeven. Het probleem is dat rtx kaarten vooralsnog niet sterk genoeg zijn om volledig raytracing te gebruiken. Het wordt dus alleen in specifieke gevallem gebruikt (light inval, schaduw) en daar moet speciaal voor geprogrammeerd worden (gameworks 2.0). Mondjesmaat games zullen het ondersteunen. Raytracing is de toekomst. Maar dat was dx12 oorspronkelijk ook. Het is nog te zien hoe ver we komen, hoe snel we gaan. Voor nu, met de normale games, presteren ze buitenmaats. De 1070 was wat sneller dan een stock 980ti en de 1080 ging dat voorbij. Nu is de 2080 even sterk als de 1080ti, maar duurder, de 2070 alvast zwakker EN ook nog eens duurder dan de 1080 waarschijnlijk (men verwacht een prijs van 500€).
RTX 2080 Ti;2;0.4302045702934265;Ok. Maar als niemand die kaart gaat kopen, gaan de game ontwikkelaars hier ook niet mee aan de slag. Want waarom tijd steken in iets wat niemand wil hebben. Dan komt er nooit vooruitgang. Het is altijd zo geweest dat degenen die als eerste met nieuwe technologie aan de gang gaan te veel betalen en soms achteraf iets hebben wat nooit echt gebruikt gaat worden. Maar als iedereen zo dacht en nooit nieuwe technologie wou kopen waren we nooit ze ver gekomen als dat we nu zijn en kon je nog steeds pixels tellen in je game.
RTX 2080 Ti;2;0.49458566308021545;Het zal waarschijnlijk nog wel een tijdje duren voordat raytracing gemeengoed wordt. Net zoals bij de introductie van DirectX 10. Performance viel bij introductie ook behoorlijk tegen dus ik verwacht dat dit niet anders zal zijn bij deze eerste generatie raytracing kaarten. Wat mij betreft sla ik nog wel een generatie over met deze prijzen.
RTX 2080 Ti;2;0.4477587938308716;En wie weet is tegen de tijd dat die echt beschikbaar zijn, liefst zonder kinderziektes, de volgende generatie Nvidia-kaarten aangekondigd. Ik vind het ook te vroeg om te oordelen, maar daarmee dus ook zeker te vroeg om 1200 euro te investeren. Beetje een kip en ei verhaal, natuurlijk, maar voor dit soort bedragen zou ik zelf toch liever even afwachten wat er qua implementatie gebeurd - het is niet alsof het met DX12 nou echt hard ging.
RTX 2080 Ti;5;0.33965247869491577;De kaarten zijn en je kunt ze kopen en gebruiken om te gamen en dus kun je ze ook beoordelen. Dat zijn ook al de reviews die nu uit komen. Stel nu dat de games die de bijzondere technieken gebruiken pas uit komen over 2 jaar, ga je de kaarten dan pas beoordelen of sla je het beoordelen dan over omdat er een nieuwe generatie kaarten uit is gekomen?
RTX 2080 Ti;2;0.4384497404098511;"Op dit moment, op de release en zonder dat er games zijn die er gebruik van maken kan je ze beoordelen als ""veel te duur"". Zeker op de lagere resoluties. Maar de beelden uit nvidia's eigen presentaties waren wel veelbelovend. Als ik op zoek was naar een high end game pc zou ik op zijn minst wachten tot de eerste game met ondersteuning voor deze kaart uitgebracht is. En als ik het goed gelezen heb zou dat dit jaar nog moeten zijn. De 1080 ti is tenslotte ook niet goedkoop maar de meerwaarde van de 2080 zie ik in ieder geval als er voldoende ondersteuning komt voor de nieuwe functies. En dan heb ik het nog niet eens over driver / game optimalisaties."
RTX 2080 Ti;1;0.47751346230506897;Er zijn geen raytracing benchmarks omdat er nog vrijwel geen games zijn die dit ondersteunen. Op dit moment zouden we als consument de hele nieuwe versie van Nvidia moeten boycotten totdat ze of met bizar goede benchmarks komen voor games of met normale prijzen.
RTX 2080 Ti;2;0.5073160529136658;Maar dat is toch ook een beetje kip en ei verhaal. Een game studio kan niets maken voor iets wat nog niet bestaat (hardware raytracing) En nvidia kan geen real life resultaten laten zien zonder dat er games zijn die dit ondersteunen. Al had er natuurlijk wel wat meer samenwerking mogen zijn zo dat er direct tenminste 1 populaire game beschikbaar was. Eindelijk is er een mogelijke vernieuwing op het grafische gebied, in plaats van dat kaarten alleen maar sneller worden. En daar zullen best nodige ontwikkelingskosten aan hangen. Bovendien kost hij maar net zo veel als een telefoon in het hoogste segment, die mensen na 2 jaar inruilen. Ik heb verder niets met nvidia ofzo, maar ik kan het wel zeer waarderen dat er nieuwe technische ontwikkelingen zijn die ook nog eens beschikbaar zijn voor de consument. Het valt me op dat er op een tech site zo veel weerstand is tegen nieuwe technologie en dan voornamelijk om de prijs. Natuurlijk moet de nieuwe technologie zich nog bewijzen en hopelijk kan dat binnenkort met de eerste game die vrijgegeven wordt, maar om te gaan roepen om het product massaal te gaan boycotten vind ik gewoon idioot.
RTX 2080 Ti;3;0.28501376509666443;Kennelijk is je kaart gelocked op 1080p/60fps wanneer je raytracing toepast.
RTX 2080 Ti;1;0.44479191303253174;goh je zou ongeveer 2 jaar geleden voor 700 eu zon kaart gekocht hebben.... wat een prestatie kroon was t toen toch. nu nog geen afschrijving
RTX 2080 Ti;3;0.3026220500469208;Dat laatste stukje tekst beantwoord al julie vragen als je de sbelste wilt hebben dan moet je diep in de buidel tasten dan kom je dus op die 2080 ti uit.Maar daarintegen wat koste die 1080 ti en die titan xo vlak na de launche?Wat je het beste kunt doen is als volgt als je een videokaart wilt aanschaffen en je hebt een Nvidia lager dan de 1080 dan kun je beter voor die 1080 gaan omdat die alles draait. Als je al een 1080 hebt en je wilt sneller dan moet je die 2080 serie kopen. ik perssonlijk zal nooit zo een dure kaart kopen omdat hij gewoonweg te duur is. Maar ben er wel aan een nieuwe toe, ikdraai nu op een 630 gt van eind 2012,en ben dus ook aan vervanging toe.Het zijn keuzes wat je moet maken, maar denk eraan voor die prijs van de 2080 heb je een nieuw computer systeem.
RTX 2080 Ti;4;0.37121495604515076;Als je een beetje gamed, dan ben je inderdaad zeker aan vervanging toe lol.
RTX 2080 Ti;1;0.5099411010742188;Of in Juni dit jaar een 1080Ti OC voor 999,-
RTX 2080 Ti;3;0.5473088622093201;Je vergelijkt hier appels en peren. Beetje als een kale auto vergelijk met een met alle opties en dan zeggen dat die met aalle opties nog trager is ook omdat hij meer gewicht heeft maar wel duurder is. Je moet dus 1080 ti vergelijken die raytracing software matig emuleert van de zelfde kwaliteit en dlss emulatie doet van de zelfde kwaliteit met een 2080. Dan weet ik wel wie er een heel stuk sneller is.
RTX 2080 Ti;2;0.4903729557991028;"Sorry makker, maar als jij de 1080ti een ""kale auto"" noemt dat weet ik niet wat jij een Ferrari zou noemen. En appels met peren vergelijken? Nee dat totaal niet. Het zijn twee videokaarten dat ja, maar dar raytracing gebeuren dat moet je zeker deze generatie, nog met een korrel zou nemen. Want er zijn maar zeer weinig games die hier gebruik van kunnen maken. En waarschijnlijk is de impact op je FPS enorm. Ik bedoel, zoveel mensen zijn er toch ook niet die PhysX en HairWorks gebruiken. Dit komt doordat de standaard methode prima werk en de FPS het niet waard is. Wat wel goed is, is dat NVidia blijft vooruit gaan, wat zeer goed is voor de consumenten wat helaas niet zo goed is zijn de prijzen."
RTX 2080 Ti;1;0.6290396451950073;Ja die prijs is nog tot daaraan toe het zijn idd ook belachelijke prijzen maar aangezien Nvidia een Amerikaans bedrijf is moet je ook nog invoerrechten betalen, tenminste als je die kaart in Amerika zou kopen. Nu weet ik ook nog niet als je bij een importeur koopt die die kaarten met honderden tegelijk in een inkoop doet wat dan de meerprijs is. Buiten dat zitten we nog in Nederland waar je dus best veel belasting moet betalen en dat komt er ook nog eens bij.Plus Nvidia moet nog eens winst maken ,het bedrijf wat die kaarten met de boot of in iedergeval transsportkosten moeten allemaal met de prijs inbegrepen zijn. En dan heb ik het ook nog niet eens over de r&d kosten die met elk product kosten zijn gemaakt in de voorm van dat die ingeneur ook nog betaald moet worden.En daarom zijn die kaarten met de launche zo duur.
RTX 2080 Ti;5;0.36020565032958984;Heel eenvoudig om hierop te reageren, de nieuwe kaarten massaal links laten liggen.
RTX 2080 Ti;2;0.4380050003528595;Ik ben het met je eens dat de prijzen veel te hoog zijn, maar dat zijn de 1080ti sowieso. Nu kan je bij alternate al een 2080 voor 799€. Geef dat een maand of 2 a 3 en je zal zien dat de prijzen behoorlijk zakken, Het is toch ook logisch dat een kaart die nieuwer is en beter presteert en nog een dure extra future heeft(of dit nu wel of niet nuttig is staat daar los van ) dan vind ik het nog wel meevallen. Maar een 2080ti zou eigenlijk niet meer dan een euro of 600 mogen kosten, dat zou schappelijk zijn. Nu maar hopen dat die hele fake mining onzin eens stopt, en dan zullen de prijzen voor de videokaarten weer een beetje normaal worden.
RTX 2080 Ti;1;0.4121157228946686;Daar zit idd niks gelogen bij Geen raytracing, geen dlss, kaarten & drivers op vrijdagmiddag binnen. Gelukkig presteren ze prima op traditionele/rasterized games, maar die prijs...
RTX 2080 Ti;1;0.736368715763092;"Eigenlijk een hele slechte launch en dan ook nog een aantal dagen verzet, wat vreemd is. Er heeft veel tijd gezeten tussen de generaties. En voor een kaart die ze door hun concurrentie positie zo kunnen aanbieden behoort ook wel wat premium aangevuld te worden met games of leuke hardware extra's. Het is wachten op de ""echte"" benchmarks, vergelijk het met nu een 8k tv nschaffen. Zeker goed en nuttig maar ....."
RTX 2080 Ti;2;0.38531413674354553;Nee, dat is puur omdat generationele prijsverhoging 70% bedraagt en de prestaties dat niet reflecteren. ja, feeding the trolls
RTX 2080 Ti;2;0.44463786482810974;Humor? Volgensmij is iemand kleineren om zijn of haar (beweerde) inkomen verre van humor. Je vergelijkt appels met peren. Magic the gathering kaarten zijn altijd al vrij prijzig geweest, volgensmij. Je kan ook moeilijk een kaartspel benchmarken dus intrinsieke prijs is altijd moeilijk vast te knopen met dat soort zaken. Een auto als een Ferrari of een Lamborghini koop je niet alleen omdat deze alleen wat harder kan rijden. Een grafische kaart koop je, lijkt mij althans, om de performance. Er wordt toch nergens gezegd dat je het niet moet kopen? Er wordt alleen gezegd dat de prijsverhouding scheef is. Dat de een geen probleem heeft met de prijs is uiteraard subjectief. Zoals ik al zei, of je het wel/niet wil kopen moet je natuurlijk helemaal zelf weten. Helemaal niks mis mee als je een 20xx series kaart haalt. Het is puur een punt van kritiek dat de kaart relatief duur is. Wat je met die info doet, moet je zelf weten. Beetje raar dat je je comment wijzigd om op onze reacties te reageren, maargoed.
RTX 2080 Ti;1;0.6543413996696472;Een uitspraak van zeer laag niveau in mijn optiek. Praktisch alle reviewers hebben dezelfde conclusie: de prijs/performance verhouding is scheef bij deze kaarten. Je krijgt zo'n 30% meer performance voor 70% meer geld. Het gaat er niet om of iemand het wel/niet kan betalen. Als reviewer wordt aan de consumenten gezegd dat dit product te duur is voor wat het is. Of je het wel/niet koopt is natuurlijk je eigen keuze, maar vanuit een economisch standpunt is het niet een goede koop.
RTX 2080 Ti;1;0.5451894998550415;Dan had NV de omstandigheden moeten bieden om een eerlijke vergelijking te kunnen maken. Ik vind het onbegrijpelijk dat je een kaart op de markt brengt met een technologie, waar je op geen enkele wijze, praktisch gebruik van kan maken. Dan kan je het technologiesites niet kwalijk nemen dat ze reviews schrijven met traditionele benchmarks, want er is niks anders, in ieder geval, niet iets wat makkelijk voor handen ligt. Vervolgens vraag je een belachelijk hoge prijs en moet de consument het maar vertrouwen dat die nieuwe functies het echt waard zijn. Los van het feit of Raytracing echt iets gaat worden natuurlijk, want daar heb ik wel vertrouwen in eigenlijk. NV heeft de plank echt volledig misgeslagen. Ik hoop echt dat AMD deze kans aangrijpt om hun stempel weer op de markt te drukken, want dit soort praktijken zouden gewoon keihard afgestraft moeten worden door marktwerking.
RTX 2080 Ti;1;0.4695446193218231;"""Hee man kan je deze testbench runnnen welke letterlijk niemand meer dan 5x zal gebruiken als benchmark dan kan iedereen zien hoe goed onze 2080ti is. Ps. Alle gehypte features zijn nog in geen enkele game goed ge implementeerd, maar dat is niet erg. Het gaat gamers om benchmarks niet om games toch?"" -jij"
RTX 2080 Ti;2;0.3491159677505493;Vind je het werkelijk vreemd dat mensen niet onder de indruk zijn van een techniek die nog nergens commercieel toegepast is maar als commercieel verkocht wordt? En ja het is vrij obvious dat de NV benchmark een best-case scenario voor hun kaarten is. Dat is vanzelfsprekend onderhand. Hoe het in werkelijke games speelt is nu nog de vraag. De enige toegepaste revolutionaire improvement van de 20xx series is dat hun encoding voor streamen blijkbaar drastisch is verbeterd en GPU streaming nu goed een ding kan worden.
RTX 2080 Ti;2;0.42794641852378845;De prijzen zijn heel logisch, waarom alle kaarten van de 10XX serie €200 goedkoper maken als je de nieuwe kaarten met nieuwe features gewoon duurder in de markt zet. Waarschijnlijk zijn er nog genoeg 10XX kaarten op voorraad die eerst nog verkocht kunnen/moeten worden. Wanneer al deze kaarten eruit zijn zal de prijsdaling ingezet worden. Slimme marketing toch.
RTX 2080 Ti;2;0.36792898178100586;Of het slim en logisch is hangt er vanaf hoeveel consumenten zich willen laten naaien.
RTX 2080 Ti;1;0.4538891613483429;Alles is binnen no-time uitverkocht dus schijnbaar is er genoeg interesse. En iedereen die loopt te zeuren over een te hoge prijs koopt een 2e handse 1080ti waarvan de verkoper weer een nieuwe 2020(ti) koopt.
RTX 2080 Ti;1;0.6258710622787476;Dat iets uitverkocht is zegt niks, hoeveel hadden ze in voorraad 10? 100000000?
RTX 2080 Ti;2;0.3360326290130615;Volgens pricewatch was de introductieprijs van de 1070 ook een stuk hoger dan die van de 970. De 1070 werd ook met een flinke hype geïntroduceerd. Volgens mij is dit een beleid dat met de 10xx is ingezet
RTX 2080 Ti;1;0.5217246413230896;Bedank AMD voor deze prijzen. Zonder concurrentie zullen ze alleen maar stijgen.
RTX 2080 Ti;1;0.6078982949256897;AMD bedanken?? Als er meer mensen een AMD kaart zouden kopen, zou het bedrijf meer armslag krijgen om te zorgen voor een betere concurrentie positie. Maar iedereen veel mensen denken alleen maar aan het 'nu' en krijgen daar NU de rekening voor gepresenteerd (letterlijk). Zo lang veel mensen maar denken... Laat iemand anders maar AMD halen, zal NVIDIA nooit in een positie komen waarbij men prijzen zal verlagen.
RTX 2080 Ti;1;0.6462005972862244;Haha, dit meen je niet. Ga je nu de schuld bij de klant leggen dat een bedrijf niet innovatief genoeg is geweest en op de achtergrond is geraakt? Dit heeft AMD al jaren geleden ingezet. Er kopen geen mensen meer een AMD kaart meer omdat het gewoon vele malen minder is dan de concurrent.
RTX 2080 Ti;3;0.40526214241981506;Maar de kaarten als de rx480/580 zijn toch helemaal niet vele malen minder dan de Nvidia tegenhangers? En wat kan AMD er precies aan doen dat Nvidia al jaren lang 3/4x hun R&D budget heeft? Het is nogal makkelijk om te zeggen dat ze maar meer hebben moeten innoveren. Dit kost namelijk geld.
RTX 2080 Ti;2;0.4730503261089325;"Kaarten die qua prestaties vergelijkbaar zijn, zijn idd niet minder. Dat is wat ""tegenhangers"" inhoudt. Maar de snelste consumentenkaart van AMD (vega64) is duurder dan een vergelijkbare kaart van Nvidia (GTX1080), en op de snelste kaarten van Nvidia (incl 1080Ti) heeft AMD totaal geen antwoord. En dat zeg ik als gelukkig bezitter van een RX570."
RTX 2080 Ti;2;0.47135549783706665;Maar de kaarten zoals de 1060 worden zo'n 5 á 6 keer beter verkocht dan een RX580. De GTX 960 ook zo'n 3x. Het heeft dus niks met specs te maken maar meer met Intel/Nvidia reputatie. Het is inderdaad jammer dat ze niet zo sterk zijn in de high end.
RTX 2080 Ti;3;0.3432397246360779;En met de hoeveelheden die Nvidia op de markt kan brengen na launch. Bij AMD waren er de laatste jaren een paar paper launches, die pas maanden later de markt haalden. Het is niet allemaal toeval dat Nvidia het beter doet, ook al is dat niet altijd terecht. Maar grosso modo heeft Nvidia zijn zaakjes beter voor elkaar qua distributie, drivers, koeling etc. En ja, ik heb naast mijn Nvidia ook een 290x die super fijn draait. Ik heb geen merkentrouw of zo. Maar ik ben gewoon voorzichtig met kiezen voor AMD kaarten. Na de 290 vind ik het allemaal wat minder geworden. Natuurlijk, ik kan undervolten, maar waarom? Ik koop een kaart prak hem erin en ga spelen. Helemaal als ik het voor de kinderen en aangetrouwde familie koop. Geen gezeik, gewoon out of the box spelen.
RTX 2080 Ti;2;0.4468628168106079;En met een lager rd budget wil je ook geen antwoord op die snelste kaarten gezien dat het kleinste taart puntje is van de markt. Beter goed middange concurerrn daar verkoop je immers het meerendeel van je kaarten.
RTX 2080 Ti;1;0.44597965478897095;Toen AMD net Ati overnam, voor 5 miljard, sprong het R&D budget per kwartaal van 278M naar 510M per kwartaal. Dan praten we over 2006>2007. Vervolgens is elk jaar steevast het R&D budget verlaagt tot het diepte punt van 229M in 2015. Ati had zelfstandig dus meer R&D budget dan AMD gecombineerd had in 2015. Dat is nog voor inflatie correctie (!). Dus om antwoord te geven op je vraag. Het was juist AMD die het R&D budget inclusief talent bij Radeon heeft leeggezogen en dit vervolgens 10 jaar lang in stand heeft gehouden waardoor het niet anders kon gebeuren dat er een moment kwam dat Radeon niet meer zou mee komen met Nvidia. Dit is iets waarvoor ik al sinds de eerste daling van het R&D budget in 2008 voor heb gewaarschuwd. Het heeft lang geduurd maar helaas is het probleem werkelijkheid geworden. Het goede nieuws is dat na Zen het R&D budget per kwartaal weer is verhoogd naar 357M per kwartaal. Dat zit nog echter ver onder het record uit 2007. Radeon moet het dus ook dit jaar nog met een fractie van het originele budget doen. Daar is AMD 100% verantwoordelijk voor. Die hebben Ati gekocht.
RTX 2080 Ti;2;0.48827242851257324;Maar had ATI het zelfstandig wel gered? Oprechte vraag hoor, geen idee. Ik heb sterk de indruk dat investeringen nu vooral mogelijk zijn door succes in consoles, wat natuurlijk niet had gekund zonder poolen van kennis over CPUs en GPUs. Als dat zo is, zou het op langere termijn stiekem alsnog een redelijke keuze kunnen zijn geweest, gezien de beschikbare middelen. Volgens mij is AMD in huidige en volgende generatie consoles marktleider, of niet? Dat zou weer een stabiele basis kunnen bieden voor betere investeringen. Ik heb iig goede hoop, maar helaas, gezien de koersen, geen aandelen 😋
RTX 2080 Ti;1;0.4110568165779114;Eens. In 2017 gaf Nvidia ongeveer 1.8 miljard aan R&D. AMD is aan het groeien maar als we H1 van 2017 x2 doen komen we op ongeveer 1.1 miljard. Gelijkaardig als je naar totale omzet kijkt. 9.7 miljard voor nvidia en 5.3 voor AMD Voor de volledigheid, Intel heeft 62.8 miljard binnengehaald en gaf daarvan ongeveer 13 miljard aan R&D uit. Nvidia ontwikkel daarmee nagenoeg enkel GPUs. AMD moet met een ongeveer 15x kleiner budget opboksen tegen Intel en nvidia om zowel concurrerende GPUs als CPUs te maken. In dat opzicht: jammer dat ze op het moment niet de prestatiekroon hebben bij GPUs, maar in de midend concurreren ze goed en lowend levert AMD zelfs iGPUs aan Intel.
RTX 2080 Ti;1;0.467494398355484;Dat is dan ook meer een uitzondering dan een regel. Stel jij maakt een project 2x zo snel af als gepland en jouw werkgever eist dit nu voor al jouw projecten, klinkt dat redelijk denk je? Intel en Nvidia zijn beiden beursgenoteerde bedrijven die zo veel mogelijk winst willen behalen. Met andere woorden, die zullen ook wel nauw letten op hun R&D uitgaven.
RTX 2080 Ti;2;0.449810653924942;Je geeft dus in principe zelf het antwoord. AMD moet zowel concureren in de CPU als in de GPU markt. Op het moment lijkt het erop dat hun efforts vooral in de CPU's worden gestoken. Om ook nog eens high end GPU's te ontwikkelen uit hetzelfde budget is gewoon niet realistisch. Natuurlijk hebben ze kneiterhard gewerkt, goede ontwerpbeslissingen en andere meevallers hebben wellicht ook geholpen. Infinity fabric is immers hun redder in nood geworden. Wellicht heb jij het project zo snel afgerond omdat je 2x zo hard werkte en veel over werkte omdat je naar de trouwerij van je zus moest, weet ik veel. Volgensmij snap je m'n punt wel. Hoe dan ook, je praat wel echt heel erg makkelijk. AMD heeft gewoon een veel kleiner budget. En dat ze toevallig voor een product minder geld dan de concurrentie nodig hadden, dan kun je dat niet zomaar extrapoleren naar de GPU-ontwikkeling.
RTX 2080 Ti;1;0.3053411543369293;intel steekt daarvan ook geld in de ontwikkeling van het productieproces. Dat is een dure aangelegenheid en is exact de reden waarom AMD dat heeft afgestoten.
RTX 2080 Ti;2;0.5217524766921997;4 jaar geleden zou je net andersom gesproken hebben, toen AMD met GPU behoorlijk meekwam in de highend maar qua CPUs het niet zo goed deed. Met Ryzen hebben ze een goede strategie, juiste mensen op het juiste moment op de juiste plaats gezet en schaalt de modulaire setup heel goed van embedded met iGPU (hier is Vega wél zuinig en goed) tot 64(+?)-core multi-socket systemen. Zover ik weet hadden ze hetzelfde voor ogen met Vega: één chip die alles aankan, want geld om aparte chips voor compute en graphics te ontwikkelen is er niet. Daar liep het dan iets minder Wie weet wat er nog aan komt in de GPU divisie. Sowieso is de markt voor aparte kaarten al een relatief klein aandeel.. Intel is al jaren de grootste graphics-verdeler op de markt De high end is hoge kosten, grotere marges, grote risico's. En ik geloof dat AMD ook al jaren het merendeel van de consolemarkt voorziet: PS4, Xbox One (S/X), Wii, Wii U. Allemaal continu inkomen, doch lage marge. Zó slecht doen ze het ook weer niet.
RTX 2080 Ti;2;0.4683462083339691;Op zich klopt je verhaal, maar er is dus te concluderen dat AMD niet dé concurrent kan zijn op CPU en GPU gebied die we nodig hebben. Daar zijn ze simpelweg te klein voor. Als we op beide gebieden goede concurrentie willen zien tegenover Intel en Nvidia dan zal er wat moeten gebeuren.
RTX 2080 Ti;2;0.4946536719799042;Amd hoeft niet op high end mee te doen. In het lage en middensegment wordt nog veel meer verkocht dan dat je denkt.
RTX 2080 Ti;2;0.3329770267009735;In de low en mid range heeft AMD nooit onder gedaan aan Nvidia, en de 290x bijvoorbeeld ging nek aan nek met de 780TI. Maar tja als iedereen intel koopt, dan komt AMD niet aan haar geld voor R&D. Vandaar dat je AMD ook ziet verdwijnen in de super high end sectie.
RTX 2080 Ti;3;0.4237033724784851;Wat betreft GPU's heb je gelijk, CPU's niet echt. AMD heeft, min of meer, een flop gehad met de Bulldozer architecure, en dat heeft hun pijn gedaan. Maar met de Ryzen en Threadripper CPU's, Zen en Zen+, is AMD heel erg concurrerend. Het heeft vergelijkbare prestaties met de Intel line-up voor over het algemeen een iets lagere prijs. En straks met de Zen2 generatie, heeft Intel NIETS om het daar tegen op te nemen zolang de 10nm process niet draait bij hun. Persoonlijk kijk ik daar wel naar uit na zo'n lange tijd.
RTX 2080 Ti;3;0.26383447647094727;Hier een lijstje met de rauwe (zonder optimalisaties) rekenkracht. Hier kun je dus zien dat als er meer mensen een AMD kopen en de developers vervolgens meer voor AMD gaan optimaliseren het best goed komt. 970 = 3,9 Tflops 980 = 4,6 290x = 5,6 980ti = 5,6 480 = 5,8 580 = 6,2 1070 = 6,5 Fury = 7,2 Fury x = 8,6 1080 = 8,9 2080 = 10,1 Vega56 = 10,5 1080ti = 11,3 Vega64 = 12,7 2080ti = 13,4 Vega64lc = 13,7 Tflops
RTX 2080 Ti;2;0.2697007358074188;Rauwe TFlops zijn het probleem niet, het toevoeren van genoeg data om al die TFlops volledig te kunnen benutten is de grote uitdaging. Daarom wordt er zoveel energie gestoken in het verlagen van cache-latencies, verhogen van bandbreedte etc.
RTX 2080 Ti;3;0.33828026056289673;En daar gaan we weer. Weet je zeker dat je er echt niet over wil praten hoe je in elk artikel van een GPU of CPU AMD moet bashen? Altijd maar AMD fanboy dit en AMD fanboy dat.. Sorry pik maar er begint een fanboy in jezelf te groeien . ff ontopic: Dit is een goede innovatieve kaart van Nvidia en ik ben echt benieuwd als er optimaal gebruik van gemaakt kan worden van alle functies in-game. Alleen dan zou ik prijs/performance goed kunnen beoordelen. Zoals het nu lijkt is het allemaal een beetje aan de dure kant
RTX 2080 Ti;2;0.295426607131958;Als de klant kortzichtig is ligt dat aan de klant ja. Beide merken hebben hun eigen voor en nadelen, en er zijn ook zat beweegredenen te verzinnen om voor het ene of andere merk te kiezen. b.v. nu een AMD kopen omdat hij niets slechter is en in de toekomst ook nog een betaalbare kaart te kunnen kopen, of nu Nvidia kopen omdat het het beste merk is en één merk grafische kaarten in de toekomst wel voldoende is
RTX 2080 Ti;3;0.4300881624221802;Vele malen minder is ook overdreven, een vega56 kan aardig mee met een 1070ti, en als je een freesync monitor hebt is de keuze al helemaal makkelijk gemaakt. Vega56 is zelfs sneller in sommige titels. Niet zo overdrijven met je '' AMD is gewoon vele malen minder dan de concurrent''
RTX 2080 Ti;3;0.3039424419403076;Voor de echte leken is de 1070ti veel zuiniger, maar voor iedereen die zich er ook maar een beetje in verdiept, youtube is genoeg, dan weet je als eigenaar van een vega dat je hem moet undervolten en dan mag je dat ''veel'' ook wel weglaten bij meer energieverbruik. Geen fanboy hier, ik heb zelf nota bene een 1070ti, omdat die simpelweg sneller is in overwatch dan de vega56, en ik veel overwatch speel, en freesync voor mij geen toevoeging was op hoge fps.
RTX 2080 Ti;2;0.3277019262313843;Ik ga gewoon amd kaart kopen, ik speel toch op 1980 netzoals veel mensen. Alleen even wachten nog tot de prijzen verder droppen
RTX 2080 Ti;1;0.7135403752326965;De AMD Polaris en Vega kaarten waren juist nergens te verkrijgen… Nvidia heeft zijn omzet verdriedubbeld naar miljarden per kwartaal. AMD zat bij benadering de hele mining hype op dezelfde omzet (de extra omzet kwam van Zen, aldus de CEO). Meer mensen die AMD Radeon kopen was dus juist niet mogelijk...
RTX 2080 Ti;2;0.44302311539649963;Niet helemaal waar, de extra omzet kwam ook van Radeon Instinct in datacentra.
RTX 2080 Ti;1;0.5897849202156067;AMD heeft tijden lang iedere kaart die ze produceerden direct kunnen verkopen aan de miners, hierdoor waren er voor gamers tijden lang geen AMD kaarten voorhanden, terwijl nvidia ruim te krijgen was, pas veel later werden nvidia kaarten ook lastiger leverbaar. Zomaar even AMD kaarten kopen als consument zat er gewoon niet in, want ze waren er simpel weg niet. Ik zou de schuld, als je al van schuld kan spreken, hier niet bij de consument leggen.
RTX 2080 Ti;2;0.4423884451389313;"AMD heeft besloten meer nadruk te leggen op de CPU en APU ontwikkeling, en met Navi op consoles. En dan zou ik een ""inferieur"" product moeten kopen om hun (in mijn ogen) slechte beleid te corrigeren?"
RTX 2080 Ti;1;0.5233912467956543;alsof iedereen enkel een 1080+ koopt..... typische reactie . de hoogste is de beste, dan al de rest ook....
RTX 2080 Ti;3;0.4206194579601288;Dit artikel gaat over de 2080/2080Ti, dus de high-end kaarten, en op dat gebied laat AMD het momenteel afweten. Maar zelfs in het midden-segment heeft nvidia in mijn ogen de betere kaarten, alleen low-end zou ik mogelijk voor AMD gaan, alleen hebben die segmenten voor mij geen enkele relevantie.
RTX 2080 Ti;1;0.4131133556365967;In jouw ogen? Waarom? Dat AMD op dit moment geen reactie heeft op een RTX2080Ti, sure. Maar midend? RX580 en GTX1060 zijn zowel qua prijs als prestaties min of meer nek aan nek, afhankelijk van het merk dat je koopt of de game die je speelt. Er is geen eenduidige winnaar te noemen. Drivers van AMD schijnt dan een argument te zijn, maar die zijn al jaaaaaa-aaaaaren echt geen probleem, terwijl die van nvidia ook echt niet heilig zijn. Verder staat AMD voor open technieken terwijl nvidia met programma's als Gameworks en GPP de concurrentie probeert te benadelen danwel op oneerlijke wijze uit te schakelen.
RTX 2080 Ti;2;0.3944704532623291;Verbruik/koeling van AMD kaarten zijn wat mij betreft echt een enorm issue. Dat zijn dingen die ook de levensduur bepalen.
RTX 2080 Ti;2;0.297041654586792;De RX580 is een opgepompte RX480, klopt. Vega is ook geshipped met een hoog voltage voor een paar MHz meer. Veel extra verbruik voor weinig resultaat, domme keuzes in een strijd om de laatste paar procentjes. Dat kan je dus eventueel nog zelf regelen. Mijn eigen kaart heeft bvb een OC BIOS en een trager BIOS. 10% meer verbruik, 2% sneller. Dus dat kan ik ook niet goedpraten, maar je kan het wel zelf weer goed zetten. Of ze daar korter van meegaan, mwah? Lijkt me dat dit in het verleden meer dan genoeg is gebeurd zonder nadelige gevolgen. Zie ook CPUs
RTX 2080 Ti;5;0.5672320127487183;De selling point van veel AMD kaarten is de bandwidth, al helemaal met de vega reeks die HBM2 gebruikt. Hoe hoger de bandwidth van het geheugen hoe soepeler het presteert op de hoge resoluties. Iets wat je ook steeds weer terug ziet bij AMD over de jaren. Nieuwe APU's, hogere resoluties, het is bijna altijd AMD die daar het meeste uit haalt en nVidia die het laat afweten.
RTX 2080 Ti;2;0.5384247899055481;Ik meen te herinneren dat HBM oorspronkelijk door de geheugenproducenten aangekondigd was met hogere snelheden en dat dit in de praktijk dan ook weer tegenviel toen het eenmaal geproduceerd werd. Ook een tegenvaller
RTX 2080 Ti;1;0.4624675512313843;koeling op NVidia FE kaarten is ook gewoon ruk, en vergeet niet dat een Vega 64 50W meer verbruikt dan de 1080 GTX, mijn 1080 draait in iedergeval standaard op 250W te blazen onder load..... en de nieuwe Turing kaartjes zijn ook niet bepaald zuinig. dat argument vind ik BS en ook niet echt relevant voor gamers, ga jij kijken naar energie verbruik van je PC? Ik in ieder geval niet, nu niet nooit niet. Bespaar liever in dingen die het wel waard zijn (Horizon box van ziggo anyone? ding verbruikte gewoon €150 aan stroom per jaar als ie uit staat). Vind daarnaast het argument wat vele maken dat AMD slechter kaarten uit brengt onzin, ja hun High end segment is slecht op het moment maar in de low en mid end zijn ze gewoon goed. Maar omdat Nvidia hun marketing beter op orde heeft en met de High end de scepter zwaait geloven mensen meteen dat alle Nvidia kaarten beter zijn ongeacht prijs point. Maar we moeten vooral zo doorgaan als schapen Nvidia te blijven kopen, is dadelijk de ENIGE relevante concurent weg en gaat je de 3080TI richting de €2000,-.
RTX 2080 Ti;5;0.25531667470932007;Tja, nvidia weet hoe belangrijk de high-end kaarten zijn voor de reputatie van een merk en profiteert daar van. AMD heeft dat zelf laten liggen. Ik kan niet wachten. Intel kan dan mooi de resten van de GPU tak van AMD overnemen, een deel van het personeel zit er toch al, en AMD kan zich concentreren op hun sterke punt, CPU's maken.
RTX 2080 Ti;1;0.5589650273323059;hoor je jezelf nu? AMD beter in CPU? volgens mij heeft AMD tot de laatste jaren altijd goed kunnen concurreren met Nvidia en omgedraaid. Intel de markt van AMD afpakken? wat heb je gerookt, wil ik namelijk ook..... Intel heeft nog NOOIT een degelijke GPU op de markt kunnen brengen, 20 jaar geleden niet en larrabee ook niet (al was deze wel een goede schop in de richting) Intel komt pas over 2-3 jaar met een discrete GPU op de markt en dan is het nog afwachten waarmee ze komen, vergeet niet dit is hun zoveelste poging om een GPU te ontwerpen. Hoop dat ze iets fatsoenlijks op de markt kunnen brengen (meer concurrentie is altijd welkom), als het doorgaat zoals nu kunnen ze misschien in de low end blijven en hoeven ze geen APU meer van AMD af te nemen nu koduri daar zit. dit soort berichten is weer typisch kort termijn visie (toekomst en verleden) waar de hele economie van naar de knoppe gaat.
RTX 2080 Ti;4;0.35626018047332764;Intel i740 was anders een prima kaart indertijd. Die kon heel goed met de concurrentie meekomen.
RTX 2080 Ti;2;0.43076327443122864;Een release met heleboel bombarie, maar de uiteindelijke performance middelmatig tot slecht bedoel je? Beetje zoals de Turing release
RTX 2080 Ti;1;0.4961811900138855;Dat was een 2D kaart met basis opties. Dat was dan ook het laatste wat Intel uitgebracht heeft omdat de concurrentie Intel stompte op alle vlakken.
RTX 2080 Ti;2;0.3383096754550934;Wellicht eens goed lezen wat ik precies zeg, dat is namelijk niet dat Intel markt van AMD af zal pakken, maar dat Intel de GPU divisie over zou kunnen nemen als deze faalt, juist omdat zij nog niet veel kennis van GPU's in huis hebben. AMD heeft de laatste jaren zich op CPU's gefocust, wat ten koste is gegaan van de ontwikkelingen op GPU gebied, maar onmiskenbaar goede resultaten op CPU gebied heeft opgeleverd. Intel wil wat met GPU's gaan doen, heeft al personeel van AMD in huis gehaald, werkt al met AMD samen op GPU gebied, en heeft een groot budget voor R&D. Overname van de GPU divisie van AMD zou dus juist een sterke concurrent voor nvidia betekenen, en daar hebben we meer aan, aangezien AMD momenteel niet echt de juiste concurrentie weet te bieden op GPU gebied. Maar ik weet het, kortzichtig van me...
RTX 2080 Ti;2;0.5559879541397095;Toen ik echt veel gamede ging ik door meerdere koelers en gingen de kaarten na een paar jaar echt gewoon stuk (Artifacts, crashes, bootloops). Dus ja, hoeveelheid hitte is gewoon een dingetje, los van de irritatie van een stofzuiger op je bureau. Dus nee, energie rekening kijk ik niet naar, maar slijtage aan de kaart is gewoon een merkbaar nadeel. Dus ja, ik kies voor de kaart die het koelste draait. een paar procent frames vind ik minder spannend. Maar dat ligt voor iedereen natuurlijk anders.
RTX 2080 Ti;1;0.38295409083366394;is dit niet het geval bij alle reference kaarten (AMD en Nvidia)? De benchmarks van de turing liegt er ook niet om dat de 2080 en 2080TI ook richting thermal limit gaan
RTX 2080 Ti;5;0.4983155429363251;Helemaal waar! Turing gaat het voor mij ook niet worden. Duidelijk een model waar we gezeik mee gaan krijgen. Generatietje overslaan Nvidia is ook niet heilig hoor, ook wel eens rare modellen tussen
RTX 2080 Ti;2;0.5563963055610657;Omdat ik sinds mijn eerste TNT gebaseerde kaart goede ervaringen heb met nvidia, en mijn uitstapjes naar ATI/AMD niet echt goed zijn bevallen. Juist, er is dus voor mij geen meerwaarde om over te stappen naar AMD. Daarnaast is het verbruik van de 10-serie een sterk punt, en dan kijk ik niet direct naar de besparing op energie-kosten, maar wel naar de lagere eisen aan de voeding. Helaas zijn de huidige 20-serie kaarten helaas weer meer gaan verbruiken.
RTX 2080 Ti;2;0.40521541237831116;De 1060 zit eigenlijk consequent een paar 10tje onder de prijs van een 580. Momenteel is het bij MSI bijvoorbeeld 259 voor de 1060 6 GB en 279 voor de 580 8 GB. Gemiddeld genomen is de RX 580 net even een stukje sneller. Power consumption praten we over 116W vs 239W tijdens gaming, aldus Samengevat is de 1060 goedkoper, zuiniger en dus stiller. De rx580 is duurder, sneller, verbruikt meer en maakt dus meer lawaai. Mantle, True audio? Probleem van AMD is dat het marktaandeel onder gamers erg laag is en dan kun je eigenlijk niet wegkomen met een gesloten standaard.
RTX 2080 Ti;2;0.5212571024894714;Die prijs hangt erg af van welk model je kiest, ik had snel even door de pricewatch gescrolled (filters lijken me niet onredelijk) en zag prijzen voor beiden in een zeer vergelijkbare range. Heb het nog even in een grafiekje proberen gooien en ze overlappen echt wel, voor nvidia zijn er gewoon meer keuzes. En dat marktaandeel... Veel mensen lijken, ook met de CPUs, iets te hebben dat het merk dat ze ooit hebben gekocht werkt en niet verder kijken, ook al is het (afhankelijk van het moment/de stiuatie) duurder.
RTX 2080 Ti;2;0.5560814738273621;Klopt. Het gekke is alleen dat de meeste van die kaarten (570/580) zeer gemakkelijk te undervolten zijn wat ze eigenlijk veel betere kaarten maakt. In sommige gevallen worden de prestaties er ook nog beter door, omdat er geen thermal throttling meer optreedt. Puur en alleen voor de benchmakrs zijn die kaarten zo tot hun maximum geduwd dat ze onzuinig en warm worden. Hetzelfde is met Vega gedaan. Die cores doen het gewoon prima als je ze niet helemaal tot het uiterste duwt, wat goed te zien is in de 2200G processors bijvoorbeeld. Persoonlijk heb ik een 570 gekocht (Gigabyte) die klonk alsof er een helikopter in de kamer stond. Na een uurtje tweaken met voltages en kloksnelheden is de kaart nu overclocked en undervolted. De prestaties zijn nu beter, en de fan hoeft niet hoger dan 2500rpm, je hoort hem amper meer. De gewone consument heeft hier niks aan natuurlijk. Maar vind de keuzes om die kaarten zo te pushen een foute keuze. Het levert 2% meer op in een benchmark, maar alle andere prestaties van de kaart lijden er onder.
RTX 2080 Ti;1;0.6080730557441711;Waarschijnlijk was het gat met Nvidia op papier dan te groot. Vrijwel alle keuzes van AMD rondom Radeon zijn vreemd. De RX480 had een enorm potentie maar is zo goedkoop in de markt gezet dat AMD er uiteindelijk maar zeer weinig van heeft verkocht. Zonde... Dus de suggestie dat er enorme volumes naar miners zijn gegaan kan relatief zeker waar zijn. Ongetwijfeld hebben miners veel kaarten gekocht. Alleen er zijn er gewoon niet veel geproduceerd...Op V&A zie je ze ook niet: De voorspelde exodus van mining kaarten is nooit gekomen en die zal wellicht ook nooit komen. Het zijn momenteel de gamers waar miljarden per kwartaal aan omzet bij wordt gegenereerd.
RTX 2080 Ti;5;0.2990281283855438;Het is CPU die gpu afdeling de afgrond mee in trok. Het is dus ook de CPU omzet van Zen die middelen voor de GPU afdeling kan verhogen dus massaal Ryzen TR kopen. En wat Vega .
RTX 2080 Ti;5;0.4132268726825714;Als AMD eens normale prijzen zou gaan vragen voor hun vega kaarten. Ze kosten (veel) meer dan hun Nvidia gelijken en presteren er net onder. Ga jij lekker AMD voorzien van geld. Vroeger was AMD iets minder goed, maar dan dus ook iets goedkoper. Dat zijn ze al jaren niet meer. Na de HD7970 is het alleen maar berg afwaarts gegaan. Pas met de CPUs nu gaan ze weer de goede kant op met Ryzen.
RTX 2080 Ti;1;0.3657332956790924;Die moet je in Duitsland kopen, in Nederland gooien de winkeliers er te veel op. En hier op de pricewatch staan de goedkope jongens er niet bij. Hier 489, was pas nog 469 dus gaat ook heen en weer.
RTX 2080 Ti;1;0.7271104454994202;Nee amd had zijn roadmap op orde moeten hebben en niet de gok op dx12 en hbr moeten nemen. Hbr is helemaal niet nodig voor foede prestaties nog maar is wel uiterst duur, dx12 is nogsteeds geen gemeengoed. Daarnaast is de gaming laptop markt erg groot en hierbij heeft AMD nu al 5 jaar niks deftigs gedaan. Wat er was, was slecht presterend en zeer energie slurpend. Waarom zouden klanten daarvoor moeten opdraaien. Nvidia gooit de prijzen nu omhoog ja maar het is ook new tech en handels embargos hebben er nu al een effect op. Het is niet alleen alsof het door hun monopolie komt. Laten we hopen dat intel het met hun gpu oplossing in 2020 beter gaat doen.
RTX 2080 Ti;1;0.5635652542114258;Op console gebied heeft AMD zo wat een monopolie, buiten de Switch. 120 miljoen consoles verkocht. Dat moet toch wat opgeleverd hebben? Ze hebben gewoon alle research in de CPU gestoken en de gpu markt laten verslonsen. Dat is niet de schuld van de consument maar van AMD zelf.
RTX 2080 Ti;1;0.5109859704971313;Graag, heb sinds de 6600GT alleen maar AMD kaarten gehad. Moeten die krengen wel te krijgen zijn! Na ruim een jaar lang gepoogd te hebben een AMD kaart voor een redelijke prijs te kunnen kopen en maar blijven wachten tot Vega eindelijk eens kwam was ik het spuugzat! Ben niet te beroerd om een paar tientjes meer te betalen voor een GPU op een build van €1800 maar als AMD al die tijd liever zijn kaarten voor de hoofdprijs aan miners verkoopt moeten ze niet huilen of wijzen naar dezelfde community die ze genaaid hebben. Dus na zo'n 10 generaties en denk 20 AMD kaarten hebben ze zelfs deze fan naar de groene kant gejaagd Voor de liefhebber deze kaarten heb ik na de 6600GT gehad (vraag alstjeblieft de logica en volgorde er niet van....Tweaker, jong, ect ) X800, X800XL (2x), X800XT (2x), X1600XT, X1800XL, X1800XT, HD4650, HD4850, HD6850 (2x), HD7700 GHz, R9 380 en toen was er schaarste. En tussendoor nog onboard HD3450, HD3200, HD3300, HD3000, HD4200 en HD4250 gebruikt Maar nee ik heb geen voorkeur voor AMD graphics hoor
RTX 2080 Ti;2;0.43079981207847595;Dus je moet een AMD kaart kopen (ook al presteert die aanzienlijk minder) om zo ervoor te zorgen dat er geen monopolie ontstaat? Zo werkt de wereld natuurlijk niet... Ik heb de vega gehaald, doet het goed voor wat ik ermee doe (niet gaming gerelateerd) maar voor gaming is hij 'waardeloos' comfort is 0 je moet hem waterkoelen anders word je doodziek van het ding (vega 64 red devil). Ik ga gewoon nV halen want hij schijnt goed te zijn in productiviteits suits (volgens linus) is stiller VEEL sneller en alsnog een lager verbruik. 1200 euro is veel geld, maar ik gebruik mijn PC flink veel, dan is 1200 euro een kleine investering.
RTX 2080 Ti;2;0.30287647247314453;Succes met dag dromen, ik draai alles op 4k, jij waarschijnlijk niet.... Farcry 5 (gekregen bij de kaart) Starcitizen en andere titels die enige moeite kosten gaat hij op volle toeren. Willen fanboys niet horen maar het heeft een reden waarom de kaart niet verkoopt (behalve aan miners). Ik gebruik hem bijna alleen voor CAD. Gamen is een nachtmerrie, welk bewijs wil je hebben? (zeker nep!!!) Sneller dan een 1080... zucht.. ja in een paar titels (forza en doom of paar andere titels die vulcan gebruiken) over het algemeen niet. Een een vega verbruikt 300 watt, een 1080, 180watt is dus cooler en dus stiller en OC'd makkelijker. P.S. niet waarschijnlijk langzamer, feitelijk vastgesteld door 99% van de review sites.
RTX 2080 Ti;3;0.4140252470970154;Als jij die vega hebt, dan moet je ook weten dat hij gewoon op 4K 60 fps kan draaien. Ik moet wel stellen dat ik de games die je noemt niet heb. Maar bij mij zit die vega 64 nitro in een relatief kleine htpc behuizing onder de reciever in het tv meubel. Gamen doen we op 4K en op tv is 60 fps voldoende, is geen monitor. Het klopt dat hij meer stroom verbruikt dan een 1080, maar ik vind het zo overtrokken. de temp is goed, de fanspeed is goed, waar hebben we het over. Een tientje meer stroom per jaar. Ik heb een redelijk sportieve motor in mij auto liggen, die verbruikt zeker 100 euro meer aan benzine per maand, daar klaag ik ook niet over. De real life performance is gewoon prima van die vega, tuurlijk het kan beter en nvidia presteert ook beter, maar dat wil niet zeggen dat die AMD kaart k.u.t. is. Ik ben absoluut geen fanboy, hekel aan.
RTX 2080 Ti;2;0.43541428446769714;60fps 4k.... what ever, welke titels dan, dragon age inq... niet, the witcher3.... niet. farcry 5... niet. bf4 net aan maar dipt constant naar de 30 / 40fps ( multiplayer ). civ5 / 6, skyrim le ( lol ) dit zijn allemaal games van een paar jaar oud behalve farcry 5 die geoptimaliseerd was voor amd. nieuwe games : starcitizen, ( shadow of the ) tombraider, ac odyssey, ( target fps 30 op high ) kan je hem op medium zetten om de 30fps te halen. stroom kosten zijn het probleem niet ( alleen een fan kan dat bedenken, ook al vind ik verspilling schandelijk.. ) het probleem is natuurlijk 0. 0 oc potentieel en standaard een 300watt heater in je kast hebt. ( kijk toevallig even hier, tweakers ook geen real life ervaring? noem mij de aantal games op met 60fps.... . ) de kaart undervolt en underclocked ( vergeet die 30fps dan maar ), ik heb een ryzen 2700 die ik niet meer oc omdat hij anders lekker warme lucht van de gpu binnen krijgt. de top van mij pc kast is een grill na een game sessie. real life performance is k * t op 4k, speel je op lagere res is de vega interessant maar aangezien de 1080 goedkoper is heb ik geen idee waarom jij een vega boven een 1080 zou verkiezen aangezien ze onder de 4k toch standaard hoge fps aan kunnen, koeler zijn, stiller, goedkoper en makkelijker verkrijgbaar zijn = nogmaals een lagere prijs. ik heb mijn vega 64 red devil voor een goede prijs gekocht = vergelijkbaar met een 1080, voor mijn applicaties presteert de vega 64 aanzienlijk beter dan nv. maar voor gaming.. zal ik deze kaart nooit aanraden. ik kan niet eens fatsoenlijk discord gebruiken met maten door die k * t fans. jij speelt overduidelijk geen gpu intensieve games in je kleine kast of je bs ' t over je vega 64... met je echte ervaring...
RTX 2080 Ti;3;0.40774005651474;Dat is zo maar het verschil is nooit zo groot geweest als nu. AMD is gewoon onbestaande in het high end segment en dat is best droevig.
RTX 2080 Ti;4;0.463595986366272;"allereerst, top review weer, wederom lekker uitgebreid, zoals het hoort ( althans naar mijn idee ) alleen nu had ik bij dit stuk hieronder zo mijn bedenkingen, of deze ook gaan kloppen is vraag 2 natuurlijk, maar toch : "" wat we wel kunnen staven, is dat de rtx 2080 in de meeste gevallen nauwelijks sneller is dan gtx 1080 ti, die iets goedkoper is. je kunt er dus voor kiezen om iets meer geld uit te geven en een kaart te kopen met toekomstbestendige features die je al dan niet gaat kunnen gebruiken, of je kunt een oudere generatie kopen die nu nog goed meekomt, maar hardware ontbeert die turing uniek maakt. "" aangezien ik het ook zo zie gebeuren dat er in eerste instantie wel een paar games zullen verschijnen die het zullen gaan ondersteunen ( het realtime ray tracing gebeuren ), een aantal uitgevers hebben immers ookal aangegeven dit zullen te gaan doen, en welke dat zullen zijn, en deze staan dan ook in deze review vermeld. en zoals een stukje uit het gelinkte. pdf bestand uit mijn reactie hierboven ookal aangeeft : it is not the intention of intel, nvidia or other graphics companies, like it is often stated in the last months, that ray tracing will be the replacement of rasterization. for that the speed advantage of rasterization is just too big. however, by the use of hybrid approaches, some computationally intensive effects could be bound into the content pipeline a lot simpler and some others would even been just made possible by that. current real - time effects, which could profit of this, would be ( soft - ) shadow algorithms or real - time global illumination algorithms that use ray - tracing - based photon mapping to determine the indirect light radiation. within this semester thesis for the advanced seminar, the two topics of new ray tracing apis and optimization of an existing gpu ray tracer, called rayglsl will be focused. furthermore, two more responses to intels plans have arrived on the market. nvidias nvirt api for nvidia graphics cards, based on the gpgpu2 tool cuda, and also a hardware solution for ray tracing by the company caustic graphics. both systems claim to be able to display interactive real - time ray tracing results."
RTX 2080 Ti;3;0.49421775341033936;Ik vind dat je daar een goed punt maakt. Niet alleen of het wel de toekomst is of niet, maar hoe lang, naast dat handjevol games, het gaat duren voor die toekomst er is? Dan zijn we wellicht een of twee video kaart generaties verder en zijn deze sowieso, maar de nieuwe misschien ook, een stuk goedkoper.
RTX 2080 Ti;1;0.6165357232093811;True, en als ik hier en daar dan ookal lees dat de goedkopere versies van deze generatie kaarten die nog komen gaan het realtime ray tracing al helemaal niet gaan meekrijgen/ondersteunen, maakt dat game studio's het in eerste instantie dus alleen voor een twee tal kaarten (en daarmee dus ook een relatief kleine groep mensen) mag gaan lopen toevoegen/ondersteunen wanneer zij dit zouden willen. (En dan reken ik de Quadro varianten van deze kaarten dus niet mee) Ikzelf ga dit dan ook eerst eens een tijdje in de gaten lopen houden, voordat ik er mijzelf überhaupt een zal gaan aanschaffen.
RTX 2080 Ti;2;0.4939095973968506;Dat klopt. Het probleem is is dat je een beroep op de toekomst doet. En niemand weet precies hoe mooi die toekomst er uit ziet. Misschien is RTX wel een flop die de meerprijs niet waard is. Misschien wel, we weten het niet. Of misschien is RTX pas volwassen in de volgende generatie. We weten het niet. We kunnen pas oordelen als de toekomst er is. Dan pas heb je ook wat aan de RTX. En misschien is dan de 1080 Ti de minder goede keuze voor de moderne spellen met deze nieuwe eigenschappen. Of misschien nog steeds een goede optie.
RTX 2080 Ti;3;0.36798420548439026;"Mee eens. Die ""toekomstbestendige features"" moet zich nog bewijzen. Deze kaarten zijn in vergelijking met de 10 serie is absoluut meer future-ready (wat je niet hebt kan je uiteindelijk wel gaan missen), maar hoever, hoeveel en hoe snel deze features het naar de markt gaan maken is nog volledig onbekend. Ik vrees zelf dat we eerst een generatie van ""overdone"" raytracing titles gaan krijgen, waarbij raytracing te aanwezig zal zijn. Dat omdat huidige games in zekere zin ook veel met lighting en reflections ""faken"" om de game leuk er uit te laten zien, en je dus wel verschil moet kunnen zien om ""profijt"" te hebben van raytracing. Een beetje zoals overmatig veel motion blur in race games.. visueel geinig maar wel hinderlijk. Tevens moet nog maar blijken dat elke title support gaat krijgen voor deze nieuwe features. Afhankelijk welke games je speelt, bijvoorbeeld simulatie games in VR, kan het nog wel eens makkelijk paar jaar duren voordat die graphic engines worden geüpdatet, als dat al binnen afzienbare tijd gebeurd. In dat geval kan je beter voor fps/$ kiezen of nog even wachten."
RTX 2080 Ti;1;0.6779513359069824;Sorry, maar al dat gezeur over te hoge prijzen zijn compleet de weg kwijt. Simpel: je kan de advies prijs bij introductie totaal niet vergelijken met de huidige prijzen, 1,5 of 2 jaar na de introductie van het vorige top model. De 1080 Ti was ten tijde van de introductie ook bijna 900 euro. Ik heb 14 oktober (half jaar of zo na introductie) bij Cool Blue 889 euro betaald voor een EVGA GeForce GTX 1080 Ti FTW3 Gaming. Bij de introductie was dat apparaat 1000 (!!!!!) euro! Toen hoorde je bijna niemand. Nvidia levert hier compleet nieuwe techniek, niet zomaar een snellere geoptimaliseerde versie van de vorige generatie. Hier zit compleet nieuw ontwikkelde techniek in. Ik vind het redelijk normaal dat als je de eerste wilt zijn, je de hoofdprijs betaalt (kijk naar Intel's snelste, dikste, meeste cores, die is ook onbetaalbaar en onevenredig duur ten opzichte van het modelletje er onder). Wacht een maand of 3 en de prijzen van Asus, MSI en al die andere zakken ook onder de 1000 euro. Lang verhaal kort: stop met zeuren en mekkeren over die prijzen en koop gewoon zo'n ding als je hem wilt hebben
RTX 2080 Ti;4;0.2409479320049286;@willemdemoor Is the Witcher 3 getest met Anti-Aliasing uit of aan op Ultra? Dat schijnt een flinke performance boost te geven wanneer je deze uitzet. Ik speel zelf op een GTX1080 SLI en haal dezelfde resultaten als een 2080 ti op 4k maar dus wel met Anti Aliasing uit.
RTX 2080 Ti;3;0.45901089906692505;AA op 4x We zoeken niet de soepelste prestaties, maar proberen de kaarten tot het uiterste te testen Medium is zonder AA, maar daar kopen we geen 2080 Ti voor
RTX 2080 Ti;3;0.5244374871253967;Uiteraard, maar ik zie wel lagere prestaties met de kaarten die jullie getest hebben en waar dus AA aanstaat. In de witcher kan AA alleen aan- of uitgezet worden. Waar hebben jullie de 4x AA ingesteld? Bijvoorbeeld: de GTX1080ti haalt met AA aan iets van 46fps gemiddeld. Net zoals de GTX1080 die ongeveer 35fps haalt dan. Jullie resultaten zijn dus hoger, dan de meeste tests weergeven. Ik kan het fout hebben hoor maar ben gewoon benieuwd. De AA op hairworks is wel apart in te stellen op 4x maar dat bedoelde ik niet en jij denk ik ook niet?
RTX 2080 Ti;1;0.3515937626361847;Excuus, ik zat idd bij Hairworks AA te kijken, die staat op 4x. De gewone AA staat gewoon op 'On'. Misschien zijn de hogere resultaten door de overgeklokte cpu veroorzaakt? We proberen die bottleneck te vermijden door de 7900X op 4,5GHz te draaien.
RTX 2080 Ti;3;0.5990867018699646;Toch lijkt het er sterk op dat er op de lagere resoluties een cpu bottleneck is. Wat dat betreft verwacht ik dat de verschillen groter zijn wanneer een 8700K op 5+ Ghz wordt gebruikt (en straks een 9700K die mogelijk nog wat hoger clockt).
RTX 2080 Ti;3;0.671722412109375;Jazeker, op lagere reso's is het in veel games de cpu, vandaar 10x 4,5GHz. Maar 8700K @5GHz zou iets hogere scores op 1440p/1080p kunnen geven, iig voor DX11-games.
RTX 2080 Ti;3;0.336239755153656;Vooral interessant daaraan is kijken of de onderlinge verhoudingen bij een snellere cpu veranderen. Laten we eerlijk zijn 4.5 Ghz op een 7800K is niet high-end anno 2018. Helaas is de realiteit daarnaast dat veel games nog altijd voor hun main thread zwaar op 1 core leunen. Ik doelde hierdoor specifiek op grotere verschillen. Dus stel met 4.5 Ghz is het verschil op 1080p 10% dan zou het op 5.0 Ghz beide sores beter uitvallen maar het verschil zou dan ook weleens kunnen stijgen naar 15% (gegeven de veel hogere verschillen op 4K). Techpowerup test ook 'slechts' op 4.8 Ghz dus ik hoop op een hertest zodra de 9700K uit is .
RTX 2080 Ti;1;0.4742199778556824;Een tijd terug was er besloten Hairworks uit de testen te houden, of iig dubbel te testen. Dat is er stiekem weer in geslopen?
RTX 2080 Ti;3;0.33545514941215515;Dat moet ik even met Jelle afstemmen, da's zeg maar 'voor mijn tijd'
RTX 2080 Ti;2;0.3945772051811218;Aan de andere kant ziek ik op 4k eigenlijk geen enkel voordeel meer van AA, tenzij ik naar een stilstaand beeld zit te kijken en dan ook nog eens dicht op het scherm. Dat gebeurt dus eigenlijk nooit en ik zet het standaard uit voor de performance boost. Op een 1080Ti overigens die het dan prima doet op 4k. Ik zie vooralsnog geen enkele reden om deze dure kaarten te kopen. OT: Ik denk dat ik eerder een Vega voor mijn tweede PC aan zal schaffen omdat ik daarop ook compute werkzaamheden doe die double precission floating point gebruiken. Dan moet je bij nVidia meteen naar de dure Tesla kaarten grijpen. (Of een Titan V). Die PC heeft een 1440p scherm, daar is Vega prima voor.
RTX 2080 Ti;3;0.4951038658618927;Wel serieuze performance gain. Ik pak op witcher 3 4k ultra 60 fps steady met mijn 1080ti maar dan hairworks off. Zal zelf niet upgraden maar performance verschil is super!
RTX 2080 Ti;3;0.5073290467262268;Jullie vergeten dat het vooral de verhoging van het aantal cores en het gebruik van GDDR6 geheugen zijn die bijdragen aan de verhoging van de prestaties. 34% sneller met 21% meer cores vind ik niet zo bijzonder en dus relatief.
RTX 2080 Ti;1;0.5094375014305115;Verklaart inderdaad direct het hogere stroomverbruik. Ik wist dat met de 1080TI het tijdperk was aangebroken om 4k 60fps te gamen, juich dus de ontwikkelingen toe! Echter is het zo dat Nvidia zoiets heeft van: succes er mee, AMD komt met niks dus ik verhoog mijn prijzen lekker! Gebrek aan concurrentie is erg jammer te noemen..
RTX 2080 Ti;3;0.43344846367836;~35% meer performance, maar de extra prijs is absurd.
RTX 2080 Ti;1;0.4458369314670563;Ja, ik lees in het artikel alleen 70% duurder. Ik ga voor het gemak uit van de goedkoopste 1080ti, is 699 euro. Dus bijna 1200 euro voor de kaart, kan toch niet waar zijn?
RTX 2080 Ti;2;0.445667564868927;Dat is het dus wel. Een 1080 Ti is een veel betere keuze op dit moment. Prestatie verlies tozv een 2080 (non Ti) is nog geen 10%. Ben overigens wel benieuwd naar de overclockbaarheid van beide kaarten. Edit: overclock resultaat. Custom kaarten met overclock is zo'n 8-9% sneller als de stock kaart. Erg veel potentie is op dat gebied niet te halen dus.
RTX 2080 Ti;2;0.45978015661239624;"Dat is dus 8-9% op de founders edition, die standaard alweer 10% overclocked zijn, dat is dus bijna 20% vanaf de ""stock"" kaart.... Lijkt mij niet een heel slecht resultaat op een nieuwe kaart met een nieuw procedé."
RTX 2080 Ti;5;0.2573181688785553;Ik denk dat je verkeerd om kijkt... de 2000 serie is de founder, dus de stock 2080 = 1080ti qua performance.
RTX 2080 Ti;2;0.4366229176521301;Hoe duur was de 1080TI op release? Als je vanaf die prijs rekent met de performance increase kom je aardig uit. op release was de 1080ti ook 999 en als deze kaart 30% beter presteert zou je dus ook op ongeveer 1200 uitkomen. Ik denk dat deze prijs omhoog gegaan is omdat de huidige kaarten door de mining gekte niet ver genoeg in prijs gedaald zijn en daardoor dus de meer prijs voor de nieuwe kaart op een Hogere prijs komt dan de vorige keer. Was deze verder gedaald met 200+- dan was de nieuwe 2080Ti waarschijnlijk ook 1000 geworden
RTX 2080 Ti;1;0.5217928290367126;Punt één is dan dus dat zowel sinds de release van de GTX1080Ti, en nu met de release van de RTX2080Ti de prijs/prestatie verhouding nul verbeterd zou zijn. Dat is geen positief teken voor ons als consument, en ook nou niet echt een teken van een 'redelijke' prijs. Vervolgens van wat ik zo snel in de Pricewatch heb gechecked is elke GTX1080ti die ik nu nog populair is voor tussen de €800 en €850 ruwweg gereleased, dus heel veel goedkoper dan €999.
RTX 2080 Ti;2;0.4113234281539917;Tuurlijk als consument wil ik ook alles goedkoper hebben, dan bedoel ik niet onredelijk goedkoop. Maar ook daarin tegen gok ik dat de prijzen meer in verhouding zijn als 1080Ti = 2080 en 2080Ti is dan de zo geheten nieuwe top kaart ipv de Titan. het moest in mijn vergelijking inderdaad niet de 1080 Ti zijn maar de Titan X
RTX 2080 Ti;3;0.495127409696579;Maar misschien haalt de kaart straks wel 400% betere fps in een game met raytracing.
RTX 2080 Ti;2;0.4231574535369873;De prijs is omhoog gegaan omdat het grotere chips zijn dus kosten yield bin hoger zijn dan vorige gen. Je krijgt meer chip wat beter op 10nm tot zijn recht zou komen ipv 12nm , nV kan dus zonder concurentie de marge hoog houden.
RTX 2080 Ti;3;0.2697345018386841;pricewatch: Inno3D GeForce RTX 2080 Ti X2 OC Goedkoopste 2080ti die ik in de pricewatch kan vinden is nagenoeg 1200
RTX 2080 Ti;3;0.4434202313423157;Ik las bij Hardwareinfo het volgende M.a.w. als de beschikbaarheid goed is, en er geen te grote hype komt moet dat enigzins haalbaar zijn.
RTX 2080 Ti;1;0.5838048458099365;De 2080Ti wordt volgens de pricewatch anders gewoon vrolijk aangeboden ergens tussen de 1200 en 2200 (!) euro. Geen idee hoe HW,info erbij komt dat MSRP gehanteerd gaat worden, want dat is nooit.
RTX 2080 Ti;3;0.26614540815353394;Doorsnee prijs voor de 1080ti is 780+...
RTX 2080 Ti;1;0.3782038986682892;Ik heb ze al voor € 640 gezien, nu uitverkocht, maar ik sluit niet uit dat we ze nog voor 600 euro gaan zien.
RTX 2080 Ti;2;0.5933809280395508;35% in 2 jaar is niet indrukwekkend eerlijk gezegd. Lijkt me een perfect normale generatie bump.
RTX 2080 Ti;2;0.48864439129829407;ik vind dat men hier echt belachelijk aan het vergelijken is. De verhoudingen zijn ongeveer gelijk met de vorige generaties, ook toen was het verschil ongeveer gelijk. Je moet bv de 1080 vergelijken met de 2080 en dan is verhoudingsgewijs het verschil gelijk aan de 980 vs 1080..
RTX 2080 Ti;1;0.4322708249092102;Dat bedoel ik idd. Een normale evolutie. Maar dat verantwoordt de prijsverhoging niet. Na 2 jaar zou die 2080 eigenlijk maar voor 600-650€ over de toonbank mogen gaan en daarmee de vorige generatie vervangen. Nu vervangen ze pascal niet, maar krijgen we een tier erboven en blijft het in de 0-600€ range stil staan. Enthusiasts gaan kopen, maar mainstream blijft op pascal en minder. Dus de vraag blijft hoever die nieuwe technieken op RTX echt gaan van de grond komen. Zeker bij dat AI gebeuren vraag ik me af of dit gratis is? Moeten developers betalen om dat te gebruiken of niet?
RTX 2080 Ti;3;0.38362884521484375;Maar de 1080 etc was indertijd ook een tier die boven de 900 serie kwam bij de introductie.. Is gewoon precies dezelfde situatie als bij de vorige series.. Enige verschil is dat ze nu gelijk de TI variant uitgebracht hebben. En nee developers hoeven niet te betalen om RTX extentie te gebruiken, dit kan gewoon aangestuurd worden van DirectX en Vulcan, die hebben voor Raytracing inmiddels ook specifieke API's voor.
RTX 2080 Ti;2;0.5068669319152832;Toch niet. De 1080 verving rechtstreeks de 980 en had ook een prijs die in dezelfde buurt zat. uiteraard is die enkele maanden later gestegen door de bitcoinhype. Maar een 980 had dit ook gedaan mocht het een jaar eerder gebeurd zijn. Nu is de 2080 véél duurder dan een 1080 die eigenlijk ook nog boven zijn 'end of life' prijs zit. maw een totaal andere prijsklasse en dus ook een andere markt wat gebruikers betreft. Maar ik verwacht dat ook die prijs zal dalen het komende jaar. Gebeurt dat niet zitten we met een groot gat in de markt in de prijsklasse 400-600€ en zal een concurrent die zeker invullen. Maar we hebben geduld nodig hiervoor.
RTX 2080 Ti;3;0.31763124465942383;Zou de i7 4770k een bottleneck voor de rtx 2080 zijn? Heb nu een gtx 1070..
RTX 2080 Ti;2;0.3617872893810272;Ligt helemaal aan de game. Ik heb een i5 6600 geklokt op 4.8 met een 1080 Ti. Guild Wars 2 op ultra is op 1080p al niet meer te spelen en ik heb een 1440x3440 scherm. CPU is duidelijk de bottleneck in deze game. GPU is 40% load, CPU alle cores 100% capped. Ik moet specifiek een aantal settings naar beneden halen om een paar procent meer GPU load te halen en een gemiddelde 60 fps vast te kunnen houden. CPU load zit dan op 80% over alle cores. Meeste games zijn voor mij geen probleem op hoge res (ik game niet op 1080p, zonde van de 1080 Ti). Ik weet niet precies hoeveel sneller mijn 6600 is op 4.8 ten opzichte van jouw 4770. Ik hoop niet dat je met de 2080 op 1080p gaat gamen haha! Misschien 1080p op 244 hz scherm met g-sync? Dan gaat de CPU verwacht ik wel wat vaker een bottleneck vormen. de 20-serie lijkt wel echt gemaakt voor hogere resoluties.
RTX 2080 Ti;1;0.3870864510536194;Nee denkt ‘t niet. Mooie is dat ‘t een i7 is.
RTX 2080 Ti;3;0.4892288148403168;Dit zijn betere prestaties dan verwacht! Ik had verwacht dat het verschil tussen de 1080Ti en de 2080Ti niet groot zou zijn. Maar gemiddeld 30-35% sneller op 4K resolutie maak het eigenlijk dat de 1080Ti net niet klaar was voor 60+ FPS en de 2080Ti wel.
RTX 2080 Ti;2;0.4613989293575287;Alleen de prijs is jammer. De kaarten zijn 30% sneller maar kosten €1.300,- (!!!!) bij Alternate. De prijs/kwaliteitsverhouding is gewoon bar en bar slecht. Vergelijk dat met de 1080 en de 980 in die tijd, ook 30% sneller maar de 1080 was destijds iets van €800,- De RTX 2080 zou een prima kaartje zijn als de prijs rond de €800 bleef. Maar een belachelijk bedrag van €1300 voor maar 30% winst is gewoon een domme investering.
RTX 2080 Ti;2;0.39280620217323303;De 1080 was qua prijs veel meer in lijn met zijn voorganger. Wat Nvidia nu heeft gedaan is de 2080 even snel én even duur gemaakt als de oude kaart. Bravo. Qua prijs-prestatie schiet je dus niets op
RTX 2080 Ti;2;0.38313624262809753;"Nee hoor. Prijs en prestatie zijn allebei hoger en daarom is de prijs-prestatie verhouding gelijk. gebleven. Mede daarom denk ik ook dat deze kaart toch wel weer goed gaat verkopen. Dat is dan het ""excuus"" om er dat geld voor uit te geven."
RTX 2080 Ti;1;0.6543689966201782;inderdaad, de GTX980 en de opvolgende GTX1080 lagen niet in verhouding qua prestatie en prijs. Vergeet niet dat iedere nieuwe kaart in het verleden gewoon veel sneller was dan zijn voorganger, maar tegen redelijk dezelfde prijs werd verkocht als zijn voorganger. Het is net of iedereen helemaal debiel is geworden... hoe vaak ik niet lees dat de prijsstelling van Nvidia wordt goedgepraat. Het is niet zo dat als een nieuwe generatie GPU twee keer beter presteert de prijs ook maar twee keer hoger moet worden. Het probleem zit h'm in het feit dat er geen concurrentie meer is. Nvidia kan vragen wat ze willen. Maar mijn inziens is dit een doodsteek voor de PC game industrie. Ik heb lang gevolgd, maar ik ben bang dat ik ook een keer ga afhaken. dan maar een console, zwaar tegen m'n principes in, maar met deze prijzen is het niet meer te doen. De PC game markt staat al flink onder druk door grote concurrentie van de consoles. Dan helpt heel dure hardware daar niet meer bij. Ik heb veel vrienden zien overstappen naar een Xbox of playstation vanwege deze reden.
RTX 2080 Ti;3;0.43951621651649475;Dat kan natuurlijk. Ik zeg op mijn beurt - als ik een console zou kopen, dan ga ik er qua resolutie op achteruit (PC draait op 3440x1440). Mijn R9 Fury draait de meeste games nog best lekker, misschien niet met alle instellingen op Ultra, maar met een beetje schuiven komt hij nog prima mee. Shadow of the Tomb Raider bijvoorbeeld draait prima. Dan maar geen 4K @ 60 fps met alle instellingen op Ludicrous, maar die haal je op de PS4Pro of de XB1X ook niet.
RTX 2080 Ti;2;0.45506736636161804;Eens, ik heb een tripple monitor opstelling op een single RX480 (patch naar geknepen RX580)... En daar kan ik vooralsnog wel aardig mee uit de voeten zolang ik genoegen neem met minder prestaties. En dat doe ik ook, vooralsnog, het houd wel een keer op. De Beta van BF 5 was praktisch onspeelbaar en ik mag hopen dat dit nog opstart problemen zijn, maar ik vrees van niet. Ik zou graag upgraden naar iets snellers, maar niet tegen elke prijs. Punt is dat Nvidia de prijs zo kan opdrijven omdat er niets in de markt is dat net zo goed of beter presteert, een AMD alternatief in de vorm van een Vega zou ik overwegen, maar die zijn ook schreeuwend duur.
RTX 2080 Ti;1;0.5063698887825012;"Vreemd.... BFV draaide op mijn RX480 zonder problemen dus snap ik je ""praktisch onspeelbaar"" opmerking niet. Ik heb ook een triple monitor opstelling maar ik ga er vanuit dat je op 1 monitor aan het spelen ben en niet in 5760x1080. Zo ja, dan denk dat het ""onspeelbare"" aan je CPU lag en niet aan je GPU!"
RTX 2080 Ti;4;0.29821285605430603;Ik heb geen idee, het speelde wel op de volledige resolutie, dus 5760x1080. Laten we de final game afwachten voor ik verder commentaar geef, want eerlijk gezegd weet ik ook niet zeker of het aan de GPU lag. mn pc is iig voorzien van 16GB ddr4 op een 6600K (@4.4Ghz) en volledig SSD ingericht. vrienden hadden minder problemen met minder specs, dus wie weet was het een ander probleem. thanks iig voor je reactie.
RTX 2080 Ti;2;0.36922377347946167;Tja, op het moment dat je als developer een spel maakt dat enkel op bleeding-edge kaarten wil draaien, dan durf ik te stellen dat het niet de GPU-makers zijn die de PC-gaming markt om zeep helpen, dan zijn het de PC games zélf die dat doen. En dan nog - de XB1 en de PS4, daar zal BF5 ook op verschijnen. Die hebben geen 1080 (Ti or otherwise) aan boord. Als het dan op de PC wél nodig is om een 1080 te gebruiken, dan is er iets fundamenteel mis met de PC-versie, IMHO. Een RX480 is een redelijk recente kaart, en zeker geen lichtgewicht. That being said, hoewel ik denk dat AMD een goed punt heeft in dat ze de top-end op de PC links laten liggen (dat begint steeds meer een niche-markt te worden, heb ik het idee) en meer mikken op de midrange en IGP's, waar veel meer marktaandeel zit, is het wel zo dat in de top-end dan nVidia zich van de lelijkste kant kan laten zien, want die laten nu pas echt zien wat voor smerige marktpraktijken ze erop na houden. Monopolisme ten top, protectionisme ten top en open standaarden, ho maar.
RTX 2080 Ti;2;0.5598922371864319;Eens, Maar geef het enkele maanden en die prijs komt echt wel naar beneden. Het prestatieverschil is echt zijn meerprijs niet waard. Er is een opening voor AMD eens ze klaar staan met Navi binnen een half jaar of zo.
RTX 2080 Ti;2;0.26149263978004456;wat let je om gewoon op oude hardware te blijven, de ontwikkeling in requirements is best wel gestagneerd in de laatste 5-10 jaar. En onder druk?, PC game markt groeit nog steeds exponentieel uit boven de consoles
RTX 2080 Ti;2;0.6020369529724121;Tja, heel begrijpelijk. Ik heb mijzelf ook al meerdere keren afgevraagd wat ik zou doen wanneer de consoles toetsenbord en muis zouden gaan ondersteunen (Ik kan echt niet meer met een controller overweg in shooter games ). Al is het maar voor naast de PC voor de exclusives die uitkomen op vooral de PS. Maar aan de andere kant is gamen op 30FPS (wat nog regelmatig voorkomt) toch wel pijnlijk denk ik en dan ga ik ook enigszins de exclusives bemoedigen waar ik ook weer op tegen ben... enz enz. Zakelijk gezien snap ik het wel, maar het is gewoon erg jammer. Laten we hopen dat zelf de diehard fans deze kaarten iig even met rust laten om een seintje af te geven naar nvidia. Mag hopen dat met deze prijzen er toch wel een stuk minder mensen zijn die early adopter willen worden.
RTX 2080 Ti;3;0.4193729758262634;Maarja, de 2080 is ook een stuk sneller dan de 1080 (zo'n 40%), dus op dat gebied ligt het gewoon exact in lijn met de vorige generatie, en ook bij introductie was de 1080 ronde de 800+ euro. de 980ti lag in lijn met de 1080 indertijd, net zoals nu de 1080ti in lijn ligt met de 2080.. dus op dat gebied is er weinig veranderd.. (Wil niet zeggen dat ik sowieso vind dat de prijzen veel te hoog zijn, maar dat vond ik al bij de 1080 reeks).
RTX 2080 Ti;2;0.3753146827220917;Dat denk ik niet want er zijn nog grote onzekerheden wat RTX enabled games met de FPS doen. Omdat er toch ook bij de aanzienlijke meerprijs deels gecompenseerd wordt met performance gain is de consument die meer bang for tha buck gericht is zit nu in de twijvel zone . De consument die vorige gen inruild voor nextgen de vaak upgrader is huidige resultaat overwegen waard omdat RTX minder toedoet en hun toch als RTX op gang komt voor refresh van turing gaan. Voor diegene die langer met hardware doen en ook meer RTX feature voor willen gaan is uiteraard de RTX HQ resultaten en performance resultaten belangrijk. En met gebrek aan zero RTX games nu geen RTX review ook geen beeld vergelijking en RTX fps performance . Want heeft 2070 wel genoeg RTX power voor voldoende FPS in full RTX enable setting bij release RTX enabled games of heb minstens 2080 nodig. Of heeft 2080Ti nog moeite. Komt RTX pas goed tot zijn recht met 2 2080ti in nvlink mode? Kwa prijs komt voor kij maximaal de 2070 kwa budget in 2019 als 2060 ook RTX heeft en nog zin heeft met RTX enabled games mits die RT cores krijgt.
RTX 2080 Ti;2;0.5036483407020569;Wat Nvidia nu heeft gedaan is de 2080 even snel én even duur gemaakt als de oude kaart. Ok, ik denk dat ik snap wat je bedoelt. Maar dan moet je bovenstaande zin even aanpassen, want dat klopt toch niet met wat je wil zeggen? Dit komt namelijk heel vreemd over. Tenminste bij mij. Los hiervan vergelijk ik geen launch-prijzen met de actuele prijzen van een kaart die al 2 jaar uit is. Vergelijk dan de prijzen eerlijk en pak de MSRP prijzen bij launch van beide kaarten. Bij HWI doen ze dit wel en dan kun je zeggen dat de prijs/prestatieverhouding gelijk is gebleven. Ps niet meteen op de man gaan spelen ... beetje jammer! Ehm, ik denk dat je even terug moet naar de basisschool.
RTX 2080 Ti;3;0.2989374101161957;Alsof €800 al niet veel te veel is voor een 2080.
RTX 2080 Ti;2;0.35232871770858765;Neem je dan wel even een flinke zak geld mee voor de extra performancewinst? Of vergeet je dat gemakshalve? Want als je eerlijk kijkt naar de prijs/prestatieverhouding tussen de 1080ti/2080 zijn we nauwelijks iets opgeschoten.
RTX 2080 Ti;2;0.45068296790122986;Eens, maar feitelijk klopt de opmerking van @mprkooij wel. ik had ook een lager verschil verwacht en wellicht stort die prestatie ook wel in met RTX aan, RTX is echter alleen beschikbaar op die 2080(Ti). De prijs vind ik zelf belachelijk tot het absurde toe, maar dat vond ik de 10xx serie ook al. Gelukkig heeft Tweakers hier ook aandacht voor. Nvidia heerst hoog in de ivoren GPU toren en daar betalen wij helaas de prijs voor...
RTX 2080 Ti;5;0.5079717636108398;Wij hebben NVIDIA daar ook in geplaatst. Je kan ook 'stemmen' met je portemonnee hè
RTX 2080 Ti;1;0.4938909411430359;"Ik niet, ik heb een Rx480 gekocht vanwege bovengenoemde reden! Het verbaast mij dat er mensen staan te springen om een RTX 2080Ti aan te schaffen, of uberhaupt een 1080(Ti) hebben gekocht laat staan in SLI.... Hoe in hemelsnaam kun je dat betalen, hoe maak je de overweging om 600+ euro uit te geven aan een videokaart, en in het geval van een RTX gaat dat gewoon x2. ik kan er met m'n verstand niet bij.... Edit: als antwoord op onderstaande; Misschien is de gemiddelde gamer rijker dan ik kan inschatten, maar ik vrees dat die gemiddelde gamer dat niet is. gerelateerd aan mijn uitgave patroon en voor wat ik over heb voor een gemiddelde videokaart relateer ik dat ook aan wat ik in het verleden kreeg voor een bedrag waar ik nog voor mij zelf een verantwoorde keus kon maken. Ik kocht gemiddeld een kaart die net onder de top zat, zoals mijn laatste nvidia, een GTX770. echter die was toen nog betaalbaar en ruim sneller dan zijn voorganger. Prijzen zijn sinds de GTX 10xx serie dermate hoog gestegen dat ik een enorme antipathie heb gevormd richting Nvidia, als de kaarten nu echt een enorme verandering hebben doorgemaakt tav eerdere ontwikkelingen, dan zou ik er het geld voor over hebben. Echter heb ik het idee dat Nvidia extreem geld aan het binnenharken is voor relatief weinig effort. Er is praktisch geen concurrentie waardoor zij zich deze prijsopdrijving kunnen permitteren."
RTX 2080 Ti;1;0.45107361674308777;dat is natuurlijk allemaal heel relatief per persoon waar het inkomen en geluksfactor heel veel in bepalen. als jij 5k per maand verdiend en je enige hobby daarnaast is gamen en je verdere uitgaven zijn minimaal dan is een kaart van 600 + en zelfs 1300 euro natuurlijk prima te verantwoorden als geluksfactor hiermee stijgt. en hoe ze het kunnen betalen? de it sector betaald gewoon erg goed in verhouding met andere sectors voorals als je freelancer bent, en dan nog, als je enkel als hobby gamen hebt en niet elke week je geld uitgeeft aan uitgaan of restaurantjes, dan hou je natuurlijk een aardig bedrag over om aan gamen uit te geven. zo koopt een hobbymatige wielrenner weer een fiets van rond de 2k ( en volgens mij is dit nog een mild bedrag zelfs ), wat voor velen alweer als een onmenselijk bedrag wordt gezien. toen later een tweedehands gtx 980 gekocht ( toen net de 1080 uitkwamen ) voor €350. en sinds kort een €1080ti voor €550 tweedehands, voor mij een behoorlijke jump in prijs, maar er is gewoon geen goed alternatief ( en geld is nou ook niet echt een issue ik ben meer gaan verdienen en had voorheen zo ' n bedrag als te gek gevonden, maar blijf wel prijsbewust ), daarbij ik had deze performance gains al redelijk voorspeld voor 2080 en zag die prijs totaal niet zitten en raytracing op 1080p60fps is gewoon een gimmick dan wat de markt echt wil, maarja ik denk dat ik het voor mijzelf zo goed kan praten als ik nu letterlijk geen videokaart zou hebben... je punt was hoe mensen een prijs van 600 + kunnen verantwoorden voor een videokaart, en volgens mij heb ik dat redelijk uitgelegd en is het dan toch echt niet zo gek dat dit uitgave patroon bestaat?
RTX 2080 Ti;1;0.5005244612693787;Tja een pakje sigaretten per dag roken kost je €2372,50 per jaar. Daar koop je meerdere RTX kaarten voor. Het is maar waar je je geld aan uitgeeft.
RTX 2080 Ti;1;0.599791944026947;"Ik rook niet, nooit gedaan ook...;)"
RTX 2080 Ti;5;0.33552300930023193;Ach, gelukkig, was al op zoek naar deze comment.
RTX 2080 Ti;1;0.5648109912872314;D'r staat hier een machine met zelfs 3 1080Ti's. En dat was alleen maar omdat het RAM in de weg zat voor een vierde kaart. Geen game PC, uiteraard. Deze staan aan Neurale Netwerken te rekenen. Maar ook Crypto Miners zijn NVidia klanten. En dan gaat de wet van vraag&aanbod werken, ja.
RTX 2080 Ti;5;0.43087735772132874;juist, vraag en aanbod waar Nvidia op dit moment flink z'n zakken mee aan het vullen is. Ze hebben een goed product, dat zal ik niet ontkennen. Beter dan AMD en daar profiteren ze nu van door prijzen te vragen die hoger liggen dan (in mijn beleving) noodzakelijk. En ik weiger daarin mee te doen.
RTX 2080 Ti;1;0.43711936473846436;Ik snap je boycot mentaliteit, maar wat als AMD of ander bedrijf nooit met een alternatief gaat komen of het met beetje geluk in 2020 gaat uitkomen (met Navi, wat net als Vega het prodigal wonder moet gaan zijn, we weten hoe het met Vega is afgelopen, en daar hadden ze letterlijk een boywonder voor ingelijfd, die nadat het faalde net zo hard is vertrokken). Blijf je dan koppig en ontzeg je dan je eigen plezier, of zwicht je dan toch want reeel gezien je boycot zal in het grotere plaatje geen zak uitmaken Nvidia vult zijn zakken toch wel, behalve dat je jezelf wellicht te kort doet. Ik ben ook geen fan van nvidia, wat ze met GPP probeerde te flikken, en hoe ze letterlijk AMD in de hoek drijven op elk vlak wanneer AMD denkt iets unieks in handen te hebben (1070 TI anyone?). Closed source metaliteit, G-Sync vs Freesync, GPUopen vs GameWorks, Hairworks met opzet cripplen op AMD cards (bron: Letterlijk conccurrentie moord. Het cruwe is dat AMD ze wel altijd de hand toerijkt, in wat zij hebben developed Nvidia daar net zo royaal van gebruik mag/kan maken. Maar ik ga mezelf echt niet te kort doen en een AMD kaart kopen om maar een idealist te zijn, daar heb ik alleen mezelf mee, de beste technologie ligt op dit moment nu eenmaal bij Nvidia.
RTX 2080 Ti;3;0.4551123082637787;Ik boycot Nvidia niet bewust, ik heb een AMD gekozen vanwege het gunstiger alternatief. Ik zal ook niet ontkennen dat mijn plezier factor het soms wint van rationeel denken. Echter ergens heb ik wel sterke beperkingen tav wat ik over heb voor een kaart, wat vind ik nog verantwoord. Als Nvidia nu de GTX 1070Ti of hoger tegen een gunstiger prijs kan aanbieden dan nu het geval, koop ik die kaart. Want dan voldoet die kaart aan mijn maximum gestelde verantwoorde prijs. en dat is altijd een afweging. Ik verdien prima (IT baan), maar heb ook een gezin te onderhouden.
RTX 2080 Ti;2;0.3625484108924866;Dat is toch echt wel AMD's schuld ook omdat ze alles open source doen. Je kan zeggen ja dat is idealistisch maar kijk hoever het hen al gebracht heeft. Ze zouden beter meer closed source doen en er een grote hoop geld mee vangen want ze hebben het echt nodig. AMD vind veel nieuwe technieken uit maar dan maken ze het voor iedereen beschikbaar. Ik vind dat dom. Sommige dingen zijn wel goed dat je het open source doet zoals bijv. Freesync maar toch niet alles.
RTX 2080 Ti;3;0.3793600797653198;Uiteindelijk is het wel bewezen dat voorsprong in techniek versneld wordt door Opensource. Ik denk niet dat AMD hier van gaat afstappen, ook om juist de mentaliteit ook anders kan te laten zien (aan Nvidia). AMD GPU tak redt het voorlopig nog wel, ze zijn nog steeds hof leverancier voor de consoles (zowel xbox als ps4). Echter op de pc highend markt is het voor nu wel bekeken, hoop dat ze er van terug kunnen komen het is ze eerder gelukt. Hier uitleg van een AMD engineer zelf over het belang van GPUopen
RTX 2080 Ti;2;0.4191896617412567;Als men braaf iNtel en nV met hun beleid blijven belonen krijgt AMD kapitaal niet om in R&D en highend klasse chips te investeren . De situatie nu is wat gamers gewoon verdienen dure hardware omdat men voor paar procent verschil al de concurrent kiest. CPU tak heeft de GPU tak de afgrond mee ingesleurt het is dus ook nu met Zen comback dat de GP.u tak juist de omzet in de CPU tak nodig heeft om in de toekomst aan slag kracht te groeien. Dus als nvidiot betaalbare nV wilt hebben in de toekomst koop je nu Ryzen aan je nV als meer wilt pushen ook Vega. Ik verwacht iig voor near future geen big chip gpu van AMD. Gezien API in samenwerking met hardware vendor tot stand komen verwacht ik dat ook iNtel GPU en AMD nextgen ook in hardware DXR gaat ondersteunen . Mits iNtel met een consumenten versie komt en van AMD meer kleinere sub highend chip zal worden. Maar pas voor de gen na Navi. Dan zou het als nV hard gepush heeft ook aardig wat games er al zijn die RTX ondersteunen.
RTX 2080 Ti;2;0.5183842778205872;Terwijl het gros van de potentiële kopers graag de performancewinst hadden willen zien voor de prijs van een 1080ti. Nu is dat niet bereikbaar voor veel gamers naar mijn bescheiden mening.
RTX 2080 Ti;1;0.5609596371650696;"Wat een loze opmerking. Waarom zouden ""kopers van deze kaart"" het niet interesseren dat de kaart peperduur is. Deze logica ontgaat me volledig. Dat deze groep ze hoe dan ook koopt is een hele andere stelling."
RTX 2080 Ti;3;0.4636576771736145;Precies, misschien lig je er wel van wakker omdat je je vrouw moet vertellen dat je gezin 1 week korter vakantie kan hebben. Ze hadden beter 'geen belemmering' op kunnen voeren. Want ik had hem echt liever voor 1Euro dan voor 1200Euro gezien.
RTX 2080 Ti;2;0.41346728801727295;Omdat het VEEL duurder is dan toen de Geforce GTX 1080 (Ti) uit kwam, en dat ze niet heel veel sneller zijn dan de Geforce GTX 1080 (Ti), als een grote groep net aan die konden kopen, en nu deze veel duurder zijn en niet heel veel sneller zijn, dan kan ik je garanderen dat het verkoop met de Geforce RTX 2080 Ti veel minder is, en dat mensen langer gaan wachten tot ze goedkoper woorden, of zelfs wachten op de volgende generatie die dan wel een STUK sneller is dan de Geforce GTX 1080 (Ti). De Geforce RTX 2080 Ti maar 30 a 40% sneller is dan de Geforce GTX 1080 Ti. De verschil tussen de Geforce GTX 980 (Ti) en Geforce GTX 1080 (Ti) is VEEL groter en is +/- 65 a 75%, zo als je goed kan zien op Guru3D, vind het erg jammer dat Tweakers geen eens de Geforce GTX 980 (Ti) hebben mee genomen. De verschil tussen de Geforce GTX 1080 Ti en Geforce RTX 2080 maar 5 a 10% is (behalve Wolfenstein II: The New Colossus), en in meerdere maar een paar procent, wanneer de verschil tussen Geforce GTX 980 Ti en Geforce GTX 1080 veel groter is, 30% en meer. Maar de meeste mensen kopen geen duure grafische kaarten, en kopen Geforce kaarten zo als de Geforce GTX 1050 (Ti) en 1060, de mensen die 1080 of hoger kopen zijn maar een kleine groep, en zo zal het ook gaan met de RTX 20*0 serie, zo als je duidelijk kan zien op de Steam hardware survey. steampowered.com/hwsurvey
RTX 2080 Ti;2;0.3720020651817322;Er is groep die alleen absolute performance interresseert die pakt er meteen 2 met nalink bruggetje. De grotere High-end groep kijkt toch ook naar de prijs en speelt bang for the Buck deels ook een rolletje of meer of minder. De normale gang van zaken bij een nestgen is dat productlijn binnen de prijs punten door snellere nestgen vervangen worden voor die prijs. Dus upgrade pad van toen een 10xx nu 20xx gaat niet op wordt model lager 20yy als dezelfde upgrade budget aanhoud.
RTX 2080 Ti;2;0.5171788334846497;Ehh, het is een STUK kleiner dan de Geforce 980 Ti en de Geforce GTX 1080 Ti, daar was het +/- 65 a 75%, en dat de verschil tussen de Geforce GTX 1080 Ti en Geforce RTX 2080 maar 5 a 10% is (behalve Wolfenstein II: The New Colossus), en in meerdere maar een paar %, wanneer de verschil tussen Geforce GTX 980 Ti en Geforce GTX 1080 veel groter is, 30% of meer. Dus ja ik vind het heel erg tegen vallen, en , maar dat had ik al gezien in de uitgelekte benchmarks.
RTX 2080 Ti;3;0.34905484318733215;Wat ik niet uit het artikel kan opmaken is welke GTX1080TI kaart is gebruikt. Is dit ook de Nvidia GTX0180TI FE geweest of is een AIB kaart (zoals de Asus Strix) gebruikt. Er zit namelijk nogal wat performance verschil tussen een Asus GTX1080TI Turbo en een Asus Strix GTX1080TI.. Uiteraard komen vandaag ook de andere reviewers met hun reviews en zijn tonen in de regel wat wel merken zijn gebruiken in hun tests..
RTX 2080 Ti;5;0.3575684130191803;De 1080 Ti FE is gebruikt.
RTX 2080 Ti;3;0.425790399312973;Dat is jammer ee had wel een custom variant meegenomen moeten worden, die is wel zo’n 5/10% beter dan een FE.
RTX 2080 Ti;1;0.5492212176322937;Juist niet, anders heb je geen objectieve vergelijking.
RTX 2080 Ti;3;0.33643704652786255;Je hebt gelijk als de 2080TI dezelfde koeler had gehad. Nu is de koeloplossing van de 2080TI FE editie beduidend beter dan de koeler van de 1080TI FE editie. Dus denk dan ook dat de custom varianten niet onwijs veel beter zullen zijn.
RTX 2080 Ti;5;0.3109898567199707;Oei, dat wordt kiezen tussen 1080 ti en RTX 2080 voor mij
RTX 2080 Ti;3;0.4684368968009949;"De 1080TI zal nu een goede koop zijn gezien de prijs. Helemaal als je deze in de V&A kan halen kan het best heel veel schelen in vergelijking met een 2080; dat terwijl de performance dus redelijk gelijk is."
RTX 2080 Ti;3;0.5252076387405396;Ik vind het persoonlijk toch niet zo´n makkelijke keus. Ja, de 1080TI is nu +- 130 euro goedkoper dan de 2080, maar de 2080 heeft toch wat winst(jes) in fps en bovenal nieuwe(re) technologie. Het is maar net wat je belangrijker acht denk ik zo..
RTX 2080 Ti;2;0.4599016606807709;Vergeet ook niet dat de 2080 nog wat optimalisaties te gaan heeft wat betreft de drivers. Dit duurt altijd een paar maanden voordat de optimale prestaties zichtbaar worden. Echter zijn kinderziektes dan weer ook nog niet opgedoken.
RTX 2080 Ti;1;0.40078458189964294;Vergeet ook niet dat de basis gewoon hetzelfde is als een 1080ti, als er nog noemenswaardige winst in zit is dat voornamelijk in die Ai en tensor cores, dat is namelijk het enige wat echt nieuw is.
RTX 2080 Ti;3;0.5323097705841064;In de basis is ook wel wat veranderd hoor.
RTX 2080 Ti;1;0.33834928274154663;Eerst maar eens bij jezelf nagaan of je op dit moment uberhaupt wel een nieuwe kaart nodig hebt...
RTX 2080 Ti;3;0.3768557012081146;Mijn gtx 970 is echt geen drama hoor, maar na een paar jaar ben ik wel toe aan iets nieuws. Als ik nu al een 1080 ti had gehad, dan zou ik het niet eens in mijn hoofd halen om te upgraden naar een 2080.
RTX 2080 Ti;2;0.5790889859199524;Nee, ik vind het ook logisch als je al een 1080ti zou hebben om dan niet voor de 2080 te gaan. Heb zelf nog steeds de 760 en ben ook al heel lang toe aan een nieuwe, maar ik vind de prijzen te achterlijk voor woorden, 300+ voor nog niet eens een echte midrange kaart (1060/6GB), waarbij ik denk dat die dus niet veel meer dan 200-250 zou mogen kosten.
RTX 2080 Ti;2;0.427578330039978;Hoezo logisch als ik vragen mag? Qua performance is er vrij weinig verschil en er zit een behoorlijk prijskaartje aan..
RTX 2080 Ti;1;0.490028977394104;Uhm, sorry, was het woordje 'niet' vergeten, logisch dat je NIET voor de 2080 gaat.. haha..
RTX 2080 Ti;3;0.433194100856781;Op 4K is het toch wel een behoorlijk performance winst. Dus ook al zou je deze niet voor de RTX ondersteuning kopen zit je nog steeds goed. Wat betreft de prijs, die zal echt nog wel gaan dalen op korte termijn. Ze willen nu eerst de 10XX eruit hebben.
RTX 2080 Ti;1;0.3801722824573517;Zoals de prijzen van de 10xx serie gedaald is na de intro zeker ? AKA de kaarten zijn nooit voor de MSRP verkocht maar altijd (ver) daarboven.
RTX 2080 Ti;2;0.5372066497802734;Eens, maar met zeer korte termijn bedoel ik wel een aantal maanden. Je ziet dan ook een behoorlijke daling in prijs (GTX1080) echter door een tekort aan kaarten (mining) is de prijs vrij kort daarna weer gigantisch omhoog geschoten.
RTX 2080 Ti;2;0.2961908280849457;Wat is het verschil tussen een nvidia titan X en een RTX 2080 TI?
RTX 2080 Ti;1;0.5414571166038513;De Titan X (pascal versie) scoort nagenoeg even veel als een 1080 Ti.
RTX 2080 Ti;5;0.46243858337402344;aha dank voor de uitleg was al benieuwd
RTX 2080 Ti;3;0.44838130474090576;Veel.. iets minder dan een 1080. Dus reken maar uit.
RTX 2080 Ti;2;0.46283021569252014;Waar is de tijd gebleven dat prestaties elke 2 jaar verdubbelden? Wat je al jaren ziet bij CPU's, wordt nu ook zichtbaar bij GPU's. Het wordt steeds moeilijker om grote prestatiewinst te boeken.
RTX 2080 Ti;2;0.3844441771507263;Ik gok dat het gebrek aan harde concurrentie ervoor heeft gezorgd dat Nvidia minder hoefde te presteren
RTX 2080 Ti;1;0.7265936136245728;Ze hadden in de afgelopen 2 jaar de Ray Tracing technologie moeten delen met fabrikanten van de games die zijn uitgekomen in de afgelopen 2 jaren. Laat ze NDA's tekenen en stel kleine teams samen om de RT features in te bouwen. Ook in populaire bestaande games. Daar hadden ze geld tegenaan moeten gooien, desnoods met hun eigen teams van ontwikkelaars. Dan hadden we nu bij de launch wat updates gekregen van de meeste populaire games die honderden duizenden mensen dagelijks streamen via Twitch.tv, en dát zou een perfecte motivatie zijn voor consumenten om ook snel te gaan kopen. Nu heb je dus buitensporig dure kaarten (70% duurder) met een matige snelheidswinst (tot 30%) wat totaal niet in verhouding staat. Zelfs op 4k resoluties lijkt het niet de moeite waard. Want bijna niemand heeft een 4k scherm met een hogere refresh rate dan 60Hz. Dus wat moet je met FPS die gemiddeld boven de 60fps liggen? Dan zou je de grafische settings ietje naar beneden kunnen schroeven om altijd 60fps te halen, en dat kost je niks. Tweakers zou deze kaart dan ook hartstochtelijk en overtuigend mogen afraden vind ik persoonlijk. De 1080-series zijn op dit moment gewoon de beste kaart qua prijs/kwaliteit. En neem de ti-versie als je op resoluties van 1440p of hoger speelt. Deze hele RTX reeks slaat helemaal nergens op. Wat een bijzonder slechte marketingbeslissingen hebben ze genomen daar bij Nvidia.
RTX 2080 Ti;3;0.412802517414093;Klopt op dit moment is voor de 1080 de prijs kwaliteit verhouding prima, heeft wel 2 jaar geduurd voordat die prijs flink omlaag ging. Dat zal voor de 2080(ti) ook zo zijn.
RTX 2080 Ti;2;0.42621028423309326;En jij denkt dat dit niet gebeurd is? Wat denk je dat die engine-bouwers gedaan hebben de afgelopen tijd? Voor oa UE4 en Unity is het eigenlijk niets meer dan een switch omgooien mits je al op een fatsoenlijke manier je models/textures gebruikte (omdat die informatie ook al nodig was voor de huidige belichtingsberekeningen etc). Ja, de ti versie is een heel stuk duurder, maar ga ik kijken in de pricewatch, en vergelijk even een doorsnee 1080ti met een doorsnee 1080, dan zie ik dat het prijsverschil procentueel gezien niet heel veel anders is dan nu tussen de 2080 en de 2080ti. En vergeet niet dat de 1080 FE bij introductie ook niet veel goedkoper was dan de 2080 FE nu is.
RTX 2080 Ti;1;0.6319372653961182;Kijk op Twitch.tv naar de meest populaire games. Geen van allen bouwen eventjes die switch in. Dat kost tijd en geld, en in het geval van veel games ligt de focus vast geheel anders dan een paar procent van hun gebruikers die ray tracing kunnen zien. Dus ik denk inderdaad dat Nvidia hier niet mee bezig is geweest. Want ik zie alleen opkomende games met de RT feature, niet bestaande games. Er zijn op dit moment letterlijk nul games die het ondersteunen.
RTX 2080 Ti;2;0.3497259020805359;Als je je eigen engine hebt geschreven, nee, dan zul je die niet even inbouwen, maak je nog gebruik van een oudere engine (bv UE3), nee dan bouw je het niet even om nee. Maar ben je nu in development en maak je gebruik van de huidige UE/Unity, dan ja, is het in principe niets meer dan een switch omgooien.. Maar ik heb het ook nergens gehad over bestaande games, maar over games die nu in development zijn. Je modellen/textures (als je fatsoenlijk bezig bent in iedergeval) geef je nu al bv wat voor materiaal het is zodat de engine de (fixed) belichting kan berekenen, alleen met de RTX extentie gebeurd dat dus voor een hoop effecten dan realtime ipv pre-calculated.
RTX 2080 Ti;2;0.6298213005065918;Vooral onder de indruk hoe goed ze het doen op 4k. 4k gamen op ultra is toch gewoon waar we naar toe moeten. Maar de 2 grote nadelen blijven voor mij onacceptabel. 1. Energieverbruik: gebaseerd op mijn verbruik en game tijd kost dit me op jaarbasis al 50 euro meer! Dat is over de levensduur van 4 jaar wat realistisch is voor mij 200 euro meer! Dat moet je dus al bij de aanschaf peijs van de kaart optellen. Het is dan ook jammer om te zienndat eeze boost wel puur ten koste gaat van stroomverbruik. Het feit dat hij goed te coolen is is dan wel weer netjes en maakt mij nieuwsgierig wat voor oc versies er gaan komen. 2. De prijs. Dit is gewoon niet op te brengen dus misschien over 2 generaties heb ik een kaart met deze prestaties voor een normalere prijs.
RTX 2080 Ti;3;0.4613267183303833;Dat is natuurlijk omdat je op 2560x1440/3440x1440 en vooral 3840x2160 minder snel tegen CPU bottlenecks aanloopt. Ter ziener tijd wel, maar je moet daarvoor diep in de buidel tasten en het is niet een gegeven dat alles erg vlot met lage frametimes zal lopen op Ultra 4K op dit moment. (of instellingen terugschroeven) Maar dit telt vooral voor mensen die erg kritisch zijn op de respons van de muis, liever op hoge refreshrates gamen met honderd(en) frames per seconde. En spellen blijven zwaarder worden dus je zal weer snel moeten upgraden om dat niveau bij te benen. Dat alles in 4K zo super makkelijk draait zoals het op 1920x1080 nu het geval is duurt het nog wel even.
RTX 2080 Ti;3;0.3101579248905182;Helemaal mee eens maar bij de introductie van 4k monitoren dacht ik dat het nog langer zou duren voordat het mogelijk was met acceptabele frame rates ook maar 1 grafisch goeie game te spelen.
RTX 2080 Ti;5;0.5441628694534302;Zolang je een high-end videokaart wil, zul je altijd met deze twee aspecten te maken hebben. Zie het als een exclusieve sportauto. Die zuipen ook en zijn ook flink aan de prijs.
RTX 2080 Ti;2;0.49997490644454956;Nou de 1080 was significant zuiniger en goedkoper in aanschaf en de 1070 was nog zuiniger. Kortom deze generatie is gewoon fors duurder en duurder in gebruik. duurder en duurder in gebruik lijkt overigens elke generatie een groter probleem te worden. de prijzen stijgen veel harder dan inkomens.
RTX 2080 Ti;3;0.5585611462593079;Een GTX 780 trok ook 250 Watt, dus op zich valt het verbruik wel mee (vergeleken met oudere kaarten). Enkel de RTX 2080 Ti gaat hier overheen met 250W/260W (FE) De RTX 2080 zit netjes op 215W/225W (FE). Dat vindt ik niet overdreven veel voor zo'n topkaart.
RTX 2080 Ti;2;0.4270906150341034;Idd maar ik vond het na vele jaren juist een opluchting dat de gtx 1xxx eindelijk zuiniger was. Echter lijkt dat concept nu compleet overboord gegooid en zitten we weer aan de stevige kabels.
RTX 2080 Ti;2;0.4254484176635742;Geachte auteur... hoe zit het eigenlijk met de VR prestaties?! Ik speel namelijk vooral simulatie games in VR tegenwoordig, en dat is nu net waar ik benieuwd naar ben. Al die shooters en arcade racers zijn eigenlijk niet zo mijn ding. Echter de VR prestaties zijn totaal niet onder de aandacht geweest.
RTX 2080 Ti;5;0.6418787837028503;Hier heb je helemaal een goed punt. Ik ben ook erg benieuwd naar Sim games benchmarks zoals Elite en iRacing.
RTX 2080 Ti;3;0.5242498517036438;Goede overzichtelijke review. Prijs toename staat niet in verhouding met de performancewinst iig. 60FPS in 4k is dus nu wel bereikbaar.
RTX 2080 Ti;2;0.4350486397743225;bereikbaar, maar niet veilig. die 10fps extra kan elk moment droppen. je zit pas veilig bij 100fps+.
RTX 2080 Ti;2;0.42296937108039856;Klopt ja. Als ik zou besluiten om zo veel geld te besteden voor een kaart die futureproof moet zijn dan zou ik idd liever rond de 100FPS zitten. Wat dat betreft vind ik deze kaarten net te licht qua performance. Denk dat je al snel settings omlaag moet gaan gooien over 1 a 2 jaar. Dat wil je natuurlijk niet als High end pc nerd
RTX 2080 Ti;3;0.5519462823867798;Nou PC is voor mij singleplayer only en dan vind ik games met mooie grote omgevingen met slowpace gameplay fijner dan is 30fps nog te doen en 60fps zat voor RTX gfx bevorderende features lijkt mij ook meer interresanter voor games waar je wel meer ingaat op de omgeving slowpace met nadruk op exploring in mooie omgeving. Waar meer online competatief bezig bent is voor mij console waar iedereen meer gelijk getrokken is dan op PC. Met 60FPS en minder gecheat.
RTX 2080 Ti;3;0.746961772441864;Dan maar de settings iets omlaag. Ik geef niet meer dan 500 euro uit voor een videokaart. En de 1070 Ti is daarmee nog geschikt genoeg voor de komende tijd. Ik heb ook 4 jaar met de 660 Ti gedaan, en dat ging op 1080p helemaal prima. Console games zitten vaak eerder rond de 30 fps en die hoor je er ook weinig over klagen. Maar ja, misschien ben ik te casual wat dat betreft.
RTX 2080 Ti;2;0.31218940019607544;Maar voor volk dat geld over de balk gooit en zodra er iets sneller is paartje koopt in nvlink mode . Zou 2080ti na de 2080 uitkomen dan waren er dus eerst 2 2080 en later 2 2080ti dat wordt hun nu bespaart , door meteen voor de 2080ti te gaan.
RTX 2080 Ti;2;0.509508490562439;Helaas niet heel veel winsten. De 2080TI is maar zo’n 30/35% beter dan de 1080TI en de 2080 is gelijk aan de 1080TI. Jammer er had meer in kunnen zitten zeker gezien de prijs.
RTX 2080 Ti;3;0.3804870843887329;En kunnen we nu eens realistisch vergelijken? toen de 1080 uit kwam, was die ook niet sneller dan de 980ti, en het prijsverschil was ook gelijk aan nu. Ga je de 1080 vergelijken met de 2080 dan zie je dat het verschil ongeveer gelijk is aan indertijd tussen de 980 en 1080... Wel even realistisch blijven...
RTX 2080 Ti;3;0.5456769466400146;1080 was wel een heel stuk sneller dan de 980TI, zelfs de 1070 was ietsje sneller. Nu is de 2080 vergelijkbaar dan een 1080TI en de 2070 zal rond de 1080 zitten.
RTX 2080 Ti;3;0.5209471583366394;Zijn er al benchmarks voor Tensor cores? Want die zijn natuurlijk wel leuk voor Tensorflow.
RTX 2080 Ti;3;0.41101688146591187;redelijk veel games getest zie ik, goodjob tweakers team !
RTX 2080 Ti;5;0.7862298488616943;Wow, indrukwekkende resultaten vergeleken met de vorige generatie kaarten, zowel van NVIDIA als AMD.
RTX 2080 Ti;4;0.4330002963542938;Het is helemaal afhankelijk op welke resolutie je wilt gamen of deze nieuwe generatie kaarten interessant is om aan te schaffen. De meerprijs die je betaald is vooral een gok op nieuwe techniek die zichzelf nog moet waarmaken. Prijs/kwaliteitswijs is de Pascal generatie op dit moment toch wel de betere optie. Maar dit is natuurlijk afhankelijk van de continue beschikbaarheid van deze kaarten in de webshops, ik verwacht dat ze er op den duur wel uitgaan.
RTX 2080 Ti;3;0.4675866365432739;Met GDDR6 en extra bandbreedte is er wel voordeel te verwachten in 4K gamen.
RTX 2080 Ti;5;0.46641433238983154;Ligt er dus helemaal aan wat je wilt en welke je met elkaar gaat zitten vergelijken. Zelf zou ik dus de 2080 gaan kopen ipv de 1080ti. Ligt er dus ook helemaal aan hoe je budget is.
RTX 2080 Ti;3;0.6432529091835022;Mijn eVGA GTX1080 heeft het soms lastig op ultrawide gaming om daar altijd boven 100fps uit te komen (monitor gaat tot 120fps). Dus een RTX2080Ti lijkt voor mijn verwachtingen wel ok.
RTX 2080 Ti;3;0.41583287715911865;Iets lagere settings en probleem opgelost.
RTX 2080 Ti;2;0.425371378660202;"tja daarvoor koop je imo niet echt een gamepc hé, dan kan ik evengoed op PS4 spelen. Ik noem mezelf een ""pc liefhebber"" en dan zoek ik naar ofwel maximale fps, ofwel maximale beeldkwaliteit en als het even kan, beide. Weinig nut om een 34"" 120fps IPS ultrawide te kopen om dan met lage settings te gaan gamen hé."
RTX 2080 Ti;2;0.46244147419929504;"Sommige settings zijn onevenredig belastend. Tesselation was daar vroeger een voorbeeld van. Zette je die lager, zag je vaak 0 verschil, maar wel tientallen fps meer. Alles op max zetten is niet perse ""pc liefhebber"" zijn."
RTX 2080 Ti;2;0.4863528907299042;Nou nee je vergelijking met consoles gaat niet helemaal op die halen zelden 60fps (vaak 30) en dat voornamelijk op 1080p. Tussen het uiterste wat jij zoekt en een console zit nog best veel ruimte en dat is waar genoeg pc gamers genoegen mee nemen.
RTX 2080 Ti;4;0.5111545920372009;Goed filmpje om te kijken: Zal je een hoop geld/zorgen besparen ULTRA settings is al jaren een beetje een farce.
RTX 2080 Ti;2;0.42918071150779724;Dat is toch enkel met DLSS dat ze hogere prestaties kunnen gaan geven? met Raytracing aan zullen de prestaties juist zakken en zal hij wellicht niet meer sneller zijn dan de oude generatie kaarten (die zonder Raytracing draaien natuurlijk)
RTX 2080 Ti;2;0.3008035123348236;door de raytracing cores zou je toch eigenlijk geen performance drop mogen verwachten lijkt me?
RTX 2080 Ti;2;0.43197184801101685;Eigenlijk niet, maar gezien de kaart nu al tegen zijn TDP aan hickt zal er toch iets minder energie moeten verbruiken als er andere cores ook energie nodig hebben
RTX 2080 Ti;3;0.5523048639297485;Hoewel goede prestaties gains zie ik wat prijs/performance betreft echt geen reden om een 2080 aan te schaffen als je al een 1080 hebt. 26% sneller @ 1080p en 33% @ 1440p, maar wel 70% (??) duurder? Upgraden voor die resoluties lijkt me dan dus niet heel verstandig, of zie ik hier iets over het hoofd? Lijkt mij ook verstandiger om van bv. een gtx980 naar een gtx1080 te gaan wat prijs betreft right? Mits je niet 4k gamed.
RTX 2080 Ti;5;0.5392275452613831;Volgends mij bij de volgende drivers. Neemt de db ook nog af op de rtx series. Ben benieuwd naar de tests met de goede drivers
RTX 2080 Ti;1;0.6656932234764099;In NL gaat de 2080 echt veel duurder zijn dan een 1080ti hoor... en dan hebben we het geeneens over 2ehands..
RTX 2080 Ti;1;0.6451133489608765;nee hoor, de 2080FE is hier 849, een doorseen 1080ti is op dit moment 790... dus nog geen 59 euro verschil.
RTX 2080 Ti;3;0.4654844105243683;Je vergelijkt de FE met een aftermarket 1080ti. Je hebt ook 1080ti's voor 715 euro (bijv gigabyte). Of te wel ~150-200 euro verschil tussen aftermarket voor redelijk identieke performance.
RTX 2080 Ti;1;0.2770252525806427;Geen blender performance ??
RTX 2080 Ti;1;0.49347516894340515;Blijkbaar kreeg Linus blender niet aan de praat met de nieuwe series dus lijkt nog niet compatibel.
RTX 2080 Ti;2;0.45810315012931824;Mijns inziens is dit een kwalijke evolutie: Vroeger waren de kaarten van de vorige generatie ook sneller, en dezelfde prijs als de vorige generatie. Of even snel als de vorige generatie, en goedkoper. Een GTX 1070 was even snel als een 980Ti, maar een PAK goedkoper ($379 vs $599). Of een GTX 1080 was veel sneller dan een GTX 980, voor dezelfde launch prijs $549. Nu zijn de kaarten ofwel even snel maar duurder (GTX 1080Ti vs 2080 even snel, maar 33% duurder, $599 tegenover $799). Of de snellere kaart is een PAK duurder (GTX 1080 TI vs 2080Ti +70% voor 3rd party kaartenof +100% voor FE). Jammere evolutie. En daarbij, tenzij je op 144Hz in 4K wil spelen heb je de 2080Ti toch helemaal niet nodig? Een 1080Ti is al lang genoeg voor 4K 60Hz. Ik heb zelf een 1440P 60Hz monitor, dus voor mij zijn deze kaarten allemaal overkill. Sneller dan een 1070 of 980Ti heb ik niet nodig. Laat ons hopen dat de 2060 serie kaart sneller is dan de 1070 voor minder geld.
RTX 2080 Ti;2;0.4771714508533478;uhh, de 1080 is nooit dezelfde launchprice geweest als de 980, en de 2080 is ook in verhouding net zoveel sneller ten opzichte van de 1080 als de 1080 was van de 980.. Het lijkt er op dat veel tweakers hier de boel een beetje aan het verdraaien zijn. En ik weet niet waar jij je prijzen vandaan haalt, maar kijk in de pricewatch en de doorsnee 1080ti kost toch echt tegen de 790 euro, waarbij de 2080FE 849 is, dus nog geen 59 euro verschil. En het verschil tussen een 1080 en 1080ti is procentueel gezien in verhouding vergelijkbaar met het verschil tussen een 2080 en 2080ti..
RTX 2080 Ti;2;0.4426767826080322;"Nope, de 1080 vs 2080 is slechts +28% sneller De 980 naar de 1080 was 66% sneller!! Het verschil was meer dan dubbel zo groot! Helemaal niet ""net zoveel sneller"" En OK, de 1080 was $599 bij launch, de 549$ is de huidige adviesprijs. De 980 was $549 bij launch. 10% duurder dus voor 66% sneller. Dat was een goede deal. Nu krijg je 0% sneller voor +33% prijs. Of +30% sneller voor +70%-+%100 prijs (in het beste geval, de 3rd party kaarten gaan nooit $999 kosten zoals Nvidia belooft, eerder tegen de $1200 van de FE aan, zoals bij de 10-serie het geval was) (Ja ik haal mijn prijzen in dollar, ik woon in de VS namelijk. De 1080Ti is hier te koop voor exact $600. Geen idee wat de verdraaide EU prijzen zijn in euros, maar ik vergelijk de prijzen in USD waar de kaarten momenteel voor verkocht worden) Punt blijft, de GTX 2080 zou NIEMAND moeten kopen, want duurder dan de 1080Ti en niet sneller. En de RTX is allemaal nog een toekomstverhaal, geen enkele game die het momenteel ondersteunt."
RTX 2080 Ti;3;0.5208141803741455;Mooie review . Uiteraard is de performance er flink op vooruit gegaan, meer dan ik had gedacht. Maar de prijzen zijn helaas exorbitant hoog. Beetje off topic: Is het verstandig om nu een laptop met GTX 1070 te kopen (FHD 144Hz GSync scherm) of kan ik hier beter mee wachten op de nieuwe mobiele chips van Nvidia? Ray tracing en dergelijke, mocht het al naar de laptops komen, intereseert mij niet zoveel. Het gaat mij erom dat ik AAA titels de komende 5 jaar op minimaal 60fps kan spelen. Ik heb het kopen al een jaar uitgesteld en hoewel ik nog langer kan wachten, denk ik zelf niet dat dat het waard is op dit moment.
RTX 2080 Ti;3;0.39571085572242737;Als het geen 4k scherm heeft zal de 1070 prima volstaan
RTX 2080 Ti;3;0.31082823872566223;Ik wil zelf binnenkort een beeldscherm en GPU upgrade doen, heb nu een standaard 60hz 1080P scherm met de 970 die alles prima runt, echter wil ik naar een Dell paneel die 144Hz kan op 1440p met G-Sync (Dell S2417DG Zwart). Welke kaart is dan het beste om te halen? Kan de 1080TI dit ook al aan bij alle games of is dit nog niet snel genoeg? Edit: Ik speel voornamelijk: Rainbow 6 Siege, BF 4, Nieuwe CoD misschien, Fortnite, SCUM, CS:GO.
RTX 2080 Ti;5;0.4715692102909088;Een nieuwe Apple is geboren in het tech landschap....
RTX 2080 Ti;3;0.5770780444145203;"De 2080 TI is krachtig, maar de prijs is voor mij en vele anderen wat teveel voor een grafische kaart. Helaas is de 2080 vs de 1080 TI niet helemaal lekker qua performance, ze zijn ongeveer hetzelfde. Als je een ""fatsoenlijk"" 1080 TI wilt hebben betaal je wel 750+ euro tot wel in de 800, en voor de 2080 zit je ook alweer 850 tot wel in de 950 of hoger. Ik zelf zou alsnog wel die 1 of 2 honderd erbij neerleggen en gaan voor de nieuwste kaart als je het geld ervoor over hebt. Puur voor het feit dat je dan toch wel een nieuw product in handen hebt en waar nvidia de komende tijd de aandacht in zal steken om de drivers te verbeteren en te optimaliseren. Na wat optimalisaties zal daar vast ook wel wat winst eruit te halen zijn. Het raytracing is een leuk extratje maar daar moet je het zeker niet voor doen als dat je hoogste prioriteit is. Zit je wat krapper met je geld, dan is de 1080 TI een betere keuze."
RTX 2080 Ti;3;0.2638477385044098;Ben ik blij dat ik toch maar die deal van Alternate gepakt heb. één 1080 ti voor €650
RTX 2080 Ti;3;0.26570457220077515;Ik ken dat gevoel gisteren ook enen gekocht bij alternate voor dezelfde prijs.
RTX 2080 Ti;1;0.3051704466342926;Welke merk?
RTX 2080 Ti;5;0.21392707526683807;Gigabyte Aorus 1080 ti
RTX 2080 Ti;5;0.2568264901638031;Ik heb de EVGA gekocht + 50 euro voor 10 jaar garantie
RTX 2080 Ti;2;0.555246889591217;Ik ben licht teleurgesteld, de 2080 presteert nauwelijks beter dan de 1080 Ti, dus de aangekondigde prestatie van de 2070 zal er zeker onder zitten (ondanks de keynote talk). Het is nog minstens een jaar wachten op de die shrink en grotere FPS verschillen. Met Black Friday zullen de 1080 Ti modellen wel met verlaagde prijs verkocht worden, daar zit wel een deal in voor mensen die nu willen kopen.
RTX 2080 Ti;1;0.3763170838356018;En dan te bedenken dat ik tot vijf jaar terug elke keer de een na snelste kaart kocht voor ongeveer driehonderd euro. Denk dat de mining er ook toe heeft bijgedragen. Men zag dat men de de kaarten ook voor drie keer zoveel kon verkopen en dat ze verkocht werden. Dus waarom dat niet volhouden. Tja je kan ze moeilijk ongelijk geven... Maar blij word ik hier niet van zoveel niet mijn hele computer onderhand Zelf meer een 1070 liefhebber. Nu eens kijken wat die gaat kosten en of hij ook 35% sneller is als raytracing wordt aangezet. Dat vind ik pas echt interessant.
RTX 2080 Ti;2;0.4328216016292572;Ehm, dat klopt niet helemaal hoor. Ik heb zelf in 2012 een GTX 670 gekocht, dat ding was toen al iets van 400 euro. En dat was niet eens de absolute top, een GTX 680 schommelde in die tijd ook al tussen de 500 en 600 euro. De 1070 en 1080 zullen nu wel snel in prijs gaan zakken verwacht ik.
RTX 2080 Ti;1;0.7420977354049683;Het lijkt op Apple-marketing, we maken de iphone heel duur, poetsen de oude zak even op, nieuw dingetje erin, hop en we maken het schreeuwend duur.... Ik heb en 1080ti, ik hou mn 1080ti nog wel even...doet t nog prima. says enough...niet kopen dus.
RTX 2080 Ti;2;0.47280353307724;Qua prijzen hanteert Nvidia op dit moment het Apple model: Een premium product met dito prijskaartje dat qua performance absolute top is (Wolfenstein 1080p met 300 FPS, wat?) Dat gezegd hebbende, in de here and now is een Pascal GPU prijstechnisch veel aantrekkelijker. Het is dan eerder wachten hoe de RTX 2070 zich zal verhouden qua prijs, maar voor nu is een Pascal GPU aantrekkelijker. Denk je echter aan de toekomst, dan is een Turing GPU het overwegen waard. Je betaalt er extra voor, maar de performance is dan wel navenant, plus dat je dan alle extra visuele goodies mee gaat trekken. Het zou mij niet verbazen als de RTX 2070/2080/2080Ti eenzelfde levensvatbaarheid gaat krijgen zoals de Radeon HD 5770/5870, Geforce GTX 750 Ti, Geforce 8800 reeks en varianten, en de Radeon 9700 Pro/9800 Pro kaarten. Langdurige ondersteuning. In dat opzicht is de belachelijke prijs wel weer te verantwoorden: Deze kaarten zullen voor lange tijd de basis vormen. Maar ja, de Radeon HD 5xxx reeks was een schijntje ivm wat je nu voor de RTXen moet neertellen.. wat dat betreft hebben die kaarten, evenals de Geforce 8800 reeks, vele malen hun waarde bewezen. Of dat ook geld voor de RTXen in de toekomst, hangt af in hoeverre de industrie Raytracing gaat omarmen. Het nut moet bewezen worden. Als Nvidia daar niet in slaagt, dan is de RTX wellicht een mislukking van jewelste. Slagen ze er wel in, dan is de RTX een kaart voor de lange toekomst.
RTX 2080 Ti;5;0.3971494436264038;Toen de eerste benches waren gelekt van de RTX 2080 heb ik ervoor gekozen om een 1080ti op te pikken voor 650 euro. Ik ben blij om te zien dat ik de juiste keuze heb gemaakt. Nu al helemaal omdat ik in eerste instantie wilde wachten tot de RTX 2070 uitgebracht werd die waarschijnlijk een stuk langzamer gaat zijn dan de 1080ti.
RTX 2080 Ti;1;0.40218111872673035;In de Pricewatch zie ik ze niet voor onder de 700 euro.. Mag ik vragen welk merk en model GTX 1080Ti het is en waar je die gekocht hebt voor 650 euro?
RTX 2080 Ti;4;0.4251713156700134;Dat was een dagaanbieding bij Alternate. Je kan het goed zien in de grafiekjes. pricewatch: Gigabyte AORUS GeForce GTX 1080 Ti 11G
RTX 2080 Ti;3;0.27649742364883423;Geen Titan V benchmarks?
RTX 2080 Ti;1;0.3258627951145172;Ik mag toch bidden dat niemand dit er voor gaat betalen.
RTX 2080 Ti;2;0.4268788695335388;Ik zou wachten op 7nm kaarten.. Die komen binnen een jaar. Dus vind het dom om nu de hoofdprijs te betalen voor deze 14/12nm kaarten die redelijk snel End Of Life zullen zijn.
RTX 2080 Ti;5;0.3153746426105499;Heb je een bron van waar je deze wijsheid vandaan hebt?
RTX 2080 Ti;5;0.29319602251052856;Zelf even zoeken was teveel moeite? Nvidia RTX 20 Series: Why You Should Jump Off The Hype Train TSMC Receives Next-Gen NVIDIA 7nm GPU Orders, More Than 50 Chip Tape Outs Expected By The End of 2018 Nvidia Ampere Architecture Will Replace Turing In 2020, Based On 7nm
RTX 2080 Ti;1;0.39811035990715027;"Zie in geen van die links een bevestiging dat er binnen een jaar 7mm kaarten komen; alleen maar speculatie en verwachtingen, en dan nog niet eens dat de 7mm kaarten ook direct gaming varianten zullen zijn."
RTX 2080 Ti;1;0.5671104192733765;haha dat we praten over '' wat krapper met geld'' en dan maar de 1080TI halen alsof 700+ euro goedkoop is Die nieuwe generatie zou 500 euro zijn als AMD er ook was. Dus geen 1400 euro voor een 2080ti En dan is de 2080ti ook nog eens'' crippled'' met 88rops en geen 96
RTX 2080 Ti;2;0.45645540952682495;Tegen de tijd dat ik van de RT features kan genieten zijn de opvolgers al op komst. Dan kan ik beter op die wachten (1080 Ti al in bezit).
RTX 2080 Ti;3;0.38957738876342773;Afgezien van hoeveel games RTX zullen gaan ondersteunen is ook nog de vraag wat de performance daarvan zal zijn, bvb of men bereid is om daarvoor van 4k 60+fps terug te stappen naar 1080 60fps (gebaseerd op wat tot dusver bekend is over RTX performance).
RTX 2080 Ti;3;0.3695078194141388;Een nieuwe video is uitgekomen om metro exodus uit te belichten betreft raytracing belichting: Ik moet zeggen dat ik wel onder de indruk ben wat de raytracing cores in samenwerking kunnen met de tensor cores. Ik weet zeker dat bij een paar verdere generaties raytracing een belangrijke feature is voor hoger realisme en dat we dan wel heel blij zijn dat nvidia deze weg in is geslagen.
RTX 2080 Ti;2;0.462419718503952;De 2080 Ti is alleen leuk voor de winst op 1440p/144hz. En de 2080 is niet snel genoeg met maar 7 of 8 % betere prestaties dan de 1080 Ti. De nieuwe serie heeft wel mooie uitschieters in Vulkan en DX12. Te duur.
RTX 2080 Ti;5;0.29822587966918945;Ik ben behoorlijk benieuwd hoe deze nieuwe kaart gaat presteren in VR gezien de nieuwere headsets zoals de Pimax. Op dit moment twijfel ik dus ook tussen een 1080ti en 2080ti.
RTX 2080 Ti;4;0.5764114856719971;Wat een leuke nieuwe generatie. Een paar dingen dat mij hier is opgevallen: De geluidsproductie is hoog en ik ben benieuwd of dit met de custom versies omlaag kan. Verder is de RTX 2080 geheel niet interessant voor ons die een 1080Ti bezitten maar een upgrade naar RTX 2080Ti is veel interessanter in 4K benchmarks
RTX 2080 Ti;1;0.4010986387729645;Ik zou zeggen, allemaal kopen! Dan vraagt Nvidia voor de 3080TI volgende keer 3000 euro Ik wacht op de volgende AMD release volgend jaar. Ondertussen gaat alles vlot genoeg met Vega 56 met Vega 64 bios.
RTX 2080 Ti;1;0.7051976919174194;Hopelijk verkoopt het voor geen meter met die belachelijke prijzen. Komt er natuurlijk van als er een monopoly is op de gpu markt.
RTX 2080 Ti;1;0.374245285987854;Geen VR games getest? Dit is voor mij de enige reden om uit te kijken naar een nieuwere grafische kaart (zit nog op GTX 970)..
RTX 2080 Ti;4;0.4135799705982208;Goede review en goed advies. Waarom nu hardware kopen, dat softwarematig nog niet gebruikt wordt. Dat is nooit slim, of je moet echt de snelste GPU willen hebben, omdat het de snelste is, maar van de features maakt nog geen enkele game optimaal gebruik. Grote kans dat als de Turing Features goed gebruikt worden, de nieuwe generatie alweer voor de deur staat en dat deze GPU's flink goedkoper zijn.
RTX 2080 Ti;1;0.5169047713279724;Heb zojuist de EVGA FTW 1080TI goedkoop op de kop kunnen tikken + voor 50 eur 10 jaar garantie afgekocht. De nieuwe generatie kaarten zijn overpriced, aangezien ik alleen in fullhd game is de 1080TI mee dan genoeg, We hebben tot al die tijd ZONDER RAYTRACING kunnen gamen, waarom moet dat nu ‘n “hebben dingetje “ zijn?
RTX 2080 Ti;4;0.29077714681625366;Op deze site vind je benchmarks gedaan met linux (Ubuntu 18.04 LTS):
RTX 2080 Ti;2;0.41057005524635315;Toch heel slim van Nvidia om die kaarten zo te prijzen dat ze niet gelijk onwijs veel beter zijn (in kwal/prijs verhouding) dan de oudere generatie. Nu blijft die hele 1080 generatie nog flink actueel. Want daarvan zal nog een flinke voorraad liggen in de fabrieken, gok ik zo.
RTX 2080 Ti;4;0.35873574018478394;Kan ook zijn dat ze gewoon dure kaarten op de markt brengen in de hoop miners aan te trekken die alleen de top kaarten willen. En dan de prijs van de vorige generatie iets omlaag en goed leverbaar zodat gamers en miners beide iets hebben.
RTX 2080 Ti;3;0.2838650643825531;Ik ben blijkbaar de enigste die ook op de benchmarks van Premiere Pro / after effects / rendering wacht? aangezien ik naast gamen daar ook gebruik van wil maken
RTX 2080 Ti;2;0.4495006799697876;"Wat mij betreft is dit de typische Geforce generatie die je best kan skippen. Ten eerste: de prijzen zijn echt belachelijk. Laat ons eerlijk zijn, de 1080Ti was vorige generatie reeds een gi-gan-tisch dure GPU, die uiteindelijk slechts door een (relatief) beperkt aantal gamers wordt gekocht; enkel wie absoluut het beste van het beste in zijn PC wil hebben koopt zoiets. Nu is de standaard high-end kaart (2080) niet alleen even duur maar zelfs significant duurder (introductieprijs 2080 699$, introductieprijs 1080Ti 599$)? En dat voor pakweg 5% betere performance? Daar komt Nvidia enkel en alleen mee weg omdat AMD geen concurrentie biedt op dit niveau. De 2080Ti is dan wel weer gemiddeld 34% sneller dan de 1080Ti, maar 1) dit is zwaar teleurstellend ivm vorige generaties (de 1080Ti was 74% sneller dan de 980Ti) 2) dit gaat deze keer gepaard met een fors toegenomen stroomverbruik en hitteproductie 3) aan een enorme prijsverhoging (introductieprijs 999$ vs 599$, dat komt bijna in de buurt van het dubbele lol) En de mega overhypte nieuwe features (Ray-tracing en DLSS AA) zijn gewoon nog niet af: zoals de review er correct op wijst zijn er momenteel NUL games beschikbaar die die features gebruiken. Da's bij mijn weten nog nooit eerder gebeurd met een GPU launch, meestal is er toch minstens één flagship game dat de nieuwe tech promoot. Tenslotte denk ik dat mensen de performance gains van raytracing zwaar overschatten; als de devs van Metro Exodus naar eigen zeggen hard aan't werken zijn om 60fps te halen op 1080p (op een RTX 20180!!) dan weet je't wel. DLSS klinkt interessanter, maar opnieuw niemand heeft het ooit al kunnen testen want er bestaat nog geen game dat dat doet, dus moeten we nVidia gewoon op hun woord geloven. Maar ik kijk met meer dan gemiddelde interesse uit naar de RTX 3080Ti Goeie review trouwens, thx."
RTX 2080 Ti;1;0.4895632863044739;Quote: Naar mijn weten betekend AIB niks anders dan (graphics) add-in-board: AMD and Nvidia also build and sell video cards, which are termed graphics add-in-board (AIBs) in the industry. Bron NVidia maakt en verkoopt dus AIB's, daar naast zijn er nog AIB Suppliers
RTX 2080 Ti;3;0.36385515332221985;Ik ben het eens met de voorlopige conclusie van deze Tweakers review dat de RTX 2080 op dit moment de beste keuze lijkt, zeker als de prijzen een beetje gaan zakken naar huidig 1080Ti niveau.. Iets meer prestaties maar vooral de ondersteuning voor bepaalde nieuwe technieken zou dan toch mijn voorkeur hebben over nog een GTX 1080Ti uit de vorige generatie te kopen.. Zelf wacht ik het allemaal rustig af, mijn GTX 980Ti kan nog wel even mee op de 2560x1440 resolutie waarop ik speel.. Waarschijnlijk wordt The Division 2 volgend jaar pas een game die me misschien weer mijn videokaart doet upgraden, als ik in die game teveel zaken moet gaan uit- of lager zetten om fatsoenlijke fps te houden.. De GTX 980Ti was destijds ook niet goedkoop vond ik met een prijs van rond de 750 euro, maar gaat inmiddels dus wel al meer dan 3 jaar mee..
RTX 2080 Ti;2;0.40483593940734863;Tuurlijk is hij duur, als je geen echte concurrentie hebt...
RTX 2080 Ti;1;0.3467690050601959;Wanneer zullen de pre-orders eigenlijk de deur uitgaan?
RTX 2080 Ti;2;0.36045828461647034;Leuk dat GTX2080 weer heel snel is, maar we kunnen extra features nog niet gebruiken. Dus je kan 200 Euro sparen of als je nog goed kaart hebt en nog steeds tevreden met 1080p speelt, beter wachten. Want AMD zal wel binnenkort met een antwoord komen en zal vast wel goedkoper zijn zoals we gewend zijn van AMD. Komt er nog iets binnenkort van AMD?
RTX 2080 Ti;3;0.5007022619247437;Ben benieuwd naar de 2070 meestal de betere kaart om te kopen. Of ie moet 4k ultra net niet aankunnen, hij zal wat trager zijn dan de 1080ti
RTX 2080 Ti;1;0.3788756728172302;Waarom niet PUBG op ultra?
RTX 2080 Ti;3;0.32695502042770386;Omdat het een multiplayer game is. Elk potje is anders dus kan je niet met elkaar vergelijken.
RTX 2080 Ti;4;0.6757437586784363;"Ik heb vandaag 10 reviews gelezen en/of bekeken, en ben zeer tevreden met de resultaten. (4k prestatie) Ik heb zelf 2x2080ti van msi met NVlink besteld, met de hoop dat lg/samsung de 5k 49"" monitoren uit brengt. Ik ben bang dat ik dan net 60 fps haal. Zoals Jayz50cents al zei: Je moet de 2080ti niet met 1080ti vergelijken, maar met de Titan. Dan vallen de prijzen reuze mee. En mocht de Bitcoin weer gaan stijgen, wat ik wel verwacht met de winter, dan zullen de prijzen nog hoger worden"
RTX 2080 Ti;2;0.41324836015701294;2080ti is een directe opvolger van de 1080ti en niet de titan. En nee de prijs ervoor valt helemaal niet mee.
RTX 2080 Ti;3;0.5170276761054993;Pas na Raytracing benchmarks kan ik pas zien, in welke categorie ze vallen. Mochten ze meer dan 30% (50% 60) performance halen, dan is het voor mij het geld wel waard.
RTX 2080 Ti;1;0.45465153455734253;Of gewoon helemaal niet naar de namen kijken, en kaarten in dezelfde prijscategorie vergelijken. Dan bieden deze kaarten niet hele geweldige FPS/$. Dat je voorheen een Titan voor dit geld kan kopen zegt eigenlijk al hoeveel NVIDIA graag (of nu kan) de prijzen omhoog duwt. Er zal ongetwijfeld ook een nieuwe Titan komen, maar moet die dan richting 2k$ gaan?
RTX 2080 Ti;5;0.24596959352493286;Hmm, nog bij mensen waar de grafieken niet doorkomen? Edit: Ze zijn er :-)
RTX 2080 Ti;1;0.339729368686676;Idem hier, dacht dat hier het netwerk weer dicht gezet was XD Edit: inderdaad
RTX 2080 Ti;1;0.5258843302726746;nee, is al gefixed
RTX 2080 Ti;2;0.4128163754940033;De minima die op veel plaatsen wordt genoemd mist nog. Zie overal waar dit vermeld wordt nog steeds maar één waarde staan, terwijl bij anderen grafieken weer duidelijk over één waarde wordt gesproken Neemt natuurlijk niet weg dat alleen de fps een zeer achterhaalde meetmethode is omdat je daarmee niet de soepelheid van een game kan meten.
RTX 2080 Ti;3;0.38607051968574524;We meten ook geen min fps, we meten waar mogelijk de frametimes. We hebben nog geen mogelijkheid gehad om de frametimes in grafieken te zetten. Ik wilde de 99th percentiel frametimes als minimum framerates in de grafieken opnemen. Dat is strikt genomen niet correct, weet ik, maar min fps leest voor de meesten makkelijker dan grafieken met frametimes van 8,3ms.
RTX 2080 Ti;2;0.4864140450954437;Erg jammer dat de Geforce RTX 2080 Ti maar 30 a 40% sneller is dan de Geforce GTX 1080 Ti. De afstaand tussen de Geforce GTX 980 (Ti) en Geforce GTX 1080 (Ti) is VEEL groter en is +/- 65 a 75%, zo als je goed kan zien op Guru3D, vind het erg jammer dat Tweakers geen eens de Geforce GTX 980 (Ti) hebben mee genomen. En dat de verschil tussen de Geforce GTX 1080 Ti en Geforce RTX 2080 maar 5 a 10% is (behalve Wolfenstein II: The New Colossus), en in meerdere maar een paar %, wanneer de verschil tussen Geforce GTX 980 Ti en Geforce GTX 1080 veel groter is, 30% of meer. Edit ik heb het over de 4k fps.
RTX 2080 Ti;1;0.7691062688827515;Werkt inderdaad niet.
RTX 2080 Ti;5;0.6846377849578857;ik moet zeggen, geweldig succes voor de marketing team van nvidia. Ze hebben nu een mainstream product, voor een Titan prijs.
RTX 2080 Ti;2;0.5744839906692505;Qua prijs/prestatie verhouding van de 2080Ti tegenover de 1080Ti kunnen deze benchmarks mij niet overtuigen om er meer dan €1300 voor neer te gaan leggen. Eerlijk gezegd vind ik de prestaties op 4K zelfs vies tegenvallen. De truuk van Nvidia om de prijs van de nieuwe topkaart dusdanig belachelijk hoog te positioneren enkel omdat ze dan van hun overtollige 1080Ti's afkomen zonder die te hoeven afprijzen vind ik ook zum kotzen. Hun goed recht, maar ook mijn goed recht om dat soort praktijken niet te gaan sponsoren. Ben toe aan een nieuwe kaart en kan het wel betalen, maar weiger gewoon om er aan mee te doen. Nu dus liever nog tweedehands, of voor in de tussentijd een wat mindere AMD kaart kopen dus.
RTX 2080 Ti;1;0.47109702229499817;Ik weet dat Hardwareinfo en Tweakers hetzelfde moederbedrijf hebben, maar dit is gewoon een kopie van de review op hardwareinfo.
RTX 2080 Ti;2;0.503635823726654;Ik heb het ook al elders neergezet, om precies te zijn onder het benchmarkfilmpje van Jayztwocents op youtube, maar er wordt nu een hoop negatiefs uitgestort over Nvidia. Nu de benchmarks er zijn, valt vooral het snelheidsverschil tussen de 10xx en de 20xx generatie op. Die is niet extreem hoog. En dus valt iedereen weer over elkaar. De nieuwe generatie kaarten wordt een marketingstunt genoemd. Er wordt zelfs beweerd dat Nvidia eigenlijk vooral van de oude kaarten af wil, en daarom een slechte generatie videokaarten de wereld in heeft gestuurd. Ik heb me ondertussen afgevraagd waarom Nvidia hun presentatie op de grootste gamesbeurs van Europa heeft gebruikt voor een Open Universiteit college over raytracing. En ik kom nu eigenlijk tot de conclusie dat Nvidia ons heeft voorbereid op iets heel anders. De prestatieverbetering zit niet in iets wat heel erg meetbaar is. Raytracing en AA zijn niet makkelijk meetbaar. Het verschil is vooral een verschil dat je met het blote oog kan zien. En dan kom ik terug mijn begintijd als computeraar en gamer. Toen werd er een stap gemaakt van ega naar vga. Van 16 naar 256 kleuren. En van plaatjes in tijdschriften die lieten zijn hoe veel dat verschil maakte. Tijdschriften zijn vervangen door internet, maar ik denk dat we een vergelijkbare tijd tegemoet gaan. En dan gaat het niet meer om fps. Wel om of een spel smooth speelt en aanvoelt, ongeacht het aantal fps.
RTX 2080 Ti;4;0.39343538880348206;Nou, leuk allemaal. Maar ik denk dat ik de goede keus heb gemaakt om tweedehands een GTX 1070 Ti aan te schaffen ter vervanging van mijn GTC770. Voorlopig kan ik weer een jaar of twee vooruit met mijn vijf jaar oude pc op 1080p.
RTX 2080 Ti;5;0.653433084487915;"Heerlijke positie hebben die videokaart makers ook he, ons steeds weer een nieuwe worst voor houden. Doen natuurlijk alle bedrijven, maar hoe revolutionair zou het zijn als er eens een bedrijf bij zat wat zou zeggen: ""Luister, dit jaar geen verbeteringen, ben gewoon eens blij met wat je hebt, ga eens wat vaker naar buiten, praat eens wat vaker met je vrienden/familie, het zijn maar spullen. Volgend jaar doen we weer mee."""
RTX 2080 Ti;1;0.5952426791191101;Hoe warm worden deze kaarten? 1080 GTX Ti vergeleken met 2080 RTX En waarom is het prestatie verschil zo weinig terwijl het RAM zoveel sneller is? DDR4 naar GDDR5 was ook huge waarom is dit nu bijna niks meer?
RTX 2080;2;0.4962785243988037;Ik moet toegeven dat vooral de RTX 2080 sneller is dan ik had verwacht. Ook de 2080 Ti doet het beter dan ik had verwacht. Maar de prijs prestatie verhouding is nog steeds ronduit slecht. Dus in mijn ogen kan je nog steeds beter voor een 1080 Ti gaan of een goedkope RX Vega als je die ergens kan krijgen. 1080 kan ook mits je niet van plan bent met HDR aan de slag te gaan want bij de oudere kaarten werkt volgens mij bepaalde memory compressie niet als er 10bit kleur gebruikt wordt en zakken de prestaties flink in. De 1080 Ti vangt dat deels nog op door zijn 384 bit bus. Tweakers heeft waarschijnlijk getest met een 1080 Ti FE welke toch snel 5-9% trager is als de non refferenced kaarten die de meeste mensen kopen en welke vaak ook nog goedkoper zijn. Als je dan de RTX 2080 er naast zet is de snelheid winst wel heel minimaal. Ik vraag mij ook af hoeveel sneller non refferenced RTX kaarten worden nu nVidia een nieuwe dual fan cooler heeft. Ik denk dat dat verschil aanzienlijk kleiner is. Als je hier kijkt naar de strix OC is dat maar 2%: Dus in mijn ogen moet je als je de review volledig wil maken ook even een aftermarket 1080 Ti kaart mee nemen. En ik wil nog wel even aanhalen dat de claim van 50% winst dus lang niet gehaald wordt. Dat is alleen maar in specifieke cherry picked gevallen in het geval van de 1080 dus 4K (waar die kaart niet voor gemaakt is) HDR (waardoor je een extra zware penalty krijgt door de beperkte geheugen bandbreedte op die kaart) Het is wel mooie hardware maar veel te duur en dus alleen maar nuttig als je perse koste wat het kost meer prestaties wil hebben dan een 1080 Ti.
RTX 2080;3;0.4414820671081543;"GamersNexus heeft een uitgebreide review waarbij ze de RTX 2080 FE vergelijken met o.a. een 1080 Ti SC van EVGA. Performance is inderdaad gelijk / verschillen zijn verwaarloosbaar tussen te 2 kaarten; Overigens wordt na het lezen van enkele reviews al snel duidelijk dat de 2080 FE eigenlijk eigenlijk een ""custom model"" is. Prima koeler, prima clocks, deze prestaties liggen dan ook veel dichter in de buurt van de custom 2080's, zoals je al aangaf is een Strix 2080 slechts 2% sneller (wel wat koeler en stiller)."
RTX 2080;3;0.4491429030895233;Inderdaad en dan betaal je dus 200 euro meer voor DLSS en RTX wat je niet altijd kan gebruiken. Niet echt de moeite als je het mij vraagt. Over 2 generaties wordt dat wel interessanter (vooral RTX, DLSS is nu opzich al nuttig maar niet nuttig genoeg om die prijs te rechtvaardigen)
RTX 2080;2;0.5447322130203247;Lijkt meer aan de manier van testen te liggen, cpu bottleneck. Zie bijvoorbeeld techpowerup bij een game waarbij de gpu het niet trok: Daar zie je 60.2 fps voor de 1080 en 79.2 fps voor de 2080. De 1080 ti zit daar op 70.7 fps en de vega64 haalt slechte 57 fps. De 4k resultaten wijzen ook op grote verschillen in ruwe prestaties. Maar op het moment dat je baseline al 150 fps is (Battlefield 1) komt er een moment dat je tegen de limieten van de cpu dan wel de engine loopt.
RTX 2080;3;0.4441544711589813;Ik kijk liever naar 1440p of 4K waar je minder last van de CPU hebt. Het verschil is daar +- 5-8% tussen de 1080 Ti en de 2080 maar Techpowerup heeft een 1080 Ti FE!! en geen aftermarket wat een eerlijkere vergelijking is dan kan je er zo 5-9% bij op tellen voor de 1080 Ti en is het gat met de 2080 dicht geslagen.
RTX 2080;3;0.3097013831138611;Waarom zou je een 1080 Ti custom met een 2080 FE gaan vergelijken? Dat is toch juist appels met peren vergelijken.
RTX 2080;3;0.46808215975761414;Omdat de meeste mensen een non refferenced gekocht vinden omdat ze die blowers niet goed genoeg vinden. De nieuwe FE hebben wel dual fan's en geen blowers meer. Daarnaast mogen ze hem van mij ook met een non refferenced 2080 vergelijken. Maar heel weinig mensen hebben een 1080 Ti FE die non ref waren een stuk beter. Dus vandaar dat dat een betere vergelijking is. De 2080 FE en 2080 Ti FE zijn in dat opzich wel beter dan de voorgaande versies. Denk dat minder mensen het nu nodig vinden een non refferenced versie te kopen.
RTX 2080;2;0.33620402216911316;Ja maar daarom testen ze FE tegen FE, dan zou het dus betekenen dat de non reference 2080(ti)'s ook sneller zijn dan de reference (en goedkoper) en dan moeten ze de non reference 2080(ti)'s testen tegen non Ref.... Wat dus per saldo het zelfde oplevert.....
RTX 2080;2;0.535461962223053;je begrijpt me niet goed. De FE kaarten van de 20x0 generatie hebben een beter PCB, betere cooling en hogere clocks dan de normale 2080 en 2080 Ti kaarten. Dus deze FE kaarten zitten dicht op de 2080 Ti OC kaarten aan qua specs waar die eerst lager zaten zitten de FE nu hoger dan een standaard 2080 / 2080 Ti. Die standaard kaarten hebben een lager TDP en lagere clocks. Voorheen hadden FE kaarten en standaard pascal 1080 Ti kaarten dezelfde clocks en de factory oc kaarten waren sneller deze generatie is dat dus anders en is het gat kleiner. Er komen straks dus stock kaarten van Asus, MSI en GA die langzamer zijn dan de FE kaarten en er komen factory OC kaarten die vergelijkbaar zijn en wellicht ook iets sneller.
RTX 2080;3;0.4402780830860138;Er komen kaarten die langzamer zijn en kaarten die sneller zijn, er zullen fabrikanten komen die een veel snellere kaart gaan leveren. % sneller dan 20x0 FE zal min of meer gelijkwaardig zijn aan %sneller dan FE 10x0. Het punt is dat een FE - FE vergelijking het beste is zowel in prijs als in features aangezien het de visie van de fabrikant was toen en nu. Een FE van 20x0 testen tegenover een kaart van no FE slaat nergens op. Het is duidelijk dat de kaarten meer cuda cores hebben, duurder zijn maar voor het aantal cuda cores- prestatie verhouding niet heel veel sneller zijn dan pascal. Maar de benchmarks laten zien dat de kaarten wel degelijk aanzienlijk sneller zijn en die 5% meer tav third party1080ti's niks doet, straks blijkt dat de 2080 een prima OC'er is en komen er bedrijven met standaard 10% meer . Verder hebben de 20x0 kaarten technieken die waarschijnlijk door grote bedrijven wel gebruikt gaan worden en dat kaarten nog sneller zullen maken. Met een 1080ti weet je zeker dat dit je max is en je geen raytray zal hebben. RT in BFV is momenteel beperkt tot 1080p maar het is de bedoeling dat de game uiteindelijk gerenderd wordt op 4k maar de raytrace effecten op 1080p of 720p wat mij een prima compromis lijkt. Aangezien de schaduwen en reflecties best iets lagere res mag zijn om nog steeds mooi over te komen. Maar de gameplay footage van RTX on/off (van anderen dan nV) ziet er gewoon indrukwekkend uit. De kaarten hebben simpelweg veel meer rekenkracht dan de specs en benchmarks nu laten vermoeden. Alleen komt dit niet tot zijn recht in simpele fps. Voor mij is dit een prima ontwikkeling, ik hoef geen 200fps een constante fps maar met betere schaduwen en licht effecten zijn voor mij belangrijker en dit is een goede eerste stap van nV. Ze hadden ook gewoon simpwel weg een nieuwe kaart kunnen introduceren met hogere klok minder extra's, je kan het al grootste gewoon ook nooit goed genoeg doen voor sommige.
RTX 2080;1;0.3870369493961334;"De FE's worden nu gemarket als 'de' kaart die het maximale uit Turing haalt; ze krijgen zelfs een hoger TDP mee. Dus dit is het omgekeerde van wat de voorgaande FE's waren. Er zit nu een fatsoenlijke koeler op en ze hebben meer ruimte voor GPU Boost. De FE/FE vergelijking gaat dus totaal niet op, integendeel, dan ben je jezelf voor de gek aan het houden."
RTX 2080;2;0.33924224972724915;een discussie is vrij moeizaam als er niet gelezen wordt. precies, nv heeft dit jaar een andere strategie dan bij de 10x0, wie is de fabrikant van deze gpu ' s... nv. dus wiens product en visie wil je bekijken die van de fabrikant, niet die van asus, niet die van gigabyte en niet van nog een andere. nv levert dit jaar kaarten die het maximale uit turing halen, ze nemen simpel weg de best gebinde gpu ' s, maar oc ' en de kaarten niet tot max. dat gaan 3den wel doen en kan je zelf ook doen met de fe ' s ( want je weet dat je voor die prijs de besten krijgt ) dit jaar kan je dus het beste een fe halen als je zeker wilt zijn van de beste ( binned ) gpu. ( nv levert ook de software : api / dll ' s voor third parties om oc utilities te maken ) de 3th party 1080 ' s zijn niks anders dan factory oc ' d kaarten met hogere tdp ' s en betere coolers. ik ben geen fanboy maar ik hou van techniek en deze kaart heeft nieuwe features die echt niet alleen maar marketing gebral is. ( ik hoop ook dat de modding community gebruik gaat maken van de nieuwe features - > civ 6 - > tensor cores : met echte intelligentie vs nu verder is misschien ook nog wel handig om te melden dat dice letterlijk een ' aantal ' weken ( weet niet exact hoeveel, maar kort ) de tijd heeft gehad om rtx toe te passen. dus je kan je voorstellen dat een ontwikkelaar met meer tijd iets leuks kan bedenken. zeggen dat rtx / tensor nu niks toevoegt is is correct maar wel een zeer beperkte visie, want het potentieel is er. naast het feit dat de kaart gewoon snel is. en een enkele blik op steam laat zien dat nv heer en meester is in gaming kaarten iets van niet onopgemerkt is bij ontwikkelaars. interessant is ook wat amd gaat doen aangezien zij ( als het goed is ) al ver zijn met navi en waarschijnlijk niet snel antwoord gaan hebben op rtx / tensor of de toch al hoge game prestaties van nv.
RTX 2080;1;0.8552237749099731;"een heel verhaal dat nul komma nul toevoegt aan die ' discussie ' die volgens jou niet gelezen wordt. je noemt niets dat ik niet al weet of dat er nog niet stond. de discussie is niet ingewikkeld en geklets over ' de visie ' van nvidia... alsjeblieft zeg. turing is een belachelijk slecht geprijsde generatie waarin er qua perf / dollar niets verandert en de fe is slechts een van de vele aspecten waarmee nvidia probeert om dat beeld iets bij te draaien ( met geklets over enorme oc ruimte, terwijl dat vantevoren al vast ligt. pascal overklokt vrij weinig buiten het eigen gpu boost en turing gaat nog een stap verder. de fe is nu gebouwd om out of the box al te doen wat ie kan ). dat lukt ze niet en rtx is tot zover een technologie die op geen enkele van de drie turing releases tot echte meerwaarde leidt. het is te zwaar voor fatsoenlijk 60 fps of het is te licht waardoor je beter gewoon op de oude manier kunt werken omdat er anders van accurate reflecties nog minder overblijft. je zegt ik ben geen fanboy, dan zou het je sieren om een beetje nuchter te blijven en niet zo mee te gaan in het marketing gelul van de fabrikant. want meer dan dat breng je eigenljk niet in. je hebt geen enkele bron aangevoerd die mij overtuigt dat rtx tot iets substantieels gaat leiden. want die bronnen bestaan simpelweg niet... overigens nog zoiets waaruit blijkt dat je er weinig van snapt : denken dat de fe "" s beter gebinned zijn. als dat het geval was, dan kregen ze geen hoger tdp budget mee. binning betekent namelijk dat je met hetzelfde tdp hogere clocks haalt. pascal toonde al aan dat binning vrijwel niets meer oplevert dankzij gpu boost 3. 0, en turing verandert daar niets aan. verder is bij pascal geen enkele aib kaart gezegend met een hoger tdp budget - de bios is gewoon locked en overal hetzelfde. ook dat schijn je nog niet te hebben ontdekt. kortom : tijd om je wat beter te gaan informeren ipv louter ' enthousiast ' te zijn over nieuwe technologie."
RTX 2080;2;0.40891319513320923;"waar precies ben ik niet nuchter? jij leest alleen wat je wilt lezen en je stuk hierboven gaat niet zozeer over wat ik zeg maar voornamelijk wat jij over nv denkt. ( oooi je kon niet wachten om dat fanboy stukje er tussen te gooien, daar heb je mij te pakken!! ) de rek is uit de gpu scaling ( zie amd ervoor ) ze gaan overstappen naar multi - chip ontwerpen ik verwacht dat dit de laatste van de echte single die gpu ' s zal zijn ( van nv ) dat zie je ook wel en amd zal wel al de aanstaande gen komen met multi chip en interconnect ontwerp ( navi ). om terug te komen op nv, dit zie je ook aan het feit dat ze weer volop inzetten op sli ( nvlink ) functionaliteit. of de benchmarks van gamers nexus. ik zeg dus nergens dat turing zo snel is, ik zeg dat hij 20 % sneller is dan de xp en heeft de extra goodies. voor de zelfde prijs. die goodies vertalen nu nog niet naar fps. fps is het enige wat telt!!! deze kaart heeft een factor 10x meer raytracing kracht dan een 1080ti. "" maar je hebt er nog niks aan "" er zijn meer mensen enthousiast. ( nog zo ' n fanboy ) - - > lees ook de conclusie. grootste fout die nv heeft gemaakt is dat er geen software voor is, maar dit begint het kip - ei verhaal te worden. is dit nog steeds de ' vega ' frustraties? je toont alleen maar aan niks anders te zien dan fps, niks te begrijpen van wat de toekomst gaat brengen ( want nv is marktleider ), niet snapt wat rtx is en hoe belangrijk het is ( ook al is het nog maar een pril begin ). rtx gaat meer toevoegen dan fps of resolutie ooit heeft gedaan en rtx is het keerpunt van fps naar foto realisme in gaming, want zonder raytracing is dat namelijk niet mogelijk. rtx memes heb ik ook goed om kunnen lachen, ik laat het hierbij. blijf lekker de frustraties van je afschrijven, dat houdt je van de straat."
RTX 2080;2;0.46768325567245483;je doet nog steeds net alsof ray tracing iets nieuws is. het enige verschil is dat het realtime gebeurt. dat is voor content creation een prachtig gegeven en daar ben ik het helemaal mee eens. de nieuwe quadros - prima producten. rt cores in een gaming gpu? complete waanzin - er is nog altijd een goede reden dat we dat lekker vooraf deden en met wat trucjes om het efficient te houden. daar veranderen 10 giga rays nog niets aan, en als je nu al kijkt naar wat voor die space er nodig is om dat te produceren... maar goed we vallen in herhaling - agree to disagree. vega frustraties zijn mij vreemd en fps ( raw performance ) is inderdaad het enige dat telt - dat klopt. de markt bepaalt inderdaad, en tot zover is rtx op een geforce gpu zwaar ondervertegenwoordigd en gezien het prijspunt gaat dat voorlopig niet veranderen. ondertussen heeft amd de consoles in handen en nul komma nul hardwarematige rtrt ondersteuning. en wat is ook alweer de echte drijvende kracht achter vooruitgang in games tegenwoordig? oh ja. consoles want : aantallen, massa, en laagdrempelig. rtx : niche, duur, hoge drempel, geen massa. vergelijk het met vr : dat komt als niche ook zo lekker van de grond. en daarover riep men hier precies hetzelfde : ' dit is de heilige graal van gaming '. nee. echt niet. de heilige graal van gaming is content, inhoud, goede gameplay. de sterkste games aller tijden zijn nog altijd degenen met hoge replay value, met sterke mechanics. dat zijn de system sellers, en dat zijn de zaken die mensen overhalen om ergens in te stappen. en dat produceer je alleen maar met talent - niet met een slim algoritme om lichtstralen te berekenen. dromen mag, dus droom lekker verder maak alleen niet de fout om een kritische blik te verwarren met ' frustraties ', want dan houd je alleen jezelf voor de gek.
RTX 2080;2;0.4390271306037903;Ben ik het niet mee eens was ik het ook niet met de vergelijking 980 Ti vs 1080. En nu met de nieuwe FE helemaal niet. De meeste mensen gebruiken non refferenced. Dus door die mee te nemen krijg je een realistischer beeld over wat de nieuwe kaarten bieden tov wat mensen hebben. Allemaal leuk en aardig die extra features die nVidia ons wil aansmeren maar het gaat er om wat we er aan hebben. DLSS zie ik wel als iets nuttigs RTX simpelweg nog niet tijd zal het leren. Misschien hebben we nog 1-3 generaties nodig. Dat is opzich niet erg. Maar als je dan kijkt naar de prijs zeg ik nee. En dan zeg ik dan liever een 1080 Ti of goedkope Vega kaart en dan kijken we de volgende generatie wel weer. 980 FE > 1080 FE was 71% verschil 1080 > 2080 is slechts 40% in het geval van de FE en stock versie 35%. Maar ze zijn wel een stuk duurder Ik vind de RTX 20x0 kaarten slechte value for money geven. Als jij dat niet vind prima ik houd je niet tegen om zo'n kaart te kopen.
RTX 2080;1;0.3102377951145172;Waarschijnlijk doet @Astennu dat omdat de nieuwe reference koeler dual fan heeft en die van de 1080 Ti maar een single fan.
RTX 2080;4;0.37339863181114197;Ook bij DLSS is het maar de vraag of het van toegevoegde waarde is. Op hogere ppi monitoren is aa voornamelijk bedoeld om 'pixel-crawling' tegen te gaan. Over het algemeen slagen standaard aa technieken daar prima in, vaak volstaat de laagste instelling prima bij hardware matige aa. Ray-tracing is leuk voor professionals. Mooie maar vooral vlotte renders en geloofwaardige animatie/cgi/games. In-game gaat niemand met 40-50fps gamen op 1080p met een peperdure gpu. Van den zotte. Eigenaardige keuze's van Nvidia, zowel qua productie als marketing.
RTX 2080;3;0.47150564193725586;"Op hoge PPI monitoren is het idd niet nodig. Dit is meer voor 1080p en 1440p maar 27""4K heeft niet echt AA nodig."
RTX 2080;2;0.2969549894332886;Het rtx deel van de 2080ti komt bij rendersoftware veel beter tot zijn recht. Games profiteren nauwelijks. Unicorn profiteerde optimaal van de nieuwe kaarten. Rendersoftware speelt sneller in op de nieuwe mogelijkheden. Het is een onderdeel dat ondergeschikt is aan de game industrie, maar toch een belangrijk onderdeel
RTX 2080;3;0.6547603011131287;Maar dat kon opzich ook al redelijk snel op de cuda cores of op de shaders van AMD kaarten. Als je die RT cores en Cuda cores tegelijk kan gebruiken om te RT te renderen is het wel een voordeel bij die software.
RTX 2080;2;0.600323498249054;Mij valt het tegen. Ik rekende op 35% bij techpowerup en dat is toch aanzienlijk minder. Op 1080p is het te verklaren door cpu bottleneck en de hoge framerates die bij sommige games worden behaald. Maar ook op 4K wordt de 35% relatief niet gehaald (50% onderling). De 2080 FE blijft steken op 31%:
RTX 2080;3;0.3560175895690918;Je gaat hier wel compleet voorbij aan het Raytracing gebeuren, wat eigenlijk de echte seller feature van deze kaart is. Iets wat geen enkele andere kaart nog kan. Wat dat uiteindelijk gaat doen weten we nog niet. Wat mij betreft is deze review ook gewoon nog niet af. Ik wacht rustig af tot we kunnen zien wat raytracing echt gaat doen.
RTX 2080;3;0.42620500922203064;Ray-tracing is al uitgebreid gedemonstreerd in bf5. Prachtige reflecties in waterplassen, diffuse reflecties op auto's en vieze ramen. Maar op 1080p met matige framerate. Een paar stappen vooruit, en nog meer stappen terug.
RTX 2080;3;0.4692477583885193;"Daar heb ik al genoeg over gezegd in het verleden. RTX is nu een gemmick niet meer niet minder. Het is er voor de game Dev's dat die aan de slag kunnen maar de performance hit is nog veel te groot. BF5 draaide 1080p met minder dan 60FPS. Er gaan nu ook al geruchten dat de effencten in de final release flink terug geschroefd worden om het speelbaarder te maken maar daardoor dus ook minder ""mooi"" daarnaast vond ik het ook niet eens altijd mooi. Soms was het echt to much en niet realistisch hoe het ea reflecteerde. Dan vond ik de oude manieren die gebruikt werden voor reflecties mooier. Dus ja ik ga daar met opzet aan voorbij alleen DLSS vind ik echt interessant. RTX zie ik pas over 2-3 generaties nuttig worden."
RTX 2080;2;0.4611138105392456;"De rant van Linus was wel 'onpoint' over het hele release/RTX debacle; Ik persoonlijk vind de prijzen belachelijk."
RTX 2080;1;0.4629884362220764;De prijzen zijn inderdaad belachelijk. Als je ziet dat je voor 700 euro vandaag een 1080 Ti kan kopen die 0 a 5% langzaamer is dan een 2080 maar 200 euro minder kost dan is de keuze snel gemaakt... Bij elke vorige generatie kaarten kreeg je bij een nieuwe generatie voor de zelfde aantal euros (veel) meer performance. Nu krijg je ongeveer de zelfde performance voor (veel) meer geld, wtf !?!?!
RTX 2080;2;0.3374275267124176;Ik vind het nog te vroeg om te oordelen zolang er geen game benchmarks met raytracing en dlss zijn. Wie weet haal je dan een veel hogere fps en/of een veel mooier beeld. En in het verleden viel er ook nog heel veel winst te halen met driver en game optimalisatie. Het is ook maar de vraag of, en hoe lang de kaart de adviesprijs kost.
RTX 2080;2;0.47733137011528015;Dat is een amd kaart kopen in 2012-2014 omdat je dacht dat dx12/mantle/vulkan de toekomst was. Uiteindelijk maar mondjesmaat games uitgekomen die het werkelijk gebruiken. Raytracing is er nu niet, je beoordeelt een kaart voor wat hij nu kan, niet in de toekomst. Je kan het wel aangeven. Het probleem is dat rtx kaarten vooralsnog niet sterk genoeg zijn om volledig raytracing te gebruiken. Het wordt dus alleen in specifieke gevallem gebruikt (light inval, schaduw) en daar moet speciaal voor geprogrammeerd worden (gameworks 2.0). Mondjesmaat games zullen het ondersteunen. Raytracing is de toekomst. Maar dat was dx12 oorspronkelijk ook. Het is nog te zien hoe ver we komen, hoe snel we gaan. Voor nu, met de normale games, presteren ze buitenmaats. De 1070 was wat sneller dan een stock 980ti en de 1080 ging dat voorbij. Nu is de 2080 even sterk als de 1080ti, maar duurder, de 2070 alvast zwakker EN ook nog eens duurder dan de 1080 waarschijnlijk (men verwacht een prijs van 500€).
RTX 2080;2;0.4302045702934265;Ok. Maar als niemand die kaart gaat kopen, gaan de game ontwikkelaars hier ook niet mee aan de slag. Want waarom tijd steken in iets wat niemand wil hebben. Dan komt er nooit vooruitgang. Het is altijd zo geweest dat degenen die als eerste met nieuwe technologie aan de gang gaan te veel betalen en soms achteraf iets hebben wat nooit echt gebruikt gaat worden. Maar als iedereen zo dacht en nooit nieuwe technologie wou kopen waren we nooit ze ver gekomen als dat we nu zijn en kon je nog steeds pixels tellen in je game.
RTX 2080;2;0.49458566308021545;Het zal waarschijnlijk nog wel een tijdje duren voordat raytracing gemeengoed wordt. Net zoals bij de introductie van DirectX 10. Performance viel bij introductie ook behoorlijk tegen dus ik verwacht dat dit niet anders zal zijn bij deze eerste generatie raytracing kaarten. Wat mij betreft sla ik nog wel een generatie over met deze prijzen.
RTX 2080;2;0.4477587938308716;En wie weet is tegen de tijd dat die echt beschikbaar zijn, liefst zonder kinderziektes, de volgende generatie Nvidia-kaarten aangekondigd. Ik vind het ook te vroeg om te oordelen, maar daarmee dus ook zeker te vroeg om 1200 euro te investeren. Beetje een kip en ei verhaal, natuurlijk, maar voor dit soort bedragen zou ik zelf toch liever even afwachten wat er qua implementatie gebeurd - het is niet alsof het met DX12 nou echt hard ging.
RTX 2080;5;0.33965247869491577;De kaarten zijn en je kunt ze kopen en gebruiken om te gamen en dus kun je ze ook beoordelen. Dat zijn ook al de reviews die nu uit komen. Stel nu dat de games die de bijzondere technieken gebruiken pas uit komen over 2 jaar, ga je de kaarten dan pas beoordelen of sla je het beoordelen dan over omdat er een nieuwe generatie kaarten uit is gekomen?
RTX 2080;2;0.4384497404098511;"Op dit moment, op de release en zonder dat er games zijn die er gebruik van maken kan je ze beoordelen als ""veel te duur"". Zeker op de lagere resoluties. Maar de beelden uit nvidia's eigen presentaties waren wel veelbelovend. Als ik op zoek was naar een high end game pc zou ik op zijn minst wachten tot de eerste game met ondersteuning voor deze kaart uitgebracht is. En als ik het goed gelezen heb zou dat dit jaar nog moeten zijn. De 1080 ti is tenslotte ook niet goedkoop maar de meerwaarde van de 2080 zie ik in ieder geval als er voldoende ondersteuning komt voor de nieuwe functies. En dan heb ik het nog niet eens over driver / game optimalisaties."
RTX 2080;1;0.47751346230506897;Er zijn geen raytracing benchmarks omdat er nog vrijwel geen games zijn die dit ondersteunen. Op dit moment zouden we als consument de hele nieuwe versie van Nvidia moeten boycotten totdat ze of met bizar goede benchmarks komen voor games of met normale prijzen.
RTX 2080;2;0.5073160529136658;Maar dat is toch ook een beetje kip en ei verhaal. Een game studio kan niets maken voor iets wat nog niet bestaat (hardware raytracing) En nvidia kan geen real life resultaten laten zien zonder dat er games zijn die dit ondersteunen. Al had er natuurlijk wel wat meer samenwerking mogen zijn zo dat er direct tenminste 1 populaire game beschikbaar was. Eindelijk is er een mogelijke vernieuwing op het grafische gebied, in plaats van dat kaarten alleen maar sneller worden. En daar zullen best nodige ontwikkelingskosten aan hangen. Bovendien kost hij maar net zo veel als een telefoon in het hoogste segment, die mensen na 2 jaar inruilen. Ik heb verder niets met nvidia ofzo, maar ik kan het wel zeer waarderen dat er nieuwe technische ontwikkelingen zijn die ook nog eens beschikbaar zijn voor de consument. Het valt me op dat er op een tech site zo veel weerstand is tegen nieuwe technologie en dan voornamelijk om de prijs. Natuurlijk moet de nieuwe technologie zich nog bewijzen en hopelijk kan dat binnenkort met de eerste game die vrijgegeven wordt, maar om te gaan roepen om het product massaal te gaan boycotten vind ik gewoon idioot.
RTX 2080;3;0.28501376509666443;Kennelijk is je kaart gelocked op 1080p/60fps wanneer je raytracing toepast.
RTX 2080;1;0.44479191303253174;goh je zou ongeveer 2 jaar geleden voor 700 eu zon kaart gekocht hebben.... wat een prestatie kroon was t toen toch. nu nog geen afschrijving
RTX 2080;3;0.3026220500469208;Dat laatste stukje tekst beantwoord al julie vragen als je de sbelste wilt hebben dan moet je diep in de buidel tasten dan kom je dus op die 2080 ti uit.Maar daarintegen wat koste die 1080 ti en die titan xo vlak na de launche?Wat je het beste kunt doen is als volgt als je een videokaart wilt aanschaffen en je hebt een Nvidia lager dan de 1080 dan kun je beter voor die 1080 gaan omdat die alles draait. Als je al een 1080 hebt en je wilt sneller dan moet je die 2080 serie kopen. ik perssonlijk zal nooit zo een dure kaart kopen omdat hij gewoonweg te duur is. Maar ben er wel aan een nieuwe toe, ikdraai nu op een 630 gt van eind 2012,en ben dus ook aan vervanging toe.Het zijn keuzes wat je moet maken, maar denk eraan voor die prijs van de 2080 heb je een nieuw computer systeem.
RTX 2080;4;0.37121495604515076;Als je een beetje gamed, dan ben je inderdaad zeker aan vervanging toe lol.
RTX 2080;1;0.5099411010742188;Of in Juni dit jaar een 1080Ti OC voor 999,-
RTX 2080;3;0.5473088622093201;Je vergelijkt hier appels en peren. Beetje als een kale auto vergelijk met een met alle opties en dan zeggen dat die met aalle opties nog trager is ook omdat hij meer gewicht heeft maar wel duurder is. Je moet dus 1080 ti vergelijken die raytracing software matig emuleert van de zelfde kwaliteit en dlss emulatie doet van de zelfde kwaliteit met een 2080. Dan weet ik wel wie er een heel stuk sneller is.
RTX 2080;2;0.4903729557991028;"Sorry makker, maar als jij de 1080ti een ""kale auto"" noemt dat weet ik niet wat jij een Ferrari zou noemen. En appels met peren vergelijken? Nee dat totaal niet. Het zijn twee videokaarten dat ja, maar dar raytracing gebeuren dat moet je zeker deze generatie, nog met een korrel zou nemen. Want er zijn maar zeer weinig games die hier gebruik van kunnen maken. En waarschijnlijk is de impact op je FPS enorm. Ik bedoel, zoveel mensen zijn er toch ook niet die PhysX en HairWorks gebruiken. Dit komt doordat de standaard methode prima werk en de FPS het niet waard is. Wat wel goed is, is dat NVidia blijft vooruit gaan, wat zeer goed is voor de consumenten wat helaas niet zo goed is zijn de prijzen."
RTX 2080;1;0.6290396451950073;Ja die prijs is nog tot daaraan toe het zijn idd ook belachelijke prijzen maar aangezien Nvidia een Amerikaans bedrijf is moet je ook nog invoerrechten betalen, tenminste als je die kaart in Amerika zou kopen. Nu weet ik ook nog niet als je bij een importeur koopt die die kaarten met honderden tegelijk in een inkoop doet wat dan de meerprijs is. Buiten dat zitten we nog in Nederland waar je dus best veel belasting moet betalen en dat komt er ook nog eens bij.Plus Nvidia moet nog eens winst maken ,het bedrijf wat die kaarten met de boot of in iedergeval transsportkosten moeten allemaal met de prijs inbegrepen zijn. En dan heb ik het ook nog niet eens over de r&d kosten die met elk product kosten zijn gemaakt in de voorm van dat die ingeneur ook nog betaald moet worden.En daarom zijn die kaarten met de launche zo duur.
RTX 2080;5;0.36020565032958984;Heel eenvoudig om hierop te reageren, de nieuwe kaarten massaal links laten liggen.
RTX 2080;2;0.4380050003528595;Ik ben het met je eens dat de prijzen veel te hoog zijn, maar dat zijn de 1080ti sowieso. Nu kan je bij alternate al een 2080 voor 799€. Geef dat een maand of 2 a 3 en je zal zien dat de prijzen behoorlijk zakken, Het is toch ook logisch dat een kaart die nieuwer is en beter presteert en nog een dure extra future heeft(of dit nu wel of niet nuttig is staat daar los van ) dan vind ik het nog wel meevallen. Maar een 2080ti zou eigenlijk niet meer dan een euro of 600 mogen kosten, dat zou schappelijk zijn. Nu maar hopen dat die hele fake mining onzin eens stopt, en dan zullen de prijzen voor de videokaarten weer een beetje normaal worden.
RTX 2080;1;0.4121157228946686;Daar zit idd niks gelogen bij Geen raytracing, geen dlss, kaarten & drivers op vrijdagmiddag binnen. Gelukkig presteren ze prima op traditionele/rasterized games, maar die prijs...
RTX 2080;1;0.736368715763092;"Eigenlijk een hele slechte launch en dan ook nog een aantal dagen verzet, wat vreemd is. Er heeft veel tijd gezeten tussen de generaties. En voor een kaart die ze door hun concurrentie positie zo kunnen aanbieden behoort ook wel wat premium aangevuld te worden met games of leuke hardware extra's. Het is wachten op de ""echte"" benchmarks, vergelijk het met nu een 8k tv nschaffen. Zeker goed en nuttig maar ....."
RTX 2080;2;0.38531413674354553;Nee, dat is puur omdat generationele prijsverhoging 70% bedraagt en de prestaties dat niet reflecteren. ja, feeding the trolls
RTX 2080;2;0.44463786482810974;Humor? Volgensmij is iemand kleineren om zijn of haar (beweerde) inkomen verre van humor. Je vergelijkt appels met peren. Magic the gathering kaarten zijn altijd al vrij prijzig geweest, volgensmij. Je kan ook moeilijk een kaartspel benchmarken dus intrinsieke prijs is altijd moeilijk vast te knopen met dat soort zaken. Een auto als een Ferrari of een Lamborghini koop je niet alleen omdat deze alleen wat harder kan rijden. Een grafische kaart koop je, lijkt mij althans, om de performance. Er wordt toch nergens gezegd dat je het niet moet kopen? Er wordt alleen gezegd dat de prijsverhouding scheef is. Dat de een geen probleem heeft met de prijs is uiteraard subjectief. Zoals ik al zei, of je het wel/niet wil kopen moet je natuurlijk helemaal zelf weten. Helemaal niks mis mee als je een 20xx series kaart haalt. Het is puur een punt van kritiek dat de kaart relatief duur is. Wat je met die info doet, moet je zelf weten. Beetje raar dat je je comment wijzigd om op onze reacties te reageren, maargoed.
RTX 2080;1;0.6543413996696472;Een uitspraak van zeer laag niveau in mijn optiek. Praktisch alle reviewers hebben dezelfde conclusie: de prijs/performance verhouding is scheef bij deze kaarten. Je krijgt zo'n 30% meer performance voor 70% meer geld. Het gaat er niet om of iemand het wel/niet kan betalen. Als reviewer wordt aan de consumenten gezegd dat dit product te duur is voor wat het is. Of je het wel/niet koopt is natuurlijk je eigen keuze, maar vanuit een economisch standpunt is het niet een goede koop.
RTX 2080;1;0.5451894998550415;Dan had NV de omstandigheden moeten bieden om een eerlijke vergelijking te kunnen maken. Ik vind het onbegrijpelijk dat je een kaart op de markt brengt met een technologie, waar je op geen enkele wijze, praktisch gebruik van kan maken. Dan kan je het technologiesites niet kwalijk nemen dat ze reviews schrijven met traditionele benchmarks, want er is niks anders, in ieder geval, niet iets wat makkelijk voor handen ligt. Vervolgens vraag je een belachelijk hoge prijs en moet de consument het maar vertrouwen dat die nieuwe functies het echt waard zijn. Los van het feit of Raytracing echt iets gaat worden natuurlijk, want daar heb ik wel vertrouwen in eigenlijk. NV heeft de plank echt volledig misgeslagen. Ik hoop echt dat AMD deze kans aangrijpt om hun stempel weer op de markt te drukken, want dit soort praktijken zouden gewoon keihard afgestraft moeten worden door marktwerking.
RTX 2080;1;0.4695446193218231;"""Hee man kan je deze testbench runnnen welke letterlijk niemand meer dan 5x zal gebruiken als benchmark dan kan iedereen zien hoe goed onze 2080ti is. Ps. Alle gehypte features zijn nog in geen enkele game goed ge implementeerd, maar dat is niet erg. Het gaat gamers om benchmarks niet om games toch?"" -jij"
RTX 2080;2;0.3491159677505493;Vind je het werkelijk vreemd dat mensen niet onder de indruk zijn van een techniek die nog nergens commercieel toegepast is maar als commercieel verkocht wordt? En ja het is vrij obvious dat de NV benchmark een best-case scenario voor hun kaarten is. Dat is vanzelfsprekend onderhand. Hoe het in werkelijke games speelt is nu nog de vraag. De enige toegepaste revolutionaire improvement van de 20xx series is dat hun encoding voor streamen blijkbaar drastisch is verbeterd en GPU streaming nu goed een ding kan worden.
RTX 2080;2;0.42794641852378845;De prijzen zijn heel logisch, waarom alle kaarten van de 10XX serie €200 goedkoper maken als je de nieuwe kaarten met nieuwe features gewoon duurder in de markt zet. Waarschijnlijk zijn er nog genoeg 10XX kaarten op voorraad die eerst nog verkocht kunnen/moeten worden. Wanneer al deze kaarten eruit zijn zal de prijsdaling ingezet worden. Slimme marketing toch.
RTX 2080;2;0.36792898178100586;Of het slim en logisch is hangt er vanaf hoeveel consumenten zich willen laten naaien.
RTX 2080;1;0.4538891613483429;Alles is binnen no-time uitverkocht dus schijnbaar is er genoeg interesse. En iedereen die loopt te zeuren over een te hoge prijs koopt een 2e handse 1080ti waarvan de verkoper weer een nieuwe 2020(ti) koopt.
RTX 2080;1;0.6258710622787476;Dat iets uitverkocht is zegt niks, hoeveel hadden ze in voorraad 10? 100000000?
RTX 2080;2;0.3360326290130615;Volgens pricewatch was de introductieprijs van de 1070 ook een stuk hoger dan die van de 970. De 1070 werd ook met een flinke hype geïntroduceerd. Volgens mij is dit een beleid dat met de 10xx is ingezet
RTX 2080;1;0.5217246413230896;Bedank AMD voor deze prijzen. Zonder concurrentie zullen ze alleen maar stijgen.
RTX 2080;1;0.6078982949256897;AMD bedanken?? Als er meer mensen een AMD kaart zouden kopen, zou het bedrijf meer armslag krijgen om te zorgen voor een betere concurrentie positie. Maar iedereen veel mensen denken alleen maar aan het 'nu' en krijgen daar NU de rekening voor gepresenteerd (letterlijk). Zo lang veel mensen maar denken... Laat iemand anders maar AMD halen, zal NVIDIA nooit in een positie komen waarbij men prijzen zal verlagen.
RTX 2080;1;0.6462005972862244;Haha, dit meen je niet. Ga je nu de schuld bij de klant leggen dat een bedrijf niet innovatief genoeg is geweest en op de achtergrond is geraakt? Dit heeft AMD al jaren geleden ingezet. Er kopen geen mensen meer een AMD kaart meer omdat het gewoon vele malen minder is dan de concurrent.
RTX 2080;3;0.40526214241981506;Maar de kaarten als de rx480/580 zijn toch helemaal niet vele malen minder dan de Nvidia tegenhangers? En wat kan AMD er precies aan doen dat Nvidia al jaren lang 3/4x hun R&D budget heeft? Het is nogal makkelijk om te zeggen dat ze maar meer hebben moeten innoveren. Dit kost namelijk geld.
RTX 2080;2;0.4730503261089325;"Kaarten die qua prestaties vergelijkbaar zijn, zijn idd niet minder. Dat is wat ""tegenhangers"" inhoudt. Maar de snelste consumentenkaart van AMD (vega64) is duurder dan een vergelijkbare kaart van Nvidia (GTX1080), en op de snelste kaarten van Nvidia (incl 1080Ti) heeft AMD totaal geen antwoord. En dat zeg ik als gelukkig bezitter van een RX570."
RTX 2080;2;0.47135549783706665;Maar de kaarten zoals de 1060 worden zo'n 5 á 6 keer beter verkocht dan een RX580. De GTX 960 ook zo'n 3x. Het heeft dus niks met specs te maken maar meer met Intel/Nvidia reputatie. Het is inderdaad jammer dat ze niet zo sterk zijn in de high end.
RTX 2080;3;0.3432397246360779;En met de hoeveelheden die Nvidia op de markt kan brengen na launch. Bij AMD waren er de laatste jaren een paar paper launches, die pas maanden later de markt haalden. Het is niet allemaal toeval dat Nvidia het beter doet, ook al is dat niet altijd terecht. Maar grosso modo heeft Nvidia zijn zaakjes beter voor elkaar qua distributie, drivers, koeling etc. En ja, ik heb naast mijn Nvidia ook een 290x die super fijn draait. Ik heb geen merkentrouw of zo. Maar ik ben gewoon voorzichtig met kiezen voor AMD kaarten. Na de 290 vind ik het allemaal wat minder geworden. Natuurlijk, ik kan undervolten, maar waarom? Ik koop een kaart prak hem erin en ga spelen. Helemaal als ik het voor de kinderen en aangetrouwde familie koop. Geen gezeik, gewoon out of the box spelen.
RTX 2080;2;0.4468628168106079;En met een lager rd budget wil je ook geen antwoord op die snelste kaarten gezien dat het kleinste taart puntje is van de markt. Beter goed middange concurerrn daar verkoop je immers het meerendeel van je kaarten.
RTX 2080;1;0.44597965478897095;Toen AMD net Ati overnam, voor 5 miljard, sprong het R&D budget per kwartaal van 278M naar 510M per kwartaal. Dan praten we over 2006>2007. Vervolgens is elk jaar steevast het R&D budget verlaagt tot het diepte punt van 229M in 2015. Ati had zelfstandig dus meer R&D budget dan AMD gecombineerd had in 2015. Dat is nog voor inflatie correctie (!). Dus om antwoord te geven op je vraag. Het was juist AMD die het R&D budget inclusief talent bij Radeon heeft leeggezogen en dit vervolgens 10 jaar lang in stand heeft gehouden waardoor het niet anders kon gebeuren dat er een moment kwam dat Radeon niet meer zou mee komen met Nvidia. Dit is iets waarvoor ik al sinds de eerste daling van het R&D budget in 2008 voor heb gewaarschuwd. Het heeft lang geduurd maar helaas is het probleem werkelijkheid geworden. Het goede nieuws is dat na Zen het R&D budget per kwartaal weer is verhoogd naar 357M per kwartaal. Dat zit nog echter ver onder het record uit 2007. Radeon moet het dus ook dit jaar nog met een fractie van het originele budget doen. Daar is AMD 100% verantwoordelijk voor. Die hebben Ati gekocht.
RTX 2080;2;0.48827242851257324;Maar had ATI het zelfstandig wel gered? Oprechte vraag hoor, geen idee. Ik heb sterk de indruk dat investeringen nu vooral mogelijk zijn door succes in consoles, wat natuurlijk niet had gekund zonder poolen van kennis over CPUs en GPUs. Als dat zo is, zou het op langere termijn stiekem alsnog een redelijke keuze kunnen zijn geweest, gezien de beschikbare middelen. Volgens mij is AMD in huidige en volgende generatie consoles marktleider, of niet? Dat zou weer een stabiele basis kunnen bieden voor betere investeringen. Ik heb iig goede hoop, maar helaas, gezien de koersen, geen aandelen 😋
RTX 2080;1;0.4110568165779114;Eens. In 2017 gaf Nvidia ongeveer 1.8 miljard aan R&D. AMD is aan het groeien maar als we H1 van 2017 x2 doen komen we op ongeveer 1.1 miljard. Gelijkaardig als je naar totale omzet kijkt. 9.7 miljard voor nvidia en 5.3 voor AMD Voor de volledigheid, Intel heeft 62.8 miljard binnengehaald en gaf daarvan ongeveer 13 miljard aan R&D uit. Nvidia ontwikkel daarmee nagenoeg enkel GPUs. AMD moet met een ongeveer 15x kleiner budget opboksen tegen Intel en nvidia om zowel concurrerende GPUs als CPUs te maken. In dat opzicht: jammer dat ze op het moment niet de prestatiekroon hebben bij GPUs, maar in de midend concurreren ze goed en lowend levert AMD zelfs iGPUs aan Intel.
RTX 2080;1;0.467494398355484;Dat is dan ook meer een uitzondering dan een regel. Stel jij maakt een project 2x zo snel af als gepland en jouw werkgever eist dit nu voor al jouw projecten, klinkt dat redelijk denk je? Intel en Nvidia zijn beiden beursgenoteerde bedrijven die zo veel mogelijk winst willen behalen. Met andere woorden, die zullen ook wel nauw letten op hun R&D uitgaven.
RTX 2080;2;0.449810653924942;Je geeft dus in principe zelf het antwoord. AMD moet zowel concureren in de CPU als in de GPU markt. Op het moment lijkt het erop dat hun efforts vooral in de CPU's worden gestoken. Om ook nog eens high end GPU's te ontwikkelen uit hetzelfde budget is gewoon niet realistisch. Natuurlijk hebben ze kneiterhard gewerkt, goede ontwerpbeslissingen en andere meevallers hebben wellicht ook geholpen. Infinity fabric is immers hun redder in nood geworden. Wellicht heb jij het project zo snel afgerond omdat je 2x zo hard werkte en veel over werkte omdat je naar de trouwerij van je zus moest, weet ik veel. Volgensmij snap je m'n punt wel. Hoe dan ook, je praat wel echt heel erg makkelijk. AMD heeft gewoon een veel kleiner budget. En dat ze toevallig voor een product minder geld dan de concurrentie nodig hadden, dan kun je dat niet zomaar extrapoleren naar de GPU-ontwikkeling.
RTX 2080;1;0.3053411543369293;intel steekt daarvan ook geld in de ontwikkeling van het productieproces. Dat is een dure aangelegenheid en is exact de reden waarom AMD dat heeft afgestoten.
RTX 2080;2;0.5217524766921997;4 jaar geleden zou je net andersom gesproken hebben, toen AMD met GPU behoorlijk meekwam in de highend maar qua CPUs het niet zo goed deed. Met Ryzen hebben ze een goede strategie, juiste mensen op het juiste moment op de juiste plaats gezet en schaalt de modulaire setup heel goed van embedded met iGPU (hier is Vega wél zuinig en goed) tot 64(+?)-core multi-socket systemen. Zover ik weet hadden ze hetzelfde voor ogen met Vega: één chip die alles aankan, want geld om aparte chips voor compute en graphics te ontwikkelen is er niet. Daar liep het dan iets minder Wie weet wat er nog aan komt in de GPU divisie. Sowieso is de markt voor aparte kaarten al een relatief klein aandeel.. Intel is al jaren de grootste graphics-verdeler op de markt De high end is hoge kosten, grotere marges, grote risico's. En ik geloof dat AMD ook al jaren het merendeel van de consolemarkt voorziet: PS4, Xbox One (S/X), Wii, Wii U. Allemaal continu inkomen, doch lage marge. Zó slecht doen ze het ook weer niet.
RTX 2080;2;0.4683462083339691;Op zich klopt je verhaal, maar er is dus te concluderen dat AMD niet dé concurrent kan zijn op CPU en GPU gebied die we nodig hebben. Daar zijn ze simpelweg te klein voor. Als we op beide gebieden goede concurrentie willen zien tegenover Intel en Nvidia dan zal er wat moeten gebeuren.
RTX 2080;2;0.4946536719799042;Amd hoeft niet op high end mee te doen. In het lage en middensegment wordt nog veel meer verkocht dan dat je denkt.
RTX 2080;2;0.3329770267009735;In de low en mid range heeft AMD nooit onder gedaan aan Nvidia, en de 290x bijvoorbeeld ging nek aan nek met de 780TI. Maar tja als iedereen intel koopt, dan komt AMD niet aan haar geld voor R&D. Vandaar dat je AMD ook ziet verdwijnen in de super high end sectie.
RTX 2080;3;0.4237033724784851;Wat betreft GPU's heb je gelijk, CPU's niet echt. AMD heeft, min of meer, een flop gehad met de Bulldozer architecure, en dat heeft hun pijn gedaan. Maar met de Ryzen en Threadripper CPU's, Zen en Zen+, is AMD heel erg concurrerend. Het heeft vergelijkbare prestaties met de Intel line-up voor over het algemeen een iets lagere prijs. En straks met de Zen2 generatie, heeft Intel NIETS om het daar tegen op te nemen zolang de 10nm process niet draait bij hun. Persoonlijk kijk ik daar wel naar uit na zo'n lange tijd.
RTX 2080;3;0.26383447647094727;Hier een lijstje met de rauwe (zonder optimalisaties) rekenkracht. Hier kun je dus zien dat als er meer mensen een AMD kopen en de developers vervolgens meer voor AMD gaan optimaliseren het best goed komt. 970 = 3,9 Tflops 980 = 4,6 290x = 5,6 980ti = 5,6 480 = 5,8 580 = 6,2 1070 = 6,5 Fury = 7,2 Fury x = 8,6 1080 = 8,9 2080 = 10,1 Vega56 = 10,5 1080ti = 11,3 Vega64 = 12,7 2080ti = 13,4 Vega64lc = 13,7 Tflops
RTX 2080;2;0.2697007358074188;Rauwe TFlops zijn het probleem niet, het toevoeren van genoeg data om al die TFlops volledig te kunnen benutten is de grote uitdaging. Daarom wordt er zoveel energie gestoken in het verlagen van cache-latencies, verhogen van bandbreedte etc.
RTX 2080;3;0.33828026056289673;En daar gaan we weer. Weet je zeker dat je er echt niet over wil praten hoe je in elk artikel van een GPU of CPU AMD moet bashen? Altijd maar AMD fanboy dit en AMD fanboy dat.. Sorry pik maar er begint een fanboy in jezelf te groeien . ff ontopic: Dit is een goede innovatieve kaart van Nvidia en ik ben echt benieuwd als er optimaal gebruik van gemaakt kan worden van alle functies in-game. Alleen dan zou ik prijs/performance goed kunnen beoordelen. Zoals het nu lijkt is het allemaal een beetje aan de dure kant
RTX 2080;2;0.295426607131958;Als de klant kortzichtig is ligt dat aan de klant ja. Beide merken hebben hun eigen voor en nadelen, en er zijn ook zat beweegredenen te verzinnen om voor het ene of andere merk te kiezen. b.v. nu een AMD kopen omdat hij niets slechter is en in de toekomst ook nog een betaalbare kaart te kunnen kopen, of nu Nvidia kopen omdat het het beste merk is en één merk grafische kaarten in de toekomst wel voldoende is
RTX 2080;3;0.4300881624221802;Vele malen minder is ook overdreven, een vega56 kan aardig mee met een 1070ti, en als je een freesync monitor hebt is de keuze al helemaal makkelijk gemaakt. Vega56 is zelfs sneller in sommige titels. Niet zo overdrijven met je '' AMD is gewoon vele malen minder dan de concurrent''
RTX 2080;3;0.3039424419403076;Voor de echte leken is de 1070ti veel zuiniger, maar voor iedereen die zich er ook maar een beetje in verdiept, youtube is genoeg, dan weet je als eigenaar van een vega dat je hem moet undervolten en dan mag je dat ''veel'' ook wel weglaten bij meer energieverbruik. Geen fanboy hier, ik heb zelf nota bene een 1070ti, omdat die simpelweg sneller is in overwatch dan de vega56, en ik veel overwatch speel, en freesync voor mij geen toevoeging was op hoge fps.
RTX 2080;2;0.3277019262313843;Ik ga gewoon amd kaart kopen, ik speel toch op 1980 netzoals veel mensen. Alleen even wachten nog tot de prijzen verder droppen
RTX 2080;1;0.7135403752326965;De AMD Polaris en Vega kaarten waren juist nergens te verkrijgen… Nvidia heeft zijn omzet verdriedubbeld naar miljarden per kwartaal. AMD zat bij benadering de hele mining hype op dezelfde omzet (de extra omzet kwam van Zen, aldus de CEO). Meer mensen die AMD Radeon kopen was dus juist niet mogelijk...
RTX 2080;2;0.44302311539649963;Niet helemaal waar, de extra omzet kwam ook van Radeon Instinct in datacentra.
RTX 2080;1;0.5897849202156067;AMD heeft tijden lang iedere kaart die ze produceerden direct kunnen verkopen aan de miners, hierdoor waren er voor gamers tijden lang geen AMD kaarten voorhanden, terwijl nvidia ruim te krijgen was, pas veel later werden nvidia kaarten ook lastiger leverbaar. Zomaar even AMD kaarten kopen als consument zat er gewoon niet in, want ze waren er simpel weg niet. Ik zou de schuld, als je al van schuld kan spreken, hier niet bij de consument leggen.
RTX 2080;2;0.4423884451389313;"AMD heeft besloten meer nadruk te leggen op de CPU en APU ontwikkeling, en met Navi op consoles. En dan zou ik een ""inferieur"" product moeten kopen om hun (in mijn ogen) slechte beleid te corrigeren?"
RTX 2080;1;0.5233912467956543;alsof iedereen enkel een 1080+ koopt..... typische reactie . de hoogste is de beste, dan al de rest ook....
RTX 2080;3;0.4206194579601288;Dit artikel gaat over de 2080/2080Ti, dus de high-end kaarten, en op dat gebied laat AMD het momenteel afweten. Maar zelfs in het midden-segment heeft nvidia in mijn ogen de betere kaarten, alleen low-end zou ik mogelijk voor AMD gaan, alleen hebben die segmenten voor mij geen enkele relevantie.
RTX 2080;1;0.4131133556365967;In jouw ogen? Waarom? Dat AMD op dit moment geen reactie heeft op een RTX2080Ti, sure. Maar midend? RX580 en GTX1060 zijn zowel qua prijs als prestaties min of meer nek aan nek, afhankelijk van het merk dat je koopt of de game die je speelt. Er is geen eenduidige winnaar te noemen. Drivers van AMD schijnt dan een argument te zijn, maar die zijn al jaaaaaa-aaaaaren echt geen probleem, terwijl die van nvidia ook echt niet heilig zijn. Verder staat AMD voor open technieken terwijl nvidia met programma's als Gameworks en GPP de concurrentie probeert te benadelen danwel op oneerlijke wijze uit te schakelen.
RTX 2080;2;0.3944704532623291;Verbruik/koeling van AMD kaarten zijn wat mij betreft echt een enorm issue. Dat zijn dingen die ook de levensduur bepalen.
RTX 2080;2;0.297041654586792;De RX580 is een opgepompte RX480, klopt. Vega is ook geshipped met een hoog voltage voor een paar MHz meer. Veel extra verbruik voor weinig resultaat, domme keuzes in een strijd om de laatste paar procentjes. Dat kan je dus eventueel nog zelf regelen. Mijn eigen kaart heeft bvb een OC BIOS en een trager BIOS. 10% meer verbruik, 2% sneller. Dus dat kan ik ook niet goedpraten, maar je kan het wel zelf weer goed zetten. Of ze daar korter van meegaan, mwah? Lijkt me dat dit in het verleden meer dan genoeg is gebeurd zonder nadelige gevolgen. Zie ook CPUs
RTX 2080;5;0.5672320127487183;De selling point van veel AMD kaarten is de bandwidth, al helemaal met de vega reeks die HBM2 gebruikt. Hoe hoger de bandwidth van het geheugen hoe soepeler het presteert op de hoge resoluties. Iets wat je ook steeds weer terug ziet bij AMD over de jaren. Nieuwe APU's, hogere resoluties, het is bijna altijd AMD die daar het meeste uit haalt en nVidia die het laat afweten.
RTX 2080;2;0.5384247899055481;Ik meen te herinneren dat HBM oorspronkelijk door de geheugenproducenten aangekondigd was met hogere snelheden en dat dit in de praktijk dan ook weer tegenviel toen het eenmaal geproduceerd werd. Ook een tegenvaller
RTX 2080;1;0.4624675512313843;koeling op NVidia FE kaarten is ook gewoon ruk, en vergeet niet dat een Vega 64 50W meer verbruikt dan de 1080 GTX, mijn 1080 draait in iedergeval standaard op 250W te blazen onder load..... en de nieuwe Turing kaartjes zijn ook niet bepaald zuinig. dat argument vind ik BS en ook niet echt relevant voor gamers, ga jij kijken naar energie verbruik van je PC? Ik in ieder geval niet, nu niet nooit niet. Bespaar liever in dingen die het wel waard zijn (Horizon box van ziggo anyone? ding verbruikte gewoon €150 aan stroom per jaar als ie uit staat). Vind daarnaast het argument wat vele maken dat AMD slechter kaarten uit brengt onzin, ja hun High end segment is slecht op het moment maar in de low en mid end zijn ze gewoon goed. Maar omdat Nvidia hun marketing beter op orde heeft en met de High end de scepter zwaait geloven mensen meteen dat alle Nvidia kaarten beter zijn ongeacht prijs point. Maar we moeten vooral zo doorgaan als schapen Nvidia te blijven kopen, is dadelijk de ENIGE relevante concurent weg en gaat je de 3080TI richting de €2000,-.
RTX 2080;5;0.25531667470932007;Tja, nvidia weet hoe belangrijk de high-end kaarten zijn voor de reputatie van een merk en profiteert daar van. AMD heeft dat zelf laten liggen. Ik kan niet wachten. Intel kan dan mooi de resten van de GPU tak van AMD overnemen, een deel van het personeel zit er toch al, en AMD kan zich concentreren op hun sterke punt, CPU's maken.
RTX 2080;1;0.5589650273323059;hoor je jezelf nu? AMD beter in CPU? volgens mij heeft AMD tot de laatste jaren altijd goed kunnen concurreren met Nvidia en omgedraaid. Intel de markt van AMD afpakken? wat heb je gerookt, wil ik namelijk ook..... Intel heeft nog NOOIT een degelijke GPU op de markt kunnen brengen, 20 jaar geleden niet en larrabee ook niet (al was deze wel een goede schop in de richting) Intel komt pas over 2-3 jaar met een discrete GPU op de markt en dan is het nog afwachten waarmee ze komen, vergeet niet dit is hun zoveelste poging om een GPU te ontwerpen. Hoop dat ze iets fatsoenlijks op de markt kunnen brengen (meer concurrentie is altijd welkom), als het doorgaat zoals nu kunnen ze misschien in de low end blijven en hoeven ze geen APU meer van AMD af te nemen nu koduri daar zit. dit soort berichten is weer typisch kort termijn visie (toekomst en verleden) waar de hele economie van naar de knoppe gaat.
RTX 2080;4;0.35626018047332764;Intel i740 was anders een prima kaart indertijd. Die kon heel goed met de concurrentie meekomen.
RTX 2080;2;0.43076327443122864;Een release met heleboel bombarie, maar de uiteindelijke performance middelmatig tot slecht bedoel je? Beetje zoals de Turing release
RTX 2080;1;0.4961811900138855;Dat was een 2D kaart met basis opties. Dat was dan ook het laatste wat Intel uitgebracht heeft omdat de concurrentie Intel stompte op alle vlakken.
RTX 2080;2;0.3383096754550934;Wellicht eens goed lezen wat ik precies zeg, dat is namelijk niet dat Intel markt van AMD af zal pakken, maar dat Intel de GPU divisie over zou kunnen nemen als deze faalt, juist omdat zij nog niet veel kennis van GPU's in huis hebben. AMD heeft de laatste jaren zich op CPU's gefocust, wat ten koste is gegaan van de ontwikkelingen op GPU gebied, maar onmiskenbaar goede resultaten op CPU gebied heeft opgeleverd. Intel wil wat met GPU's gaan doen, heeft al personeel van AMD in huis gehaald, werkt al met AMD samen op GPU gebied, en heeft een groot budget voor R&D. Overname van de GPU divisie van AMD zou dus juist een sterke concurrent voor nvidia betekenen, en daar hebben we meer aan, aangezien AMD momenteel niet echt de juiste concurrentie weet te bieden op GPU gebied. Maar ik weet het, kortzichtig van me...
RTX 2080;2;0.5559879541397095;Toen ik echt veel gamede ging ik door meerdere koelers en gingen de kaarten na een paar jaar echt gewoon stuk (Artifacts, crashes, bootloops). Dus ja, hoeveelheid hitte is gewoon een dingetje, los van de irritatie van een stofzuiger op je bureau. Dus nee, energie rekening kijk ik niet naar, maar slijtage aan de kaart is gewoon een merkbaar nadeel. Dus ja, ik kies voor de kaart die het koelste draait. een paar procent frames vind ik minder spannend. Maar dat ligt voor iedereen natuurlijk anders.
RTX 2080;1;0.38295409083366394;is dit niet het geval bij alle reference kaarten (AMD en Nvidia)? De benchmarks van de turing liegt er ook niet om dat de 2080 en 2080TI ook richting thermal limit gaan
RTX 2080;5;0.4983155429363251;Helemaal waar! Turing gaat het voor mij ook niet worden. Duidelijk een model waar we gezeik mee gaan krijgen. Generatietje overslaan Nvidia is ook niet heilig hoor, ook wel eens rare modellen tussen
RTX 2080;2;0.5563963055610657;Omdat ik sinds mijn eerste TNT gebaseerde kaart goede ervaringen heb met nvidia, en mijn uitstapjes naar ATI/AMD niet echt goed zijn bevallen. Juist, er is dus voor mij geen meerwaarde om over te stappen naar AMD. Daarnaast is het verbruik van de 10-serie een sterk punt, en dan kijk ik niet direct naar de besparing op energie-kosten, maar wel naar de lagere eisen aan de voeding. Helaas zijn de huidige 20-serie kaarten helaas weer meer gaan verbruiken.
RTX 2080;2;0.40521541237831116;De 1060 zit eigenlijk consequent een paar 10tje onder de prijs van een 580. Momenteel is het bij MSI bijvoorbeeld 259 voor de 1060 6 GB en 279 voor de 580 8 GB. Gemiddeld genomen is de RX 580 net even een stukje sneller. Power consumption praten we over 116W vs 239W tijdens gaming, aldus Samengevat is de 1060 goedkoper, zuiniger en dus stiller. De rx580 is duurder, sneller, verbruikt meer en maakt dus meer lawaai. Mantle, True audio? Probleem van AMD is dat het marktaandeel onder gamers erg laag is en dan kun je eigenlijk niet wegkomen met een gesloten standaard.
RTX 2080;2;0.5212571024894714;Die prijs hangt erg af van welk model je kiest, ik had snel even door de pricewatch gescrolled (filters lijken me niet onredelijk) en zag prijzen voor beiden in een zeer vergelijkbare range. Heb het nog even in een grafiekje proberen gooien en ze overlappen echt wel, voor nvidia zijn er gewoon meer keuzes. En dat marktaandeel... Veel mensen lijken, ook met de CPUs, iets te hebben dat het merk dat ze ooit hebben gekocht werkt en niet verder kijken, ook al is het (afhankelijk van het moment/de stiuatie) duurder.
RTX 2080;2;0.5560814738273621;Klopt. Het gekke is alleen dat de meeste van die kaarten (570/580) zeer gemakkelijk te undervolten zijn wat ze eigenlijk veel betere kaarten maakt. In sommige gevallen worden de prestaties er ook nog beter door, omdat er geen thermal throttling meer optreedt. Puur en alleen voor de benchmakrs zijn die kaarten zo tot hun maximum geduwd dat ze onzuinig en warm worden. Hetzelfde is met Vega gedaan. Die cores doen het gewoon prima als je ze niet helemaal tot het uiterste duwt, wat goed te zien is in de 2200G processors bijvoorbeeld. Persoonlijk heb ik een 570 gekocht (Gigabyte) die klonk alsof er een helikopter in de kamer stond. Na een uurtje tweaken met voltages en kloksnelheden is de kaart nu overclocked en undervolted. De prestaties zijn nu beter, en de fan hoeft niet hoger dan 2500rpm, je hoort hem amper meer. De gewone consument heeft hier niks aan natuurlijk. Maar vind de keuzes om die kaarten zo te pushen een foute keuze. Het levert 2% meer op in een benchmark, maar alle andere prestaties van de kaart lijden er onder.
RTX 2080;1;0.6080730557441711;Waarschijnlijk was het gat met Nvidia op papier dan te groot. Vrijwel alle keuzes van AMD rondom Radeon zijn vreemd. De RX480 had een enorm potentie maar is zo goedkoop in de markt gezet dat AMD er uiteindelijk maar zeer weinig van heeft verkocht. Zonde... Dus de suggestie dat er enorme volumes naar miners zijn gegaan kan relatief zeker waar zijn. Ongetwijfeld hebben miners veel kaarten gekocht. Alleen er zijn er gewoon niet veel geproduceerd...Op V&A zie je ze ook niet: De voorspelde exodus van mining kaarten is nooit gekomen en die zal wellicht ook nooit komen. Het zijn momenteel de gamers waar miljarden per kwartaal aan omzet bij wordt gegenereerd.
RTX 2080;5;0.2990281283855438;Het is CPU die gpu afdeling de afgrond mee in trok. Het is dus ook de CPU omzet van Zen die middelen voor de GPU afdeling kan verhogen dus massaal Ryzen TR kopen. En wat Vega .
RTX 2080;5;0.4132268726825714;Als AMD eens normale prijzen zou gaan vragen voor hun vega kaarten. Ze kosten (veel) meer dan hun Nvidia gelijken en presteren er net onder. Ga jij lekker AMD voorzien van geld. Vroeger was AMD iets minder goed, maar dan dus ook iets goedkoper. Dat zijn ze al jaren niet meer. Na de HD7970 is het alleen maar berg afwaarts gegaan. Pas met de CPUs nu gaan ze weer de goede kant op met Ryzen.
RTX 2080;1;0.3657332956790924;Die moet je in Duitsland kopen, in Nederland gooien de winkeliers er te veel op. En hier op de pricewatch staan de goedkope jongens er niet bij. Hier 489, was pas nog 469 dus gaat ook heen en weer.
RTX 2080;1;0.7271104454994202;Nee amd had zijn roadmap op orde moeten hebben en niet de gok op dx12 en hbr moeten nemen. Hbr is helemaal niet nodig voor foede prestaties nog maar is wel uiterst duur, dx12 is nogsteeds geen gemeengoed. Daarnaast is de gaming laptop markt erg groot en hierbij heeft AMD nu al 5 jaar niks deftigs gedaan. Wat er was, was slecht presterend en zeer energie slurpend. Waarom zouden klanten daarvoor moeten opdraaien. Nvidia gooit de prijzen nu omhoog ja maar het is ook new tech en handels embargos hebben er nu al een effect op. Het is niet alleen alsof het door hun monopolie komt. Laten we hopen dat intel het met hun gpu oplossing in 2020 beter gaat doen.
RTX 2080;1;0.5635652542114258;Op console gebied heeft AMD zo wat een monopolie, buiten de Switch. 120 miljoen consoles verkocht. Dat moet toch wat opgeleverd hebben? Ze hebben gewoon alle research in de CPU gestoken en de gpu markt laten verslonsen. Dat is niet de schuld van de consument maar van AMD zelf.
RTX 2080;1;0.5109859704971313;Graag, heb sinds de 6600GT alleen maar AMD kaarten gehad. Moeten die krengen wel te krijgen zijn! Na ruim een jaar lang gepoogd te hebben een AMD kaart voor een redelijke prijs te kunnen kopen en maar blijven wachten tot Vega eindelijk eens kwam was ik het spuugzat! Ben niet te beroerd om een paar tientjes meer te betalen voor een GPU op een build van €1800 maar als AMD al die tijd liever zijn kaarten voor de hoofdprijs aan miners verkoopt moeten ze niet huilen of wijzen naar dezelfde community die ze genaaid hebben. Dus na zo'n 10 generaties en denk 20 AMD kaarten hebben ze zelfs deze fan naar de groene kant gejaagd Voor de liefhebber deze kaarten heb ik na de 6600GT gehad (vraag alstjeblieft de logica en volgorde er niet van....Tweaker, jong, ect ) X800, X800XL (2x), X800XT (2x), X1600XT, X1800XL, X1800XT, HD4650, HD4850, HD6850 (2x), HD7700 GHz, R9 380 en toen was er schaarste. En tussendoor nog onboard HD3450, HD3200, HD3300, HD3000, HD4200 en HD4250 gebruikt Maar nee ik heb geen voorkeur voor AMD graphics hoor
RTX 2080;2;0.43079981207847595;Dus je moet een AMD kaart kopen (ook al presteert die aanzienlijk minder) om zo ervoor te zorgen dat er geen monopolie ontstaat? Zo werkt de wereld natuurlijk niet... Ik heb de vega gehaald, doet het goed voor wat ik ermee doe (niet gaming gerelateerd) maar voor gaming is hij 'waardeloos' comfort is 0 je moet hem waterkoelen anders word je doodziek van het ding (vega 64 red devil). Ik ga gewoon nV halen want hij schijnt goed te zijn in productiviteits suits (volgens linus) is stiller VEEL sneller en alsnog een lager verbruik. 1200 euro is veel geld, maar ik gebruik mijn PC flink veel, dan is 1200 euro een kleine investering.
RTX 2080;2;0.30287647247314453;Succes met dag dromen, ik draai alles op 4k, jij waarschijnlijk niet.... Farcry 5 (gekregen bij de kaart) Starcitizen en andere titels die enige moeite kosten gaat hij op volle toeren. Willen fanboys niet horen maar het heeft een reden waarom de kaart niet verkoopt (behalve aan miners). Ik gebruik hem bijna alleen voor CAD. Gamen is een nachtmerrie, welk bewijs wil je hebben? (zeker nep!!!) Sneller dan een 1080... zucht.. ja in een paar titels (forza en doom of paar andere titels die vulcan gebruiken) over het algemeen niet. Een een vega verbruikt 300 watt, een 1080, 180watt is dus cooler en dus stiller en OC'd makkelijker. P.S. niet waarschijnlijk langzamer, feitelijk vastgesteld door 99% van de review sites.
RTX 2080;3;0.4140252470970154;Als jij die vega hebt, dan moet je ook weten dat hij gewoon op 4K 60 fps kan draaien. Ik moet wel stellen dat ik de games die je noemt niet heb. Maar bij mij zit die vega 64 nitro in een relatief kleine htpc behuizing onder de reciever in het tv meubel. Gamen doen we op 4K en op tv is 60 fps voldoende, is geen monitor. Het klopt dat hij meer stroom verbruikt dan een 1080, maar ik vind het zo overtrokken. de temp is goed, de fanspeed is goed, waar hebben we het over. Een tientje meer stroom per jaar. Ik heb een redelijk sportieve motor in mij auto liggen, die verbruikt zeker 100 euro meer aan benzine per maand, daar klaag ik ook niet over. De real life performance is gewoon prima van die vega, tuurlijk het kan beter en nvidia presteert ook beter, maar dat wil niet zeggen dat die AMD kaart k.u.t. is. Ik ben absoluut geen fanboy, hekel aan.
RTX 2080;2;0.43541428446769714;60fps 4k.... what ever, welke titels dan, dragon age inq... niet, the witcher3.... niet. farcry 5... niet. bf4 net aan maar dipt constant naar de 30 / 40fps ( multiplayer ). civ5 / 6, skyrim le ( lol ) dit zijn allemaal games van een paar jaar oud behalve farcry 5 die geoptimaliseerd was voor amd. nieuwe games : starcitizen, ( shadow of the ) tombraider, ac odyssey, ( target fps 30 op high ) kan je hem op medium zetten om de 30fps te halen. stroom kosten zijn het probleem niet ( alleen een fan kan dat bedenken, ook al vind ik verspilling schandelijk.. ) het probleem is natuurlijk 0. 0 oc potentieel en standaard een 300watt heater in je kast hebt. ( kijk toevallig even hier, tweakers ook geen real life ervaring? noem mij de aantal games op met 60fps.... . ) de kaart undervolt en underclocked ( vergeet die 30fps dan maar ), ik heb een ryzen 2700 die ik niet meer oc omdat hij anders lekker warme lucht van de gpu binnen krijgt. de top van mij pc kast is een grill na een game sessie. real life performance is k * t op 4k, speel je op lagere res is de vega interessant maar aangezien de 1080 goedkoper is heb ik geen idee waarom jij een vega boven een 1080 zou verkiezen aangezien ze onder de 4k toch standaard hoge fps aan kunnen, koeler zijn, stiller, goedkoper en makkelijker verkrijgbaar zijn = nogmaals een lagere prijs. ik heb mijn vega 64 red devil voor een goede prijs gekocht = vergelijkbaar met een 1080, voor mijn applicaties presteert de vega 64 aanzienlijk beter dan nv. maar voor gaming.. zal ik deze kaart nooit aanraden. ik kan niet eens fatsoenlijk discord gebruiken met maten door die k * t fans. jij speelt overduidelijk geen gpu intensieve games in je kleine kast of je bs ' t over je vega 64... met je echte ervaring...
RTX 2080;3;0.40774005651474;Dat is zo maar het verschil is nooit zo groot geweest als nu. AMD is gewoon onbestaande in het high end segment en dat is best droevig.
RTX 2080;4;0.463595986366272;"allereerst, top review weer, wederom lekker uitgebreid, zoals het hoort ( althans naar mijn idee ) alleen nu had ik bij dit stuk hieronder zo mijn bedenkingen, of deze ook gaan kloppen is vraag 2 natuurlijk, maar toch : "" wat we wel kunnen staven, is dat de rtx 2080 in de meeste gevallen nauwelijks sneller is dan gtx 1080 ti, die iets goedkoper is. je kunt er dus voor kiezen om iets meer geld uit te geven en een kaart te kopen met toekomstbestendige features die je al dan niet gaat kunnen gebruiken, of je kunt een oudere generatie kopen die nu nog goed meekomt, maar hardware ontbeert die turing uniek maakt. "" aangezien ik het ook zo zie gebeuren dat er in eerste instantie wel een paar games zullen verschijnen die het zullen gaan ondersteunen ( het realtime ray tracing gebeuren ), een aantal uitgevers hebben immers ookal aangegeven dit zullen te gaan doen, en welke dat zullen zijn, en deze staan dan ook in deze review vermeld. en zoals een stukje uit het gelinkte. pdf bestand uit mijn reactie hierboven ookal aangeeft : it is not the intention of intel, nvidia or other graphics companies, like it is often stated in the last months, that ray tracing will be the replacement of rasterization. for that the speed advantage of rasterization is just too big. however, by the use of hybrid approaches, some computationally intensive effects could be bound into the content pipeline a lot simpler and some others would even been just made possible by that. current real - time effects, which could profit of this, would be ( soft - ) shadow algorithms or real - time global illumination algorithms that use ray - tracing - based photon mapping to determine the indirect light radiation. within this semester thesis for the advanced seminar, the two topics of new ray tracing apis and optimization of an existing gpu ray tracer, called rayglsl will be focused. furthermore, two more responses to intels plans have arrived on the market. nvidias nvirt api for nvidia graphics cards, based on the gpgpu2 tool cuda, and also a hardware solution for ray tracing by the company caustic graphics. both systems claim to be able to display interactive real - time ray tracing results."
RTX 2080;3;0.49421775341033936;Ik vind dat je daar een goed punt maakt. Niet alleen of het wel de toekomst is of niet, maar hoe lang, naast dat handjevol games, het gaat duren voor die toekomst er is? Dan zijn we wellicht een of twee video kaart generaties verder en zijn deze sowieso, maar de nieuwe misschien ook, een stuk goedkoper.
RTX 2080;1;0.6165357232093811;True, en als ik hier en daar dan ookal lees dat de goedkopere versies van deze generatie kaarten die nog komen gaan het realtime ray tracing al helemaal niet gaan meekrijgen/ondersteunen, maakt dat game studio's het in eerste instantie dus alleen voor een twee tal kaarten (en daarmee dus ook een relatief kleine groep mensen) mag gaan lopen toevoegen/ondersteunen wanneer zij dit zouden willen. (En dan reken ik de Quadro varianten van deze kaarten dus niet mee) Ikzelf ga dit dan ook eerst eens een tijdje in de gaten lopen houden, voordat ik er mijzelf überhaupt een zal gaan aanschaffen.
RTX 2080;2;0.4939095973968506;Dat klopt. Het probleem is is dat je een beroep op de toekomst doet. En niemand weet precies hoe mooi die toekomst er uit ziet. Misschien is RTX wel een flop die de meerprijs niet waard is. Misschien wel, we weten het niet. Of misschien is RTX pas volwassen in de volgende generatie. We weten het niet. We kunnen pas oordelen als de toekomst er is. Dan pas heb je ook wat aan de RTX. En misschien is dan de 1080 Ti de minder goede keuze voor de moderne spellen met deze nieuwe eigenschappen. Of misschien nog steeds een goede optie.
RTX 2080;3;0.36798420548439026;"Mee eens. Die ""toekomstbestendige features"" moet zich nog bewijzen. Deze kaarten zijn in vergelijking met de 10 serie is absoluut meer future-ready (wat je niet hebt kan je uiteindelijk wel gaan missen), maar hoever, hoeveel en hoe snel deze features het naar de markt gaan maken is nog volledig onbekend. Ik vrees zelf dat we eerst een generatie van ""overdone"" raytracing titles gaan krijgen, waarbij raytracing te aanwezig zal zijn. Dat omdat huidige games in zekere zin ook veel met lighting en reflections ""faken"" om de game leuk er uit te laten zien, en je dus wel verschil moet kunnen zien om ""profijt"" te hebben van raytracing. Een beetje zoals overmatig veel motion blur in race games.. visueel geinig maar wel hinderlijk. Tevens moet nog maar blijken dat elke title support gaat krijgen voor deze nieuwe features. Afhankelijk welke games je speelt, bijvoorbeeld simulatie games in VR, kan het nog wel eens makkelijk paar jaar duren voordat die graphic engines worden geüpdatet, als dat al binnen afzienbare tijd gebeurd. In dat geval kan je beter voor fps/$ kiezen of nog even wachten."
RTX 2080;1;0.6779513359069824;Sorry, maar al dat gezeur over te hoge prijzen zijn compleet de weg kwijt. Simpel: je kan de advies prijs bij introductie totaal niet vergelijken met de huidige prijzen, 1,5 of 2 jaar na de introductie van het vorige top model. De 1080 Ti was ten tijde van de introductie ook bijna 900 euro. Ik heb 14 oktober (half jaar of zo na introductie) bij Cool Blue 889 euro betaald voor een EVGA GeForce GTX 1080 Ti FTW3 Gaming. Bij de introductie was dat apparaat 1000 (!!!!!) euro! Toen hoorde je bijna niemand. Nvidia levert hier compleet nieuwe techniek, niet zomaar een snellere geoptimaliseerde versie van de vorige generatie. Hier zit compleet nieuw ontwikkelde techniek in. Ik vind het redelijk normaal dat als je de eerste wilt zijn, je de hoofdprijs betaalt (kijk naar Intel's snelste, dikste, meeste cores, die is ook onbetaalbaar en onevenredig duur ten opzichte van het modelletje er onder). Wacht een maand of 3 en de prijzen van Asus, MSI en al die andere zakken ook onder de 1000 euro. Lang verhaal kort: stop met zeuren en mekkeren over die prijzen en koop gewoon zo'n ding als je hem wilt hebben
RTX 2080;4;0.2409479320049286;@willemdemoor Is the Witcher 3 getest met Anti-Aliasing uit of aan op Ultra? Dat schijnt een flinke performance boost te geven wanneer je deze uitzet. Ik speel zelf op een GTX1080 SLI en haal dezelfde resultaten als een 2080 ti op 4k maar dus wel met Anti Aliasing uit.
RTX 2080;3;0.45901089906692505;AA op 4x We zoeken niet de soepelste prestaties, maar proberen de kaarten tot het uiterste te testen Medium is zonder AA, maar daar kopen we geen 2080 Ti voor
RTX 2080;3;0.5244374871253967;Uiteraard, maar ik zie wel lagere prestaties met de kaarten die jullie getest hebben en waar dus AA aanstaat. In de witcher kan AA alleen aan- of uitgezet worden. Waar hebben jullie de 4x AA ingesteld? Bijvoorbeeld: de GTX1080ti haalt met AA aan iets van 46fps gemiddeld. Net zoals de GTX1080 die ongeveer 35fps haalt dan. Jullie resultaten zijn dus hoger, dan de meeste tests weergeven. Ik kan het fout hebben hoor maar ben gewoon benieuwd. De AA op hairworks is wel apart in te stellen op 4x maar dat bedoelde ik niet en jij denk ik ook niet?
RTX 2080;1;0.3515937626361847;Excuus, ik zat idd bij Hairworks AA te kijken, die staat op 4x. De gewone AA staat gewoon op 'On'. Misschien zijn de hogere resultaten door de overgeklokte cpu veroorzaakt? We proberen die bottleneck te vermijden door de 7900X op 4,5GHz te draaien.
RTX 2080;3;0.5990867018699646;Toch lijkt het er sterk op dat er op de lagere resoluties een cpu bottleneck is. Wat dat betreft verwacht ik dat de verschillen groter zijn wanneer een 8700K op 5+ Ghz wordt gebruikt (en straks een 9700K die mogelijk nog wat hoger clockt).
RTX 2080;3;0.671722412109375;Jazeker, op lagere reso's is het in veel games de cpu, vandaar 10x 4,5GHz. Maar 8700K @5GHz zou iets hogere scores op 1440p/1080p kunnen geven, iig voor DX11-games.
RTX 2080;3;0.336239755153656;Vooral interessant daaraan is kijken of de onderlinge verhoudingen bij een snellere cpu veranderen. Laten we eerlijk zijn 4.5 Ghz op een 7800K is niet high-end anno 2018. Helaas is de realiteit daarnaast dat veel games nog altijd voor hun main thread zwaar op 1 core leunen. Ik doelde hierdoor specifiek op grotere verschillen. Dus stel met 4.5 Ghz is het verschil op 1080p 10% dan zou het op 5.0 Ghz beide sores beter uitvallen maar het verschil zou dan ook weleens kunnen stijgen naar 15% (gegeven de veel hogere verschillen op 4K). Techpowerup test ook 'slechts' op 4.8 Ghz dus ik hoop op een hertest zodra de 9700K uit is .
RTX 2080;1;0.4742199778556824;Een tijd terug was er besloten Hairworks uit de testen te houden, of iig dubbel te testen. Dat is er stiekem weer in geslopen?
RTX 2080;3;0.33545514941215515;Dat moet ik even met Jelle afstemmen, da's zeg maar 'voor mijn tijd'
RTX 2080;2;0.3945772051811218;Aan de andere kant ziek ik op 4k eigenlijk geen enkel voordeel meer van AA, tenzij ik naar een stilstaand beeld zit te kijken en dan ook nog eens dicht op het scherm. Dat gebeurt dus eigenlijk nooit en ik zet het standaard uit voor de performance boost. Op een 1080Ti overigens die het dan prima doet op 4k. Ik zie vooralsnog geen enkele reden om deze dure kaarten te kopen. OT: Ik denk dat ik eerder een Vega voor mijn tweede PC aan zal schaffen omdat ik daarop ook compute werkzaamheden doe die double precission floating point gebruiken. Dan moet je bij nVidia meteen naar de dure Tesla kaarten grijpen. (Of een Titan V). Die PC heeft een 1440p scherm, daar is Vega prima voor.
RTX 2080;3;0.4951038658618927;Wel serieuze performance gain. Ik pak op witcher 3 4k ultra 60 fps steady met mijn 1080ti maar dan hairworks off. Zal zelf niet upgraden maar performance verschil is super!
RTX 2080;3;0.5073290467262268;Jullie vergeten dat het vooral de verhoging van het aantal cores en het gebruik van GDDR6 geheugen zijn die bijdragen aan de verhoging van de prestaties. 34% sneller met 21% meer cores vind ik niet zo bijzonder en dus relatief.
RTX 2080;1;0.5094375014305115;Verklaart inderdaad direct het hogere stroomverbruik. Ik wist dat met de 1080TI het tijdperk was aangebroken om 4k 60fps te gamen, juich dus de ontwikkelingen toe! Echter is het zo dat Nvidia zoiets heeft van: succes er mee, AMD komt met niks dus ik verhoog mijn prijzen lekker! Gebrek aan concurrentie is erg jammer te noemen..
RTX 2080;3;0.43344846367836;~35% meer performance, maar de extra prijs is absurd.
RTX 2080;1;0.4458369314670563;Ja, ik lees in het artikel alleen 70% duurder. Ik ga voor het gemak uit van de goedkoopste 1080ti, is 699 euro. Dus bijna 1200 euro voor de kaart, kan toch niet waar zijn?
RTX 2080;2;0.445667564868927;Dat is het dus wel. Een 1080 Ti is een veel betere keuze op dit moment. Prestatie verlies tozv een 2080 (non Ti) is nog geen 10%. Ben overigens wel benieuwd naar de overclockbaarheid van beide kaarten. Edit: overclock resultaat. Custom kaarten met overclock is zo'n 8-9% sneller als de stock kaart. Erg veel potentie is op dat gebied niet te halen dus.
RTX 2080;2;0.45978015661239624;"Dat is dus 8-9% op de founders edition, die standaard alweer 10% overclocked zijn, dat is dus bijna 20% vanaf de ""stock"" kaart.... Lijkt mij niet een heel slecht resultaat op een nieuwe kaart met een nieuw procedé."
RTX 2080;5;0.2573181688785553;Ik denk dat je verkeerd om kijkt... de 2000 serie is de founder, dus de stock 2080 = 1080ti qua performance.
RTX 2080;2;0.4366229176521301;Hoe duur was de 1080TI op release? Als je vanaf die prijs rekent met de performance increase kom je aardig uit. op release was de 1080ti ook 999 en als deze kaart 30% beter presteert zou je dus ook op ongeveer 1200 uitkomen. Ik denk dat deze prijs omhoog gegaan is omdat de huidige kaarten door de mining gekte niet ver genoeg in prijs gedaald zijn en daardoor dus de meer prijs voor de nieuwe kaart op een Hogere prijs komt dan de vorige keer. Was deze verder gedaald met 200+- dan was de nieuwe 2080Ti waarschijnlijk ook 1000 geworden
RTX 2080;1;0.5217928290367126;Punt één is dan dus dat zowel sinds de release van de GTX1080Ti, en nu met de release van de RTX2080Ti de prijs/prestatie verhouding nul verbeterd zou zijn. Dat is geen positief teken voor ons als consument, en ook nou niet echt een teken van een 'redelijke' prijs. Vervolgens van wat ik zo snel in de Pricewatch heb gechecked is elke GTX1080ti die ik nu nog populair is voor tussen de €800 en €850 ruwweg gereleased, dus heel veel goedkoper dan €999.
RTX 2080;2;0.4113234281539917;Tuurlijk als consument wil ik ook alles goedkoper hebben, dan bedoel ik niet onredelijk goedkoop. Maar ook daarin tegen gok ik dat de prijzen meer in verhouding zijn als 1080Ti = 2080 en 2080Ti is dan de zo geheten nieuwe top kaart ipv de Titan. het moest in mijn vergelijking inderdaad niet de 1080 Ti zijn maar de Titan X
RTX 2080;3;0.495127409696579;Maar misschien haalt de kaart straks wel 400% betere fps in een game met raytracing.
RTX 2080;2;0.4231574535369873;De prijs is omhoog gegaan omdat het grotere chips zijn dus kosten yield bin hoger zijn dan vorige gen. Je krijgt meer chip wat beter op 10nm tot zijn recht zou komen ipv 12nm , nV kan dus zonder concurentie de marge hoog houden.
RTX 2080;3;0.2697345018386841;pricewatch: Inno3D GeForce RTX 2080 Ti X2 OC Goedkoopste 2080ti die ik in de pricewatch kan vinden is nagenoeg 1200
RTX 2080;3;0.4434202313423157;Ik las bij Hardwareinfo het volgende M.a.w. als de beschikbaarheid goed is, en er geen te grote hype komt moet dat enigzins haalbaar zijn.
RTX 2080;1;0.5838048458099365;De 2080Ti wordt volgens de pricewatch anders gewoon vrolijk aangeboden ergens tussen de 1200 en 2200 (!) euro. Geen idee hoe HW,info erbij komt dat MSRP gehanteerd gaat worden, want dat is nooit.
RTX 2080;3;0.26614540815353394;Doorsnee prijs voor de 1080ti is 780+...
RTX 2080;1;0.3782038986682892;Ik heb ze al voor € 640 gezien, nu uitverkocht, maar ik sluit niet uit dat we ze nog voor 600 euro gaan zien.
RTX 2080;2;0.5933809280395508;35% in 2 jaar is niet indrukwekkend eerlijk gezegd. Lijkt me een perfect normale generatie bump.
RTX 2080;2;0.48864439129829407;ik vind dat men hier echt belachelijk aan het vergelijken is. De verhoudingen zijn ongeveer gelijk met de vorige generaties, ook toen was het verschil ongeveer gelijk. Je moet bv de 1080 vergelijken met de 2080 en dan is verhoudingsgewijs het verschil gelijk aan de 980 vs 1080..
RTX 2080;1;0.4322708249092102;Dat bedoel ik idd. Een normale evolutie. Maar dat verantwoordt de prijsverhoging niet. Na 2 jaar zou die 2080 eigenlijk maar voor 600-650€ over de toonbank mogen gaan en daarmee de vorige generatie vervangen. Nu vervangen ze pascal niet, maar krijgen we een tier erboven en blijft het in de 0-600€ range stil staan. Enthusiasts gaan kopen, maar mainstream blijft op pascal en minder. Dus de vraag blijft hoever die nieuwe technieken op RTX echt gaan van de grond komen. Zeker bij dat AI gebeuren vraag ik me af of dit gratis is? Moeten developers betalen om dat te gebruiken of niet?
RTX 2080;3;0.38362884521484375;Maar de 1080 etc was indertijd ook een tier die boven de 900 serie kwam bij de introductie.. Is gewoon precies dezelfde situatie als bij de vorige series.. Enige verschil is dat ze nu gelijk de TI variant uitgebracht hebben. En nee developers hoeven niet te betalen om RTX extentie te gebruiken, dit kan gewoon aangestuurd worden van DirectX en Vulcan, die hebben voor Raytracing inmiddels ook specifieke API's voor.
RTX 2080;2;0.5068669319152832;Toch niet. De 1080 verving rechtstreeks de 980 en had ook een prijs die in dezelfde buurt zat. uiteraard is die enkele maanden later gestegen door de bitcoinhype. Maar een 980 had dit ook gedaan mocht het een jaar eerder gebeurd zijn. Nu is de 2080 véél duurder dan een 1080 die eigenlijk ook nog boven zijn 'end of life' prijs zit. maw een totaal andere prijsklasse en dus ook een andere markt wat gebruikers betreft. Maar ik verwacht dat ook die prijs zal dalen het komende jaar. Gebeurt dat niet zitten we met een groot gat in de markt in de prijsklasse 400-600€ en zal een concurrent die zeker invullen. Maar we hebben geduld nodig hiervoor.
RTX 2080;3;0.31763124465942383;Zou de i7 4770k een bottleneck voor de rtx 2080 zijn? Heb nu een gtx 1070..
RTX 2080;2;0.3617872893810272;Ligt helemaal aan de game. Ik heb een i5 6600 geklokt op 4.8 met een 1080 Ti. Guild Wars 2 op ultra is op 1080p al niet meer te spelen en ik heb een 1440x3440 scherm. CPU is duidelijk de bottleneck in deze game. GPU is 40% load, CPU alle cores 100% capped. Ik moet specifiek een aantal settings naar beneden halen om een paar procent meer GPU load te halen en een gemiddelde 60 fps vast te kunnen houden. CPU load zit dan op 80% over alle cores. Meeste games zijn voor mij geen probleem op hoge res (ik game niet op 1080p, zonde van de 1080 Ti). Ik weet niet precies hoeveel sneller mijn 6600 is op 4.8 ten opzichte van jouw 4770. Ik hoop niet dat je met de 2080 op 1080p gaat gamen haha! Misschien 1080p op 244 hz scherm met g-sync? Dan gaat de CPU verwacht ik wel wat vaker een bottleneck vormen. de 20-serie lijkt wel echt gemaakt voor hogere resoluties.
RTX 2080;1;0.3870864510536194;Nee denkt ‘t niet. Mooie is dat ‘t een i7 is.
RTX 2080;3;0.4892288148403168;Dit zijn betere prestaties dan verwacht! Ik had verwacht dat het verschil tussen de 1080Ti en de 2080Ti niet groot zou zijn. Maar gemiddeld 30-35% sneller op 4K resolutie maak het eigenlijk dat de 1080Ti net niet klaar was voor 60+ FPS en de 2080Ti wel.
RTX 2080;2;0.4613989293575287;Alleen de prijs is jammer. De kaarten zijn 30% sneller maar kosten €1.300,- (!!!!) bij Alternate. De prijs/kwaliteitsverhouding is gewoon bar en bar slecht. Vergelijk dat met de 1080 en de 980 in die tijd, ook 30% sneller maar de 1080 was destijds iets van €800,- De RTX 2080 zou een prima kaartje zijn als de prijs rond de €800 bleef. Maar een belachelijk bedrag van €1300 voor maar 30% winst is gewoon een domme investering.
RTX 2080;2;0.39280620217323303;De 1080 was qua prijs veel meer in lijn met zijn voorganger. Wat Nvidia nu heeft gedaan is de 2080 even snel én even duur gemaakt als de oude kaart. Bravo. Qua prijs-prestatie schiet je dus niets op
RTX 2080;2;0.38313624262809753;"Nee hoor. Prijs en prestatie zijn allebei hoger en daarom is de prijs-prestatie verhouding gelijk. gebleven. Mede daarom denk ik ook dat deze kaart toch wel weer goed gaat verkopen. Dat is dan het ""excuus"" om er dat geld voor uit te geven."
RTX 2080;1;0.6543689966201782;inderdaad, de GTX980 en de opvolgende GTX1080 lagen niet in verhouding qua prestatie en prijs. Vergeet niet dat iedere nieuwe kaart in het verleden gewoon veel sneller was dan zijn voorganger, maar tegen redelijk dezelfde prijs werd verkocht als zijn voorganger. Het is net of iedereen helemaal debiel is geworden... hoe vaak ik niet lees dat de prijsstelling van Nvidia wordt goedgepraat. Het is niet zo dat als een nieuwe generatie GPU twee keer beter presteert de prijs ook maar twee keer hoger moet worden. Het probleem zit h'm in het feit dat er geen concurrentie meer is. Nvidia kan vragen wat ze willen. Maar mijn inziens is dit een doodsteek voor de PC game industrie. Ik heb lang gevolgd, maar ik ben bang dat ik ook een keer ga afhaken. dan maar een console, zwaar tegen m'n principes in, maar met deze prijzen is het niet meer te doen. De PC game markt staat al flink onder druk door grote concurrentie van de consoles. Dan helpt heel dure hardware daar niet meer bij. Ik heb veel vrienden zien overstappen naar een Xbox of playstation vanwege deze reden.
RTX 2080;3;0.43951621651649475;Dat kan natuurlijk. Ik zeg op mijn beurt - als ik een console zou kopen, dan ga ik er qua resolutie op achteruit (PC draait op 3440x1440). Mijn R9 Fury draait de meeste games nog best lekker, misschien niet met alle instellingen op Ultra, maar met een beetje schuiven komt hij nog prima mee. Shadow of the Tomb Raider bijvoorbeeld draait prima. Dan maar geen 4K @ 60 fps met alle instellingen op Ludicrous, maar die haal je op de PS4Pro of de XB1X ook niet.
RTX 2080;2;0.45506736636161804;Eens, ik heb een tripple monitor opstelling op een single RX480 (patch naar geknepen RX580)... En daar kan ik vooralsnog wel aardig mee uit de voeten zolang ik genoegen neem met minder prestaties. En dat doe ik ook, vooralsnog, het houd wel een keer op. De Beta van BF 5 was praktisch onspeelbaar en ik mag hopen dat dit nog opstart problemen zijn, maar ik vrees van niet. Ik zou graag upgraden naar iets snellers, maar niet tegen elke prijs. Punt is dat Nvidia de prijs zo kan opdrijven omdat er niets in de markt is dat net zo goed of beter presteert, een AMD alternatief in de vorm van een Vega zou ik overwegen, maar die zijn ook schreeuwend duur.
RTX 2080;1;0.5063698887825012;"Vreemd.... BFV draaide op mijn RX480 zonder problemen dus snap ik je ""praktisch onspeelbaar"" opmerking niet. Ik heb ook een triple monitor opstelling maar ik ga er vanuit dat je op 1 monitor aan het spelen ben en niet in 5760x1080. Zo ja, dan denk dat het ""onspeelbare"" aan je CPU lag en niet aan je GPU!"
RTX 2080;4;0.29821285605430603;Ik heb geen idee, het speelde wel op de volledige resolutie, dus 5760x1080. Laten we de final game afwachten voor ik verder commentaar geef, want eerlijk gezegd weet ik ook niet zeker of het aan de GPU lag. mn pc is iig voorzien van 16GB ddr4 op een 6600K (@4.4Ghz) en volledig SSD ingericht. vrienden hadden minder problemen met minder specs, dus wie weet was het een ander probleem. thanks iig voor je reactie.
RTX 2080;2;0.36922377347946167;Tja, op het moment dat je als developer een spel maakt dat enkel op bleeding-edge kaarten wil draaien, dan durf ik te stellen dat het niet de GPU-makers zijn die de PC-gaming markt om zeep helpen, dan zijn het de PC games zélf die dat doen. En dan nog - de XB1 en de PS4, daar zal BF5 ook op verschijnen. Die hebben geen 1080 (Ti or otherwise) aan boord. Als het dan op de PC wél nodig is om een 1080 te gebruiken, dan is er iets fundamenteel mis met de PC-versie, IMHO. Een RX480 is een redelijk recente kaart, en zeker geen lichtgewicht. That being said, hoewel ik denk dat AMD een goed punt heeft in dat ze de top-end op de PC links laten liggen (dat begint steeds meer een niche-markt te worden, heb ik het idee) en meer mikken op de midrange en IGP's, waar veel meer marktaandeel zit, is het wel zo dat in de top-end dan nVidia zich van de lelijkste kant kan laten zien, want die laten nu pas echt zien wat voor smerige marktpraktijken ze erop na houden. Monopolisme ten top, protectionisme ten top en open standaarden, ho maar.
RTX 2080;2;0.5598922371864319;Eens, Maar geef het enkele maanden en die prijs komt echt wel naar beneden. Het prestatieverschil is echt zijn meerprijs niet waard. Er is een opening voor AMD eens ze klaar staan met Navi binnen een half jaar of zo.
RTX 2080;2;0.26149263978004456;wat let je om gewoon op oude hardware te blijven, de ontwikkeling in requirements is best wel gestagneerd in de laatste 5-10 jaar. En onder druk?, PC game markt groeit nog steeds exponentieel uit boven de consoles
RTX 2080;2;0.6020369529724121;Tja, heel begrijpelijk. Ik heb mijzelf ook al meerdere keren afgevraagd wat ik zou doen wanneer de consoles toetsenbord en muis zouden gaan ondersteunen (Ik kan echt niet meer met een controller overweg in shooter games ). Al is het maar voor naast de PC voor de exclusives die uitkomen op vooral de PS. Maar aan de andere kant is gamen op 30FPS (wat nog regelmatig voorkomt) toch wel pijnlijk denk ik en dan ga ik ook enigszins de exclusives bemoedigen waar ik ook weer op tegen ben... enz enz. Zakelijk gezien snap ik het wel, maar het is gewoon erg jammer. Laten we hopen dat zelf de diehard fans deze kaarten iig even met rust laten om een seintje af te geven naar nvidia. Mag hopen dat met deze prijzen er toch wel een stuk minder mensen zijn die early adopter willen worden.
RTX 2080;3;0.4193729758262634;Maarja, de 2080 is ook een stuk sneller dan de 1080 (zo'n 40%), dus op dat gebied ligt het gewoon exact in lijn met de vorige generatie, en ook bij introductie was de 1080 ronde de 800+ euro. de 980ti lag in lijn met de 1080 indertijd, net zoals nu de 1080ti in lijn ligt met de 2080.. dus op dat gebied is er weinig veranderd.. (Wil niet zeggen dat ik sowieso vind dat de prijzen veel te hoog zijn, maar dat vond ik al bij de 1080 reeks).
RTX 2080;2;0.3753146827220917;Dat denk ik niet want er zijn nog grote onzekerheden wat RTX enabled games met de FPS doen. Omdat er toch ook bij de aanzienlijke meerprijs deels gecompenseerd wordt met performance gain is de consument die meer bang for tha buck gericht is zit nu in de twijvel zone . De consument die vorige gen inruild voor nextgen de vaak upgrader is huidige resultaat overwegen waard omdat RTX minder toedoet en hun toch als RTX op gang komt voor refresh van turing gaan. Voor diegene die langer met hardware doen en ook meer RTX feature voor willen gaan is uiteraard de RTX HQ resultaten en performance resultaten belangrijk. En met gebrek aan zero RTX games nu geen RTX review ook geen beeld vergelijking en RTX fps performance . Want heeft 2070 wel genoeg RTX power voor voldoende FPS in full RTX enable setting bij release RTX enabled games of heb minstens 2080 nodig. Of heeft 2080Ti nog moeite. Komt RTX pas goed tot zijn recht met 2 2080ti in nvlink mode? Kwa prijs komt voor kij maximaal de 2070 kwa budget in 2019 als 2060 ook RTX heeft en nog zin heeft met RTX enabled games mits die RT cores krijgt.
RTX 2080;2;0.5036483407020569;Wat Nvidia nu heeft gedaan is de 2080 even snel én even duur gemaakt als de oude kaart. Ok, ik denk dat ik snap wat je bedoelt. Maar dan moet je bovenstaande zin even aanpassen, want dat klopt toch niet met wat je wil zeggen? Dit komt namelijk heel vreemd over. Tenminste bij mij. Los hiervan vergelijk ik geen launch-prijzen met de actuele prijzen van een kaart die al 2 jaar uit is. Vergelijk dan de prijzen eerlijk en pak de MSRP prijzen bij launch van beide kaarten. Bij HWI doen ze dit wel en dan kun je zeggen dat de prijs/prestatieverhouding gelijk is gebleven. Ps niet meteen op de man gaan spelen ... beetje jammer! Ehm, ik denk dat je even terug moet naar de basisschool.
RTX 2080;3;0.2989374101161957;Alsof €800 al niet veel te veel is voor een 2080.
RTX 2080;2;0.35232871770858765;Neem je dan wel even een flinke zak geld mee voor de extra performancewinst? Of vergeet je dat gemakshalve? Want als je eerlijk kijkt naar de prijs/prestatieverhouding tussen de 1080ti/2080 zijn we nauwelijks iets opgeschoten.
RTX 2080;2;0.45068296790122986;Eens, maar feitelijk klopt de opmerking van @mprkooij wel. ik had ook een lager verschil verwacht en wellicht stort die prestatie ook wel in met RTX aan, RTX is echter alleen beschikbaar op die 2080(Ti). De prijs vind ik zelf belachelijk tot het absurde toe, maar dat vond ik de 10xx serie ook al. Gelukkig heeft Tweakers hier ook aandacht voor. Nvidia heerst hoog in de ivoren GPU toren en daar betalen wij helaas de prijs voor...
RTX 2080;5;0.5079717636108398;Wij hebben NVIDIA daar ook in geplaatst. Je kan ook 'stemmen' met je portemonnee hè
RTX 2080;1;0.4938909411430359;"Ik niet, ik heb een Rx480 gekocht vanwege bovengenoemde reden! Het verbaast mij dat er mensen staan te springen om een RTX 2080Ti aan te schaffen, of uberhaupt een 1080(Ti) hebben gekocht laat staan in SLI.... Hoe in hemelsnaam kun je dat betalen, hoe maak je de overweging om 600+ euro uit te geven aan een videokaart, en in het geval van een RTX gaat dat gewoon x2. ik kan er met m'n verstand niet bij.... Edit: als antwoord op onderstaande; Misschien is de gemiddelde gamer rijker dan ik kan inschatten, maar ik vrees dat die gemiddelde gamer dat niet is. gerelateerd aan mijn uitgave patroon en voor wat ik over heb voor een gemiddelde videokaart relateer ik dat ook aan wat ik in het verleden kreeg voor een bedrag waar ik nog voor mij zelf een verantwoorde keus kon maken. Ik kocht gemiddeld een kaart die net onder de top zat, zoals mijn laatste nvidia, een GTX770. echter die was toen nog betaalbaar en ruim sneller dan zijn voorganger. Prijzen zijn sinds de GTX 10xx serie dermate hoog gestegen dat ik een enorme antipathie heb gevormd richting Nvidia, als de kaarten nu echt een enorme verandering hebben doorgemaakt tav eerdere ontwikkelingen, dan zou ik er het geld voor over hebben. Echter heb ik het idee dat Nvidia extreem geld aan het binnenharken is voor relatief weinig effort. Er is praktisch geen concurrentie waardoor zij zich deze prijsopdrijving kunnen permitteren."
RTX 2080;1;0.45107361674308777;dat is natuurlijk allemaal heel relatief per persoon waar het inkomen en geluksfactor heel veel in bepalen. als jij 5k per maand verdiend en je enige hobby daarnaast is gamen en je verdere uitgaven zijn minimaal dan is een kaart van 600 + en zelfs 1300 euro natuurlijk prima te verantwoorden als geluksfactor hiermee stijgt. en hoe ze het kunnen betalen? de it sector betaald gewoon erg goed in verhouding met andere sectors voorals als je freelancer bent, en dan nog, als je enkel als hobby gamen hebt en niet elke week je geld uitgeeft aan uitgaan of restaurantjes, dan hou je natuurlijk een aardig bedrag over om aan gamen uit te geven. zo koopt een hobbymatige wielrenner weer een fiets van rond de 2k ( en volgens mij is dit nog een mild bedrag zelfs ), wat voor velen alweer als een onmenselijk bedrag wordt gezien. toen later een tweedehands gtx 980 gekocht ( toen net de 1080 uitkwamen ) voor €350. en sinds kort een €1080ti voor €550 tweedehands, voor mij een behoorlijke jump in prijs, maar er is gewoon geen goed alternatief ( en geld is nou ook niet echt een issue ik ben meer gaan verdienen en had voorheen zo ' n bedrag als te gek gevonden, maar blijf wel prijsbewust ), daarbij ik had deze performance gains al redelijk voorspeld voor 2080 en zag die prijs totaal niet zitten en raytracing op 1080p60fps is gewoon een gimmick dan wat de markt echt wil, maarja ik denk dat ik het voor mijzelf zo goed kan praten als ik nu letterlijk geen videokaart zou hebben... je punt was hoe mensen een prijs van 600 + kunnen verantwoorden voor een videokaart, en volgens mij heb ik dat redelijk uitgelegd en is het dan toch echt niet zo gek dat dit uitgave patroon bestaat?
RTX 2080;1;0.5005244612693787;Tja een pakje sigaretten per dag roken kost je €2372,50 per jaar. Daar koop je meerdere RTX kaarten voor. Het is maar waar je je geld aan uitgeeft.
RTX 2080;1;0.599791944026947;"Ik rook niet, nooit gedaan ook...;)"
RTX 2080;5;0.33552300930023193;Ach, gelukkig, was al op zoek naar deze comment.
RTX 2080;1;0.5648109912872314;D'r staat hier een machine met zelfs 3 1080Ti's. En dat was alleen maar omdat het RAM in de weg zat voor een vierde kaart. Geen game PC, uiteraard. Deze staan aan Neurale Netwerken te rekenen. Maar ook Crypto Miners zijn NVidia klanten. En dan gaat de wet van vraag&aanbod werken, ja.
RTX 2080;5;0.43087735772132874;juist, vraag en aanbod waar Nvidia op dit moment flink z'n zakken mee aan het vullen is. Ze hebben een goed product, dat zal ik niet ontkennen. Beter dan AMD en daar profiteren ze nu van door prijzen te vragen die hoger liggen dan (in mijn beleving) noodzakelijk. En ik weiger daarin mee te doen.
RTX 2080;1;0.43711936473846436;Ik snap je boycot mentaliteit, maar wat als AMD of ander bedrijf nooit met een alternatief gaat komen of het met beetje geluk in 2020 gaat uitkomen (met Navi, wat net als Vega het prodigal wonder moet gaan zijn, we weten hoe het met Vega is afgelopen, en daar hadden ze letterlijk een boywonder voor ingelijfd, die nadat het faalde net zo hard is vertrokken). Blijf je dan koppig en ontzeg je dan je eigen plezier, of zwicht je dan toch want reeel gezien je boycot zal in het grotere plaatje geen zak uitmaken Nvidia vult zijn zakken toch wel, behalve dat je jezelf wellicht te kort doet. Ik ben ook geen fan van nvidia, wat ze met GPP probeerde te flikken, en hoe ze letterlijk AMD in de hoek drijven op elk vlak wanneer AMD denkt iets unieks in handen te hebben (1070 TI anyone?). Closed source metaliteit, G-Sync vs Freesync, GPUopen vs GameWorks, Hairworks met opzet cripplen op AMD cards (bron: Letterlijk conccurrentie moord. Het cruwe is dat AMD ze wel altijd de hand toerijkt, in wat zij hebben developed Nvidia daar net zo royaal van gebruik mag/kan maken. Maar ik ga mezelf echt niet te kort doen en een AMD kaart kopen om maar een idealist te zijn, daar heb ik alleen mezelf mee, de beste technologie ligt op dit moment nu eenmaal bij Nvidia.
RTX 2080;3;0.4551123082637787;Ik boycot Nvidia niet bewust, ik heb een AMD gekozen vanwege het gunstiger alternatief. Ik zal ook niet ontkennen dat mijn plezier factor het soms wint van rationeel denken. Echter ergens heb ik wel sterke beperkingen tav wat ik over heb voor een kaart, wat vind ik nog verantwoord. Als Nvidia nu de GTX 1070Ti of hoger tegen een gunstiger prijs kan aanbieden dan nu het geval, koop ik die kaart. Want dan voldoet die kaart aan mijn maximum gestelde verantwoorde prijs. en dat is altijd een afweging. Ik verdien prima (IT baan), maar heb ook een gezin te onderhouden.
RTX 2080;2;0.3625484108924866;Dat is toch echt wel AMD's schuld ook omdat ze alles open source doen. Je kan zeggen ja dat is idealistisch maar kijk hoever het hen al gebracht heeft. Ze zouden beter meer closed source doen en er een grote hoop geld mee vangen want ze hebben het echt nodig. AMD vind veel nieuwe technieken uit maar dan maken ze het voor iedereen beschikbaar. Ik vind dat dom. Sommige dingen zijn wel goed dat je het open source doet zoals bijv. Freesync maar toch niet alles.
RTX 2080;3;0.3793600797653198;Uiteindelijk is het wel bewezen dat voorsprong in techniek versneld wordt door Opensource. Ik denk niet dat AMD hier van gaat afstappen, ook om juist de mentaliteit ook anders kan te laten zien (aan Nvidia). AMD GPU tak redt het voorlopig nog wel, ze zijn nog steeds hof leverancier voor de consoles (zowel xbox als ps4). Echter op de pc highend markt is het voor nu wel bekeken, hoop dat ze er van terug kunnen komen het is ze eerder gelukt. Hier uitleg van een AMD engineer zelf over het belang van GPUopen
RTX 2080;2;0.4191896617412567;Als men braaf iNtel en nV met hun beleid blijven belonen krijgt AMD kapitaal niet om in R&D en highend klasse chips te investeren . De situatie nu is wat gamers gewoon verdienen dure hardware omdat men voor paar procent verschil al de concurrent kiest. CPU tak heeft de GPU tak de afgrond mee ingesleurt het is dus ook nu met Zen comback dat de GP.u tak juist de omzet in de CPU tak nodig heeft om in de toekomst aan slag kracht te groeien. Dus als nvidiot betaalbare nV wilt hebben in de toekomst koop je nu Ryzen aan je nV als meer wilt pushen ook Vega. Ik verwacht iig voor near future geen big chip gpu van AMD. Gezien API in samenwerking met hardware vendor tot stand komen verwacht ik dat ook iNtel GPU en AMD nextgen ook in hardware DXR gaat ondersteunen . Mits iNtel met een consumenten versie komt en van AMD meer kleinere sub highend chip zal worden. Maar pas voor de gen na Navi. Dan zou het als nV hard gepush heeft ook aardig wat games er al zijn die RTX ondersteunen.
RTX 2080;2;0.5183842778205872;Terwijl het gros van de potentiële kopers graag de performancewinst hadden willen zien voor de prijs van een 1080ti. Nu is dat niet bereikbaar voor veel gamers naar mijn bescheiden mening.
RTX 2080;1;0.5609596371650696;"Wat een loze opmerking. Waarom zouden ""kopers van deze kaart"" het niet interesseren dat de kaart peperduur is. Deze logica ontgaat me volledig. Dat deze groep ze hoe dan ook koopt is een hele andere stelling."
RTX 2080;3;0.4636576771736145;Precies, misschien lig je er wel van wakker omdat je je vrouw moet vertellen dat je gezin 1 week korter vakantie kan hebben. Ze hadden beter 'geen belemmering' op kunnen voeren. Want ik had hem echt liever voor 1Euro dan voor 1200Euro gezien.
RTX 2080;2;0.41346728801727295;Omdat het VEEL duurder is dan toen de Geforce GTX 1080 (Ti) uit kwam, en dat ze niet heel veel sneller zijn dan de Geforce GTX 1080 (Ti), als een grote groep net aan die konden kopen, en nu deze veel duurder zijn en niet heel veel sneller zijn, dan kan ik je garanderen dat het verkoop met de Geforce RTX 2080 Ti veel minder is, en dat mensen langer gaan wachten tot ze goedkoper woorden, of zelfs wachten op de volgende generatie die dan wel een STUK sneller is dan de Geforce GTX 1080 (Ti). De Geforce RTX 2080 Ti maar 30 a 40% sneller is dan de Geforce GTX 1080 Ti. De verschil tussen de Geforce GTX 980 (Ti) en Geforce GTX 1080 (Ti) is VEEL groter en is +/- 65 a 75%, zo als je goed kan zien op Guru3D, vind het erg jammer dat Tweakers geen eens de Geforce GTX 980 (Ti) hebben mee genomen. De verschil tussen de Geforce GTX 1080 Ti en Geforce RTX 2080 maar 5 a 10% is (behalve Wolfenstein II: The New Colossus), en in meerdere maar een paar procent, wanneer de verschil tussen Geforce GTX 980 Ti en Geforce GTX 1080 veel groter is, 30% en meer. Maar de meeste mensen kopen geen duure grafische kaarten, en kopen Geforce kaarten zo als de Geforce GTX 1050 (Ti) en 1060, de mensen die 1080 of hoger kopen zijn maar een kleine groep, en zo zal het ook gaan met de RTX 20*0 serie, zo als je duidelijk kan zien op de Steam hardware survey. steampowered.com/hwsurvey
RTX 2080;2;0.3720020651817322;Er is groep die alleen absolute performance interresseert die pakt er meteen 2 met nalink bruggetje. De grotere High-end groep kijkt toch ook naar de prijs en speelt bang for the Buck deels ook een rolletje of meer of minder. De normale gang van zaken bij een nestgen is dat productlijn binnen de prijs punten door snellere nestgen vervangen worden voor die prijs. Dus upgrade pad van toen een 10xx nu 20xx gaat niet op wordt model lager 20yy als dezelfde upgrade budget aanhoud.
RTX 2080;2;0.5171788334846497;Ehh, het is een STUK kleiner dan de Geforce 980 Ti en de Geforce GTX 1080 Ti, daar was het +/- 65 a 75%, en dat de verschil tussen de Geforce GTX 1080 Ti en Geforce RTX 2080 maar 5 a 10% is (behalve Wolfenstein II: The New Colossus), en in meerdere maar een paar %, wanneer de verschil tussen Geforce GTX 980 Ti en Geforce GTX 1080 veel groter is, 30% of meer. Dus ja ik vind het heel erg tegen vallen, en , maar dat had ik al gezien in de uitgelekte benchmarks.
RTX 2080;3;0.34905484318733215;Wat ik niet uit het artikel kan opmaken is welke GTX1080TI kaart is gebruikt. Is dit ook de Nvidia GTX0180TI FE geweest of is een AIB kaart (zoals de Asus Strix) gebruikt. Er zit namelijk nogal wat performance verschil tussen een Asus GTX1080TI Turbo en een Asus Strix GTX1080TI.. Uiteraard komen vandaag ook de andere reviewers met hun reviews en zijn tonen in de regel wat wel merken zijn gebruiken in hun tests..
RTX 2080;5;0.3575684130191803;De 1080 Ti FE is gebruikt.
RTX 2080;3;0.425790399312973;Dat is jammer ee had wel een custom variant meegenomen moeten worden, die is wel zo’n 5/10% beter dan een FE.
RTX 2080;1;0.5492212176322937;Juist niet, anders heb je geen objectieve vergelijking.
RTX 2080;3;0.33643704652786255;Je hebt gelijk als de 2080TI dezelfde koeler had gehad. Nu is de koeloplossing van de 2080TI FE editie beduidend beter dan de koeler van de 1080TI FE editie. Dus denk dan ook dat de custom varianten niet onwijs veel beter zullen zijn.
RTX 2080;5;0.3109898567199707;Oei, dat wordt kiezen tussen 1080 ti en RTX 2080 voor mij
RTX 2080;3;0.4684368968009949;"De 1080TI zal nu een goede koop zijn gezien de prijs. Helemaal als je deze in de V&A kan halen kan het best heel veel schelen in vergelijking met een 2080; dat terwijl de performance dus redelijk gelijk is."
RTX 2080;3;0.5252076387405396;Ik vind het persoonlijk toch niet zo´n makkelijke keus. Ja, de 1080TI is nu +- 130 euro goedkoper dan de 2080, maar de 2080 heeft toch wat winst(jes) in fps en bovenal nieuwe(re) technologie. Het is maar net wat je belangrijker acht denk ik zo..
RTX 2080;2;0.4599016606807709;Vergeet ook niet dat de 2080 nog wat optimalisaties te gaan heeft wat betreft de drivers. Dit duurt altijd een paar maanden voordat de optimale prestaties zichtbaar worden. Echter zijn kinderziektes dan weer ook nog niet opgedoken.
RTX 2080;1;0.40078458189964294;Vergeet ook niet dat de basis gewoon hetzelfde is als een 1080ti, als er nog noemenswaardige winst in zit is dat voornamelijk in die Ai en tensor cores, dat is namelijk het enige wat echt nieuw is.
RTX 2080;3;0.5323097705841064;In de basis is ook wel wat veranderd hoor.
RTX 2080;1;0.33834928274154663;Eerst maar eens bij jezelf nagaan of je op dit moment uberhaupt wel een nieuwe kaart nodig hebt...
RTX 2080;3;0.3768557012081146;Mijn gtx 970 is echt geen drama hoor, maar na een paar jaar ben ik wel toe aan iets nieuws. Als ik nu al een 1080 ti had gehad, dan zou ik het niet eens in mijn hoofd halen om te upgraden naar een 2080.
RTX 2080;2;0.5790889859199524;Nee, ik vind het ook logisch als je al een 1080ti zou hebben om dan niet voor de 2080 te gaan. Heb zelf nog steeds de 760 en ben ook al heel lang toe aan een nieuwe, maar ik vind de prijzen te achterlijk voor woorden, 300+ voor nog niet eens een echte midrange kaart (1060/6GB), waarbij ik denk dat die dus niet veel meer dan 200-250 zou mogen kosten.
RTX 2080;2;0.427578330039978;Hoezo logisch als ik vragen mag? Qua performance is er vrij weinig verschil en er zit een behoorlijk prijskaartje aan..
RTX 2080;1;0.490028977394104;Uhm, sorry, was het woordje 'niet' vergeten, logisch dat je NIET voor de 2080 gaat.. haha..
RTX 2080;3;0.433194100856781;Op 4K is het toch wel een behoorlijk performance winst. Dus ook al zou je deze niet voor de RTX ondersteuning kopen zit je nog steeds goed. Wat betreft de prijs, die zal echt nog wel gaan dalen op korte termijn. Ze willen nu eerst de 10XX eruit hebben.
RTX 2080;1;0.3801722824573517;Zoals de prijzen van de 10xx serie gedaald is na de intro zeker ? AKA de kaarten zijn nooit voor de MSRP verkocht maar altijd (ver) daarboven.
RTX 2080;2;0.5372066497802734;Eens, maar met zeer korte termijn bedoel ik wel een aantal maanden. Je ziet dan ook een behoorlijke daling in prijs (GTX1080) echter door een tekort aan kaarten (mining) is de prijs vrij kort daarna weer gigantisch omhoog geschoten.
RTX 2080;2;0.2961908280849457;Wat is het verschil tussen een nvidia titan X en een RTX 2080 TI?
RTX 2080;1;0.5414571166038513;De Titan X (pascal versie) scoort nagenoeg even veel als een 1080 Ti.
RTX 2080;5;0.46243858337402344;aha dank voor de uitleg was al benieuwd
RTX 2080;3;0.44838130474090576;Veel.. iets minder dan een 1080. Dus reken maar uit.
RTX 2080;2;0.46283021569252014;Waar is de tijd gebleven dat prestaties elke 2 jaar verdubbelden? Wat je al jaren ziet bij CPU's, wordt nu ook zichtbaar bij GPU's. Het wordt steeds moeilijker om grote prestatiewinst te boeken.
RTX 2080;2;0.3844441771507263;Ik gok dat het gebrek aan harde concurrentie ervoor heeft gezorgd dat Nvidia minder hoefde te presteren
RTX 2080;1;0.7265936136245728;Ze hadden in de afgelopen 2 jaar de Ray Tracing technologie moeten delen met fabrikanten van de games die zijn uitgekomen in de afgelopen 2 jaren. Laat ze NDA's tekenen en stel kleine teams samen om de RT features in te bouwen. Ook in populaire bestaande games. Daar hadden ze geld tegenaan moeten gooien, desnoods met hun eigen teams van ontwikkelaars. Dan hadden we nu bij de launch wat updates gekregen van de meeste populaire games die honderden duizenden mensen dagelijks streamen via Twitch.tv, en dát zou een perfecte motivatie zijn voor consumenten om ook snel te gaan kopen. Nu heb je dus buitensporig dure kaarten (70% duurder) met een matige snelheidswinst (tot 30%) wat totaal niet in verhouding staat. Zelfs op 4k resoluties lijkt het niet de moeite waard. Want bijna niemand heeft een 4k scherm met een hogere refresh rate dan 60Hz. Dus wat moet je met FPS die gemiddeld boven de 60fps liggen? Dan zou je de grafische settings ietje naar beneden kunnen schroeven om altijd 60fps te halen, en dat kost je niks. Tweakers zou deze kaart dan ook hartstochtelijk en overtuigend mogen afraden vind ik persoonlijk. De 1080-series zijn op dit moment gewoon de beste kaart qua prijs/kwaliteit. En neem de ti-versie als je op resoluties van 1440p of hoger speelt. Deze hele RTX reeks slaat helemaal nergens op. Wat een bijzonder slechte marketingbeslissingen hebben ze genomen daar bij Nvidia.
RTX 2080;3;0.412802517414093;Klopt op dit moment is voor de 1080 de prijs kwaliteit verhouding prima, heeft wel 2 jaar geduurd voordat die prijs flink omlaag ging. Dat zal voor de 2080(ti) ook zo zijn.
RTX 2080;2;0.42621028423309326;En jij denkt dat dit niet gebeurd is? Wat denk je dat die engine-bouwers gedaan hebben de afgelopen tijd? Voor oa UE4 en Unity is het eigenlijk niets meer dan een switch omgooien mits je al op een fatsoenlijke manier je models/textures gebruikte (omdat die informatie ook al nodig was voor de huidige belichtingsberekeningen etc). Ja, de ti versie is een heel stuk duurder, maar ga ik kijken in de pricewatch, en vergelijk even een doorsnee 1080ti met een doorsnee 1080, dan zie ik dat het prijsverschil procentueel gezien niet heel veel anders is dan nu tussen de 2080 en de 2080ti. En vergeet niet dat de 1080 FE bij introductie ook niet veel goedkoper was dan de 2080 FE nu is.
RTX 2080;1;0.6319372653961182;Kijk op Twitch.tv naar de meest populaire games. Geen van allen bouwen eventjes die switch in. Dat kost tijd en geld, en in het geval van veel games ligt de focus vast geheel anders dan een paar procent van hun gebruikers die ray tracing kunnen zien. Dus ik denk inderdaad dat Nvidia hier niet mee bezig is geweest. Want ik zie alleen opkomende games met de RT feature, niet bestaande games. Er zijn op dit moment letterlijk nul games die het ondersteunen.
RTX 2080;2;0.3497259020805359;Als je je eigen engine hebt geschreven, nee, dan zul je die niet even inbouwen, maak je nog gebruik van een oudere engine (bv UE3), nee dan bouw je het niet even om nee. Maar ben je nu in development en maak je gebruik van de huidige UE/Unity, dan ja, is het in principe niets meer dan een switch omgooien.. Maar ik heb het ook nergens gehad over bestaande games, maar over games die nu in development zijn. Je modellen/textures (als je fatsoenlijk bezig bent in iedergeval) geef je nu al bv wat voor materiaal het is zodat de engine de (fixed) belichting kan berekenen, alleen met de RTX extentie gebeurd dat dus voor een hoop effecten dan realtime ipv pre-calculated.
RTX 2080;2;0.6298213005065918;Vooral onder de indruk hoe goed ze het doen op 4k. 4k gamen op ultra is toch gewoon waar we naar toe moeten. Maar de 2 grote nadelen blijven voor mij onacceptabel. 1. Energieverbruik: gebaseerd op mijn verbruik en game tijd kost dit me op jaarbasis al 50 euro meer! Dat is over de levensduur van 4 jaar wat realistisch is voor mij 200 euro meer! Dat moet je dus al bij de aanschaf peijs van de kaart optellen. Het is dan ook jammer om te zienndat eeze boost wel puur ten koste gaat van stroomverbruik. Het feit dat hij goed te coolen is is dan wel weer netjes en maakt mij nieuwsgierig wat voor oc versies er gaan komen. 2. De prijs. Dit is gewoon niet op te brengen dus misschien over 2 generaties heb ik een kaart met deze prestaties voor een normalere prijs.
RTX 2080;3;0.4613267183303833;Dat is natuurlijk omdat je op 2560x1440/3440x1440 en vooral 3840x2160 minder snel tegen CPU bottlenecks aanloopt. Ter ziener tijd wel, maar je moet daarvoor diep in de buidel tasten en het is niet een gegeven dat alles erg vlot met lage frametimes zal lopen op Ultra 4K op dit moment. (of instellingen terugschroeven) Maar dit telt vooral voor mensen die erg kritisch zijn op de respons van de muis, liever op hoge refreshrates gamen met honderd(en) frames per seconde. En spellen blijven zwaarder worden dus je zal weer snel moeten upgraden om dat niveau bij te benen. Dat alles in 4K zo super makkelijk draait zoals het op 1920x1080 nu het geval is duurt het nog wel even.
RTX 2080;3;0.3101579248905182;Helemaal mee eens maar bij de introductie van 4k monitoren dacht ik dat het nog langer zou duren voordat het mogelijk was met acceptabele frame rates ook maar 1 grafisch goeie game te spelen.
RTX 2080;5;0.5441628694534302;Zolang je een high-end videokaart wil, zul je altijd met deze twee aspecten te maken hebben. Zie het als een exclusieve sportauto. Die zuipen ook en zijn ook flink aan de prijs.
RTX 2080;2;0.49997490644454956;Nou de 1080 was significant zuiniger en goedkoper in aanschaf en de 1070 was nog zuiniger. Kortom deze generatie is gewoon fors duurder en duurder in gebruik. duurder en duurder in gebruik lijkt overigens elke generatie een groter probleem te worden. de prijzen stijgen veel harder dan inkomens.
RTX 2080;3;0.5585611462593079;Een GTX 780 trok ook 250 Watt, dus op zich valt het verbruik wel mee (vergeleken met oudere kaarten). Enkel de RTX 2080 Ti gaat hier overheen met 250W/260W (FE) De RTX 2080 zit netjes op 215W/225W (FE). Dat vindt ik niet overdreven veel voor zo'n topkaart.
RTX 2080;2;0.4270906150341034;Idd maar ik vond het na vele jaren juist een opluchting dat de gtx 1xxx eindelijk zuiniger was. Echter lijkt dat concept nu compleet overboord gegooid en zitten we weer aan de stevige kabels.
RTX 2080;2;0.4254484176635742;Geachte auteur... hoe zit het eigenlijk met de VR prestaties?! Ik speel namelijk vooral simulatie games in VR tegenwoordig, en dat is nu net waar ik benieuwd naar ben. Al die shooters en arcade racers zijn eigenlijk niet zo mijn ding. Echter de VR prestaties zijn totaal niet onder de aandacht geweest.
RTX 2080;5;0.6418787837028503;Hier heb je helemaal een goed punt. Ik ben ook erg benieuwd naar Sim games benchmarks zoals Elite en iRacing.
RTX 2080;3;0.5242498517036438;Goede overzichtelijke review. Prijs toename staat niet in verhouding met de performancewinst iig. 60FPS in 4k is dus nu wel bereikbaar.
RTX 2080;2;0.4350486397743225;bereikbaar, maar niet veilig. die 10fps extra kan elk moment droppen. je zit pas veilig bij 100fps+.
RTX 2080;2;0.42296937108039856;Klopt ja. Als ik zou besluiten om zo veel geld te besteden voor een kaart die futureproof moet zijn dan zou ik idd liever rond de 100FPS zitten. Wat dat betreft vind ik deze kaarten net te licht qua performance. Denk dat je al snel settings omlaag moet gaan gooien over 1 a 2 jaar. Dat wil je natuurlijk niet als High end pc nerd
RTX 2080;3;0.5519462823867798;Nou PC is voor mij singleplayer only en dan vind ik games met mooie grote omgevingen met slowpace gameplay fijner dan is 30fps nog te doen en 60fps zat voor RTX gfx bevorderende features lijkt mij ook meer interresanter voor games waar je wel meer ingaat op de omgeving slowpace met nadruk op exploring in mooie omgeving. Waar meer online competatief bezig bent is voor mij console waar iedereen meer gelijk getrokken is dan op PC. Met 60FPS en minder gecheat.
RTX 2080;3;0.746961772441864;Dan maar de settings iets omlaag. Ik geef niet meer dan 500 euro uit voor een videokaart. En de 1070 Ti is daarmee nog geschikt genoeg voor de komende tijd. Ik heb ook 4 jaar met de 660 Ti gedaan, en dat ging op 1080p helemaal prima. Console games zitten vaak eerder rond de 30 fps en die hoor je er ook weinig over klagen. Maar ja, misschien ben ik te casual wat dat betreft.
RTX 2080;2;0.31218940019607544;Maar voor volk dat geld over de balk gooit en zodra er iets sneller is paartje koopt in nvlink mode . Zou 2080ti na de 2080 uitkomen dan waren er dus eerst 2 2080 en later 2 2080ti dat wordt hun nu bespaart , door meteen voor de 2080ti te gaan.
RTX 2080;2;0.509508490562439;Helaas niet heel veel winsten. De 2080TI is maar zo’n 30/35% beter dan de 1080TI en de 2080 is gelijk aan de 1080TI. Jammer er had meer in kunnen zitten zeker gezien de prijs.
RTX 2080;3;0.3804870843887329;En kunnen we nu eens realistisch vergelijken? toen de 1080 uit kwam, was die ook niet sneller dan de 980ti, en het prijsverschil was ook gelijk aan nu. Ga je de 1080 vergelijken met de 2080 dan zie je dat het verschil ongeveer gelijk is aan indertijd tussen de 980 en 1080... Wel even realistisch blijven...
RTX 2080;3;0.5456769466400146;1080 was wel een heel stuk sneller dan de 980TI, zelfs de 1070 was ietsje sneller. Nu is de 2080 vergelijkbaar dan een 1080TI en de 2070 zal rond de 1080 zitten.
RTX 2080;3;0.5209471583366394;Zijn er al benchmarks voor Tensor cores? Want die zijn natuurlijk wel leuk voor Tensorflow.
RTX 2080;3;0.41101688146591187;redelijk veel games getest zie ik, goodjob tweakers team !
RTX 2080;5;0.7862298488616943;Wow, indrukwekkende resultaten vergeleken met de vorige generatie kaarten, zowel van NVIDIA als AMD.
RTX 2080;4;0.4330002963542938;Het is helemaal afhankelijk op welke resolutie je wilt gamen of deze nieuwe generatie kaarten interessant is om aan te schaffen. De meerprijs die je betaald is vooral een gok op nieuwe techniek die zichzelf nog moet waarmaken. Prijs/kwaliteitswijs is de Pascal generatie op dit moment toch wel de betere optie. Maar dit is natuurlijk afhankelijk van de continue beschikbaarheid van deze kaarten in de webshops, ik verwacht dat ze er op den duur wel uitgaan.
RTX 2080;3;0.4675866365432739;Met GDDR6 en extra bandbreedte is er wel voordeel te verwachten in 4K gamen.
RTX 2080;5;0.46641433238983154;Ligt er dus helemaal aan wat je wilt en welke je met elkaar gaat zitten vergelijken. Zelf zou ik dus de 2080 gaan kopen ipv de 1080ti. Ligt er dus ook helemaal aan hoe je budget is.
RTX 2080;3;0.6432529091835022;Mijn eVGA GTX1080 heeft het soms lastig op ultrawide gaming om daar altijd boven 100fps uit te komen (monitor gaat tot 120fps). Dus een RTX2080Ti lijkt voor mijn verwachtingen wel ok.
RTX 2080;3;0.41583287715911865;Iets lagere settings en probleem opgelost.
RTX 2080;2;0.425371378660202;"tja daarvoor koop je imo niet echt een gamepc hé, dan kan ik evengoed op PS4 spelen. Ik noem mezelf een ""pc liefhebber"" en dan zoek ik naar ofwel maximale fps, ofwel maximale beeldkwaliteit en als het even kan, beide. Weinig nut om een 34"" 120fps IPS ultrawide te kopen om dan met lage settings te gaan gamen hé."
RTX 2080;2;0.46244147419929504;"Sommige settings zijn onevenredig belastend. Tesselation was daar vroeger een voorbeeld van. Zette je die lager, zag je vaak 0 verschil, maar wel tientallen fps meer. Alles op max zetten is niet perse ""pc liefhebber"" zijn."
RTX 2080;2;0.4863528907299042;Nou nee je vergelijking met consoles gaat niet helemaal op die halen zelden 60fps (vaak 30) en dat voornamelijk op 1080p. Tussen het uiterste wat jij zoekt en een console zit nog best veel ruimte en dat is waar genoeg pc gamers genoegen mee nemen.
RTX 2080;4;0.5111545920372009;Goed filmpje om te kijken: Zal je een hoop geld/zorgen besparen ULTRA settings is al jaren een beetje een farce.
RTX 2080;2;0.42918071150779724;Dat is toch enkel met DLSS dat ze hogere prestaties kunnen gaan geven? met Raytracing aan zullen de prestaties juist zakken en zal hij wellicht niet meer sneller zijn dan de oude generatie kaarten (die zonder Raytracing draaien natuurlijk)
RTX 2080;2;0.3008035123348236;door de raytracing cores zou je toch eigenlijk geen performance drop mogen verwachten lijkt me?
RTX 2080;2;0.43197184801101685;Eigenlijk niet, maar gezien de kaart nu al tegen zijn TDP aan hickt zal er toch iets minder energie moeten verbruiken als er andere cores ook energie nodig hebben
RTX 2080;3;0.5523048639297485;Hoewel goede prestaties gains zie ik wat prijs/performance betreft echt geen reden om een 2080 aan te schaffen als je al een 1080 hebt. 26% sneller @ 1080p en 33% @ 1440p, maar wel 70% (??) duurder? Upgraden voor die resoluties lijkt me dan dus niet heel verstandig, of zie ik hier iets over het hoofd? Lijkt mij ook verstandiger om van bv. een gtx980 naar een gtx1080 te gaan wat prijs betreft right? Mits je niet 4k gamed.
RTX 2080;5;0.5392275452613831;Volgends mij bij de volgende drivers. Neemt de db ook nog af op de rtx series. Ben benieuwd naar de tests met de goede drivers
RTX 2080;1;0.6656932234764099;In NL gaat de 2080 echt veel duurder zijn dan een 1080ti hoor... en dan hebben we het geeneens over 2ehands..
RTX 2080;1;0.6451133489608765;nee hoor, de 2080FE is hier 849, een doorseen 1080ti is op dit moment 790... dus nog geen 59 euro verschil.
RTX 2080;3;0.4654844105243683;Je vergelijkt de FE met een aftermarket 1080ti. Je hebt ook 1080ti's voor 715 euro (bijv gigabyte). Of te wel ~150-200 euro verschil tussen aftermarket voor redelijk identieke performance.
RTX 2080;1;0.2770252525806427;Geen blender performance ??
RTX 2080;1;0.49347516894340515;Blijkbaar kreeg Linus blender niet aan de praat met de nieuwe series dus lijkt nog niet compatibel.
RTX 2080;2;0.45810315012931824;Mijns inziens is dit een kwalijke evolutie: Vroeger waren de kaarten van de vorige generatie ook sneller, en dezelfde prijs als de vorige generatie. Of even snel als de vorige generatie, en goedkoper. Een GTX 1070 was even snel als een 980Ti, maar een PAK goedkoper ($379 vs $599). Of een GTX 1080 was veel sneller dan een GTX 980, voor dezelfde launch prijs $549. Nu zijn de kaarten ofwel even snel maar duurder (GTX 1080Ti vs 2080 even snel, maar 33% duurder, $599 tegenover $799). Of de snellere kaart is een PAK duurder (GTX 1080 TI vs 2080Ti +70% voor 3rd party kaartenof +100% voor FE). Jammere evolutie. En daarbij, tenzij je op 144Hz in 4K wil spelen heb je de 2080Ti toch helemaal niet nodig? Een 1080Ti is al lang genoeg voor 4K 60Hz. Ik heb zelf een 1440P 60Hz monitor, dus voor mij zijn deze kaarten allemaal overkill. Sneller dan een 1070 of 980Ti heb ik niet nodig. Laat ons hopen dat de 2060 serie kaart sneller is dan de 1070 voor minder geld.
RTX 2080;2;0.4771714508533478;uhh, de 1080 is nooit dezelfde launchprice geweest als de 980, en de 2080 is ook in verhouding net zoveel sneller ten opzichte van de 1080 als de 1080 was van de 980.. Het lijkt er op dat veel tweakers hier de boel een beetje aan het verdraaien zijn. En ik weet niet waar jij je prijzen vandaan haalt, maar kijk in de pricewatch en de doorsnee 1080ti kost toch echt tegen de 790 euro, waarbij de 2080FE 849 is, dus nog geen 59 euro verschil. En het verschil tussen een 1080 en 1080ti is procentueel gezien in verhouding vergelijkbaar met het verschil tussen een 2080 en 2080ti..
RTX 2080;2;0.4426767826080322;"Nope, de 1080 vs 2080 is slechts +28% sneller De 980 naar de 1080 was 66% sneller!! Het verschil was meer dan dubbel zo groot! Helemaal niet ""net zoveel sneller"" En OK, de 1080 was $599 bij launch, de 549$ is de huidige adviesprijs. De 980 was $549 bij launch. 10% duurder dus voor 66% sneller. Dat was een goede deal. Nu krijg je 0% sneller voor +33% prijs. Of +30% sneller voor +70%-+%100 prijs (in het beste geval, de 3rd party kaarten gaan nooit $999 kosten zoals Nvidia belooft, eerder tegen de $1200 van de FE aan, zoals bij de 10-serie het geval was) (Ja ik haal mijn prijzen in dollar, ik woon in de VS namelijk. De 1080Ti is hier te koop voor exact $600. Geen idee wat de verdraaide EU prijzen zijn in euros, maar ik vergelijk de prijzen in USD waar de kaarten momenteel voor verkocht worden) Punt blijft, de GTX 2080 zou NIEMAND moeten kopen, want duurder dan de 1080Ti en niet sneller. En de RTX is allemaal nog een toekomstverhaal, geen enkele game die het momenteel ondersteunt."
RTX 2080;3;0.5208141803741455;Mooie review . Uiteraard is de performance er flink op vooruit gegaan, meer dan ik had gedacht. Maar de prijzen zijn helaas exorbitant hoog. Beetje off topic: Is het verstandig om nu een laptop met GTX 1070 te kopen (FHD 144Hz GSync scherm) of kan ik hier beter mee wachten op de nieuwe mobiele chips van Nvidia? Ray tracing en dergelijke, mocht het al naar de laptops komen, intereseert mij niet zoveel. Het gaat mij erom dat ik AAA titels de komende 5 jaar op minimaal 60fps kan spelen. Ik heb het kopen al een jaar uitgesteld en hoewel ik nog langer kan wachten, denk ik zelf niet dat dat het waard is op dit moment.
RTX 2080;3;0.39571085572242737;Als het geen 4k scherm heeft zal de 1070 prima volstaan
RTX 2080;3;0.31082823872566223;Ik wil zelf binnenkort een beeldscherm en GPU upgrade doen, heb nu een standaard 60hz 1080P scherm met de 970 die alles prima runt, echter wil ik naar een Dell paneel die 144Hz kan op 1440p met G-Sync (Dell S2417DG Zwart). Welke kaart is dan het beste om te halen? Kan de 1080TI dit ook al aan bij alle games of is dit nog niet snel genoeg? Edit: Ik speel voornamelijk: Rainbow 6 Siege, BF 4, Nieuwe CoD misschien, Fortnite, SCUM, CS:GO.
RTX 2080;5;0.4715692102909088;Een nieuwe Apple is geboren in het tech landschap....
RTX 2080;3;0.5770780444145203;"De 2080 TI is krachtig, maar de prijs is voor mij en vele anderen wat teveel voor een grafische kaart. Helaas is de 2080 vs de 1080 TI niet helemaal lekker qua performance, ze zijn ongeveer hetzelfde. Als je een ""fatsoenlijk"" 1080 TI wilt hebben betaal je wel 750+ euro tot wel in de 800, en voor de 2080 zit je ook alweer 850 tot wel in de 950 of hoger. Ik zelf zou alsnog wel die 1 of 2 honderd erbij neerleggen en gaan voor de nieuwste kaart als je het geld ervoor over hebt. Puur voor het feit dat je dan toch wel een nieuw product in handen hebt en waar nvidia de komende tijd de aandacht in zal steken om de drivers te verbeteren en te optimaliseren. Na wat optimalisaties zal daar vast ook wel wat winst eruit te halen zijn. Het raytracing is een leuk extratje maar daar moet je het zeker niet voor doen als dat je hoogste prioriteit is. Zit je wat krapper met je geld, dan is de 1080 TI een betere keuze."
RTX 2080;3;0.2638477385044098;Ben ik blij dat ik toch maar die deal van Alternate gepakt heb. één 1080 ti voor €650
RTX 2080;3;0.26570457220077515;Ik ken dat gevoel gisteren ook enen gekocht bij alternate voor dezelfde prijs.
RTX 2080;1;0.3051704466342926;Welke merk?
RTX 2080;5;0.21392707526683807;Gigabyte Aorus 1080 ti
RTX 2080;5;0.2568264901638031;Ik heb de EVGA gekocht + 50 euro voor 10 jaar garantie
RTX 2080;2;0.555246889591217;Ik ben licht teleurgesteld, de 2080 presteert nauwelijks beter dan de 1080 Ti, dus de aangekondigde prestatie van de 2070 zal er zeker onder zitten (ondanks de keynote talk). Het is nog minstens een jaar wachten op de die shrink en grotere FPS verschillen. Met Black Friday zullen de 1080 Ti modellen wel met verlaagde prijs verkocht worden, daar zit wel een deal in voor mensen die nu willen kopen.
RTX 2080;1;0.3763170838356018;En dan te bedenken dat ik tot vijf jaar terug elke keer de een na snelste kaart kocht voor ongeveer driehonderd euro. Denk dat de mining er ook toe heeft bijgedragen. Men zag dat men de de kaarten ook voor drie keer zoveel kon verkopen en dat ze verkocht werden. Dus waarom dat niet volhouden. Tja je kan ze moeilijk ongelijk geven... Maar blij word ik hier niet van zoveel niet mijn hele computer onderhand Zelf meer een 1070 liefhebber. Nu eens kijken wat die gaat kosten en of hij ook 35% sneller is als raytracing wordt aangezet. Dat vind ik pas echt interessant.
RTX 2080;2;0.4328216016292572;Ehm, dat klopt niet helemaal hoor. Ik heb zelf in 2012 een GTX 670 gekocht, dat ding was toen al iets van 400 euro. En dat was niet eens de absolute top, een GTX 680 schommelde in die tijd ook al tussen de 500 en 600 euro. De 1070 en 1080 zullen nu wel snel in prijs gaan zakken verwacht ik.
RTX 2080;1;0.7420977354049683;Het lijkt op Apple-marketing, we maken de iphone heel duur, poetsen de oude zak even op, nieuw dingetje erin, hop en we maken het schreeuwend duur.... Ik heb en 1080ti, ik hou mn 1080ti nog wel even...doet t nog prima. says enough...niet kopen dus.
RTX 2080;2;0.47280353307724;Qua prijzen hanteert Nvidia op dit moment het Apple model: Een premium product met dito prijskaartje dat qua performance absolute top is (Wolfenstein 1080p met 300 FPS, wat?) Dat gezegd hebbende, in de here and now is een Pascal GPU prijstechnisch veel aantrekkelijker. Het is dan eerder wachten hoe de RTX 2070 zich zal verhouden qua prijs, maar voor nu is een Pascal GPU aantrekkelijker. Denk je echter aan de toekomst, dan is een Turing GPU het overwegen waard. Je betaalt er extra voor, maar de performance is dan wel navenant, plus dat je dan alle extra visuele goodies mee gaat trekken. Het zou mij niet verbazen als de RTX 2070/2080/2080Ti eenzelfde levensvatbaarheid gaat krijgen zoals de Radeon HD 5770/5870, Geforce GTX 750 Ti, Geforce 8800 reeks en varianten, en de Radeon 9700 Pro/9800 Pro kaarten. Langdurige ondersteuning. In dat opzicht is de belachelijke prijs wel weer te verantwoorden: Deze kaarten zullen voor lange tijd de basis vormen. Maar ja, de Radeon HD 5xxx reeks was een schijntje ivm wat je nu voor de RTXen moet neertellen.. wat dat betreft hebben die kaarten, evenals de Geforce 8800 reeks, vele malen hun waarde bewezen. Of dat ook geld voor de RTXen in de toekomst, hangt af in hoeverre de industrie Raytracing gaat omarmen. Het nut moet bewezen worden. Als Nvidia daar niet in slaagt, dan is de RTX wellicht een mislukking van jewelste. Slagen ze er wel in, dan is de RTX een kaart voor de lange toekomst.
RTX 2080;5;0.3971494436264038;Toen de eerste benches waren gelekt van de RTX 2080 heb ik ervoor gekozen om een 1080ti op te pikken voor 650 euro. Ik ben blij om te zien dat ik de juiste keuze heb gemaakt. Nu al helemaal omdat ik in eerste instantie wilde wachten tot de RTX 2070 uitgebracht werd die waarschijnlijk een stuk langzamer gaat zijn dan de 1080ti.
RTX 2080;1;0.40218111872673035;In de Pricewatch zie ik ze niet voor onder de 700 euro.. Mag ik vragen welk merk en model GTX 1080Ti het is en waar je die gekocht hebt voor 650 euro?
RTX 2080;4;0.4251713156700134;Dat was een dagaanbieding bij Alternate. Je kan het goed zien in de grafiekjes. pricewatch: Gigabyte AORUS GeForce GTX 1080 Ti 11G
RTX 2080;3;0.27649742364883423;Geen Titan V benchmarks?
RTX 2080;1;0.3258627951145172;Ik mag toch bidden dat niemand dit er voor gaat betalen.
RTX 2080;2;0.4268788695335388;Ik zou wachten op 7nm kaarten.. Die komen binnen een jaar. Dus vind het dom om nu de hoofdprijs te betalen voor deze 14/12nm kaarten die redelijk snel End Of Life zullen zijn.
RTX 2080;5;0.3153746426105499;Heb je een bron van waar je deze wijsheid vandaan hebt?
RTX 2080;5;0.29319602251052856;Zelf even zoeken was teveel moeite? Nvidia RTX 20 Series: Why You Should Jump Off The Hype Train TSMC Receives Next-Gen NVIDIA 7nm GPU Orders, More Than 50 Chip Tape Outs Expected By The End of 2018 Nvidia Ampere Architecture Will Replace Turing In 2020, Based On 7nm
RTX 2080;1;0.39811035990715027;"Zie in geen van die links een bevestiging dat er binnen een jaar 7mm kaarten komen; alleen maar speculatie en verwachtingen, en dan nog niet eens dat de 7mm kaarten ook direct gaming varianten zullen zijn."
RTX 2080;1;0.5671104192733765;haha dat we praten over '' wat krapper met geld'' en dan maar de 1080TI halen alsof 700+ euro goedkoop is Die nieuwe generatie zou 500 euro zijn als AMD er ook was. Dus geen 1400 euro voor een 2080ti En dan is de 2080ti ook nog eens'' crippled'' met 88rops en geen 96
RTX 2080;2;0.45645540952682495;Tegen de tijd dat ik van de RT features kan genieten zijn de opvolgers al op komst. Dan kan ik beter op die wachten (1080 Ti al in bezit).
RTX 2080;3;0.38957738876342773;Afgezien van hoeveel games RTX zullen gaan ondersteunen is ook nog de vraag wat de performance daarvan zal zijn, bvb of men bereid is om daarvoor van 4k 60+fps terug te stappen naar 1080 60fps (gebaseerd op wat tot dusver bekend is over RTX performance).
RTX 2080;3;0.3695078194141388;Een nieuwe video is uitgekomen om metro exodus uit te belichten betreft raytracing belichting: Ik moet zeggen dat ik wel onder de indruk ben wat de raytracing cores in samenwerking kunnen met de tensor cores. Ik weet zeker dat bij een paar verdere generaties raytracing een belangrijke feature is voor hoger realisme en dat we dan wel heel blij zijn dat nvidia deze weg in is geslagen.
RTX 2080;2;0.462419718503952;De 2080 Ti is alleen leuk voor de winst op 1440p/144hz. En de 2080 is niet snel genoeg met maar 7 of 8 % betere prestaties dan de 1080 Ti. De nieuwe serie heeft wel mooie uitschieters in Vulkan en DX12. Te duur.
RTX 2080;5;0.29822587966918945;Ik ben behoorlijk benieuwd hoe deze nieuwe kaart gaat presteren in VR gezien de nieuwere headsets zoals de Pimax. Op dit moment twijfel ik dus ook tussen een 1080ti en 2080ti.
RTX 2080;4;0.5764114856719971;Wat een leuke nieuwe generatie. Een paar dingen dat mij hier is opgevallen: De geluidsproductie is hoog en ik ben benieuwd of dit met de custom versies omlaag kan. Verder is de RTX 2080 geheel niet interessant voor ons die een 1080Ti bezitten maar een upgrade naar RTX 2080Ti is veel interessanter in 4K benchmarks
RTX 2080;1;0.4010986387729645;Ik zou zeggen, allemaal kopen! Dan vraagt Nvidia voor de 3080TI volgende keer 3000 euro Ik wacht op de volgende AMD release volgend jaar. Ondertussen gaat alles vlot genoeg met Vega 56 met Vega 64 bios.
RTX 2080;1;0.7051976919174194;Hopelijk verkoopt het voor geen meter met die belachelijke prijzen. Komt er natuurlijk van als er een monopoly is op de gpu markt.
RTX 2080;1;0.374245285987854;Geen VR games getest? Dit is voor mij de enige reden om uit te kijken naar een nieuwere grafische kaart (zit nog op GTX 970)..
RTX 2080;4;0.4135799705982208;Goede review en goed advies. Waarom nu hardware kopen, dat softwarematig nog niet gebruikt wordt. Dat is nooit slim, of je moet echt de snelste GPU willen hebben, omdat het de snelste is, maar van de features maakt nog geen enkele game optimaal gebruik. Grote kans dat als de Turing Features goed gebruikt worden, de nieuwe generatie alweer voor de deur staat en dat deze GPU's flink goedkoper zijn.
RTX 2080;1;0.5169047713279724;Heb zojuist de EVGA FTW 1080TI goedkoop op de kop kunnen tikken + voor 50 eur 10 jaar garantie afgekocht. De nieuwe generatie kaarten zijn overpriced, aangezien ik alleen in fullhd game is de 1080TI mee dan genoeg, We hebben tot al die tijd ZONDER RAYTRACING kunnen gamen, waarom moet dat nu ‘n “hebben dingetje “ zijn?
RTX 2080;4;0.29077714681625366;Op deze site vind je benchmarks gedaan met linux (Ubuntu 18.04 LTS):
RTX 2080;2;0.41057005524635315;Toch heel slim van Nvidia om die kaarten zo te prijzen dat ze niet gelijk onwijs veel beter zijn (in kwal/prijs verhouding) dan de oudere generatie. Nu blijft die hele 1080 generatie nog flink actueel. Want daarvan zal nog een flinke voorraad liggen in de fabrieken, gok ik zo.
RTX 2080;4;0.35873574018478394;Kan ook zijn dat ze gewoon dure kaarten op de markt brengen in de hoop miners aan te trekken die alleen de top kaarten willen. En dan de prijs van de vorige generatie iets omlaag en goed leverbaar zodat gamers en miners beide iets hebben.
RTX 2080;3;0.2838650643825531;Ik ben blijkbaar de enigste die ook op de benchmarks van Premiere Pro / after effects / rendering wacht? aangezien ik naast gamen daar ook gebruik van wil maken
RTX 2080;2;0.4495006799697876;"Wat mij betreft is dit de typische Geforce generatie die je best kan skippen. Ten eerste: de prijzen zijn echt belachelijk. Laat ons eerlijk zijn, de 1080Ti was vorige generatie reeds een gi-gan-tisch dure GPU, die uiteindelijk slechts door een (relatief) beperkt aantal gamers wordt gekocht; enkel wie absoluut het beste van het beste in zijn PC wil hebben koopt zoiets. Nu is de standaard high-end kaart (2080) niet alleen even duur maar zelfs significant duurder (introductieprijs 2080 699$, introductieprijs 1080Ti 599$)? En dat voor pakweg 5% betere performance? Daar komt Nvidia enkel en alleen mee weg omdat AMD geen concurrentie biedt op dit niveau. De 2080Ti is dan wel weer gemiddeld 34% sneller dan de 1080Ti, maar 1) dit is zwaar teleurstellend ivm vorige generaties (de 1080Ti was 74% sneller dan de 980Ti) 2) dit gaat deze keer gepaard met een fors toegenomen stroomverbruik en hitteproductie 3) aan een enorme prijsverhoging (introductieprijs 999$ vs 599$, dat komt bijna in de buurt van het dubbele lol) En de mega overhypte nieuwe features (Ray-tracing en DLSS AA) zijn gewoon nog niet af: zoals de review er correct op wijst zijn er momenteel NUL games beschikbaar die die features gebruiken. Da's bij mijn weten nog nooit eerder gebeurd met een GPU launch, meestal is er toch minstens één flagship game dat de nieuwe tech promoot. Tenslotte denk ik dat mensen de performance gains van raytracing zwaar overschatten; als de devs van Metro Exodus naar eigen zeggen hard aan't werken zijn om 60fps te halen op 1080p (op een RTX 20180!!) dan weet je't wel. DLSS klinkt interessanter, maar opnieuw niemand heeft het ooit al kunnen testen want er bestaat nog geen game dat dat doet, dus moeten we nVidia gewoon op hun woord geloven. Maar ik kijk met meer dan gemiddelde interesse uit naar de RTX 3080Ti Goeie review trouwens, thx."
RTX 2080;1;0.4895632863044739;Quote: Naar mijn weten betekend AIB niks anders dan (graphics) add-in-board: AMD and Nvidia also build and sell video cards, which are termed graphics add-in-board (AIBs) in the industry. Bron NVidia maakt en verkoopt dus AIB's, daar naast zijn er nog AIB Suppliers
RTX 2080;3;0.36385515332221985;Ik ben het eens met de voorlopige conclusie van deze Tweakers review dat de RTX 2080 op dit moment de beste keuze lijkt, zeker als de prijzen een beetje gaan zakken naar huidig 1080Ti niveau.. Iets meer prestaties maar vooral de ondersteuning voor bepaalde nieuwe technieken zou dan toch mijn voorkeur hebben over nog een GTX 1080Ti uit de vorige generatie te kopen.. Zelf wacht ik het allemaal rustig af, mijn GTX 980Ti kan nog wel even mee op de 2560x1440 resolutie waarop ik speel.. Waarschijnlijk wordt The Division 2 volgend jaar pas een game die me misschien weer mijn videokaart doet upgraden, als ik in die game teveel zaken moet gaan uit- of lager zetten om fatsoenlijke fps te houden.. De GTX 980Ti was destijds ook niet goedkoop vond ik met een prijs van rond de 750 euro, maar gaat inmiddels dus wel al meer dan 3 jaar mee..
RTX 2080;2;0.40483593940734863;Tuurlijk is hij duur, als je geen echte concurrentie hebt...
RTX 2080;1;0.3467690050601959;Wanneer zullen de pre-orders eigenlijk de deur uitgaan?
RTX 2080;2;0.36045828461647034;Leuk dat GTX2080 weer heel snel is, maar we kunnen extra features nog niet gebruiken. Dus je kan 200 Euro sparen of als je nog goed kaart hebt en nog steeds tevreden met 1080p speelt, beter wachten. Want AMD zal wel binnenkort met een antwoord komen en zal vast wel goedkoper zijn zoals we gewend zijn van AMD. Komt er nog iets binnenkort van AMD?
RTX 2080;3;0.5007022619247437;Ben benieuwd naar de 2070 meestal de betere kaart om te kopen. Of ie moet 4k ultra net niet aankunnen, hij zal wat trager zijn dan de 1080ti
RTX 2080;1;0.3788756728172302;Waarom niet PUBG op ultra?
RTX 2080;3;0.32695502042770386;Omdat het een multiplayer game is. Elk potje is anders dus kan je niet met elkaar vergelijken.
RTX 2080;4;0.6757437586784363;"Ik heb vandaag 10 reviews gelezen en/of bekeken, en ben zeer tevreden met de resultaten. (4k prestatie) Ik heb zelf 2x2080ti van msi met NVlink besteld, met de hoop dat lg/samsung de 5k 49"" monitoren uit brengt. Ik ben bang dat ik dan net 60 fps haal. Zoals Jayz50cents al zei: Je moet de 2080ti niet met 1080ti vergelijken, maar met de Titan. Dan vallen de prijzen reuze mee. En mocht de Bitcoin weer gaan stijgen, wat ik wel verwacht met de winter, dan zullen de prijzen nog hoger worden"
RTX 2080;2;0.41324836015701294;2080ti is een directe opvolger van de 1080ti en niet de titan. En nee de prijs ervoor valt helemaal niet mee.
RTX 2080;3;0.5170276761054993;Pas na Raytracing benchmarks kan ik pas zien, in welke categorie ze vallen. Mochten ze meer dan 30% (50% 60) performance halen, dan is het voor mij het geld wel waard.
RTX 2080;1;0.45465153455734253;Of gewoon helemaal niet naar de namen kijken, en kaarten in dezelfde prijscategorie vergelijken. Dan bieden deze kaarten niet hele geweldige FPS/$. Dat je voorheen een Titan voor dit geld kan kopen zegt eigenlijk al hoeveel NVIDIA graag (of nu kan) de prijzen omhoog duwt. Er zal ongetwijfeld ook een nieuwe Titan komen, maar moet die dan richting 2k$ gaan?
RTX 2080;5;0.24596959352493286;Hmm, nog bij mensen waar de grafieken niet doorkomen? Edit: Ze zijn er :-)
RTX 2080;1;0.339729368686676;Idem hier, dacht dat hier het netwerk weer dicht gezet was XD Edit: inderdaad
RTX 2080;1;0.5258843302726746;nee, is al gefixed
RTX 2080;2;0.4128163754940033;De minima die op veel plaatsen wordt genoemd mist nog. Zie overal waar dit vermeld wordt nog steeds maar één waarde staan, terwijl bij anderen grafieken weer duidelijk over één waarde wordt gesproken Neemt natuurlijk niet weg dat alleen de fps een zeer achterhaalde meetmethode is omdat je daarmee niet de soepelheid van een game kan meten.
RTX 2080;3;0.38607051968574524;We meten ook geen min fps, we meten waar mogelijk de frametimes. We hebben nog geen mogelijkheid gehad om de frametimes in grafieken te zetten. Ik wilde de 99th percentiel frametimes als minimum framerates in de grafieken opnemen. Dat is strikt genomen niet correct, weet ik, maar min fps leest voor de meesten makkelijker dan grafieken met frametimes van 8,3ms.
RTX 2080;2;0.4864140450954437;Erg jammer dat de Geforce RTX 2080 Ti maar 30 a 40% sneller is dan de Geforce GTX 1080 Ti. De afstaand tussen de Geforce GTX 980 (Ti) en Geforce GTX 1080 (Ti) is VEEL groter en is +/- 65 a 75%, zo als je goed kan zien op Guru3D, vind het erg jammer dat Tweakers geen eens de Geforce GTX 980 (Ti) hebben mee genomen. En dat de verschil tussen de Geforce GTX 1080 Ti en Geforce RTX 2080 maar 5 a 10% is (behalve Wolfenstein II: The New Colossus), en in meerdere maar een paar %, wanneer de verschil tussen Geforce GTX 980 Ti en Geforce GTX 1080 veel groter is, 30% of meer. Edit ik heb het over de 4k fps.
RTX 2080;1;0.7691062688827515;Werkt inderdaad niet.
RTX 2080;5;0.6846377849578857;ik moet zeggen, geweldig succes voor de marketing team van nvidia. Ze hebben nu een mainstream product, voor een Titan prijs.
RTX 2080;2;0.5744839906692505;Qua prijs/prestatie verhouding van de 2080Ti tegenover de 1080Ti kunnen deze benchmarks mij niet overtuigen om er meer dan €1300 voor neer te gaan leggen. Eerlijk gezegd vind ik de prestaties op 4K zelfs vies tegenvallen. De truuk van Nvidia om de prijs van de nieuwe topkaart dusdanig belachelijk hoog te positioneren enkel omdat ze dan van hun overtollige 1080Ti's afkomen zonder die te hoeven afprijzen vind ik ook zum kotzen. Hun goed recht, maar ook mijn goed recht om dat soort praktijken niet te gaan sponsoren. Ben toe aan een nieuwe kaart en kan het wel betalen, maar weiger gewoon om er aan mee te doen. Nu dus liever nog tweedehands, of voor in de tussentijd een wat mindere AMD kaart kopen dus.
RTX 2080;1;0.47109702229499817;Ik weet dat Hardwareinfo en Tweakers hetzelfde moederbedrijf hebben, maar dit is gewoon een kopie van de review op hardwareinfo.
RTX 2080;2;0.503635823726654;Ik heb het ook al elders neergezet, om precies te zijn onder het benchmarkfilmpje van Jayztwocents op youtube, maar er wordt nu een hoop negatiefs uitgestort over Nvidia. Nu de benchmarks er zijn, valt vooral het snelheidsverschil tussen de 10xx en de 20xx generatie op. Die is niet extreem hoog. En dus valt iedereen weer over elkaar. De nieuwe generatie kaarten wordt een marketingstunt genoemd. Er wordt zelfs beweerd dat Nvidia eigenlijk vooral van de oude kaarten af wil, en daarom een slechte generatie videokaarten de wereld in heeft gestuurd. Ik heb me ondertussen afgevraagd waarom Nvidia hun presentatie op de grootste gamesbeurs van Europa heeft gebruikt voor een Open Universiteit college over raytracing. En ik kom nu eigenlijk tot de conclusie dat Nvidia ons heeft voorbereid op iets heel anders. De prestatieverbetering zit niet in iets wat heel erg meetbaar is. Raytracing en AA zijn niet makkelijk meetbaar. Het verschil is vooral een verschil dat je met het blote oog kan zien. En dan kom ik terug mijn begintijd als computeraar en gamer. Toen werd er een stap gemaakt van ega naar vga. Van 16 naar 256 kleuren. En van plaatjes in tijdschriften die lieten zijn hoe veel dat verschil maakte. Tijdschriften zijn vervangen door internet, maar ik denk dat we een vergelijkbare tijd tegemoet gaan. En dan gaat het niet meer om fps. Wel om of een spel smooth speelt en aanvoelt, ongeacht het aantal fps.
RTX 2080;4;0.39343538880348206;Nou, leuk allemaal. Maar ik denk dat ik de goede keus heb gemaakt om tweedehands een GTX 1070 Ti aan te schaffen ter vervanging van mijn GTC770. Voorlopig kan ik weer een jaar of twee vooruit met mijn vijf jaar oude pc op 1080p.
RTX 2080;5;0.653433084487915;"Heerlijke positie hebben die videokaart makers ook he, ons steeds weer een nieuwe worst voor houden. Doen natuurlijk alle bedrijven, maar hoe revolutionair zou het zijn als er eens een bedrijf bij zat wat zou zeggen: ""Luister, dit jaar geen verbeteringen, ben gewoon eens blij met wat je hebt, ga eens wat vaker naar buiten, praat eens wat vaker met je vrienden/familie, het zijn maar spullen. Volgend jaar doen we weer mee."""
RTX 2080;1;0.5952426791191101;Hoe warm worden deze kaarten? 1080 GTX Ti vergeleken met 2080 RTX En waarom is het prestatie verschil zo weinig terwijl het RAM zoveel sneller is? DDR4 naar GDDR5 was ook huge waarom is dit nu bijna niks meer?
RTX 2070;1;0.44130319356918335;Videokaartmarkt blijft zeer vreemd, heb 3.5 jaar geleden voor €360,- een GTX 970 gekocht en als ik vandaag voor hetzelfde geld een nieuwe videokaart zou moeten kopen dan is dat een 1060 die min of meer hetzelfde presteert. Dus de uiteindelijke performance gain per dollar (in ieder geval in de range van 3-400 euro) is niet veranderd in de laatste 3.5 jaar, dat is toch zeer vreemd te noemen
RTX 2070;2;0.44004026055336;De GTX 1070 was in prijs/dollar iets beter dan de 970: Maar goed er is inderdaad sprake van een stilstand en hoewel niet leuk is het allemaal wel te verklaren. Spelen meerdere zaken een rol 1) Eurokoers In het verleden hadden wij tov de amerikaanse advies prijs in dollars (MSRP) een voordeel rond de 35%. Op het diepte punt was dit voordeel vrijwel volledig verdwenen en tegenwoordig is het weer opgekrabbeld naar 15%, maar het is niet meer wat het vroeger was 2) AMD AMD heeft Ati destijds gekocht. Vervolgens is het letterlijk na 1 jaar gaan snijden in het R&D budget. Dit hebben ze bijna 10 jaar vol gehouden. Ik zag de bui rond 2010 al hangen maar de eerste tekenen bij Radeon waren na southern islands toen nieuwe kaarten steeds langer op zich lieten wachten, er steeds vaker rebrands werden gebruikt, nieuwe techniek te laat in de markt werd gezet en er te weinig mee werd gedaan (denk aan fury nano etc). Qua omzet speelt Radeon anno 2018 geen rol van betekenis. Al zou iedereen morgen een AMD Radeon willen aanschaffen, de voorraad is er simpelweg niet. Nvidia kan dus zijn eigen plan trekken. 3) Welvaart Momenteel gaat het wereldwijd goed en dat merk je aan de prijzen. Bedrijven kunnen hogere prijzen vragen en klanten betalen het graag. Dit werd pijnlijk duidelijk tijdens de cryptoboom waarbij er genoeg consumenten zijn geweest die kaarten als de GTX 1060 6GB voor 400+ euro hebben aangeschaft. Goede nieuws is dat het er allerminst op lijkt dat Nvidia technisch vast zit geroest. De nieuwe architectuur is beter en het R&D budget is hoger dan ooit tevoren. Maar voorlopig zit Nvidia zit in de luxe positie dat tot zekere hoogte kunnen vragen wat ze willen.
RTX 2070;1;0.5212782621383667;Punt 3 heeft een hele andere reden. Bedrijven hebben de prijzen omhoog gegooit omdat de vraag explosief gestegen was. Dat heeft niet zoveel te maken met welvaart. De enige reden dat men 400+ euro wou betalen voor een GTX 1060 is omdat je deze kaart binnen een maand of 2 weer terug had verdient met cryptomining. Daarna maakte je pure winst (minus de stroomkosten). Er zijn vast mensen geweest die een videokaart hebben gekocht om mee te gamen. Maar deze mensen geven echt niet met plezier zoveel extra geld uit aan een videokaart die een paar maanden terug 100 euro goedkoper was. Nu de vraag vanuit cryptomining minder is geworden (en er dus weer meer voorraad is) zie je ook direct dat de prijzen flink gedaald zijn. Daarnaast is welvaart relatief. Want als de welvaart stijgt, stijgen schulden mee. Het is niet zo dat een huishouden nu opeens veel extra geld overhoudt ten opzichte van een paar jaar geleden. De voornaamste reden dat het goed gaat met de economie is omdat iedereen (bedrijven, overheden, banken, burgers) goedkoop geld kunnen lenen. En dus sneller en meer geld uitgeven. Totdat straks de rente weer omhoog gaat en de huizenmarkt instort. Maar dat is weer een hele andere discussie.
RTX 2070;2;0.3912609815597534;"Ik had het specifiek over de nieuwe hogere advies prijzen. De 1070 zat op $379/$449(FE) terwijl de 2070 nu op $499/$599(FE) zit. Dit is voor een x70 kaart een record. De 970 zat op $329, de 770 op $399 en de 670 op $399. De marktprijzen zijn inderdaad in de loop van 2017 (mei) heel erg hard gestegen. In dat jaar ken ik zelf diverse tweakers die kaarten als de 1060 hebben gekocht. Op de Steam survey zag je in die tijd ook dat de 1060 aan een flinke opmars bezig was. In november 2017 zag je bijvoorbeeld dat de 1060 bijna 4% per maand aan marktaandeel won. Nog specifieker; de 1060 stond op mei 2017 op de vierde plek met 2.79% martkaandeel en op mei 2018 op de eerste plek met 13.41% marktaandeel. Die 1060 gingen er dus gedurende de crypto boom bij gamers in als zoete broodjes. Met andere woorden; (ook) gamers kochten die kaarten. Wat Nvidia dus heeft gezien is dat ondanks die hogere prijzen de gamers die kaarten nog kochten. Deze les zien we nu helaas terug in de advies prijzen.Als nvidia had vastgehouden aan het traditionele beleid dan had de 2070 $399 - $429 moeten kosten (inflatie). Eigenlijk is alles nu doorgeschoven, de 1080 werd geïntroduceerd op $599 / $699 (FE) maar is later bijgesteld naar $499. Op dat prijspunt zit nu de x70. Op zijn 'normale' prijspunt was de 2070 een hele goede deal geweest!"
RTX 2070;2;0.4471714496612549;Om eerlijk te zijn is dat een beetje te kort door de bocht. De RTX heeft een compleet andere architectuur dan de 10X serie. De RTX heeft bijvoorbeeld een veel grotere chip. Dat betekent dat je voor dezelfde gpu bij wijze van dubbel zoveel wafers nodig hebt. En dan is nog de vraag wat de foutmarge is per wafer. Hoe meer fouten hoe duurder de chip. Dat wil niet zeggen dat de cryptomarkt geen invloed heeft op de huidige prijzen. Maar je kunt ook niet zomaar de advies prijzen doortrekken naar een nieuwe generatie met een compleet andere architectuur. Nvidia zal hun prijzen afstemmen op de marges die ze maken per product. Dat doet elk bedrijf. Misschien is de marge van de RTX serie een stuk lager dan de GTX 10 serie. Dus stel de marge van de 10 serie is 50%. Maar de marge van de RTX serie (op basis van jou genoemde prijzen) is maar 20%. Dan houden ze per verkoop veel minder over. Dus het kan zomaar zijn dat ze de marges min of meer gelijk willen houden en dat kan zorgen voor een veel hogere prijs. Een bedrijf kan niet zomaar de marge halveren. Overigens is dit slechts speculatie. Ik weet helemaal niks van de wafer productie van Nvidia en de kosten die daarbij komen kijken per chip. Ook heb ik geen idee wat hun marges zijn. Maar op basis van de architectuur en de grootte van de chips kun je wel stellen dat de nieuwe chips duurder zijn om te maken dan de voorgaande generaties. Maar ik ben het zeker met je eens dat de huidige prijzen niet sporen tot aankoop. De prestaties zijn daar gewoon niet goed genoeg voor. Maar ik denk dat de volgende generatie van Nvidia goedkoper zal zijn dan de huidige RTX serie. Simpelweg omdat ze nu niet het wiel opnieuw hoeven uit te vinden maar gaan voortbouwen op de huidige generatie. Maar het kan zomaar zijn dat de cryptomarkt weer booming business is tegen die tijd. We zullen het zien!
RTX 2070;1;0.44623464345932007;"Nvidia bewijst al jaren dat het heer en meester is in geld verdienen. Elk doemscenario van de afgelopen 15 jaar voorspeld door 'experts' op basis van de kosten bleek achteraf niet te kloppen. Nvidia maakt gewoon altijd winst. Ik zou daar dus niet aan gaan beginnen . Overigens heeft Nvidia zelf circa 2 jaar geleden het 'geheim' onthuld in de vorm van een video rapportage. Ze hebben een kamer van pak hem beet 5 bij 5 meter met daarin een nieuwe Geforce kaart volledig op hardware niveau gevirtualsieerd, aangesloten op een computer. Zo kunnen ze de prestatie van de nog te bouwen kaart exact bepalen, voor hij geminiaturiseerd wordt. Met andere woorden; Nvidia ontwerpt de hardware naar het prijspunt toe in plaats van andersom. Ze maken een zo goedkoop mogelijke kaart bij de gewenste prestaties binnen de mogelijkheden van de nieuwe architectuur. Voorspelling over de toekomst maak je ook een denkfout. Je redenatie is gebaseerd op een koppeling tussen (ontwerp)kosten en de (advies)prijs. Die is er helaas niet. We hebben Nvidia de afgelopen jaren zien groeien van enkele 10tallen miljoenen winst per kwartaal naar honderden miljoenen winst per kwartaal. De 1 miljard die pascal gekost zou hebben is in 2017 al lang en breed terug verdiend. Het is helaas niet zo dat daarna de prijzen zijn gekelderd. Zolang de wereld economie lekker draait en er geen problematische concurrentie is zal Nvidia zijn prijzen echt niet omlaag bijstellen. Er zijn dus in ieder geval drie factoren die dit kunnen veranderne -wereldwijde econimische crisis -AMD die een nieuwe high-end gpu lanceert (zie ik zelf voorlopig niet gebeuren, geruchten over Navi zijn pessimistisch. De focus ligt daar op Ryzen.) -Intel die met een dedicated gpu komt (ja die gaat echt komen)"
RTX 2070;2;0.4084470570087433;En dat kwam idd door de crypto miners, daardoor zijn de prijzen en vooral van Nvidia flink gestegen. Alhoewel het nu weer gestagneerd is.Zij hebben gezorgd dat de prijzen veel hoger waren dan voorheen. effe ontopic:ik zou best zo een 2070 willen hebben,de 2080 en 2080 ti is overkill als je het mij vraagt en veel te duur in aanschaf.
RTX 2070;3;0.47657275199890137;Ik ben het met je eens dat de voortgang niet veen snel is, maar hoe je het neerzet is wel een beetje verkeerd. Voor 30 euro kom je namelijk uit op een 1070 (pricewatch: Palit GeForce GTX 1070 Dual 8GB) die presteert als een 980ti. Dit terwijl een 1060 al voor iets boven 200 gaat, of 260 voor de 6G versie (die ook eerder als een 980 presteert).
RTX 2070;3;0.3739461898803711;"3.5 jaar terug kon je voor 360 euro een Asus GTX970 OC Strix kaart halen. De ""budget"" merken zaten eerder rond de 300 euro. Bij de introductie van de GTX1070 was de prijs al iets boven de 400 voor budget kaarten. Nu is de introductieprijs pak en beet 500 euro. Denk dat je wel kan stellen dat de prijs in 2 generaties tijd met 66% gestegen is, terwijl de prestaties t.o.v. een GTX970 maar ""slechts"" verdubbelt zijn (bron: Gamers Nexus). Ik denk dat je mensen met de dikke portemonnee niet hoort klagen, want meer beschikbare performance is altijd welkom wegens 4K, VR, etc. Maar voor FPS/$ zijn we weinig opgeschoten."
RTX 2070;2;0.3755229413509369;De gtx 1060 was de instap model van de Pascal generatie en was net zo snel en soms zelfs sneller dan de snelste kaart van de generatie daarvoor, namelijk de gtx 980 en bovendien ook zuiniger. Nu is dat niet het geval, de instap model van Turing zal de rtx 2060 worden en die kan niet op zijn minst even snel zijn als de gtx 1080 want dat is de rtx 2070 maar net... De gpu ontwikkeling is minder indrukwekkend dan de vorige keer.
RTX 2070;2;0.3906313478946686;er komt geen RTX 2060. Dat owrd de GTX 2060, ZONDER de rtx cores. deze kaart moet het hebben van de DXR layer als men raytracing wil gebruiken op een 2060. deze kaart zal ongetwijfeld voor 399 dollar gaan en hoogst waarschijnlijk niet veel sneller zal zijn dan een 1070. ALs het aan mij ligt is het verstandiger een 2e hands 1050/1070 of 1080 of een RX 570/580 (vega alleen als je blower wil of wat meer geld wil uitgeven) dan de 20xx kaarten als je zoekt naar ene performance increase. Maargoed.. ik hoop dat AMD hier op in kan spelen maar eerlijk gezegt ben ik er bang voor.. zelf stap ik liever over naar een vega 56 ivm met mijn freesync monitor, maargoed, als ik meer perf wil lijkt nvidia, 2e hands, de beste keuze.
RTX 2070;2;0.32804229855537415;En toch zijn er mensen die beweren dat we niet de grenzen van de wet v Moore hebben bereikt.
RTX 2070;3;0.3469093143939972;Toen de 10xx serie uitkwam heb ik het uitgerekend, het verschil toen met de 9xx serie daarvoor kwam voor een groot deel door de verzwakking van de Euro ten opzichte van de Dollar. Daar bovenop kwam natuurlijk de Founders Edition waar veel fabrikanten gewoon boven gingen (en bleven) zitten. Dat was denk ik onvermijdelijk maar als je de Founders Edition negeert waren de 10xx kaarten grofweg even scherp geprijsd als de 9xx kaarten (maar hier duurder door de wisselkoers). De nieuwe serie is denk ik gewoon vrij log door de nieuwe soorten kernen die nog vrij weinig meerwaarde geven. Die kosten een hoop extra transistoren en ze willen natuurlijk geen zwakker product leveren dan de vorige kaarten.
RTX 2070;1;0.49800917506217957;Misschien heb ik een simpele verklaring. Zolang ze worden gekocht blijven zij ze maken. Ik vind die prijzen ronduit belachelijk. Voor het topmodel koop ik een complete midrange PC die alles makkelijk op FullHD max draait. Durf die bewering zelfs aan met een 21:9 scherm. (2560X1080 ofzo). Maar aan de andere kant laat het wel zien dat de rijen de winkel uit gaan en de hoek om. Ze verkopen gewoon. En de volgende generatie (RTX3000) gaan voor precies dezelfde bedragen of meer. En die worden ook gewoon verkocht. Die prijzen gaan alleen omlaag als de consument massaal zoiets heeft van dit koop ik niet. Of concurrentie maar dat kan je gerust vergeten. Ja AMD doet het heel goed op de processormarkt. Die Ryzens zijn fantastisch. Maar wat is de beste videokaart die ze verkopen? RX580? Das een GTX1060 concurrent. Of de Vega's? Als ze uberhaupt verkrijgbaar zijn waarom zou je? Je hebt dan een GTX1080 concurrent die je niet zou kiezen voor zn gunstigere prijs ofzo (ze kosten beiden evenveel). En ze verbruiken meer energie. Wil hieraan toevoegen dat ik zelf hoop dat ik hierin ongelijk heb en AMD met een serieuze kaart op de markt komt.
RTX 2070;3;0.45086124539375305;Dat is niet zo vreemd. Dat heeft alles te maken met de gebruikte technologie, de kosten en de concurrentie.
RTX 2070;1;0.3979910612106323;Heb 4.5 jaar geleden een AMD HD 7950 gekocht voor 160 euro. Geloof dat ik op dit moment ook niks beters kan krijgen voor die prijs.
RTX 2070;1;0.3530597686767578;En nu eindelijk meer en meer DX12 spellen op de markt komen is Crossfire en SLI geen probleem meer. Oudere opstellingen met kaartjes van nog geen 300 euro per stuk maken in Crossfire en SLI nog gehakt van een 2080TI. De meeste mensen hebben al zo'n kaart die steken er een kaartje van 300 euro bij en hebben geen enkel belang bij een upgrade naar een kaart van 1300+ euro die slechter presteert dan hun Crossfire of SLI opstelling. Het is alleen spijtig dat relatief nieuwe games met vrij nieuwe game engines geen of slechte multigpu support bieden. Maar die games laat ik gewoon links liggen. Als de developer niet de moeite neemt om mij in staat te stellen het meeste uit mijn spel ervaring te krijgen, dan neem ik ook niet de moeite hun spellen te spelen.
RTX 2070;3;0.3017679750919342;Als ik het goed begrijp klopt het nou dat er goedkopere 1080 TI op de markt zijn en beter presteren dan de duurdere 2070 kaarten?
RTX 2070;2;0.4683230519294739;Nee dat begrijp je niet goed. de goedkoopste GTX 2070 is 660 euro. De 1080 is 500 euro, de 1080 TI is 719 euro. dus voor 60 euro meer heb je betere prestaties in huidige games. Voor 60 euro minder weet je niet wat raytracing etc gaat brengen. Ik zou zeggen, als je niet hoeft te upgraden zou ik even wachten tot het nieuwe jaar. Dan zijn de prijzen wat genormaliseerd en weten we hopelijk hoe de nieuwe features werken in games.
RTX 2070;3;0.3298887312412262;Nou ja, we weten dat de 2080Ti rond de 40 fps hangt op 1080p met RTX aan. De 2070 heeft zwakkere RTX hardware en zal dus een stuk minder halen. Leuk dus, dat 'futuure proof' gedoe, maar 40 fps op 1080p... daar geeft ik geen 600+ euro aan uit, dus leuk dat Turing raytracing 'ondersteunt', persoonlijk zou ik het buiten er even mee spelen toch geen gebruik van maken met zulke bagger performance. edit: bron
RTX 2070;1;0.3041366636753082;Bron?
RTX 2070;4;0.2846148908138275;
RTX 2070;3;0.3292785882949829;Als je verder leest in het artikel staat er ook nog dit: Dus ja de fps gaan nog een stuk beter worden met RT aan. Dat staat trouwens ook in dit artikel van Techpowerup.
RTX 2070;2;0.35196563601493835;leuke aanname, en het is ongetwijfeld waar dat de prestaties nog beter worden, maar hoeveel beter? ik heb er enorm sterke twijfels bij dat de prestaties veel beter worden, voornamelijk hierom : why the fuck lanceert nvidia hun nieuwe gpus voor echt achterlijke hoge prijzen met een usp die nog niet getest, gebruikt, en gebenchmarkt kunnen worden? het is toch een usp, ze houden hun bek niet over gigarays... dus waarom dat niet releasen met ene paar games, als het oh - zo - geweldig is? ze werken voor hun rtx api nauw samen met de game studios dus het is neit zo dat ze geen invloed hebben op of weet hebben van de voortgang bij de studios, ze konden de game en gpu release perfect op elkaar afstemmen... ik geloof dat niet, amd is doorgaans vrij open over releases als er wat noemenswaardig zit aan te komen, zie zen / zen + / zen2. ook over polaris en vega waren ze erg open. enkel de polaris refresh werd niet aangekondigd, want ja : dat was een rebrand van de 400 serie. ik reken dus enkel op een polaris refresh ( de 600 serie ) van amd op de korte termijn. optie 2 dan maar? dan zou nvidia een reden moeten hebben om niet te showen met de rtx performance gedurende de eerste maanden na release. de enige verklaring daarvoor die ik zie, is dat de raytracing performance ronduit bagger is, en niet veel beter wordt ( anders hadden ze de release makkelijk enkele maanden uitgesteld ). dan is het in het belang van nvidia om die benchmarks ver van de release weg te houden, zodat vele mensen de gpu kopen op basis van vage beloftes en geen bewezen prestaties. ik zie dat als enige realistische invulling van de vreemde release door nvidia. maar ik zou wensen dat ik er grof naast zit : ik wil niets liever dan een degelijke hoeveelheid concurrentie, dus als vega20 / 7nm er toch snel komt, ben ik dolgelukkig dat ik er naast zit.
RTX 2070;5;0.35714712738990784;Het zal dan vooral hopen worden op Vega 20 7 nm en niet denken. Zelf denk ik dat AMD pas begin 2019 met Vega 7 nm komt. En de 2000-serie bevat ook Tensor cores waar je DLSS mee kan bekomen. Dat is een soort anti-aliasing en als je daarmee de shader cores kan ontlasten kan dit ook zorgen voor een serieuze prestatieverbetering en is dit naast RT een bijkomend verkoopargument voor de 2000-serie tov de 1000-serie.
RTX 2070;3;0.35739442706108093;Persoonlijk vermoed ik ook dat de 2070 niet voldoende power zal hebben voor hybrid ray tracing. DLSS is mogelijk een gans ander verhaal. En dat kan wel eens voor veel betere frame rates zorgen dan een 1080 en mogelijk ook een 1080Ti bij gelijke beeldkwaliteit. Zelfs dan blijft de vraag: wil je een meer allround kaart (1080Ti) of een kaart die bij een aantal games piekt (2070)? Met die bijkomstigheid dat de 2070 wat minder stroom vreet.
RTX 2070;2;0.46909624338150024;Kijkend naar technieken als Tesselation en Physx, helemaal niks bijzonders. Een aantal spellen (al dan niet gesponsord) zullen de techniek van de daken schreeuwen, maar zodra de subsidie op is zal het stabiliseren ergens tussen eye-candy en performance met een implementatie die merk-onafhankelijk is (zoals Havok in het geval van Physx) zonder het te merken met wellicht een optie in het menu om het wat op te krikken. Nvidia roept wat hard om met die USP er 200 euro boven op te kunnen doen, maar de adoptie zal maar mondjes maat zijn. En dan zit je dus met een Tensorcore van 200 piek die in 95% van de spellen geen kut doet.
RTX 2070;2;0.35784316062927246;Je vergeet alleen dat er steeds meer spellen met de grote standaard engines gemaakt worden, en de grote standaard engines ondersteunen allemaal inmiddels wel de RTX toevoegingen en is het niets meer dan een setting die aangezet hoeft te worden. Degene die hun eigen engines schrijven, tja, die zullen wel extra moeite moeten doen willen ze dit ondersteunen.
RTX 2070;3;0.46659353375434875;Ik denk toch dat het stug tegenvalt daarmee. Vaak worden er veel truuks gebruikt om een speler mbv verlichting door een level heen te leiden of gewoon voor het stileren. Ik denk dat je redelijk je verlichting op de schop moet gooien om hetzelfde/natuurlijk effect te krijgen.
RTX 2070;3;0.38629528880119324;In tegenstelling tot physx (en tesselation?) wordt er nu wel geroepen dat raytracing aanzetten heel simpel en snel gaat in bestaande titels, en in nieuwe titels rapper dan zelf al je lighting regelen. Indien waar kan dat wel een impact geven op hoeveel games deze feature gaan gebruiken.
RTX 2070;2;0.5681817531585693;De enige die dat roept is nVidia. En ja, als je een game maakt die alleen werkt met ray tracing, Dan zijn de licht effecten makkelijker toe te voegen. Ontwikkelaars moeten echter nu en in de nabije toekomst de games ook nog laten werken op de oude manier, omdat er te weinig gebruikers zijn met een RTX compatibele kaart. Ontwikkelaars moeten bij implementatie van RTX dus dubbel werk doen. Ze moeten lichtbronnen toevoegen op de oude manier via rasterisation en ze moeten RTX implementeren. Ik denk dat nVidia een aantal developers zoals Dice voor de release van de RTX kaarten een enorme zak met geld heeft gegeven om een goed woordje te doen voor RTX, omdat ze zagen dat de prestaties van deze lijn te weinig bood in vergelijking met oudere kaarten. Maar dat de RTX adoptie het komende jaar erg tegen gaat vallen. Mijn twee centen...
RTX 2070;2;0.39029109477996826;Als je beredenering klopt hoop ik dat Nvidia geleerd heeft dat je er alleen met een smak geld niet komt en dat de klant eigenlijk koning is. Hier spelende op een GTX1070 die het nog prima doet en voorlopig achter mijn hoofd krabbelt of ik de keuze ga maken om up te graden. En als dit gaat gebeuren misschien een 1080ti voor een leuk prijsje.. Ik ben nl niet van plan om voor nieuwe features te betalen die het eigenlijk nog niet zijn.. Voelt mij als in een dure luxe nieuwe auto kopen, wetende dat hij 2 lekke banden heeft...
RTX 2070;2;0.3371298909187317;"De goedkoopste RTX 2070 is op dit moment in pricewatch 660 euro. Maar de buren hebben contact opgenomen met Alternate: ""Navraag bij Alternate leert ons dat er RTX 2070's daadwerkelijk vanaf 529 euro verkrijgbaar zullen zijn vanaf morgen."" Voor 529 euro is de RTX 2070 wel aantrekkelijk. Ik zit de afgelopen week te kijken naar een GTX 1080 (Gigabyte GeForce GTX 1080 G1 Gaming 8GB, huidige prijs 529 euro). Maar voor 529 euro neem ik natuurlijk de RTX 2070 ipv GTX 1080."
RTX 2070;3;0.38630813360214233;Toch blijft het een flink bedrag.. 530 euro.. voor een midrange kaart waarmee je wat meer fps krijgt dan je huidige kaart... ik speel niet genoeg games om dat te kunnen veroorloven, ik denk wel meer mensen niet.
RTX 2070;1;0.6524779796600342;Je moet afwachten totdat de 1000-serie van de markt gehaald wordt want dan kan Nvidia zijn prijs van de 2000-serie niet meer afzetten tegen deze kaarten en gaan de prijzen dalen. Nu een Turingkaart kopen is keidom. Trouwens €529 voor een 2070 is nog altijd veel te veel. Never nooit niet ga ik een videokaart kopen die meer dan €500 kost.
RTX 2070;2;0.45658785104751587;Ja, want de prijs van de 1000-serie was ook zo gedaald toen de 900-serie uitgefaseerd werd... oh wacht.... Ben wel met je eens dat 500+ veel te veel is voor een GPU, vind de 260+ al teveel voor een (nog geen echte midrange) kaart als de 1060/6GB.
RTX 2070;1;0.2661079168319702;Crypto...
RTX 2070;5;0.35034090280532837;Bij de generatie ga je straks met Raytracing actief ook geen snellere beelden krijgen maar krijg je mooiere beelden door de nieuwe technieken. De meeste crypto miners gebruiken amper nog Nvidia kaarten, juist de AMD kaarten zijn gewild
RTX 2070;3;0.2374504655599594;"was een reactie op ""Ja, want de prijs van de 1000-serie was ook zo gedaald toen de 900-serie uitgefaseerd werd"". En fijn dat je met 2-3 jaar oude informatie komt - nVidia kaarten waren zeer zeker ook in trek bij miners. Ik snap de relevantie van jouw comment met mijn comment niet"
RTX 2070;1;0.7070072293281555;Minen is allang niet meer boeiend geworden de laatste 6 maanden. Wie nog mined zit over het algemeen op verouderde projecten die nog in PoW draaien. PoS en PoC is allang het nieuwe ding. PoW is stervende
RTX 2070;2;0.5355478525161743;Ik snap de relevantie van jouw comment met mijn comment nog steeds niet. Je draagt een (tegen)argument aan alsof je je gelijk wil halen maar inhoudelijk reageren lukt niet heel erg goed, merk ik. Daarbij wordt mansplaining ook niet heel erg gewaardeerd. Al helemaal niet omdat je geen flauw benul hebt van mijn betrokkenheid bij cryptocurrency.
RTX 2070;5;0.5156004428863525;Nvidia werd pas echt in trek toen de AMD kaarten niet meer te krijgen waren. En prima je zal vast heel betrokken zijn bij crypto en dat is natuurlijk helemaal prima. Tuurlijk werd Nvidia ook gebruikt voor crypto maar AMD was toch wel echt het meest gewild. Dat kon je ook zien in alle grafieken waarbij pas te korten kwamen bij Nvidia toen AMD echt moeilijk te krijgen was. Dat is echt een feit.
RTX 2070;1;0.8059854507446289;Mee eens veel te duur voor een x70 kaart. Heb er nooit meer als €400 voor betaald en dat gaat nu ook niet gebeuren!
RTX 2070;2;0.304740309715271;De 1080ti heeft geen ray-tracing core's?
RTX 2070;5;0.5070691704750061;raytracing cores is nieuw sinds de RTX 20 serie.
RTX 2070;2;0.30314311385154724;Dit lijkt inderdaad de beste aanpak. Zeker als je nu nog een GTX1080 hebt en al die tijd het dus niet nodig vondt om om naar de 1080Ti te gaan voor de 30% dat die sneller is. (Dan is de 2080 momenteel waarschijnlijk ook nog te prijzig) Daarnaast is het nog maar de vraag hoe bruikbaar raytracing wordt in de eerste games op deze kaarten. Als 1080p 60fps is wat je krijgt voor de rt delen in hybrid RT games (full raytracing gaat em sowieso niet worden) op een 2080Ti, dan kan het op de 2070 waarschijnlijk amper ingeschakeld worden zonder naar hele lage framerates af te glijden, of door zeer lage resolutie RT te gaan gebruiken. (b.v. reflecties op 480p 50fps terwijl je normaliter 1440p 75fps kon draaien zonder RT) Nu moet ik zeggen dat deze prijzen nog meevallen (mja, die 900€ kaarten natuurlijk niet), kijkend naar 2080Ti prijzen lijken deze kaarten een stuk minder afroming te genieten. Maar dat neemt niet weg dat €650 teveel is voor wat je krijgt hier. Dit is zo'n beetje hetzelfde als 3(!!!) jaar terug immers...
RTX 2070;4;0.38682523369789124;Hoe ik het begrijp lijkt het grootste voordeel in bepaalde mining sectoren te zitten. Zo zijn er coins die erg leunen op de compute unites / cuda cores en etc. En die zijn drastisch verbeterd bij de 20** serie. We zitten zelf nog op wat uitslagen te wachten van de 2070 en de 2080 maar het ziet er zeer positief uit. Dat het voor gaming amper verschil maakt tsja. Petje af voor nvidia met deze dikke mining kaarten Vergeet dus niet dat de mining wereld ontzettend groot is (ondanks dat bepaalde websites doen alsof dit niet zo is), ook wederom deze nieuwe nvidia reeks lijkt uitermate geschikt voor mining. Juist doordat de specs die bij bepaalde coins nodig zijn (en bij gamen een stuk minder) ''zeer'' goed verbeterd zijn bij deze 20**
RTX 2070;3;0.30194398760795593;Pantostie, De goedkoopste RTX 2070 op dit moment is die van evga 519 euro standaard versie. is net zo duur als de GTX 1080. Verschil kwa snelheid is weinig maar kwa arcitectuur ga je erop vooruit.
RTX 2070;1;0.7134965658187866;Goedkoopste rtx 2070 is nu 529 en goedkoopste 1080 is nu 499. Terwijl de goedkoopste 1080ti 649 kost. Voor die 30 euro meer lijkt me de 2070 de beste koop (nieuwe technologie en iets beter) als je deze vergelijkt met de 1080 en 1080ti. De 2080 en 2080ti zijn het niet eens waard om te noemen. Belachelijke prijzen.
RTX 2070;2;0.5532079339027405;Nee, niet helemaal. Het komt er een beetje op neer of de kaarten voor de adviesprijs te kopen worden aangeboden, die is namelijk lager dan de goedkoopste 1080 TI (719 euro) die ik nu in de pricewatch zie staan. De 2070 FE staat nog nergens met prijs maar de adviesprijs is 639 euro. Tevens is het verschil naast performance ook de tensor en RT cores. De 1080 TI heeft deze niet, en als er games komen die het gaan gebruiken kan dat de 2070 uiteraard voorbij de 1080 TI pushen als ze deze efficiënt weten in te zetten (bijvoorbeeld met DLSS). Dus het is een beetje een afwacht vergelijking momenteel. De prijzen zijn eigenlijk nog niet bekend en de inzet van tensor en RT cores is ook nog niet echt gemeen goed. Persoonlijk denk ik dat de 2070 een betere investering op de langere termijn zal zijn maar dat is een beetje koffiedik kijken. Ik zou eigenlijk gewoon wachten tot dat er games zijn die het allemaal ondersteunen, dan weet je of het echt meer waarde heeft.
RTX 2070;2;0.4876554310321808;Hoe het nu lijkt, lijken de prestaties juist in te storten als RTX aangezet word, en is DLSS een leuke toevoeging, maar moet net als RTX toegepast worden in games, en is de kwaliteit er van ook niet altijd even goed Denk zelf van niet, om het moment dat er RTX games uitkomen zal het niet meer als een gimmic zijn, en op het moment dat dit breed toegepast gaat worden, zijn we al 1 of 2 generatie videokaarten verder, mocht het al zo ver komen
RTX 2070;1;0.4460233449935913;">Hoe het nu lijkt, lijken de prestaties juist in te storten als RTX aangezet word Behalve dit ben ik ook extreem bang voor Bloom 2.0. Game designers zijn over het algemeen erg goed in raytracen 'faken' (tot in zoverre dat ik geld zou zetten op dat de meeste gamers een game met RT en zonder RT op het moment 100% hetzelfde zouden ervaren, of zelfs minder positief), maar geef ze een nieuw speeltje en opeens is het ""KIJK WAT ONZE ENGINE HEEFT"" en heb je een bek vol Bloom ala (meteen op 0:01 van de video). Ik ben erg enthousiast over RT, en helemaal niet enthousiast over RT in 2018. Dit had gewoon als bonus op een zoiezo betere kaart moeten zitten, niet een 1080TI + Misschien Nuttig Cores waarvan ik eerlijk denk dat ze de komdende paar jaren de gemiddelde game alleen maar naar beneden gaan halen in kwaliteit. Misschien leuk verdiennertje. Koop een paar van deze dingen op marktplaats rond de kerst als mensen geld nodig hebben. Verpats ze in mei als het vakantiegeld eraan komt en mensen domme shit gaan kopen."
RTX 2070;5;0.2969868779182434;2070 heeft nieuwe technologie..
RTX 2070;3;0.2837248742580414;waarbij de vraag is of elke ontwikkelaar die gaat toepassen?
RTX 2070;1;0.444323867559433;Vast niet iedere ontwikkelaar, maar velen wel. Het is toch weer een verkoop argument.
RTX 2070;1;0.5228297114372253;RTX was al extreem laggy op een 2080 Ti dus op een 2070 zullen de meeste mensen die opties uitzetten IMO.
RTX 2070;2;0.49593544006347656;Ja en zodra je die RTX optie aan zet kan kom je zelfs met 1080 als resolutie niet meer boven de 60 fps. Zeker op de 2070 zie ik de nieuwe technologie niet als nuttig.
RTX 2070;1;0.7563903331756592;*buzzwords Baby vorm toepassingen en gimped showreel stellen niks voor.
RTX 2070;1;0.6468042731285095;Conclusie is verkeerd opgeschreven, ze lopen te goochelen met 1080 en 1080 TI.
RTX 2070;1;0.3981098532676697;"Dat is de vraag, aangezien men verwacht dat de RTX 2070 in reguliere edities goedkoper kan/zal zijn: ""Als de huidige prijzen van de 2070 echter standhouden, koop je voor hetzelfde geld een 1080 Ti. Dat is vrijwel altijd de snellere kaart. Je hebt dan uiteraard geen tensorcores of raytracingcores in de gpu, dus je kunt niet profiteren van die nieuwe technologie als en wanneer er games verschijnen met ondersteuning daarvoor."" ""Als we van de FE-adviesprijs uitgaan, koop je voor diezelfde pak 'm beet 650 euro de duurste GTX 1080. Volgens sommige webwinkels, met name Alternate, zou je echter vanaf woensdag 17 oktober een instap-2070 voor 529 euro moeten kunnen kopen"""
RTX 2070;2;0.3415214717388153;Dat heb je goed begrepen, de goedkoopste 1080Ti kaarten zijn momenteel goedkoper dan 2070 kaarten. Die eerste zijn de laatste tijd in prijs gedaald en die laatste is natuurlijk net nieuw, die komt nog wel iets omlaag. Ik zie uberhaupt geen reden om mijn 1080Ti de deur uit te doen, een 2080/2080Ti is veel te prijzig, ik houd al een tijdje V&A in de gaten of iemand eenzelfe 1080Ti als ik heb tweedehands weg doet, dan ga ik SLI, RTX is vooralsnog meer een gimmick als ik alle tests zo lees. Misschien dat dat bij de RTX 21xx serie een optie wordt?
RTX 2070;1;0.6291810274124146;zelfs de 1070ti is nagenoeg even snel als de 2070, en die koop je voor 400 euro.
RTX 2070;2;0.3419087529182434;Nu hanteer je wel een heel rekbare versie van het woord nagenoeg. Volgens die definitie zou je ook kunnen zeggen dat een 1080ti nagenoeg even snel is als een 1080.
RTX 2070;2;0.4063448905944824;Nee, kijk maar naar 1080p benchmarks, ze hebben bijna dezelfde fps. de 1080ti is veel sneller. Heb je de benchmarks wel bekeken?
RTX 2070;1;0.7130556106567383;Ik heb de benchmarks bekeken en zie echt totaal niet waar je het over hebt.
RTX 2070;1;0.616047739982605;Deze kaart lijkt totaal onnodig. Ogenschijnlijk te traag voor RTX, langzamer dan een 1080 Ti en ook, in tegenstelling tot wat beweerd wordt, DUURDER dan die snellere 1080 Ti. De goedkoopste 1080 Ti in de pricewatch is 649, de goedkoopste 2070 is in de pricewatch 660. Verdere 2070 AIB tussen de 750 en 900 euro! De 2070 is dus duurder EN trager dan een 1080 Ti.
RTX 2070;1;0.7070911526679993;"660 euro!!! Elke post vermeld 660 euro. ""...Alternate leert ons dat er RTX 2070's daadwerkelijk vanaf 529 euro verkrijgbaar zullen zijn vanaf morgen."" Als dat een model is van Gigabyte dan ben ik dik tevreden met een prijs van 529 euro."
RTX 2070;1;0.32946398854255676;Ik heb mijn tweedehands auto gekocht voor 660 euro.
RTX 2070;3;0.33704787492752075;Deze is nu inderdaad volop te koop voor 529 en lijkt mij ook daarom een betere koop tov de 1080 en 1080ti.
RTX 2070;1;0.46216315031051636;Volgens pricewatch is het toch echt 529 euro: pricewatch: Gigabyte GeForce RTX 2070 Windforce (Bij 3 winkels) pricewatch: MSI GeForce RTX 2070 Aero 8G (Bij Azerty) pricewatch: MSI GeForce RTX 2070 Armor 8G (Bij Azerty)
RTX 2070;1;0.2616890072822571;519 op caseking fyi
RTX 2070;2;0.5036903619766235;Zoals al aan gegeven door meerdere mensen liggen de prijzen nu nog te hoog en gaat alternate ze vanaf 525 euro verkopen,de rest gaat dan volgen is mijn verwachting. Dat er geen concurrentie is helpt helaas ook niet mee,tevens als de 1000 serie gaat uitfaseren gaan de prijzen wel veranderen. Daarnaast is dit een nieuw product en de 1080ti kan geen procenten meer winst halen dmv drivers waar dit bij de 2000 serie nog wel degelijk zal gaan gebeuren en dan ben ik benieuwd wat het verschil nog gaat zijn. Om te stellen dat deze kaart onnodig is dat vind ik nu nog veel te kort door de bocht. Laten we aankomend kwartaal eerst afwachten wat er met driver updates aan verbeteringen mogelijk is en wat de prijzen gaan doen.
RTX 2070;3;0.3473762273788452;Even op een iets nuttigeren gang, ik denk dat het nog wel wat jaren duurt voor Retracing een ding is (als het ooit iets wordt) en zo als het artikel al zegt kan je een 1080TI oppikken voor rond hetzelfe, snap deze serie ook niet kwa prijs en performance upgrade
RTX 2070;1;0.42853426933288574;Het is ray-tracing en als je denkt dat het niets wordt dan weet je duidelijk niet wat het inhoudt. Dit is de volgende logische stap in de ontwikkeling van grafische kaarten, niet enkel een 'feature'. Dat deze kaarten waarschijnlijk een generatie te vroeg zijn is een ander verhaal.
RTX 2070;2;0.5131391286849976;Het heeft zeker toekomst, maar geen enkel nut op een 2070. Het is al de vraag of een 2080ti met ray-tracing normale fps kan behouden op een resolutie van 1080. En ik zie 1440 toch echt als het minimum voor gaming dus dan is ray-tracing voor mij niet boeiend de komende jaren.
RTX 2070;1;0.43043047189712524;Ik zou als ik jou was je aandelen in de games industrie verkopen want die denken daar heel anders over. Op de GC konden insiders al demo's zien die behoorlijk overtuigend waren. Natuurlijk zonder dat je daar veel over mocht berichten en dat moet voor de Tweaker redactie best frusterend zijn. Ik stond naast een Tweaker journalist bij een demo die een 2080 werkelijk naar zijn topvermogen bracht. Alle game engines zijn over een paar maanden klaar voor de nieuwe hardware en veel studio's overwegen updates voor bestaande games (als een het doet kunnen de anderen niet achterblijven).
RTX 2070;2;0.3298344612121582;Volgens mij reageer je op de verkeerde, want ik zeg niet dat RTX niks wordt. Alleen dat een 2070 niet de kaart is voor de RTX ervaring en als we toch bezig zijn dat je beter een generatie kan wachten aangezien de 2080ti al last heeft op 1080
RTX 2070;3;0.41627079248428345;Agreed, het staat nog erg in de kinderschoenen. Tegen de tijd dat volledige ray tracing mogelijk is op ultra settings op 1080p bv zijn we verschillende generaties verder verwacht ik. Het hangt natuurlijk ook af van de game devs hierin en of ze er ook volledig voor willen gaan. Persoonlijk als ik in het komende jaar een kaart zou kopen is het voor mij geen afweging of in welke mate ray tracing ondersteunt word.
RTX 2070;1;0.6689165830612183;Is dat echt zo, en heb je daar een bron van? Vind het namelijk niet geloof waardig, vogens mij stop geen een Game developer graag geld in zaken die een heel klein gedeelte van ze gebruikers kan gebruiken, zelfde met Physix dat zit ook maar in een handje vol games Tot dat Raytracing gemeengoed is zullen er maar weinig games uitkomen waar raytracing letterlijk een meerwaarde heeft imo
RTX 2070;3;0.3969501554965973;Daar zijn die aparte RT-cores toch voor om de shader cores te ontlasten. Ik zie niet in waarom de prestaties met RT aan naar beneden zouden gaan.
RTX 2070;2;0.47086918354034424;Niet dat ik weet. Zie dat er alleen maar oud nieuws (2 maanden) over is dus misschien heb ik wat gemist, maar de 2070 krijgt 6 giga Rays tegenover de 12 van de 2080ti dus kan me niet voorstellen dat het top wordt.
RTX 2070;2;0.29853370785713196;"NVIDIA and Eidos (developers of ""Shadow of the Tomb Raider"") were quick to respond to the PCGH story. They stated that the build of the game demoed at Gamescom is pre-release, and the studio is still optimizing it for NVIDIA GeForce RTX series; and that the GeForce RTX hardware is running on pre-launch beta drivers that are yet to pack ""Game Ready"" optimization for SOTR. Dit staat toch in het artikel. Waarom zou je Nvidia niet geloven? Ik denk dat het nog een pak beter kan met RT als de drivers geoptimaliseerd zijn en ook de game zelf."
RTX 2070;1;0.6109930276870728;Echt geen idee waarom ik Nvidia wel zou geloven. Ze hebben opzettelijk vage benchmarks gepost, een hele conferentie over nieuwe features zonder te praten over wat de kaarten verder kunnen en ze sturen nu de €500 versies van de 2070 naar reviewers om net te doen of deze kaart wel betaalbaar is. Het kan zijn dat het waar is, maar bij Nvidea is het voor mij eerst zien en dan geloven. De 2070 heeft hoe dan ook de helft van de power van een 2080ti beschikbaar voor rays dus dan mag je hopen dat het geen on/off optie wordt in games.
RTX 2070;5;0.44760507345199585;De 2000-serie heeft ook nog Tensor cores voor DLSS. Dat is een soort anti-aliasing en kan zorgen voor een serieuze prestatieverbetering. Dat is een bijkomend verkoopargument voor de 2000-serie tov de 1000-serie.
RTX 2070;3;0.45415353775024414;Ik vind het jammer dat tweakers keer op keer de VEGA 56/64 kaarten onderbelicht, alsof ze niet meer bestaan. Zeker met de nieuwste drivers, verlaging in prijzen en eventuel undervolt and overclock, wordt de AMD kaart een aanzienlijk betere keuze. Dit is helemaal interesant aangezien Freesync 2.0 gratis is vs G-sync. Alleen als je echt max op 4k gaat dan is er maar 1 keuze de 2080TI, anders dan dat staat AMD op 1440p zijn mannetje ook wel.
RTX 2070;1;0.7899194955825806;Helaas is de Vega een beetje een dood product. Of een product die niet meer serieus word genomen door de kaard bakkers. Zie hier: categorie: Videokaarten Alleen 6 modellen als je kijkt naar resultaten met meer dan 2 webshops. Als je kijkt naar de 1070/1070ti/1080: categorie: Videokaarten 141 uitvoeringen gevonden..... Dat geeft bij mij aan: Dood product.Geflopt.
RTX 2070;1;0.34531381726264954;Geflopt? Als je een freesync monitor hebt is het een betere keus dan een 1070ti/1080.
RTX 2070;2;0.42579275369644165;Ja geflopt. Alleen 6 modellen als je kijkt naar resultaten met meer dan 2 webshops. En aantal gebruikers: Dat is voor mij geflopt. De RX570 en RX580 zijn wel leuke kaarten (Voor de verschrikkelijke prijsstijging door de mining). FYI ben een AMD fan. Dus ik hoop dat ze flinke klappen gaan uitdelen met Navi, en ons zoet houden met een budget klapper: RX 590/RX670/RX680.
RTX 2070;1;0.5519335865974426;Je bent zelf geflopt, zat vega kaarten in dld.
RTX 2070;2;0.43122681975364685;Want? De Vega's worden gewoon meegenomen in de benchmarks en in de conclusie. Het mag onderhand ook wel dat Vega in prijs genormaliseerd is en dat Vega eindelijk structureel beschikbaar is voor gewone consumenten, denk je niet? Voor gaming is Vega te lang overpriced geweest, en nauwelijks beschikbaar.
RTX 2070;1;0.5134115815162659;Laat me raden, jij hebt een VEGA kaart....... En of Freesync gratis is maakt helemaal niet echt uit. En doorsnee gebruikers gaan een kaart niet overclocken/undervolten/whatever, die steken de kaart in de computer, installeren de driver en gaan met die banaan. En zo zou er dus ook getest moeten worden, met hooguit een extra sectie met overclocken..
RTX 2070;1;0.3029542565345764;ik heb nu een 670 kaart, naar welke kaart zou ik (budgetfriendly) best upgraden?
RTX 2070;1;0.2856840193271637;Wat is jouw budget? En welke resolutie ga je gamen (1080p, 1440p, 4k?). Heb je 3 games als voorbeeld die je graag met ultra settings wil draaien?
RTX 2070;4;0.3519470989704132;liefst max 800e, en voorlopig op 1080p (50inch plasma), maar aan het zien voor een 4k upgrade farcry series/doom/ assassins creed series thx
RTX 2070;2;0.36725789308547974;Er is heel veel verschil tussen 1080p en 4K kwa kracht wat nodig is van je GPU. Dus ook heel veel verschil in kaart keus indien je een klein budget heb. Ik vind 800 euro niet een klein budget. Als Alternate.nl echt een Nvidia RTX 2070 kan verkopen voor 529 euro dan is dat de beste keus voor iemand met een 800 euro budget voor een GPU die ook eventueel wil overstappen naar gamen in 4K.
RTX 2070;3;0.2877207100391388;waar zie je dat op alternate?
RTX 2070;1;0.3746132552623749;Het is nog vroeg. Geef ze even de tijd. Update: Hier de eerste 2 met een 529 euro prijs pricewatch: Gigabyte GeForce RTX 2070 Windforce
RTX 2070;5;0.5206930041313171;Ik heb een 1060 3GB en die draait alles op high, zonder problemen. Het tekort aan geheugen wat iedereen roept heb ik ook werkelijk nooit last van en de framerates zijn identiek aan de 6GB variant (zie YT). Voor een kaart op budget kan ik hem zeker aanraden! PS. ik draai op 1440p. Als je 1080p draait wordt het helemaal een makkie.
RTX 2070;3;0.4093376100063324;Yordi, Er is wel een verschil tussen de GTX 1060 3GB en 6GB kaarten. Namelijk de aantal cores: 6GB kaarten: 1280 cores 3GB kaarten: 1152 cores
RTX 2070;5;0.27506595849990845;Behalve in laptops daar heb je wel een 3GB met 1280 cores
RTX 2070;3;0.26222196221351624;mijn 670 heeft 4gb :d 'k wil wat futureproof zijn
RTX 2070;3;0.30135586857795715;Zijn voor deze review de vega's opnieuw gebenched? of zijn dit gegevens uit oudere reviews? Aangezien mijn Vega 64 op Ultra 1440P toch echt makkelijk 90 FPS haalt in GTA V.
RTX 2070;1;0.4389970302581787;Die zijn medio september met 18.6.1 getest
RTX 2070;3;0.28974345326423645;De tweede groep wijst naar de maand, dus dat is een juni driver en geen driver van september. In september hadden we 18.9.x drivers. Dus dan hebben jullie Vega in juni getest, aangezien het de eerste driver van juni is. Of jullie hebben toen een driver gepakt van een aantal versies terug. Zelf heb ik op dit moment een september driver geïnstalleerd. 18.9.3. De derde driver van september dus. De nieuwste is nu trouwens 18.10.1. Edit: juni 2018 dus, aangezien ook de nVidia driver voor de non-Turing kaarten van juni is. Tja lekkere vergelijking, voor hetzelfde heeft één van die vele drivers een winst van tien procent of meer in een game, gebeurt vaak genoeg, vooral bij nieuwere games.
RTX 2070;3;0.40418314933776855;Ik kan mij voorstellen dat men niet maandelijks de drivers update. Met een driver uitgebracht in Juni testen in September is dus goed mogelijk en is zeker per definitie niet slecht. Ik denk ook niet dat er in 3 maanden tijd veel updates zijn gekomen die in alle geteste games een wereld van verschil in performance maakt.
RTX 2070;2;0.4525057077407837;De games die we draaien zullen niet bijster veel in prestatie winnen met een nieuwe driver, aangezien die games driverupdates die prestaties verbeteren al lang en breed gekregen hebben. Voor nieuwe kaarten gebruiken we uiteraard nieuwe (launch)drivers, maar omwille van coherentie van eerdere resultaten en begeleidende teksten testen we niet elke keer alle kaarten opnieuw als een nieuwe driver uitkomt. Dat is qua tijd ook niet realistisch. Dat gezegd hebbende, heb ik even de belangrijkste fixes tussen 18.6.1 en 18.9.3 op een rijtje gezet: 18.9.3 AC: Odyssey perf boost (niet relevant) 18.9.2 F1 2018 perf boost (relevant dus), Fortnite, Shadow of Tomb Raider & Star Control perf boost (niet relevant) 18.9.1 support added voor Shadow of the Tomb Raider & Star Control (niet relevant) 18.8.2 Strange Brigade perf boost, support F1 2018 18.8.1 perf boost Monster Hunter (niet relevant), support added diverse games (niet relevant) 18.7.1 perf boost Earthfall (niet relevant) En dan zijn we bij 18.6.1 aangekomen. Kortom, je kunt iets betere prestaties in F1 2018 voor de Vega-kaarten verwachten met de nieuwe drivers. Dat doet denk ik bijzonder weinig af aan de teneur en conclusie van de review of de algemene verhouding tussen de 2070 en Vega 64.
RTX 2070;3;0.3432730436325073;Ja, dat is dus precies wat ik bedoel met mijn laatste zin dus. Zes updates en er is wel 1 noemenswaardige fix die van pas kan komen bij het benchmarken.
RTX 2070;3;0.43366411328315735;OK, ik kan dus niet lezen, want sommige games zijn inderdaad met 18.6.1 getest, en sommige met 18.8.1. In het kader van transparantie hier het lijstje met drivers/games 22421 Gebruikte driverversie - 3DMark 18.6.1 22989 Gebruikte driverversie - Assassin's Creed Origins 18.6.1 22425 Gebruikte driverversie - Battlefield 1 18.6.1 23425 Gebruikte driverversie - F1 2018 18.8.1 22499 Gebruikte driverversie - Far Cry 5 18.6.1 22445 Gebruikte driverversie - Forza Motorsport 7 18.6.1 22431 Gebruikte driverversie - Ghost Recon Wildlands 18.6.1 22433 Gebruikte driverversie - GTA V 18.6.1 22437 Gebruikte driverversie - Rise of the Tombraider 18.6.1 23705 Gebruikte driverversie - Shadow of the Tomb Raider 18.9.2 22439 Gebruikte driverversie - The Witcher 3: Blood & Wine 18.6.1 22441 Gebruikte driverversie - Total War: Warhammer 2 18.6.1 22423 Gebruikte driverversie - Unigine Superposition 18.6.1 22987 Gebruikte driverversie - Wolfenstein II 18.6.1 Voor F1 is dus 18.8.1 gebruikt, nog steeds niet ideaal, maar zoals gezegd, alles hertesten is tijdtechnisch onhaalbaar.
RTX 2070;1;0.45518478751182556;Achja, je mag al blij zijn dat het versienummer van de driver benoemd wordt, gebeurd bij een hoop reviews niet.
RTX 2070;3;0.26241087913513184;Het eerste wat je doet voordat je begint met testen is de nieuwste driver installeren lijkt mij.
RTX 2070;3;0.30528101325035095;Zoals er door AMD meermaals per maand een nieuwe driver uitkomt, kun je van een tester niet verwachten dat die altijd de nieuwste driver gebruikt. Ook dat zorgt o.a. voor vervuiling van de PC en ook dat beïnvloed de benchmark. Je wilt de PC's dus zo schoon mogelijk houden. Ik zou dan dus eerder opteren om elke X tijd de PC opnieuw te installeren en dan voorzien van de courantste drivers. EDIT: Zie willemdemoor in 'reviews: Nvidia GeForce RTX 2070 - Turing op budget?', in de afgelopen zes updates van de AMD drivers wel 1 aanpassing die van noemenswaardig belang kan zijn, daar is het imo het niet waard voor om te upgraden.
RTX 2070;3;0.4601069986820221;Je mag van een review toch wel verwachten dat een test wordt uitgevoerd met de op dat moment laatste driver.. Wel kun je niet bezig blijven met testen voor opvolgende kaarten.
RTX 2070;3;0.4000265896320343;Meen je dit nou? dat is juist iets dat ik wel verwacht van testers, ja zal best veel werk zijn, maar je bent een goeie teser of je bent het niet ik zie ook redelijke verschillen hier en met Die wel altijd de nieuwste drivers gebruiken
RTX 2070;5;0.22820937633514404;Zou zeggen lees even mee vanaf willemdemoor in 'reviews: Nvidia GeForce RTX 2070 - Turing op budget?'. En ja, heb vaak zat gedoe met drivers gehad omdat ze niet goed verwijderd werden in het verleden. Daarom dat ik en zo min mogelijk upgrade en als ik al (verplicht) upgrade, dat altijd met een clean install van de drivers doe en nog hou je restjes over. Anders zouden tools als DDU ook niet nodig zijn.
RTX 2070;1;0.4478992223739624;ik heb dat gelezen, maar het is niet dat is nu denk van ohh maar dan is het goed dat stukje geeft me alleen een slechter gevoel dat er dus met verschillende driver versies is getest Zelf trouwens bijna nooit problemen gehad met drivers, simpel weg de nieuwste installeren, herstarten en gaan, simpeler kan het niet imo, een tool als DDU heb ik misschien een paar keer gebruikt en dan praten we over jaren geleden, nu al jaren nooit meer het nut er van ingezien aangezien driver updaten super simpel is BTW er is altijd zo'n mooi lijstje met wat de update doet, maar denk je echt dat AMD letterlijk elke game test om te zien of er verschil is? verschil kan er altijd zijn, of het nu aangegeven is door AMD of niet
RTX 2070;3;0.4019700288772583;Begrijp me niet verkeerd, ik zou zelf ook altijd met de nieuwst beschikbare driver testen en wel wat meer tijd inruimen voor bijwerken van de 'testbank' om het zo maar even te zeggen. Ik begrijp echter ook dat men dit niet doet, dat is wat ik wilde aanstippen.
RTX 2070;2;0.3858013451099396;Geen idee wat jij met je PC doet maar het updaten van een AMD driver heeft bij mij nog nooit voor 'vervuiling' gezorgt. Tuurlijk blijft er wel eens een orphan file over maar dat is alleen een beetje ruimte op je disk. Ik moet bij elke nieuwe driver nieuwe benchmarks laten draaien. We deden dat eerst met dri machines die elke keer 'vers' waren, toen met een die we niet vers hielden en na een jaar zal het ons aan de reet oxideren want de AMD updates routines zijn tegenwoordig gewoon okay. Ik ben er 100% zeker van dat Tweakers ook geen vers OS installeert om de verschillen tussen drivers te zien. Als jij denkt dat dat de tests van Tweakers daarom onduidelijk zijn snap ik niet goed waarom je hier reageert.
RTX 2070;2;0.39758098125457764;De files alleen zijn niet het enige. Wat dacht je van het Windows-register bijvoorbeeld? Tools als DDU zijn er niet voor niets nog altijd.
RTX 2070;1;0.31596091389656067;In September testen met een driver versie van Juni? Bijzonder.. Overigens, Witcher inmiddels zonder hairworks getest?
RTX 2070;3;0.4685949683189392;Als deze kaarten straks echt voor 530 euro in de winkel te krijgen zijn en dat ook blijven is dit de eerste kaart in de RTX reeks die interessant is tov Pascal als je het hebt over prijs vs prestaties. De RTX 2080 Ti is wel interessant als je hele diepe zakken hebt en je wilt perse op 4K gamen met hoge framerates maar buiten dat is het geen interessante kaart. Deze RTX 2070 zou dat wel kunnen zijn.
RTX 2070;3;0.3366118371486664;Fixed Vol spanning wachten op voorraad. Enkel wachten we nu ook nog op RTX 2080 Ti voorraad. Eén ding is lijkt mij eenvoudig in te koppen: totdat er genoeg aanbod is zou ik geen prijsdrops verwachten.
RTX 2070;2;0.6264647841453552;Ik vind het geen fix. MSRP prijzen vertellen al genoeg tov de huidige prijzen van de Pascal kaarten. Veel goedkoper dan MSRP gaat het toch niet snel worden zeker niet met de huidige voorraad. Hoe dan ook ze blijven niet interessant. RTX wordt nog niet ondersteund even als DLSS wat de meerwaarde daar van is is onvoldoende bekend en tegen de tijd dat wel aardig wat games het ondersteunen komt de nieuwe generatie al weer in zicht en persoonlijk is die met betrekking tot RTX denk ik interessanter. Dit is een leuke introductie maar qua bruikbaarheid verwacht ik er maar weinig van. Als we het dan over normale performance hebben zonder DLSS en RTX bied alleen de 2080 Ti iets nieuws. En de 2080 is relatief duur. Dus lijkt alleen de 2070 op dit moment echt interessant worden tov de pascal kaarten. Want hoeveel mensen hebben het budget om een kaart van 530 euro te kopen? al niet extreem veel. Laat staan een GPU van 1200+ euro.
RTX 2070;3;0.6475993990898132;Beetje herhaling van menig andere discussie. Een snelle auto is ook duur maar dat maakt hem niet per sé slecht. Maar eens hoor, sterker nog zou ik denken dat 'de normale man' niet eens op een RTX2070 zit te wachten, maar blij is dat je momenteel een RX 570 kan kopen voor <200 euro met 3 toffe games erbij. Tot dusver is het een beetje de goed nieuws show voor mensen met veel centen, en wat je zegt: vooral de 2080 Ti dan. Ik durf echter wel in te zetten op het feit dat de 2080 de 1080 Ti snel genoeg van de kaart veegt. Is het niet inhoudelijk dan is het wel op basis van voorraad. En dat is niet per sé positief voor de consument.
RTX 2070;3;0.5670963525772095;Dat klopt maar afgelopen jaren gaat het wel hard met de GPU en CPU prijzen als je het beste van het beste wil. Er zijn gewoon veel segmenten bij gekomen. Bij Telefoons zie je hetzelfde gebeuren. Vind het wel jammer want dat betekend gewoon dat je de handdoek in de ring moet gooien en dan maar niet meer voor de top gaan maar voor een midrange of lower end kaart die wel betaalbaar is waar je voorheen wel the best of the best kocht.
RTX 2070;3;0.4410100281238556;Juist wel positief. Hoe rapper de 1000-serie verdwijnt hoe rapper de prijzen van de 2000-serie gaan zakken.
RTX 2070;3;0.2956133782863617;Volgens mij kunnen we dat mooi zien bij Battlefield 5. Komt binnenkort uit en doet RTX. Als de recensie daarvan de filmpjes ondersteunen, dan moet ik toch maar eens in de buidel tasten en een 2070 aanschaffen.
RTX 2070;1;0.33864614367485046;Dat wordt een duur spelletje dan, BF5.
RTX 2070;3;0.29733824729919434;Ja, want als dat goed werkt, dan komen er natuurlijk geen andere spellen meer uit met RTX?
RTX 2070;2;0.422835111618042;Wat ze in de trailer laten zien is te zwaar en ze gaan het in de uiteindelijke game flink terug schroeven het is maar de vraag wat je voor een FPS gaat halen. Shadow of the Tomb Raider draaide regelmatig onder de 60FPS op 1080p met een RTX 2080 Ti laat staan een RTX 2070.... Dus ik moet eerst zien hoe bruikbaar het is.
RTX 2070;5;0.3186407685279846;Zoals ik aangaf: als de recensies de filmpjes ondersteunen.
RTX 2070;2;0.41018247604370117;Dat zie je ook duidelijk bij de 1080 Ti's, blijkbaar is er meer dan vraag zat naar 1080/1080Ti's waardoor ze amper zakken qua prijs. Feit dat de 2000 kaarten niet/nauwelijks leverbaar zijn en de bizar prijs zorgt ervoor dat een lager prijspunt verder weg is dan menigeen gehoopt had..... (ben blij dat ik mijn 1080 Ti direct bij launch gekocht heb, gebruik die kaart al 1,5 jaar nu, prima value for money!).
RTX 2070;2;0.44401729106903076;En ik vrees dat 1080 Ti's enkel lastiger te vinden gaan worden. Nu zie je die 'mooie' deals van ca 650-680 al nauwelijks meer. Ik weet het eerlijk gezegd ook niet, lastig om te stellen of je beter snel kan reageren of juist kan wachten. Maar wat je zegt, als je een Ti bij launch hebt gekocht zit je achteraf gezien enorm goed!
RTX 2070;3;0.341324120759964;Gaat nog wel wat komen Stephan
RTX 2070;1;0.4940774440765381;Argh! Ik heb een glazen bol nodig. Vroeg of laat gaat het op zijn in elk geval, dat durf ik nog wel te zeggen
RTX 2070;3;0.3908795714378357;Eigenlijk is de 2080 Ti meer een 1440p High FPS. Het is de eerste kaart die met de meeste games, zonder instellingen te down tunen, 144 fps op 1440p kan halen. Het is maar hoe je het bekijkt maar het is bij lange na niet een 4k high fps kaart imo.
RTX 2070;3;0.45642954111099243;Nja ze halen meer dan 60 fps. Sommige games 80-90 dat is niet super hoog maar al wel beduidend meer dan 60 dus veel soepeler als je gsync of freesync of vsync off hebt. 144fps 4k gaat nog even duren (1-2 generaties)
RTX 2070;1;0.42258381843566895;Voor degenen die mijn comment zullen lezen, er is op dit moment daadwerkelijk een 1080 ti goedkoper dan een 2070, kijk prijsvergelijker. De 1080ti (helaas wel de poep lelijke Gigabyte OC gaming in het wit, zonder backplate) is nu voor 649 te krijgen. Ik heb hem gekocht bij de desbetreffende winkel en heb nog 30 dagen tijd om te retourneren, mocht de 2070, wellicht een ultra OC versie van EVGA, redelijk geprijsd zijn en goed presteren. Zo niet, dan skip ik de 20xx series en zit ik het met deze 1080ti uit
RTX 2070;1;0.5420374870300293;Volgens mij is 649 nog steeds een hoop meer euro’s dan 529?
RTX 2070;2;0.45352575182914734;Comment was gisteravond geplaatst op het moment dat de goedkoopste 2070 nog 660,- was. Ik moet zeggen dat het mij meevalt dat er echt modellen zijn voor 529,-, ik had persoonlijk meer verwacht. De 2070 waar ik wellicht interesse in heb is de EVGA 2070 XC Ultra, maar deze is momenteel nog niet verkrijgbaar en bij EVGA is de prijs 609,-. Al met al komen de prijzen van de 'betere/high end' 2070 modellen aardig dicht in de buurt van de goedkoopste 1080 ti modellen, vaak zijn ze zelfs duurder. Zo is bij die webshop de ASUS 2070 Strix OC bijvoorbeeld 749,- en voor dat geld heb ik liever een 1080 ti.
RTX 2070;1;0.41006067395210266;LOL 600+ euro voor een budget kaartje wat een tijden joh. Ja ik zeik maar ik heb wel een 1080ti gekocht bij launch, net voor de mining Craze. Ben wel blij met die kaart, nog steeds een topper. Mooi om te zien dat nvidia zichzelf in de voet schiet - Onzinnige upgrades tov ouder kaarten - Slecht qua stroom verbruik - geen 7nm - RTX nog jong (dus ZINLOOS om op te gokken, mogelijk een dikke flop) - Achterlijke prijzen, alsof er 64GB geheugen in elke kaart zit. - Overal matige reviews, niemand prijst de kaarten aan - Verneuqing van de 1000reeks naam Tsja, goed bezig nvidia, NOT
RTX 2070;3;0.22687095403671265;Dit is geen budgetkaart, dit is voorlopig de instapper voor de 20xx-serie.. Een budget kaart is een kaart die minder dan 150 euro oid kost..
RTX 2070;2;0.3522651791572571;"Ik zie het wel vaker voorbij komen bij een aantal reviewers en ik heb er altijd mijn mond over gehouden, maar nu de redactie van tweakers dit taalgebruik gaat gebruiken, zeg ik er wat van. Als je het over zogenaamde ""straatprijs"" hebt, dan heb je het over goederen, met name illegale producten, die je op straat koopt, b.v. drugs zoals cocaïne, of bij Mr. Robot gekochte hardware. Computerhardware koop ik nooit op straat, maar in een winkel of webwinkel, dus dan wordt dat (web)winkelprijzen genoemd. Aangezien de Introductieprijs enige jaren al niet meer overeenkomt met de werkelijke prijzen, zou je het over richtprijs of adviesprijs kunnen hebben."
RTX 2070;1;0.4527992606163025;Straatprijs heeft niets met illegaal of niet te maken. Straatprijs is de prijs waarvoor je iets daadwerkelijk in de winkel(straat) kunt krijgen in plaats van de adviesprijs die zeker bij videokaarten op dit moment geen enkele relatie meer heeft met de daadwerkelijke winkelprijs..
RTX 2070;1;0.715073823928833;Dat heb je echt verkeerd begrepen. Winkelprijs: prijs waarvoor goederen in een winkel verkocht worden Straatprijs: prijs waarvoor goederen op straat verkocht worden
RTX 2070;1;0.6598803997039795;Geen enkele site voert een OC test uit. Is dat omdat Nvidia dat niet wil?
RTX 2070;1;0.41304704546928406;"Overklokken is net zoals bij cpu’s een silicon lottery; de ene kaart klokt goed over terwijl een andere kaart van hetzelfde type veel minder goed reageert op overklokken. Daarom zegt een benchmark bij overklokken alleen iets over die ene individuele kaart en niet iets over alle kaarten van dat type. Daarom zinloos om te doen in een review waarbij je wilt laten zien hoe een bepaald type kaart presteert. De buren hebben overigens al een overkloktest van een 2080Ti gepubliceert dus je suggestie dat nVidia dat zou verbieden mag ook de prullenbak in."
RTX 2070;2;0.47480833530426025;Het valt mij op dat zowel Tweakers als HWI enkel op gebied van prestaties de verschillende generaties vergelijken, en nergens het argument raytracing aan te voeren. Voor mij zou dat een hele goede reden zijn om een 2070 te verkiezen boven een 1080 of zelfs een 1080ti.
RTX 2070;4;0.3057903051376343;Niet iedereen vindt een gimmick een goede reden.
RTX 2070;2;0.49376893043518066;Tenzij er een wonder gebeurt, is raytracing geen reden om de 2070 te kopen. De 2080ti heeft al grote moeite om raytracing toe te passen op 1080p, laat staan een 2070 met significant zwakkere hardware. --- Een reden de 2070 te kopen, is dat deze bij ongeveer gelijke prijs (november 20180) iets beter presteert dan de 1080 en met de nieuwe DLSS anti aliasing techniek, de voorsprong in de toekomst nog wat verder kan uitbreiden. Echter, voor velen (1080p gamers bijv.) zullen deze redenen voor aanschaf enkel gelden, als ze een 1060 of zwakker hebben, anders is de upgrade gewoonweg te klein.
RTX 2070;2;0.5734272003173828;Vind de prestatie jump van de 1070 naar de 2070 erg mager. De jump van de toen erg sterke 970gtx naar de 1070gtx was vele malen groter en kreeg je geforce 980ti prestaties voor de rond de 500 euro en dat was tijdens bet begin van de mining boom. Nu krijg je amper betere prestaties voor datzelfde bedrag. Nog maar een generatie wachten vrees ik.
RTX 2070;3;0.4835197925567627;Wat vind jij er mager aan? Persoonlijk vind ik die gemiddeld genomen 33% verbetering niet verkeerd en zeker vergelijkbaar met de 34% verbetering tussen de 970gtx en 1070gtx (ik weet, niet 1 op 1 helemaal vergelijkbaar, maar toch)
RTX 2070;2;0.40460383892059326;Ben neit zo fan van user benchmark. is totaal niet te verifieren en de games zijn ook niet bepaald benchmark waardig. De Geforce 970 was net wat sneller dan de 780TI De Geforce 1070 was net wat sneller dan de 980TI Maar nu is de 2070 maar net aan wat sneller dan de 1080 wat een klasse lager is. Dat dan wel met een meerprijs. De 1070 had een RRP van 450 bij launch en had je al met een deftige koeler. De 2070 is er voor rond de 600 euro voor de echt goed uitvoeringen en alleen de plastic fantastic baords voor rond de 525 en de 575.
RTX 2070;1;0.31244784593582153;Op dit moment staan er 3 RTX 2070 in de pricewatch op 529 euro, bij 4 verschillende winkels:
RTX 2070;1;0.4572656452655792;Bij alternate heb ik hem kunnen kopen voor 529 vanmorgen nu is hij uitverkocht en staat nu op 579,-
RTX 2070;2;0.5213322043418884;Sorry maar op budget vind ik een beetje vreemde titel voor een kaart van meer van € 500, het feit dat het iets voordeliger is dan de nog duurdere 2080 neemt dat niet weg. Is niet heel aantrekkelijk te upgraden de afgelopen 1-2 jaar, eerste de ram prijzen die gigantisch gestegen zijn, nu de cpu prijzen die door het dolle gaan, en de gpu's worden ook elk jaar flink duurder.
RTX 2070;2;0.5126436948776245;Blijft erg lastig om hier vooralsnog ook maar iets zinnigs over te concluderen omdat we ten eerste niet weten wat voor prijzen de AiB's op de 2070 gaan plakken (MSRP negeert iedereen) en we weten nog steeds niet wat voor RTX en DLSS performance we kunnen verwachten.
RTX 2070;2;0.6395941972732544;Voor mij is het toch duidelijk dat ik deze reeks oversla, en ga wachten tot volgende reeks. Voor het handvol games dat nu gebruik maken van raytracing vind ik het persoonlijk allemaal niet waard. Daarboven nog eens de dure prijs en de verlaging van performance per upgrade (vroeger had je minstens TI-niveau bij upgrade).
RTX 2070;5;0.4787859618663788;Wat een geld. Tuurlijk, het is een prachtproduct, maar de GPU-prijzen zijn gigantisch. Mede daarom blijf ik voorlopig bij mijn GTX970.
RTX 2070;3;0.5343418717384338;een gtx 1070ti is redelijke price -performance kaart, ongeveer even snel als de 2070 maar een stuk goedkoper.
RTX 2070;2;0.36474597454071045;Hoeveel AA is er toegepast op elke game? Want nu weet ik nog niets of er AA is toegepast. Ik ben aan het kijken voor een upgrade van mn GTX780, maar ik game op mn tv 1080p/60, dus render ik games momenteel op 2K of als het kan, 4K (oudere games) en is AA niet meer nodig, want dit is eigenlijk SS. Maar als games die amper 60halen met de RTX2070 op 4K en AA aan heeft, zou een groot verschil maken voor m'n keuze.
RTX 2070;3;0.4498765468597412;Het zou vreemd zijn als de RTX2070 voor 529 weg zou gaan, dan moet de GTX1080 voor veel minder weg. Waarom zou je immers nog een 1080 kopen voor hetzelfde geld? Of we krijgen een daling van de 1080 te zien, wat de 1080 wel weer een interessante kaart maakt voor de top van het midden segment.
RTX 2070;3;0.5064564943313599;Voor mij voelt Ray tracing heel erg aan als het begin van 4k-beeldschermen. Het bestaat, het is iets wat oprecht games beter kan maken, alleen het is nog zo jong. Als het aan mijn ligt wacht ik liever een paar generaties. Pas als er een kaart uitkomt die 1080p 144hz Ray tracing kan doen denk ik dat de markt enigzins ontwikkeld genoeg is voor een overweging
RTX 2070;1;0.33964186906814575;"tl;dr: Koop een 1080 Ti ipv een 2070."
RTX 2070;1;0.5269022583961487;Please correct me if I'm wrong: wat ik voornamelijk opmaak uit deze test opmaak is dat de 1070, voor huidige spellen op de markt, echt een prima optie is voor ultra settings op 1440. En dat die kaart uit 2016 (jaar oud!) nog steeds slechts 200 euro goedkoper is, dan de nieuwste chip. Het is een bizarre markt geworden zeg..
RTX 2070;2;0.4789584279060364;Waarom zou je uberhaupt een Founders Edition van de nieuwe kaarten willen kopen? Ik heb dat nooit gesnapt. Ze zijn duurder, ze draaien op een lagere clock-snelheid, zijn qua prestaties minder en hebben vaak minder goede koeling dan de aib-kaarten die ietsje later volgen.
RTX 2070;1;0.39253896474838257;M.a.w wacht op een deal en koop een 1080ti, als je al 600+ euro moet uitgeven ...
RTX 2070;1;0.4652768671512604;en dan voor dat geld langzamer gaan nee dankje
RTX 2070;3;0.2196464091539383;De eerste RTX 2070 met een prijs van 529 euro staat op Pricewatch: pricewatch: Gigabyte GeForce RTX 2070 Windforce
RTX 2070;2;0.3881213963031769;Ik lees in hier: dat NVLink geen een optie gaat zijn op de RTX2070 van Nvidia en op de custom kaarten Jammer, aangezien NVLink op de 2080's redelijk goed schaalt! Nu een 2070 kopen en er over een jaar of meer er nog een 2070 bij kunnen kopen zou wel een fijne optie zijn geweest
RTX 2070;2;0.3933176100254059;Ik zit nog op een GTX 550 Ti, wil dus wel gaan upgraden. Ik zit nu echt te dubben tussen 1070 ti of RTX 2070. En in die range vallen ook nog een aantal GTX 1080's. Monitor is nog een Syncmaster 2443 BW, cpu i5 4430 op Z97 bord. Is een 2070 dan een aankoop die futureproof is voor een volgend systeem tzt of is het slimmer het bij een GTX 1070ti te houden?
RTX 2070;1;0.39926716685295105;Als de prijs van een 1070ti niet snel naar 2-300 euro gaat kan je voor die 100 euro meerprijs veel beter die 2070 kopen ..
RTX 2070;1;0.36008429527282715;Met andere woorden, deze jongen wacht nog ff. Er zijn nu nog geen eens games die Raytracing ondersteunen..ik blijft rustig verder gamen op mn GTX 1070.
RTX 2070;2;0.3930698037147522;Wanneer kunnen we de eerstvolgende nieuwe kaarten van Nvidia verwachten? Komen ze ieder jaar rond dezelfde tijd met nieuwe kaarten? Of is er geen jaarlijkse cyclus? Wil ergens in de nabije toekomst een nieuwe build maken, maar neem liever niet de eerste generatie van een nieuwe architectuur.
RTX 2070;3;0.34466734528541565;Hi Koen, De GTX 1000 serie was uitgekomen op 27 Mei 2016. De RTX 2000 serie is uitgekomen op 20 september 2018. Het zal dus heel lang duren voor de RTX serie/architecture zal worden vervangen met iets nieuws.
RTX 2070;3;0.4485216438770294;Aah. Dan ga ik alleen even afwachten hoeveel mooier RayTracing in games gaat worden en hoe goed de RTX kaarten presteren met 'ray-tracing on'. Dank!
RTX 2070;3;0.5908889770507812;Er zullen wel wat games gaan komen met ray tracing ondersteuning zoals Battlefield 5, de aankomende Metro game en Shadow of the Tombraider. Verwacht maar dat ray tracing behoorlijk veel performance kost, zodat eigenlijk alleen de 2080Ti hiervoor de geschikte kaart is. Je wil namelijk ook nog een knappe resolutie hebben en niet op 1080p willen game. Mocht ray tracing echt de next best thing worden, dan is de volgende generatie waarschijnlijk pas echt krachtig genoeg daarvoor. Voor alle overige games zou ik niet weten waarom je de eerste generatie (20xx reeks) van een nieuwe architectuur niet zou moeten nemen. De prijs is wellicht wat heftig
RTX 2070;1;0.4719502925872803;Ik heb anders gewoon nog een 1080p scherm als je 4k wilt gamen op hoogste settings moet je elke 2 jaar een nieuwe video kaart kopen, daar heb ik niet zo'n zin in en zo'n groot verschil zie ik niet tussen 4k en 1080p. Daarom wil ik nu ook niet de hoofdprijs betalen voor zo'n gruwelijke kaart omdat ik het toch niet volledig uitnut. Misschien is de 2070 wat voor mij als dat gemakkelijk 1080P aan kan + raytracing.
RTX 2070;1;0.7855606079101562;De conclusie is vreemd opgeschreven jullie hebben het over de 1080, 1080 TI, 2070 adviesprijzen en echte prijzen en halen volgens mij zelf de boel door elkaar. Misschien nog even naar kijken.
RTX 2070;3;0.4672519564628601;Het is ook een beetje lastig op het moment, omdat er nog geen straatprijzen van de RTX 2070 zijn. Bij introductie was het als volgt: RTX 2070 639 euro GTX 1080 657 euro GTX 1080 Ti 825 euro Momenteel liggen de straatprijzen als volgt: RTX 2070 vanaf 529 euro? (momtenteel 660-900 euro) GTX 1080 vanaf 500 tot ~650 euro GTX 1080 Ti vanaf ~750 - ~900 euro Weet je wat? Ik zet wel even een tabelletje erbij om het duidelijker te maken
RTX 2070;1;0.45428457856178284;Een RTX 2070 heeft een MSRP van 499 dollar, omgerekend met btw is dat net iets meer dan 521 euro. De 519 euro die nVidia noemt als MSRP voor Nederland klopt dus prima met de omgerekende prijs. Daarnaast zit ook Alternate met de 529 euro hier heel dicht bij in de buurt. Het probleem is alleen dat de RTX 2080 op 599 dollar zit, omgerekend dus 625 euro en nog wat. Kijken we dan in de pricewatch dan zien we dat de prijs, gewoon 200 euro/30% hoger ligt en Alternate is de goedkoopste. Gaan we dat dus doortrekken naar de RTX 2070 dan is 719 euro (200 euro meer) of 675 euro (30% meer) waarschijnlijk een betere gok. Of gewoon de prijzen aanhouden die er werkelijk zijn, dus tussen 660 euro en 967 euro. Edit: @willemdemoor Waar zijn die RTX 2070 voor 529 euro bij Alternate? Het is nu een dag later en er de snelst leverbare RTX 2070 is 929 euro. Edit 2: @willemdemoor Die van 929 euro hierboven is ook niet meer snel leverbaar, maar nu staat er één van 649 euro en 689 euro, leverbaar vanaf morgen. Daarmee lijkt de prijs van 529 euro dus zoals verwacht gewoon onzin.
RTX 2070;1;0.28225913643836975;Dit is idd het meest waarschijnlijke qua prijs
RTX 2070;2;0.5530139803886414;"Maar op basis van wat er NU bekend is, is de 2070 duurder dan de 1080 Ti. Of dat blijvend is zal de tijd leren maar ik vind het, om eerlijk te zijn, dan onjuist om op DIT moment te concluderen dat de 1080 Ti veel duurder is. Met de nadruk ook nog op veel. EDIT Nergens staat trouwens ""veel"" dat lijk ik verkeerd gezien te hebben en bij de conclussie staat het overigens beter dan het onderdeel daarvoor."
RTX 2070;1;0.31110525131225586;koop je die dingen op straat? dan had ik het goed koper verwacht
RTX 2070;1;0.43261128664016724;Was beide jaren gebruiker van Intel & Nvidia. Gauw beide malafide bedrijven boycotten, tijd voor AMD met ware passie! Zonder AMD waren we nog in de steen tijd en hadden we duo core en i9 quad core, met maandelijkse driver degrade voor de gpu. Tijdens launch omhoogkrikken en minimale dailing der loop der jaren tot nieuw product. Intel is de grootste Boris boef!
RTX 2070;1;0.44844555854797363;Waarom heeft AMD ware passie? Omdat ze een underdog zijn t.o.v. de echt grote jongens? AMD is compleet zoek gespeeld op zowel de CPU als GPU markt. In het verleden kon ATi het nog winnen van het veel grotere nVidia, maar sinds AMD dat heeft overgenomen zijn de gpu's van AMD niet veel meer. Het is nog dat ze via de consolemarkt veel gpu's kunnen slijten.....
RTX 2070;3;0.26587459444999695;Ik hou het nog wel even op mijn gtx 1070 tot de prijzen zakken.
RTX 2070;1;0.491431325674057;Ging in november bij men verjaardag, valt op black friday dit jaar voor een Gtx 1070ti of een 1080 gaan, zag nu net: World of warcraft Bfa Dx12 2560 x 1440p 4xmssa, settings 10. Gtx 1080: 78.4 fps Vega 64: 71.1fps Rtx 2070 FE: 80.9 fps Denk dat men keuze nu wel gemaakt is. Evga Rtx 2070 is al voor €519 te krijgen
RTX 2070;2;0.4596269428730011;Verbazend dat de prijzen voor de RTX2070 modellen zo ver uit elkaar lopen. Ik tel momenteel vier modellen met een prijs van €529, maar voor de luxere modellen komt daar tot wel 300 euro bovenop. Okay, daar krijg je dan waarschijnlijk een flinke overklok en relatief superieure koeling voor terug, maar toch... Om verder maar te zwijgen over de nog belachelijkere prijzen van boven de 800 euro die sommige webshops in deze introductiefase -wegens schaarste- durven te vragen voor de RTX2070....Voor dat bedrag koop je tenslotte al een RTX2080.
RTX 2070;1;0.48182931542396545;Hoe overclockbaar zijn de RTX 2070 kaarten? Is daar al iets over bekend?
RTX 2070;1;0.47718632221221924;Huppetaa, een eind onder de 500 euro! En nu niet meer,,,,
RTX 2070;3;0.3329658508300781;Tweakers “Nvidia GeForce RTX 2070 Review” query betreffende vergelijk GeForce RTX 2070 videokaarten: categorie: Videokaarten Waarom zijn de Asus GeForce RTX 2070 videokaarten (relatief) duurder dan de overige merken?
RTX 2060;2;0.41265133023262024;"Ik heb net even m'n factuur van m'n Palit GTX 1060 6GB Super Jetstream erbij gepakt. Daar was ik bijna precies 2 jaar geleden €330 aan kwijt; niet de goedkoopste after-cooler, niet de goedkoopste webshop maar wel een indicatie van wat custom-cooler GTX 1060's deden...zeg €300. De voorganger (MSI GTX 760 Gaming 2GB) was €240 eind 2013. Onder garantie is die kaart eind 2015 omgeruild voor de MSI GTX 960 Gaming 4 GB die destijds een vergelijkbaar prijskaartje had (paar tientjes hoger wellicht). Van een €240 eind 2013 via een €250 eind 2015 en een €300 eind 2016 nu dus naar (waarschijnlijk) de €350-400 range (wat speculatie want of de echte prijzen van de custom coolers hoger of lager dan de reference uitvallen is altijd maar de vraag...de goedkoopste modellen zullen lager uitvallen, maar erboven zal ook gebeuren. Helemaal onverwacht is dit prijspunt wat mij betreft dan ook niet; de gaten tussen de xx50 en xx60 en dan naar de xx70 zijn altijd al wat scheef geweest; xx50 naar xx60 klein verschil, xx60 naar xx70 een behoorlijk groot verschil. Het lijkt erop dat de prijs van de xx60 meer naar het midden tussen de xx50 en xx70 verschoven wordt. Of het het waard is? Daar ben ik nog niet helemaal uit voor mezelf. Ik zit op een 1440p 60 Hz monitor. De 1060 die ik nu heb kan dat in de spellen die ik speel doorgaans net niet helemaal hebben op ultra, dus wat kleine tweaks down op de settings voor de 60 FPS. De 1070 kan de 1440p wel op 60 FPS houden, maar kostte destijds dan ook ruim 50% meer. Wat de echte prijzen gaan zijn wordt wel doorslaggevend. Daarnaast hebben we ook nog de geruchten rondom een GTX 1160. Als dat inderdaad een 2060 zonder raytracing blijkt te zijn, dan is dat wellicht de meest interessante kaart van deze generatie omdat die wel eens ruim 60 FPS op 1440p ultra tegen een lagere prijs dan de 2060 kunnen zijn."
RTX 2060;2;0.42304548621177673;Hmm, ik ben in dubio of dit eerlijk is. Ok, 1070 Ti was toen duurder bij launch, maar we hebben het erover wat we -nu- willen kopen. Gelijkwaardige kaarten op dit moment dus, dus kijk je NU hoe duur/goedkoop ze zijn. En nee, de 2060 is niet meer future proof want de de RT-features werken al amper goed op de hoger geplaatste kaarten en er zijn amper games voor.
RTX 2060;3;0.33939898014068604;Een RTX 2060 is een nieuwere GPU en daardoor wel degelijk hoogstwaarschijnlijk iets meer futureproof dan een GTX 1070Ti.. Voor dezelfde prestaties en kosten zou ik altijd de nieuwere GPU kopen, een RTX 2060 dus in dit geval..
RTX 2060;2;0.26424822211265564;Ik blijf het triest vinden dat ik 4 jaar geleden voor €360 een GTX970 kon kopen, waar alleen de 980 beter was(Ti versie was nog niet uit) en nu kan je van datzelfde geld net de budgetkaart kopen. Ik hoop echt dat amd met navi wat leuke betaalbare kaarten uitbrengt.
RTX 2060;2;0.4312511682510376;De 2060 is echter geen budgetkaart meer, dit ding trekt alle nieuwe titels op 1440p/ultra ruim boven de 60 FPS, enkel 4K/ultra is te hoog gegrepen, wat dat betreft is de performance vrij goed te vergelijken met waar de 970 destijds op gemikt werd, daar was 4K ook te doen als je bereid was de settings wat te laten zakken. de 960 was (in ieder geval in mijn optiek, toen ik em kocht) echt een 1080p kaart, en was ultra ook vrijwel zeker een no-go bij nieuwe titels De 3gb 1060 begint weer te zakken naar een prijs waar nvidia weer een fatsoenlijk budgetkaart heeft, maar dan is een RX570 nog steeds de betere koop, de 1050Ti is gewoon nog steeds te duur
RTX 2060;1;0.43114739656448364;Hoezo is dit geen budget kaart? dit is toch de goedkoopste instap kaart in de RTX line up of zie ik iets over het hoofd? prestaties staan daar los van, tuurlijk kon de GTX970 geen 1440p op ultra aan, de tijd heeft niet stil gestaan, we zijn inmiddels 4 jaar verder.
RTX 2060;2;0.49271172285079956;"Als veel mensen die in de markt zijn voor een budget kaart deze kaart niet kopen omdat ze die te duur vinden, dan is het geen budget kaart. Hun budget gaat niet zomaar omhoog omdat Nvidia die kaart duurder heeft gemaakt. De prijs van deze kaart is aanzienlijk minder gunstig dan die van budget kaarten 5 tot 10 jaar geleden. En nee, het is niet zo dat betere hardware altijd al zo veel duurder is geweest. De relatief hoge prijs heeft twee verklaringen: qua ontwikkelingen begint de rek er een beetje uit te raken (einde wet v Moore), en Nvidia heeft een defacto monopolie ; een reëel monopolie op de snelste consumer kaarten, en veel betere naamsbekendheid in het lage- en midden segment."
RTX 2060;2;0.3010445237159729;bekijk het eens anders. de rtx 2060 is een midrange kaart zoals de gtx970 ooit was, gelijkaardige prijs zeker als je inflatie meerekend. en de rtx 2060 is in moderne games op 1440p ( volgende stap na fullhd ) echt wel krachtig genoeg om alle a - titels vlot te draaien. relatief gezien is de rtx 2060 zelf krachtiger dan de gtx 970 was 4jaar geleden aan een gelijkaardige prijs met gelijkaardige a - titels op 1080p. nieuw segment wat er anders is dan 4jaar geleden is dat er nu 4k gpu [UNK] s zijn die 4x de pixels aansturen tov 1080p. factor 4 heeft geresulteerd in giga - chips met veel cores en transtors en toen werd de gtx1080, 1080ti en de titan geboren. veel te snel voor 1080p en 1440p, overkill zelfs. 4jaar geleden bestond die mate van overkill niet. er is dus naast de bestaande high - end line up een ultra high end line up gekomen. x2. midrange on top als je de rtx 2060 benchmarks erbij neemt zie je dat bf1 op 1040p ultra kan gespeeld worden op 102fps gemiddeld. 4jaar geleden kon de gtx970 helemaal geen 100fps halen op battlefield 4 ultra op 1080p. relatief gezien zijn we er dus zeker op vooruit gegaan en dat is wat telt. nieuwe eend en dan heb je nog [UNK] real time ray tracing [UNK]. na 4k weer een totaal nieuwe evolutie in gaming. wie had dit ooit gedacht? raytracing was toch voor renders only? nooit gedacht als je 4jaar geleden ( of zelfs vorig jaar ) had gezegd dat je met een midrange kaart a - titels op 4k kon draaien en met gemak real time kon ray tracen op 1440p voor een gtx970 budget had iedereen direct getekend of u voor gek verklaard. conclusie midrange kaarten zijn ano 2019 nog zeer relevant als je van games wil genieten aan een gelijkaardige prijs als 4jaar geleden. geen nood om uw midrange budget te verhogen.
RTX 2060;2;0.4508090913295746;Ik ben het gedeeltelijk met je eens toch voelt het niet zo en dat is de schuld van hoe men de kaarten positioneert. Men wil het liefst de kaart met de hoogst mogelijke cijfers, omdat men er vanuit gaat dat deze beter zijn. Voor mensen die niet de top tier kaarten kunnen aanschaffen was er een xx70 lijn. Die een hele goede prijs/kwaliteit verhouding had. De prijzen van de kaarten zijn veel harder gestegen dan de inflatie, ik heb eind 2014 een Asus GTX970OC aangeschaft voor €369,-. Een vergelijkbare kaart uit de nieuwe serie kost op dit moment €629,- Ik weet niet wat jou inflatie is, maar ik wou dat mijn salaris met hetzelfde percentage was gestegen de afgelopen 4 jaar Het mag dan wel Real time Raytracing genoemd worden, toch worden er allerlei trucjes gebruikt om er voor te zorgen dat men niet zoveel rekenkracht nodig heeft. Bekijk eens de technische filmpjes over Raytracing en hoe men bij BF5 ervoor bezorgt heeft dat men het real time aan het werk krijgt. Nog steeds een hele knappe prestatie, begrijp me niet verkeerd. Maar de nauwkeurigheid is niet te vergelijken met die van renders.
RTX 2060;2;0.4886559545993805;Je mist de insteek van men post denk ik. Ik vergelijk niet de benaming van Nvidia, die is immers vrij te kiezen, en bijgevolg geen goed eikpunt. Zo beland je on ermijdelijk in een nutteloze semantische discussie men gaat morren over wat nu een midrangekaart is en wat niet. Zinloos. Ik vegrelijk de 970 met de 2060 omdat die wat prijs betreft gelijk was met de 2060 van nu . Als je naar A-titels van vandaag kijkt zie je dat de 2060 best goed presteerd en gebruik huidige A-titels met hogere standaardresolutie en dan pure fps als eikpunt. Maar goed.. ik dacht dat ik dat deel goed had beschreven in mijn vorige post? Het klopt dat Raytracing nog niet top is, het lijkt enkel te werken op reflecterende delen en het voelt wat gemaakt aan maar dat zal wel iets beter worden met de jaren. Bf1 is natuurlijk de eerste implementatie. Wat vast staat is dat ray tracing the next big thing is in consoleland en dat de echte RT evolutie pas echt gaat boomen als consoles dit ook gaan ondersteunen.
RTX 2060;2;0.4763741195201874;De 970 neerzetten als voorloper vd 1060 gaat wat mij betreft niet op. Dat we er op vooruit gaan is niet bijzonder, het een gegeven bij opeenvolgende generaties hardware. Wat overblijft is hoeveel meer het kost, ook al lijk je te suggereren dat dat niet uitmaakt. De 1060 kostte ongeveer evenveel als de 960, maar de 2060 kost aanzienlijk meer dan de 1060.
RTX 2060;2;0.33430016040802;Ik vind dat hij toch wel gelijk heeft. Dit zijn geen budget kaarten en zijn zo ook niet bedoeld, dit is gewoon een reeks voor de toch al wat meer hardcore gamer. Net zoals OLED TV's er ook zijn voor de meer veeleisende thuiskijker. De 10xx reeks is de nieuwe budget reeks, vele van deze kaarten zijn meer dan goed genoeg voor de doorsnee casual gamer en zullen dat ook nog jaren zijn.
RTX 2060;1;0.3546164035797119;Anderen argumenteren juist dat het wel een budget kaart is, in reactie op anderen en ik die zeggen dat het geen budget kaart is...
RTX 2060;1;0.5169050097465515;Eo zo geraakt de cirkel rond
RTX 2060;2;0.2970399856567383;En je vindt dat 'hij' ongelijk heeft en dat ik gelijk heb.
RTX 2060;1;0.4964476525783539;Lees eens opnieuw: Ik schrijf niet over de 1060, wel over de 970 en de 2060 die relatief ongeveer evenveel kosten. Hoe jii heel de tijd die 1060 erbij haalt weet ik niet want ik of niemand anders maakt die vergelijking dus je kan moeilijk iemand verwijten een verkeerde vergelijking te maken want ze is nooit gemaakt
RTX 2060;2;0.42083561420440674;Ik haal de 1060 (een budget kaart) er bij omdat die de voorloper is van de 2060 (zogenaamd ook een budget kaart, maar dan zonder budget prijs), en de opvolger is van de 960 (ook een budget kaart, met budget prijs) maar niet de opvolger is vd 970. Mij is niet duidelijk waarom jij de 970 er bij haalt.
RTX 2060;1;0.6010194420814514;Puur prijs niet typenummer, niemand koopt toch een kaart opmdat het de ‘opvolger’ is? Toch omdat de price range gelijk is en kikt dan wat je er voor terug krijgt of andersom. Ik spreek me niet uit over de 1060, enkel over de 970 en 2060. Dat noem ik geen budget kaarten meer. De 1060 trouwens ook geen budgetkaart, budget is eerder €100 €200 nieuwprijs bij lancering.
RTX 2060;1;0.6884532570838928;GTX 260/460/560/660/760/960/1060 en de laatste is dus de 2060. Al die xx60 kaarten zitten rond de 199/249 dollar/euro en de nieuwe xx60 is plots €370!
RTX 2060;3;0.4307864010334015;Ja, maar je vergelijkt GTX 1op1 met RTX. Appelen met peren vergelijken. De 1160 GTX is nog niet uit (komt er aan) en die zal ook in die prijsklasse vallen. Dus mensen die rayetracing niet wensen kunnen binnenkort rond de €270 een full HD gpu kopen.
RTX 2060;1;0.6009423136711121;1160 is een gerucht dus is helemaal niet zeker of die wel komt. En als zo’n kaart wel komt is dat een grootte blunder van Nvidia, daarmee word de RTX 2060 in 1 klap niet meer relevant. Maar zo te lezen trappen er een hoop mensen in de marketing, dat is ook de rede waarom bedrijven daar erg veel geld in stoppen. De RTX naamgeving is 100% marketing, de 2060 is gewoon de opvolger van de 1060.
RTX 2060;1;0.35494378209114075;Waarom zou de RTX 2060 plots irelevant worden? Die kaart presteerd op gtx1070-1080 niveau. Die stap is historisch gezien erg groot. Ik luister niet naat marketing, ik kijk gewoon naar de facts. En uit alles blijkt dat er ruimte is voor een gtx versie die onder de rtx valt en die perfect de opvolger kan zijn van de gtx 1060 wat betreft prijs, naming en prestaties.
RTX 2060;2;0.4428846836090088;Je vergeet nog één verklaring: de cryptocurrency mining hype. Die galmt nog steeds flink na wat je merkt aan de (nog steeds) te hoge prijzen.
RTX 2060;1;0.6540393233299255;Goedkoopste betekent niet budget....de goedkoopste Lamborghini is nog altijd geen budget car
RTX 2060;1;0.43501272797584534;Sinds wanneer vergelijken we Nvidia met een Lamborghini? Tot op de dag van vandaag kon je elke generatie wel een betaalbare kaart kopen uit de nieuwe line up, nu word je al in de opmerking doorverwezen naar 1050 Ti als ''budget'' wat natuurlijk inmiddels gewoon oud prul is en niks meer te maken heeft met budget/instap.
RTX 2060;2;0.45303475856781006;Omdat de gehele lineup/generatie nooit compleet is vanaf het eerste moment. Vanuit Nvidia gezien, is de -60 altijd mainstream, daarboven enthousiast en daaronder budget, met misschien wat crossover ertussen. Het is geen budget kaart vanwege de modelnaam en de plaatsing , Nvidia vindt dit een mainstream kaart (en voor mainstream is hij best aan de dure kant, dat was de gtx1060 6gb eigenlijk ook al ). Ook al zijn ze er nu nog niet, er komen echt wel goedkopere Turing kaarten . Of die raytracing hebben of niet is de vraag, maar dat zijn de budget kaarten van deze generatie , de 2050 en 2030 (of het nu een GTX of RTX wordt , of een 2050 of een 1150 of zo, ..). Dus de 2060 is op dit moment de goedkoopste raytracing kaart, maar hij is absoluut niet als budget kaart gepositioneerd.
RTX 2060;2;0.5375385284423828;Het blijft bijzonder dat de een niets met RTX te maken wil hebben terwijl de ander de 1050 en 1060 al oud prul noemt. Mij lijkt een RTX feature die je relatief tot de frame rate verlaging niets nuttigs oplevert op een lager performance level helaas een luxe die er gewoon nog even niet bij past binnen een 'normaal' budget. NVidia heeft daar dan op dit moment misschien domweg nog niets te bieden wat echt die stap beter is...
RTX 2060;2;0.502269446849823;Niets nuttigs? Belichting is zo'n beetje het belangrijkste bij graphics. Ik heb liever raytracing dan 4k(aangenomen dat ik één van beide moet kiezen voor 60fps), maar dat is persoonlijk...
RTX 2060;2;0.498412549495697;Ik bedoelde vooral dat je met een theoretische RTX2050 zo weinig rauwe performance hebt dat je met ray tracing actief zelfs op 1080p al geen 60 fps meer haalt. De RTX2060 haalt een 60fps minimum framerate al niet met medium ray tracing actief in Battlefield 5. Dan moet je dus wel op je kwaliteit inleveren om toch aan 60 fps te komen en ray tracing zal dan al snel het eerste zijn dat je moet gaan uitzetten. RTX hardware maakt de chips ook groter, complexer, duurder. Ik zie vanuit NVidia dan ook nog weinig heil in een RTX2050 of lager, Battlefield 5 ray tracing reviews gaan die niet helpen verkopen met te lage frame rates. Zal in de toekomst met 7nm straks hopelijk voor een RTX3050 of RTX4050 wel kunnen.
RTX 2060;3;0.3390892446041107;"We vergelijken de RTX lijn met lamborghini (of althans, @yvez doet dat), Nvidia heeft natuurlijk goedkoper spul, maar al die ""leuke"" RTX features zijn het premium gedeelte, tuurlijk zie je dat in de eerste generatie nog niet zomaar terug op een budget kaartje"
RTX 2060;1;0.3489512503147125;Hoezo zou de RTX serie dan een budget kaart moeten hebben? Was dan tot vandaag de 2070 de budgetkaart?
RTX 2060;1;0.39800649881362915;Nee want het was allang bekend dat RTX 2060 nog zou worden uitgebracht als instapmodel.
RTX 2060;2;0.444095641374588;"Dat de 2060 een instapmodel en/of budgetmodel zou zijn is verder nergens terug te vinden. Het is typische zo'n aanname die een eigen leven gaat leiden op de techsites en fora; ook op Tweakers. Op dit moment is geen enkele 20xx kaart budget, als je budget wil dan is er de 1050Ti of de RX580. Prima budget kaarten. En anders moet je echt nog even wachten op een budgetmodel 20xx series. Als die er al komt, want er is 0,0 reden voor nVidia om die uit te brengen zo lang ze nog met bergen 10xx series stock zitten."
RTX 2060;1;0.41812005639076233;"nee dat weten ze goed te vermarkten, de RTX 2060 is de opvolger van de GTX1060, zoals die ook weer de opvolger was van de 960 enz enz... Die kaarten waren allemaal ""instap"" modellen om leuk te kunnen gamen, de enthousiast card. de kaart die betaalbare performance leverde tegen een compromis. Noem het budget of whatever, je weet donders goed dat dit een kaart zou moeten zijn voor enthousiaste gamer met een beperkt budget. en volgens mij voldoet deze prijsstelling niet aan die verwachting. Is het een verkeerde kaart? Nee zeker niet, maar ben het wel eens met de conclusie van de (vele) review(s). Oei... ik heb wat mensen op de zere teentjes getrapt met een -1 als reactie score?! Nvidia enthousiastelingen? aandelen?"
RTX 2060;3;0.4330955147743225;Niet van mij iig, want dit is wel een spotlightje waard
RTX 2060;2;0.528428316116333;Nogal logisch dat mensen dat denken als dat met de vorige generaties ook zo was, nvidia is gewoon slim met nummertjes gooien en de prijzen te verhogen Ondertussen heb je dus mensen die dat allemaal wel prima vinden, dus ja waarom zou nVidia het niet doen Mooi dat je trouwens als Budget de 1050ti en de 580 noemt, de 1050TI is echt slecht qua prijs prestatie en zal ik ook nooit als budget GPU aanraden. De 580 is echt vele malen sneller, aangezien de 570 al vele malen sneller is (en goedkoper)
RTX 2060;4;0.42376908659935;"Fair point, maar ik bedoelde vooral dat RTX gewoon geen budgetspul is momenteel, net als dat je bij Ferrari met de Portofino wel een instapper hebt, maar geen ""budget sportauto"" de RTX feature set is momenteel 100% enthousiastelingen grade spul, de budgetgamer komt nog steeds prima weg met een RX570/580/1060, en speelt lekker op 1080p zn spelletjes, en vaak genoeg op high/ultra ook."
RTX 2060;2;0.3931643068790436;Dat is dus het hele probleem. Je verwacht vooruitgang qua snelheid, juist zonder dat het er (veel) duurder op wordt.
RTX 2060;2;0.4242263734340668;"Die vooruitgang is er toch ook? Hoezeer ik ook een probleem heb met het idee van een ""x60 kaart voor €370"", is dit ook gewoon GTX1080 performance voor €370 Nvidia probeert gebruikers te upsellen door lagere typenummers hoger te trekken kwa prijs en performance (hier komen ze mee weg omdat AMD ook rare dingen doet kwa typenummers, en weinig concurrentie biedt in dit segment momenteel), traditioneel was de x60 pakweg gelijk aan de (x-1)70, dat de 1060 een 980 evenaarde, en nu de 2060 de 1080 aantikt betekent niet dat je daadwerkelijk meer hoeft te betalen voor adequate performance, alleen dat nivida probeert met getalletjesgegochel je meer te verkopen dan je nodig hebt."
RTX 2060;2;0.41364601254463196;Tuurlijk is er vooruitgang, zou een beetje te gek voor woorden zijn als die er niet was, of wordt dat tegenwoordig als een + gezien? Vergelijkbare performance van een kaart die over 2 maanden precies 2.5 jaar geleden is uitgebracht, nou joepie. Voor sommige games is een 1070 al recommended, leg je dan mooi bijna 400 euro van je zuurverdiende centen neer voor een videokaart die binnen nu en 2 jaar al is achterhaald. Like i said, toen ik mijn GTX 970 kocht was er maar 1 gamekaart beter en duurder en dat was de 980 die rond de 550 euro lag detstijds en nu 4 jaar later nog redelijk mee kan in sommige games. Nu koop je voordatzelfde geld een 2060 en heb je een 2070/2080/1080Ti/2080 Ti die stuk voor stuk allemaal beter zijn, IK vind dat dat gewoon bezopen.
RTX 2060;2;0.426670104265213;Sja, maar als je PC master race will zijn dan, zeker in het bovensegment dan weet je toch al lang dat je gemolken word door fabrikanten. Je had de 1080Ti nodig voor 4k 60hz en nu de 2080Ti voor 4k RayTrace 120hz en dan zometeen de 3080Ti voor 240hz 5k RayTrace 2.0 Supra. Elke keer mag je weer de hoofdprijs betalen. Want het is een steeds kleiner wordende niche aan mensen die er aan mee doen. Ondertussen draaien de games die populair zijn op 2 aardappelen met een Celeron er tussen. Of je koopt gewoon een Xbox One X voor dezelfde prijs en laat de boel de boel...
RTX 2060;5;0.35590028762817383;Je had de 1080Ti nodig voor 4k 60hz en nu de 2080Ti voor 1080P RayTrace 60hz Fixed that for ya...
RTX 2060;3;0.4566725492477417;Mja, dat roept men steeds wel... Ondertussen kan ik BFV spelen met DXR op Low en de rest om Ultra op 3440x1440 en ik krijg 67-88fps te zien. Helaas zitten er wat gestotter bij. En ja, ook uitschieters naar 100fps. Uiteindelijk is het probleem ermee dat je in de MP juist minder goed de tegenstand kan spotten. Dus kies ik ervoor om het uit te zetten. Ondertussen kan een 2080Ti gewoon op 4K bij Ultra instellingen met DXR aan (ja, op low...) 60fps draaien. Maar alles bijelkaar is het nog steeds te vroeg om hier echt conclusies over te trekken. Er is maar één titel waar een nog experimentele implementatie van deze techniek te zien is. En dan alleen de MS DX12 versie, niet de Vulkan API versie. Maar goed, om de RT prijs kwaliteitsverhouding hoef je momenteel niet echt naar de 2080Ti of Titan RTX. Juist op lagere resoluties zijn de goedkopere kaarten veeeel kosten-effectiever.
RTX 2060;2;0.24294181168079376;Ik hoop persoonlijk dat toetsenbord en muis binnenkort volledig ondersteund gaat worden voor alle games. Dan kan de PC de deur uit, en heeft iedereen dezelfde graphics voor een betaalbare prijs. Dan kun je zelf nog beslissen of je op tv of monitor gaat gamen. Ik ben bout met controllers, mocht daat een toetsenbord en muis voor in de plaats (En aangepaste matchmaking met bijvoorbeeld alleen toetsenbord/muis of pc spelers) komen ben ik verkocht.
RTX 2060;1;0.6381188035011292;"Het maakt toch niet uit wat er allemaal nog meer te koop is boven jouw keuze? Of koop jij je videokaart om een hoger getalletje te hebben dan de andere jochies op het internet? Met een 2060 kan je de nieuwste games (die ook al een goed stuk zwaarder zijn dan ten tijde van de 970) spelen op 1440p/ultra, or 4K als je wat water bij de wijn doet, wat is dan het probleem? Nvidia probeert een kaart die prima de 2070 had kunnen zijn te 2060 te noemen zodat alle ""ik koop elke 2 jaar een x60 nvidia kaart"" mensen nu blindelings 370 euro uitgeven ipv 250, als je die getalletjes inderdaad belangrijker vind dan de performance, betaal je inderdaad ineens 370 euro voor ""budgetkaart"""
RTX 2060;1;0.6313925981521606;"Wat een onzin. Niet alleen is er veel minder vooruitgang nu dan vroeger, maar de prijzen gaan onevenredig omhoog. ""Vroeger"" kocht ik altijd mid-range kaarten voor rond de 200 euro en soms zelfs veel goedkoper (mijn GTX260 216 was bijv. 165 euro). Het maakte niet uit of die sneller waren, daar ging je vanuit. Nieuwe kaarten in de mid-range hadden altijd ongeveer dezelfde prijs. Daar is met de Pascal kaarten opeens een hele hoop mee veranderd. De GTX1060 werd al iets duurder gelanceerd dan wat de meeste mensen hadden verwacht en de bitcoin koorts maakte het er niet beter op qua prijzen. Nvidia zag dat mensen bereid waren (veel) meer te betalen omdat er ook weinig concurrentie was. Nu hebben ze er nog een schepje bovenop gedaan met de nieuwe Turing kaarten. De lonen zijn marginaal gestegen, RAM is ook een goed stuk duurder nu dan 2 jaar geleden en dat maakt het hele PC bouwen een beetje kut. Ik heb 2 jaar geleden voor een laptop met GTX1060 kaart gekozen omdat dat goedkoper was dan een soortgelijke desktop te bouwen en qua performance weinig onderdoet. Dat had ik voor deze prijsstijgingen nooit overwogen. Het zou nu zelfs lonen om bijvoorbeeld een Xbox One X te kopen die qua performance ongeveer gelijk is aan een PC met GTX1060 (ja, andere usecase maar voor puur gamen niet zo'n groot verschil). Ray Tracing op de 2080Ti is al van bedroevend niveau nu, laat staan dat een 2060 moet gaan ray-tracen. De GTX 1160 zal waarschijnlijk ook niet bijzonder goedkoop zijn."
RTX 2060;5;0.22745563089847565;"Ik ben helemaal met je eens dat de mining craze en het hele DRAM gedoe nu compleet kut is, ik krijg mezelf ook maar niet over de drempel om naar AM4 over te stappen Maar als je nou eens even verder kijkt dan typenummertjes, zal je zien dat je met die 200 euro nog steeds een uitstekende budget kaart kan kopen, zoals de RX580. Deze 2060 kan verdomme gewoon 1440p op ultra draaien en met wat tuning trekt ie 4K ook, dat is toch geen ""budget"" meer? Sure, spelletjes worden niet elk jaar netzoveel mooier als 10 jaar terug, maar dat kan ook gewoon niet, ook 10 jaar terug was die vooruitgang al aan het stagneren. Desondanks is die vooruitgang er nog steeds, en kan je nog steeds voor 200 euro prima een kaart kopen die nieuwe spellen prima draait op reguliere settings. 15 jaar terug kocht je voor die 200 euro een 6600GT, en je hoeft echt niet te denken dat je daarmee Far cry op 1080p/ultra kon spelen.."
RTX 2060;4;0.32624518871307373;toen werd die resolutie ook amper gebruikt. FarCry kon je echter wel op ultra spelen op de meer standaard 1280x1024 resolutie. Ook leuk om te weten is dat deze kaart op bijna alle games sneller was dan de 5950 ultra, het topmodel van de voorgaande reeks.
RTX 2060;1;0.4719621539115906;Klopt, 1080p was toen niet populair, maar ook al had je zon scherm (of 1920*1200), dan had een 6600GT het niet getrokken, 1080p is een hele berg extra pixels tov 1280*1024 (en volgens mij speelde ik op 1024*768 en later 1440*900) De vergelijking met de 5950U is een leuke idd, maar dat was ook wel de perfect storm, de FX serie was een absolute flop, de 6 serie was gewoon geweldig, monsterlijk goede kaarten destijds.
RTX 2060;2;0.48255690932273865;op 1600*1200 (ongeveer evenveel pixels als 1080p) kon de 6600GT inderdaad maar 40fps halen gemiddeld. Toen werd dat nog wel als speelbaar beschouwd, maar tegenwoordig is dat net niet goed genoeg. Indien ik de index op de verkoopsprijs bij release toepas kom ik uit op een prijs van 284 Euro, voor prestaties die ongeveer 20% achterliepen op het topmodel.
RTX 2060;2;0.4216933250427246;"[ quote [ maar als je nou eens even verder kijkt dan typenummertjes, zal je zien dat je met die 200 euro nog steeds een uitstekende budget kaart kan kopen, zoals de rx580 [ / quote ] klopt, maar de rx580 is een rx570 met ietwat hogere clocks, op zich niet zo spectaculair aangezien de rx570 met moeite de gtx1060 kon bijhouden. de gtx1060 zit nu pas rond de prijs waarop hij rond de launch moest zitten je vergeet alleen even dat we vroeger kaarten hadden die voor een hoge adviesprijs geintroduceerd werden, onder adviesprijs verkocht werden en die prijs na 3 maanden zo laag was dat het betaalbaar werd voor de meeste mensen. de huidige kaarten doen qua prijs niet zo veel en kunnen zelfs in prijs omhoog gaan wat al helemaal absurd is. videokaarten zijn geen collector ' s items, maar zo gaan we er wel mee om in deze tijden van schaarste en de monopoliepositie van nvidia. laten we even heel eerlijk zijn, amd doet niet echt mee in deze race nu. en dan nog iets : als tweakers en consumenten moeten we niet de fabrikanten gaan verdedigen die ons uitbuiten. het hele dram cartel, de nvidia prijzen en releases ( 2060 heeft 3 verschillende geheugencapaciteiten die weer onderverdeeld zijn in 3 verschillende soorten ram per kaart ), intel die net als nvidia lui was geworden en het nu heet onder de voeten krijgt omdat amd opeens weer meedoet. de "" 60 kaarten zijn altijd mid - range kaarten geweest. dat is waar de consument en enthousiast het mee associeerd. technologie wordt sneller, dat is toch logisch en dat was vroeger ook zo ( en ging een stuk sneller ), maar destijds betaalde je niet steeds meer voor producten omdat ze sneller waren dan de vorige generatie. ik ben blij dat we nu met ray tracing beginnen, maar de instapprijzen zijn niet te verantwoorden. ook het hele "" founder ' s edition "" schiet bij mij in het verkeerde keelgat. reference kaarten, want dat zijn het, horen goedkoper te zijn dan board partner kaarten. deze video legt het hele founder ' s edition gebeuren beter uit dan ik dat kan :"
RTX 2060;2;0.39169272780418396;Ik ben het geheel met je eens dat wij als tweakers/consument uitbuiting niet moeten verdedigen, maar dat doe ik ook niet. De 2060 is gewoon geen budgetkaart meer, ongeacht wat eerdere x60 kaarten waren, de prijs is er niet naar en de performance is veel hoger dan je als budget bewust gamer nodig hebt, het enige probleem hier is dat nvidia het ding 2060 heeft genoemd en niet 2070 (wat als je bijvoorbeeld naar de stap 780>970 kijkt, en 2060>1080 prima te verdedigen zou zijn) Ik ben het ook met je eens dat al die FE onzin van nvidia nergens op slaat, maar het probleem hier is niet dat nvidia 370 euro durft te vragen voor een 2060, maar dat ze het ding 2060 hebben genoemd, of dat ze geen duidelijk genoeg onderscheid maken tussen GTX en RTX, als je puur kijkt naar performance/€ is er niks mis met deze kaart en heb je ook meteen door dat je als budgetgamer hier niet naar hoeft te kijken, 1440p/ultra is hardcore-gamer land. (overigens, de 580 is geen overklockte 570, de 580 heeft extra compute units, de 590 daarintegen is wel een 580 met iets hogere clocks, en die kaart heb ik ook weinig goeds over te zeggen)
RTX 2060;2;0.3860686123371124;Wat veel mensen lijken te vergeten is dat er op deze rtx kaarten daadwerkelijk extra hardware zit... Die tensor en rt cores zijn gewoon silicon waarvoor betaald moet worden, is de prijs voor waar je voor betaald hardware technisch gezien mis? Nee. Doet die extra hardware waar je voor betaald iets wat het 100 euro meer waard maakt? Tja....
RTX 2060;1;0.6935670375823975;Hier klopt dus helemaal niets van. Jij gaat er dus van uit dat videokaarten niet met de tijd meegaan. Vroeger was de 970 gewoon de mid-range kaart, en de 960 de 'budget'kaart. Tegenwoordig zijn dat gewoon de 2060 en 2070. Het enige wat er is verandert is dat de prijzen omhoog geschoten zijn. Dat de nieuwe kaart meer kan dan de 'oude' mid-range kaart heeft hier dus helemaal niets mee te maken, omdat het kaarten van een paar jaar geleden zijn. Als deze kaarten vergelijkbaar zouden zijn met de prijzen zoals ze nu zijn, zou dat diep triest zijn.
RTX 2060;2;0.37588000297546387;"Met een 2060 kan je huidige games op veel hogere settings draaien dan met een 960 de games van die tijd, nvidia probeert gewoon een kaart die eigenlijk best een 2070 had kunnen zijn als 2060 te verkopen om daarmee alle ""ik koop gewoon de nvidia budgetkaart"" klanten een treetje hoger te krijgen. 1440p ultra is gewoon x70 territory, niet x60/budget"
RTX 2060;2;0.42470768094062805;Wist zelf niet goed hoe je het goed moest uitleggen, totdat ik deze video tegenkwam. Gewoon een mooi verkoop praatje van Nvidia. Gewoon altijd ervan uitgaan dat bedrijven met gebakken lucht komen en mooie praatjes. Gewoon common sense gebruiken, totdat iemand die kritisch is en er wel alles van begrijpt iets zegt, of je begrijpt alles zelf al. De prijzen van de nieuwe 'budget'kaarten zijn gewoon veels te hoog.
RTX 2060;2;0.3779451847076416;Je kan ook nog verder teruggaan. In 2008 had je een Radeon HD4850 voor pakweg 150 EUR. Die zat toen in een vergelijkbaar segment als de GTX970. De prijzen van kaarten binnen hetzelfde segment zijn meer dan ver-3-dubbeld als je naar de huide prijzen de RTX2070 kijkt. Het zou wel leuk zijn om dit eens op een grafiekje te zien. Heeft Tweakers historische data van videokaart prijzen 10 jaar terug? In de pricewatch vond ik niet veel.
RTX 2060;3;0.33187100291252136;"Lijkt me interessant om dit eens te bekijken, natuurlijk wel de inflatiecorrectie meenemen! Heb geen harde cijfers, maar 150,- 10 jaar terug is natuurlijk niet hetzelfde als 150,- in 2018. (zal eerder rond de 200-210 zitten). Ook is het moment dat de prijs bepaald wordt belangrijk om vast te stellen: launch met adviesprijs of 1 maand na release en dan goedkoopste uit de pricewatch? Wellicht een leuk artikel waard: ""20 jaar 3d gaming"""
RTX 2060;3;0.3194902539253235;Naast inflatiecorrectie moeten we natuurlijk ook de component kosten meenemen. Er zit nogal andere hardware in een nieuwe GPU vergeleken met 10 jaar terug. Net zoals een Nokia van 20 jaar terug niet te vergelijken is met een moderne high-end smartphone.
RTX 2060;2;0.44014668464660645;Hmm, amd / ATI had toen de 4850 en 4870 die het opnemen tegen de gtx260, daarboven nog twee kaarten . En dan reken ik (van beide partijen ) de dual-gpu kaarten niet mee. Een 4850 / 4870 was absoluut een mainstream kaart voor een topprijs (vergeleken met Nvidia sowieso) maar het was mainstream, niet enthousiast en hoger. Dan nog, de 'mainstream' kaart van de gtx10xx serie was de 1060, en die was (zelfs in uitgeklede versies ) niet onder de 200 te krijgen en deze gtx2060 kost nu meer dan 300, dus ik snap je punt wel. Maar een hd4850 zat niet in vergelijkbaar segment als de gtx979, maar eerder de 960 of zelfs de gtx950. (En je moet ze dus eigenlijk vergelijken met de Nvidia kaarten van die tijd, de gtx2xx serie).
RTX 2060;3;0.5205234289169312;De HD4870 legde het toen enkel af tegen de GTX280 (die was een dikke 10% sneller) en een dual gpu kaart. In mijn ogen is dat toch echt wel boven mainstream. De HD4850 scoorde iets van 20% onder de HD4870. Om dan de HD4850 in hetzelfde segment te stoppen als de GTX960 en zelfs GTX950 vind ik wel wat ver gaan... Die zitten nogal wat verder van de topmodellen vandaan.
RTX 2060;3;0.4508534371852875;reviews: Club3D Radeon HD 4870 versus MSI Geforce GTX260 OC 4870 en gtx260 waren netjes aan elkaar gewaagd, en na prijsverlagingen van Nvidia ook qua prijs. Daar komen dan de gtx275 en gtx285 nog bovenop, en AMD met de HD4890. Vandaar dat ik (ook toen) het om het 'mainstream' segment vond gaan. Het verscheelt ook wel erg waar in tijd de review genomen is (bij launch van de hd48xx of later) want de drivers maakten best wat meer verschil in die tijd.
RTX 2060;2;0.4548223614692688;Maak je niet dik. Dit kaartje is getest met een 1000 Euro processor 10 core / 20 threads met 32GB DDR4 geheugen een lollige 1600 Watt voeding en twee SSD's (RAID 0?) Valt een beetje uit het lood om dan de simpelste kaart te stoppen in een dure test setup die de meeste gamers niet eens hebben. Tweakers heeft over heet algemeen leuke topics, maar hardware tests en de best-buy roundups zijn knullig.
RTX 2060;1;0.2679671347141266;Wat was de launchprijs van die 970?
RTX 2060;5;0.4179046154022217;dat staat in mijn post.
RTX 2060;3;0.44348984956741333;Er staat wat je betaalde, niet wat de msrp is. Op zich denk ik dat je deze kaart wel kan vergelijken met de 970 in die tijd. Je kan met deze kaart op de meestgebruikte resoluties meer dan prima gebruiken. 1440p is prima te doen, zelfs 4k nog.
RTX 2060;2;0.40559494495391846;"Vreemd resultaat w.b.t. Battlefield 1. De RTX 2070 is op 1440p sneller qua minimum fps dan op 1080p @ ultra settings? En de RTX 2060 is op 1080p sneller qua minimum fps dan de RTX 2070. Prestaties/stroomverbruik verder best wel mooi... maar ik vraag me af of je in 2019 een videokaart met ""maar"" 6gb moet willen kopen. Met Wolfenstein op 4k ultra gaat de 1070ti de RTX 2060 ruim voorbij qua minimum fps. Zou dat door 8 gb versus 6 gb vram komen?"
RTX 2060;1;0.38222360610961914;Dat rondje zal vast nooit helemaal 100% hetzelfde verlopen.
RTX 2060;2;0.3590424656867981;Maar als dat al zo'n verschil geeft tussen dezelfde kaart met twee verschillende resoluties, hoe serieus kun je deze test dan noemen tussen 2 verschillende kaarten?
RTX 2060;3;0.5662408471107483;De minima uit die benchmark zouden ze eigenlijk moeten schrappen. Die hebben weinig waarde juist omdat de pieken kwa belasting aardig kunnen uitschieten waardoor ze niet goed te vergelijken zijn. Maar gemiddeld zal zich dat grotendeels opheffen. Bij de gemiddeldes zijn de resultaten ook logisch. Bij de minima zijn er wel meer gekkigheden. De Vega 56 krijgt met Ultra ook ineens hogere minima dan met Medium. Dat kan gewoon niet.
RTX 2060;3;0.49052944779396057;Ik denk toch dat de meeste gamers de minimum fps het belangrijkste vinden.
RTX 2060;2;0.5596414804458618;Waarschijnlijk zelfs in combinatie met 'hoe vaak' dat voorkomt. Als zo'n minimum maar een seconde van een paar uur durende game-sessie voorkwam, is dat alweer heel wat anders dan wanneer je bij elke keer dat je scherm wat drukker wordt niet lekker meer kunt spelen. Maarja, om nou met allerlei percentielen of lopende gemiddeldes (moving average) enzo te gaan werken, daar wordt de presentatie van de resultaten natuurlijk ook weer niet duidelijker op.
RTX 2060;2;0.5088515877723694;"Aan de andere kant: is minder overzichtelijke maar wel meer inzicht verschaffende informatieverstrekking niet te verkiezen boven simpeler maar ""misleidender"" informatieverstrekking? Het feit dat het alternatief moeilijk te begrijpen is betekent niet dat de huidige tekortschietende manier van weergeven dan maar moet blijven. Is er niet een derde optie te bedenken?"
RTX 2060;2;0.49321165680885315;Ja, ze zijn belangrijk maar de waarden die hier worden gepresenteerd zijn gewoon niet betrouwbaar omdat er geen vaste benchmark is. Om die waarde wel betrouwbaar te maken zouden ze een veel langere speelsessie moeten gebruiken.
RTX 2060;1;0.4970039129257202;Gemiddeldes geven de speelbaarheid van een game helaas absoluut niet weer. Om het even extreem te nemen: als mijn benchmark van twee minuten de helft op 5fps draait en de andere helft op 200 fps dan heb ik een gemiddelde van 62,5 fps. 60+ fps is prima, maar dit voorbeeld is dus de helft van de tijd niet om aan te zien.
RTX 2060;5;0.4408378303050995;En daarom is de review van de buren veel nuttiger: zij geven ook de frametimes.
RTX 2060;3;0.5182079672813416;"Beetje vreemd ook dat de ""buren"" dezelfde kaart testen. Ik ga er vanuit dat dit geheel losstaande tests zijn geweest. Als we even eerlijk zijn dat doorgaans hw info wat beter is in hardware testen, snap ik niet dat tweakers het niet gewoon uitbesteed? Het is immers hetzelfde bedrijf. Ik kan er natuurlijk naast zitten, wellicht is er wel samengewerkt."
RTX 2060;2;0.46827369928359985;Dat zei ik ook niet. Minima zijn belangrijk, MAAR ze zijn alleen nuttig voor een vergelijking indien je een identieke benchmark hebt. Zodra er variatie komt in de benchmark is deze waarde nutteloos voor een vergelijking van de grafische hardware.
RTX 2060;2;0.31769776344299316;Bovendien geldt volgende regel in de statistiek: outliers mag je niet verwijderen tenzij je kunt bewijzen dat die onterecht worden meegerekend, dat is hier niet het geval. De minima zijn dus essentieel voor het correct interpreteren van de benchmark. eigenlijk zou je dan ook maxima moeten weergeven maar die zijn in deze context al helemaal niet relevant.
RTX 2060;1;0.45535242557525635;Outliers mag je niet zomaar zonder reden verwijderen. Dat klopt. Maar wat hier wordt gedaan is dat de grootste outlier als een benchmark wordt gebruikt en dat is natuurlijk statistisch totaal onbruikbaar als het testscenario niet helemaal zuiver is (en dat is hier absoluut niet het geval).
RTX 2060;3;0.2741943895816803;Dat + ik ga er vanuit dat ze 1 test gedaan hebben per instelling waardoor het mogelijk kan zijn dat het een uitschieter (onderschieter?) is geweest
RTX 2060;1;0.3843986988067627;RTX gimmick... Toch maar een tweedehands GTX1080 scoren!
RTX 2060;3;0.2624596059322357;Als je met 'RTX gimmick' doelt op de raytracing techniek dan ben ik het niet met je eens. Voor zover ik mij erin heb verdiept lijkt raytracing een bijzonder natuurgetrouwe techniek om graphics te genereren. Het werkt vergelijkbaar met hoe de fotonen vanuit een lichtbron rondstuirteren op allerlei opvlakken voordat het in onze ogen valt. In dit filmpje wordt het uitgelegd.
RTX 2060;1;0.6547417044639587;Tegelijkertijd wordt het door niets ondersteund en is het nvidia specifiek. Dit is een gevalletje Hairworks. Als er voor 3 tientjes minder exact dezelfde kaart verkocht zou worden zonder raytracing dan zou niemand nog de RTX variant kopen.
RTX 2060;1;0.5581194758415222;hoewel ik het met je eens ben dat RTX niks wordt (in de huidige impl), geef ik je op een briefje dat 90% van de huidige RTX kopers die 3 tientjes graag extra zouden lappen, kijk maar eens naar wat mensen overhebben voor een factory OC van 3-5%, een koeler die 5 graden koeler blijft, of *gruwel* RGB ledjes...
RTX 2060;2;0.3369452655315399;Fair enough maar mijn gamesysteem is dan ook een absolute ghettobuild met een passief gekoelde Xeon, 1 hele case fan, overal kabels en de goedkoopste 1060 die ik kon kopen. Als je me 10 euro zou geven om hem nog lelijker te maken dan zou ik het doen
RTX 2060;2;0.31823983788490295;Dat het mooi en echt is, maakt het nog niet geen Gimmick het is een Gimmick omdat het bijna nergens wordt gebruikt en als het al wordt gebruikt zijn de prestaties bagger
RTX 2060;2;0.44764524698257446;Even los van de definitie van een gimmick, ben ik het met je eens dat de huidige techniek nog niet ver genoeg is ontwikkeld. Ik doel daarentegen vooral op het potentieel van de raytracing techniek, die is wat mij betreft verre van een gimmick te noemen. Hoe dan ook, de RTX serie zie ik meer als de eerste stapjes op een (vrij) zekere weg van het realtime renderen van 'realistische' graphics. Sterker nog: je zou de huidige techniek een gimmick kunnen noemen gezien daar d.m.v. trucjes allerlei grafische effecten worden gegenereerd, waar raytracing deze trucjes niet nodig heeft omdat het gewoon doet wat er in werkelijkheid ook gebeurt (tot op zekere hoogte in elk geval).
RTX 2060;3;0.4632265269756317;Het geeft anders een waanzinnig effect. Op de 2060 lijkt het inderdaad niet geweldig maar met 1 driver update konden ze 50% meer fps krijgen. Dat maakt het iig op 2080 TI dat ik ruim 110 fps heb in Battlefield op ultra en RTX aan.
RTX 2060;1;0.2854959964752197;Ja, op 1080P. Zit je dan met je €1350 kostende kaart op een resolutie uit 2005.
RTX 2060;1;0.4264695346355438;Heb voor €550 een tweedehands msi 1080ti gaming x gehaald. Geen RT, wel 140 fps ultra 1440p. En dat voor de helft
RTX 2060;3;0.501645028591156;zonder rt ga ik naar de 200. resolutie is wat mij betreft prima. Het verschil in kleuren en licht is wel aanzienlijk. Misschien zie ik zo niet alles maar de game is wel echt prachtig zo.
RTX 2060;1;0.5616728067398071;Ieder zo zijn ding. Ik vind 200 fps 1080p geen €1400 waard.
RTX 2060;5;0.38971734046936035;Ieder zijn ding. Dit is ook de eerste keer dat ik zo veel uitgeef. Maar van de andere kant zit ik veel achter de computer en heb ik er uiteindelijk veel plezier van.
RTX 2060;2;0.45723214745521545;Hoewel de GTX1080 hier ontbreekt, kunnen we wel wat extrapoleren. En als ik dan een beetje extrapoleer en gok, denk ik dat een 1080 ietsjes sneller is een een 2060, maar geen schokkend verschil. Een tweedehands non-founders 1080 gaat voor €400-€450. Dus als ik echt heel optimistisch reken, dan heeft een tweedehands 1080 een vergelijkbare prijs/kwaliteit als een nieuwe 2060. Tot zover waarom ik tweedehands GPU prijzen niet begrijp. Maar ik zou ook echt dan niet aanraden om een tweedehands te kopen.
RTX 2060;2;0.35739102959632874;Die prijzen zakken dan nog wel wat na deze launch, hoewel ik 2e hands prijzen ook verder niet snap hoor, voor bijvoorbeeld RX570/580s loopt de 2e hands prijs uiteen tussen 80-110% van een nieuwe, maar dan zonder/minder lang garantie, en gegarandeerd gebruikt in een mining rig...
RTX 2060;5;0.5178979635238647;Ik vind een tweedehands 1080ti gaming voor €550 een zeer goede koop tbh.
RTX 2060;2;0.5004658102989197;"Of je wacht tot ray tracing met mid range ook prima speelbaar word, ik vermoed ergens als Nvidia in 2019/2020 op 7nm zit en DLSS ook door ""zelf leren"" beter gaat werken. Jammer dat AMD wacht tot ray tracing prima speelbaar word op low end en tot dan nog geen hw ondersteuning bied wat wel nodig is . SLI schaalt overigens ook niet lekker voor een 2e kaart na mijn menig."
RTX 2060;5;0.35284146666526794;of gewoon w8ten tot de 11 serie
RTX 2060;2;0.5029454231262207;De weg is vrij voor AMD in het budgetsegment. Vreemde keuze van Nvidia om de xx60 opeens zo'n stuk hoger de positioneren. Ten tijde van de 700-generatie kocht je voor dezelfde prijs bijna een 780. De 960 was ook een stuk goedkoper. De 1060 is niet echt mee te rekenen in verband met de mininggekte. Achja, het is slechts naamgeving. Ik ben benieuwd wat een RTX2050 dan wordt. Als Battlefield 1 geen ingebouwde benchmark heeft, waarom blijven jullie er dan op testen? Nu spelen er te veel variabelen. Deze vergelijking zegt bijzonder weinig, het lijkt mij nuttiger om die tijd in een andere game te stoppen, een game die wel over een ingebouwde benchmark bezit?
RTX 2060;1;0.25554126501083374;zo denk ik ook. De mining gekte is gebruikt om mensen aan hogere prijzen te laten wennen. Zie de overstock van 1060 ed. En iedereen vind het normaal en raar dat mensen klagen over de prijs van de 2060. Eigenlijk vanaf de 400 serie uit mijn hoofd waren de kaarten in de 60 serie altijd rond de 200 euro. Met verbetering etc.
RTX 2060;3;0.2762065827846527;Je kunt binnen kort nog wel wat meer 2060's verwachten IMHO. 3 GB, DDR5 versies en alle mogelijke itteraties.
RTX 2060;1;0.6092435717582703;Ik vraag me af waarom Nvidia haar producten zo duur heeft gemaakt. Ik vraag me ook af hoeveel mensen een rtx 2070 of hoger hebben gekocht. Ergens jammer voor de normale consument, ergens positief voor de workstation segment als ik Linus van YouTube mag geloven. Klinkt gek, ik heb ooit een keer een EVGA 8800 GT gekocht. Het was toen een mooie midden segment kaart. Mooie tijden. Daarna had ik een 460 GT gehaald. Ook een mooie kaart. Jammer Nvidia.
RTX 2060;2;0.32214653491973877;Misschien omdat de hardware gewoon duurder is om te ontwikkelen/produceren
RTX 2060;1;0.47681882977485657;Dat is wel een mogelijke reden. Bedankt. Niet aan gedacht.
RTX 2060;3;0.6377753019332886;Ik vind de resultaten van de AMD Vega 56 een beetje teleurstellend. Waarschijnlijk is dit op basis dat de kaard stock draait. Maar er is algemeen bekend dat de Vega ge-undervolt moet worden met een kleine OC om op zijn best te presteren. Zo draait mijn Sapphire Vega 56 Pulse in games continu op 1600 MHz met geheugen op 945 MHz. En presteert mijn kaart een stuk beter dan wat bovenstaande grafieken laten zien. Nu is met benchmarks natuurlijk niet uitgegaan van OC omdat lang niet iedereen zal OC-en. Maar ik denk dat 80 procent van de mensen die juist een Vega hebben gekocht vanwege de performance met een undervolt en OC combi. Ik zou graag in dit soort grafieken een undervolted Vega ook nog terug zien komen. Ik verwacht dan dat deze zo'n 2 posities zal stijgen en tussen de RTX 2070 en GTX 1070ti uitkomt. Zeker gezien ik in veel games en benches dezelfde performance haal als een GTX 1080.
RTX 2060;2;0.3638797700405121;1070ti's draaien standaard ook op veel te lage settings, alleen al de power en temp target omhoog gooien geeft je 10%, mijn MSI gaming x draait op 2025mhz core en 4500 memory, ipv de standaard 1700 en 4000
RTX 2060;1;0.46502256393432617;Vreemd dat mensen een nagenoeg bijna performance van een 1070TI als budget bestempelen. Of mis ik iets?
RTX 2060;3;0.3436211347579956;"1070Ti is van een vorige generatie. Next GEN behoort beter te presteren en de ""waardering"" verschuift. Los daarvan heeft budget meer met de prijs te maken dan prestaties en daarom is deze kaart eigenlijk nog te duur om onder budget te vallen."
RTX 2060;2;0.4304734170436859;Betere prestaties dan ik had verwacht van deze kaart. De prijs is inderdaad hoog, maar qua prijs prestatie is het in ieder geval geen achteruitgang, zoals het bij de andere 20xx kaarten. De kaart komt akelig dicht bij de 2070 in de buurt met de overklok. Qua RTX is het natuurlijk nog maar de vraag of iemand dit uberhaubt wil of weet te gebruiken. Het is nu afwachten op AMD. Ik hoop dat ze een gelijkwaardig of beter product neer kunnen zetten voor een betere prijs. Ik ben zelf op zoek naar een nieuwe GPU, maar dan moeten de prijs en prestatie er wel naar zijn.
RTX 2060;2;0.5238614678382874;Ik wacht al jaren op een GPU van AMD, maar sinds de R290 is daar geen innovatie meer te vinden. Ik verwacht helemaal niets meer van AMD en deze kaart maakt dat weer pijnlijk duidelijk. Jammer, dat wel.
RTX 2060;2;0.47094541788101196;De vega is opzichte niks mis, de crypto markt heeft die kaart geen goede reputatie gegeven en HBM2 geheugen helpt ook niet echt met de prijs.
RTX 2060;1;0.2404751181602478;nog 2 nachtjes slapen en je weet het (Woensdag 18:00 uur Nederlandse tijd).
RTX 2060;3;0.4712181091308594;Ik denk wel dat de VEGA kaarten innovatief te noemen waren. Als ze verkijgbaar waren voor de adviesprijzen dan zouden vast ook een hoop meer mensen ze in hun PC's hebben. VEGA bevatte al wat meer compute mogelijkheden waar NVIDIA zich nu pas op focust met de RTX kaarten. Bovendien was HBM2 een stuk sneller dan GDDR5, maar helaas ook duurder.
RTX 2060;3;0.36896196007728577;Wat ik me afvraag, onder welke omstandigheden wordt opgenomen vermogen precies gemeten? Ik ben eigenlijk benieuwd wat de impact op verbruik is wanneer RTX wordt ingeschakeld, gezien toch een aardig deel van de chip voor RTX is.
RTX 2060;5;0.5303651690483093;Eindelijk een kaart die het beter doet dan de vorige generatie voor hetzelfde geld. Laten we hem vergelijken met de GTX 1070, die ook een adviesprijs van $379 had, ruwweg hetzelfde. Tov die kaart is deze 25% sneller. Dus nu krijgen we EINDELIJK vooruitgang voor hetzelfde geld. Bij de andere Turing kaarten was dit namelijk NIET het geval. (Daar kreeg je ruwweg dezelfde prestaties voor MEER geld dan de vorige generatie). Al bij al nog steeds een trieste stand van zaken, dat je beter af was met de vorige generatie kaarten dan met deze. Een GTX 1080 of 1080Ti was een betere koop dan de huidige RTX 2070 / RTX 2080 enz. En nog steeds zijn er geen kaarten in het lagere segment te krijgen (al zijn de GTX 1070 en 1060 enz. nog steeds te koop).
RTX 2060;1;0.4804908335208893;Ik hoop* dat door de prijs/prestatieverhouding van de RTX 2060 de prijs van een 2e hands 1070 Ti gaat drukken. Als ik zo de V&A bekijk gaan de goedkoopste 2e hands 1070 TI's weg voor +/- €325-350. Dit is geen houdbare prijs als een instapper RTX 2060 nieuw voor €369 wordt verkocht. *: zoekende naar een scherpgeprijsde 1070 TI mini voor in HTPC. Het is een bekend gegeven dat tweedehands kaarten ( vooral van de vorige generatie) een betere prijs/prestatie verhouding hebben dan de huidige/nieuwe modellen.
RTX 2060;2;0.43355825543403625;Hmm.. RT is een No Go voor de 2060, wat heeft het nut als BF 5 nu al amper speelbaar is op 60 Fps met 1080P laat staan als straks nieuwere games met RT komen die grafisch nog beter zijn. Of als veel mensen net als ik een 1440p scherm hebben dan heb je er geen drol aan. overigens is de 2060 wellicht geen slechte aankoop als je nog een oudere kaart hebt. Mensen zeggen wel dat je beter een 1070 Ti kan halen maar, op dit moment is dat nog een duurdere kaart dan de 2060. Wel krijg je er 2 GB meer bij de 1070 Ti
RTX 2060;2;0.2409859448671341;Net even een raytracing comparison filmpje bekeken in BF5. Je levert de helft van je frames (ultra DRX) in voor een amper waarneembaar verschil. Het zal vast nog beter geïmplementeerd worden in de toekomst maar het is voor nu meer een gimmick.
RTX 2060;3;0.32438716292381287;Tja, wat jij amper noemt, noem iktoch aanzienlijk, en daarmee geen gimmick (niet minder dan vb antialiasing of bumpmapping jaten geleden).
RTX 2060;3;0.32550153136253357;(...)De RTX 2060 kost 369 euro en voor een mainstreamkaart vinden we dat eigenlijk te veel.(...) (...)Daarmee heeft Nvidia nog altijd geen mainstreamkaart op de markt.(...) Dus het is geen mainstreamkaart maar hij is wel te duur voor een mainstreamkaart. Goed punt
RTX 2060;3;0.38066229224205017;Aan de ene kant qua prijs en prestaties is de RTX een Top kaart voor 370€ in vergelijking met een GTX 1070 voor pricewatch: MSI GeForce GTX 1070 Aero ITX 8G OC 360€ en 1070TI voor pricewatch: Gigabyte GeForce GTX 1070 Ti Windforce 8G 440€ en R56 voor pricewatch: Gigabyte Radeon RX VEGA 56 GAMING OC 8G 340€ Aan de andere kant RTX is nieuwe technologie en dat is eigenlijk niet te vergelijken met prestaties van oudere techniek zoals Pascal en Vega en RTX is in feite ook anders mede door Raytracing. De potentie van RTX is ook nog niet duidelijk daar heb je meer updates voor nodig voor de kaart zelf en voor de spellen die je speelt of programma's waar je mee werkt en het is ook afwachten of Raytracing omarmt wordt door alle bekende Studio's. Ben benieuwd waar AMD met Navi mee komt want qua prijs en prestaties blijft AMD toch net zo interessant als Nvidia maar door die extra stroomverbruik van R56 en R64 heb ik zo iets van nee dank je... zolang stroom niet gratis en 100% Groen is blijft het een Factor voor mij. Zelf zie ik het al helemaal zitten een mooi en strak systeem met Aluminium en glazen behuizing van Inwinn of Phanteks met zwart en zilveren componenten met deze 2060FE maad toch nog even Ryzen 3000 en Navi afwachten en ik was al aan het twijfelen tussen een i3 8100, i5 8400, 2200G, 2600.
RTX 2060;1;0.8449960350990295;En niemand valt over het feit dat ze hier oude games blijven testen en maar liefst 6 verschillende Nvidia drivers en 2 verschillende AMD drivers gebruikt hebben? De laatst AMD drivers brengen aardig was prestatie verbeteringen met zich mee.. 2 van de 10 geteste games worden zelfs letterlijk verleldt in de changelogs. Waardeloze grafiekjes, sorry.
RTX 2060;1;0.5578707456588745;Hilarisch dat mensen denken 'het recht' te hebben op een grafische kaart die 1.5x zo snel is voor dezelfde prijs of even snel zijn voor 0.5x van de prijs. Mensen het is een commercieel bedrijf die WINST wil maken. AMD heeft goede kaarten met de RX570/580, maar dan heb je het over het €100-220 segment, en rond de €300-350 kunnen ze nog wat met Vega. Hopelijk komt AMD met een goede '7nm' Navi kaart deze CES, dan kan die $350 adviesprijs uitvallen als €330 en heb je een super mid range kaart voor die prijs. Als we pech hebben valt het tegen en gaan deze kaarten €400+, omdat er niks beters is. Ja ze misbruiken dan hun machtspositie, maar dat is nu eenmaal hoe het werkt in deze wereld. Je bent consument, je moet zelf bepalen waar je wel of niet je geld aan uit geeft. Als je vindt dat Nvidia niks extra's levert ten opzichte van de vorige generatie dan koop je de kaart toch lekker niet. Al die negatieve reacties hier.
RTX 2060;3;0.6923799514770508;Beetje jammer dat de GTX1060 (6GB) niet in alle vergelijkingen is meegenomen...
RTX 2060;5;0.6583641171455383;Ik vind die nieuwe coolers echt super tof.
RTX 2060;3;0.5017456412315369;Tja, alles is beter dan zuigers van de vorige generaties. Deze zien er wél mooi basic en functioneel uit, kan ik wel waarderen (ipv neonlichtjes en overdreven design onzin).
RTX 2060;3;0.37561699748039246;Voor mij als laptopgamer is dit best een relevante kaart gok ik zo (uiteraard nog geen prijzen gezien van laptops met deze kaart) Ik vind 1080p prima, en een tweedehands gtx1080 bestaat uiteraard ook niet. Mijn huidige met 960m is inmiddels wel aan vervanging toe. Als ik een 1060 had gehad, dan zou ik deze eerst nog even een jaartje links laten liggen.
RTX 2060;1;0.525989294052124;Conclussie: 2080ti traag... 2080 traag.. 2070 traag.. 2060 traag Het gaat immers om raytracing.. de aanverwant 1000 serie is antiek onderhand
RTX 2060;1;0.33482372760772705;Als je echt niks kunt verzinnen... Doe wat Linus doet!
RTX 2060;1;0.44744864106178284;"Ik game op 1080p en wil in de nabije toekomst graag een VR set. Meestal doe ik flight-sims. Ik vraag me af in hoeverre deze kaart het VR geweld verdraagt? Betreffende de discussie wat het budget voor een ""midrange"" kaart zou moeten zijn, ik zie zoiets als wat de gek er voor geeft. Wellicht ben ik gek maar ik vind 369 euro helemaal geen gekke prijs, zeker als je het afzet tegen een 1080ti of 1070ti."
RTX 2060;5;0.25590160489082336;Prijs en snelheid zijn gepositioneerd om de VEGA56 enigzins te verslaan op prijs en performance, dit lijkt een bewuste zet een stap harder dan dat hoeft Nvidia niet werken voor jouw centen, en bovendien is deze zometeen ook Freesync compatible. Dat is de deal die Nvidia hier bied.
RTX 2060;1;0.49606674909591675;350$ lijkt mij 450+ euro wat ik kan zien op tweaker prijswatch rip Kunnen we bij nvidea kopen voor 350$?
RTX 2060;1;0.48066556453704834;Ik zat ook al te kijken, flink teleurstellend dit.
RTX 2060;1;0.5395793318748474;"Ik speel vooral games van meer ""indie"" ontwikkelaars, zoals Escape from Tarkov, Post Scriptum en Squad. Allemaal games met behoorlijke performance issues zonder ray tracing. Blijft geoptimaliseerde ray tracing alleen iets voor de AAA industrie of ook voor andere developers? Ik zie deze developers echt niet binnen 5 jaar ray tracing in games stoppen en ook nog eens een speelbare framerate op zo'n kaart. Daarnaast we hebben maar 1 benchmark met ray tracing omdat er maar 1 ray tracing game is, die vanaf moment 1 in conclaaf met Nvidia zijn zodat de game als stokpaardje voor de RTX kaarten kan fungeren. Als we er 10 hebben, ook van devs die niks met Nvidia te maken hebben. wil ik de performance nog wel eens zien."
RTX 2060;2;0.39078089594841003;Dus als andere firm'a raytracing niet correct implementeren betekend dat dat de hardware niet goed is? Game devs krijgen prima hulp van Nvidia en er is geen enkele reden dat de makers van BF iets kunnen wat andere niet kunnen. Het feit ligt er dat vrijwel niemand interesse had in raytracing, terwijl het echt enorm veel kan toevoegen.
RTX 2060;2;0.5815399289131165;Wat voegt het precies toe dan wat zo enorm is? Je krijg natuurgetrouw licht en reflecties, echter is dit in games wat mij betreft niet zo belangrijk, tijdens intensief gamen merk je er helemaal niks van En heb gelezen dat dit makkelijker te programmeren is, wat leuk is voor de ontwikkelaar maar niet voor de consument, ik zie dus nog steeds niet wat het zo erorm veel kan toevoegen
RTX 2060;2;0.3963545560836792;Datzelfde kun je eigenlijk over iedere grafische verbetering zeggen: hogere resolutie, weereffecten, whatever. Toch zijn we van ver gekomen en niet iedere game is zo intensief als een FPS. Verder wordt ook iedere game met matige of slechte graphics compleet afgeschoten, dus blijkbaar is grafische praal wel degelijk belangrijk, of wordt zo ervaren. Ray tracing staat aan het begin maar is in feite wel de heilige graal van computer graphics. De rek is er al bijna uit in wat je kunt faken met textures. Ondanks alle kritiek op Nvidia zetten ze hier wel een eerste stap.
RTX 2060;2;0.35879218578338623;Denk dat je even van ganre moet wisselen in horror games maajt dit een enorm verschil. Omdat het licht nu ook het hoekje om kan terwijl er objecten tussen source en destination bewegen. Meshien even de metro demo kijken daar laten ze het goed zien
RTX 2060;2;0.49819421768188477;Ik bedoel meer dat ik het gevoel heb dat de groei in grafische kracht de laatste jaren gepaard gaat met een daling in optimalisatie, voor bij de NIET-AAA studios met beperkte middelen / optimalisatieskills. Daarom denk ik dat het nog even duurt voor er een game is met ray tracing en goede framerate door zo'n dev gemaakt. Daarnaast vind ik zulk soort games representatiever voor de markt dan grondig gepolijste AAA games, dus ik zie liever een benchmark van een game die niet gebundeld is met een RTX-kaart. Dus ja, als andere firma's niet zo optimaliseren als Dice/EA in R ray tracing, ben ik niet geïnteresseerd. En het tegendeel moet nog bewezen worden. TLDR: Ik vertrouw nvidia en haar partners niet vanwege hun gedrag in het verleden.
RTX 2060;2;0.3902316987514496;"Sommige mensen noemen een minimum van 60 fps speelbaar (ikzelf zou niets minder willen voor fps games). Alles onder de 60 - 50 fps merk en ""voel"" ik direct."
RTX 2060;3;0.45392361283302307;alle high en kaarten hebben toch 60+ FPS , alleen deze midrange kaart heeft 40+ FPS maar dan wel op ultra settings. Je kunt niet verwachten dat midrange op ultra goed presteerd. men heeft specifiek voor ultra gekozen om de vergelijking te maken zou niet eerlijk zijn om de 2060 o een lagere setting te laten draaien en dan vergelijken. Je moet bij een review benchmark natuurlijk wel tussen de regels lezen wat er gezegt wordt.
RTX 2060;2;0.39876341819763184;"Ik geef alleen maar aan dat ik 40 fps niet speelbaar vind en dat ik denk dat sommigen dat ook vinden. In jouw reactie lijkt het alsof jij 40 fps speelbaar vind? Kun je even duidelijk uitleggen wat ik dan tussen de regels door moet lezen? Ik snap namelijk geen bal van je manier van reageren. Kun je dan ook even aangeven waar ik gezegd heb dat ik verwacht dat midrange op ultra ""goed"" zou moeten presteren?"
RTX 2060;1;0.44775986671447754;Op 1080p ja. Nu zal het gros van de mensen dat nog gebruiken, zelf zit ik al tijden op 1440p (nu zelfs 3440x1440), waar die waardes dus niet haalbaar zijn.
RTX 2060;2;0.34399786591529846;"Afgezien van de discutabele 40 fps als ""speelbaar"": Een RTX2060, of eigenlijk alle xx60 kaarten voorheen, waren niet gericht op hogere resoluties, maar op de ""gewone"" gamer. Momenteel is dat volgens de Steam Hardware Survey nog steeds overduidelijk 1920x1080. Naar 1440p kijken voor een RTX2060 vind ik dus niet zinnig. Dat is het domein voor de 2070."
RTX 2060;3;0.2734619677066803;Degene waar ik op reageerde benoemde alle kaarten, waarbij specifiek de 2060 minimaal 40 fps zou halen met RTX aan. Waar ik op doelde was dat dit dus voor de 2080ti wel op 1080p was getest.
RTX 2060;1;0.41960108280181885;Raytracing stond niet eens aan, waar heb je het over
RTX 2060;3;0.5257828831672668;euhh, even iets beter lezen , raytracing staat wel degelijk aan daarom heet het ook raytracing benchmark.
RTX 2060;1;0.6929912567138672;Ik zie het idd, fps duikt onder de 50 fps, onspeelbaar voor mij, speel niet voor niets op pc.
RTX 2060;1;0.5815228819847107;1 benchmark en nog beweren dat deze kaart presteert? Is meer dure miskoop.
RTX 2060;2;0.34654659032821655;ze presteren allemaal 40+ FPS, op lage RT en ultra game settings. Dus dat houd in dat het meer dan voldoende is als je met een medium range kaart verwacht alles op ultra te kunnen spelen met 100fps dan zit er al iets mis bij jouw gedachte dat halen de high range kaarten niet eens. BF is zeker niet de enig game elke game die draait op de unreal engine en dat zijn er veel kunnen dit nu releasen netzoals metro.
RTX 2060;2;0.39505788683891296;Manmanman de 1070 presteert beter...dus waarom meer betalen?
RTX 2060;2;0.35378000140190125;De 1070 kun je nuet vergelijken met 2060 apples en peren. Je hebt daar geen tensor cores en ook geen rtx cores. Dus je mist die features. De vraag is wat wil je zelf snellere kleine auto of een die trager is maar wel alle luxe heeft. Dit zijn first gen kaarten. Net als jaren gelden fsaa etc kwam met de nieuwe eye candy aan was het trager. En chips zeker coprocessors zijn niet gratiz. Speel je hardcore fps dan zul je hier niet heel veel van zien. Speel je single player horror dan is rtx een hele verbetering
RTX 2060;1;0.7034898996353149;De technologie is zo nieuw dat er weinig games zijn die het gebruiken. Vervolgens wel de hoofdprijs betalen ? Ik koop liever de 1070 of de 1070 ti voor dat geld. Pas over 2/4 jaar zal het wel een verbetering zijn met hoop games. Maar goed hoop mensen staren zich blind nu op dit ontwerp. S6 verder ik stink er niet in. Luxe met 2060 kaart? Er grappig. Maybe de 2080ti de grote jonge .....yup luxe.maar dat geld? Wat zij vragen?echt schandalig VEEL!!!.
GTX 1660 Ti;3;0.5837558507919312;Aardige prestaties, maar voor wie is deze kaart nu bedoeld? ik heb zelf een 1060 en als ik meer prestaties zou willen, dan zou ik destijds wel een 1070 (Ti) hebben gekocht of tussentijds zijn overgestapt op een 1070 (evt. tweedehands). mijn overstap nu zou eerder een 2070 zijn. Deze kaart komt op mij een beetje over als een upgrade die eigenlijk al lang aanwezig was in de vorm van de Gtx1070, tenzij je misschien een nieuwe build doet of nog bijvoorbeeld een Gtx960 hebt. Desondanks denk ik dat dit een betere keus is voor een nieuwe build, dan de RTX2060, waar de raytracing eigenlijk overbodig is omdat de kaart niet krachtig genoeg is.. of zie ik het verkeerd?
GTX 1660 Ti;3;0.431652694940567;"""Aardige prestaties, maar voor wie is deze kaart nu bedoeld?"" Als niet-meer gamer ben ik denk ik een vreemde eend in de bijt, maar voor mij een mooie kaart op het goede moment. 4 jaar geleden een 4770K incl 750ti gekocht. Als GPU zuinig, stil en gaf wat extra voor editing icm een 4770K. De CPU en GPU ontberen beide helaas hardware matige decoding van actuele zwaardere video formaten. Dus bv HEVC 100+ Mbps materiaal kan met geen mogelijkheid vloeiend worden bekeken en ook zocht ik iets met 6GB VRAM. Ipv een volledige systeemupgrade (cpu/mobo/ram/koeler) a ~1100euro excl GPU, heb ik nu enkel een 1660ti genomen waardoor ik een bredere upgrade nog even kan uitstellen. HEVC 10bit 400Mbps wordt nu met 6-10% CPU belasting vloeiend afgespeeld. Keuze voor de 1660ti ipv de 1060 bv is dat de Turing modellen weer een bredere hardware encoding/decoding support hebben dus ik hoop daarmee voor mijn gebruik weer wat future proof te zijn. En daarnaast weer een zuiniger dan de vorige generatie, wat op de lange termijn ook telt. En misschien met deze gaming paardenkrachten toch maar weer een wat spellen oppakken;-)"
GTX 1660 Ti;2;0.5010349750518799;heb een bijna 3 jaar oude gtx1060 gamelaptop en die gaat dit jaar een upgrade krijgen. Vind raytracing nog niet interessant met maar 2-3 games die het nu ondersteunen en de 2060 kan het amper aan heb ik begrepen. Laptops met een 2070 of 2080 zijn onbetaalbaar voor mij. Conclusie voor 1080p of 1440p gamen is die 1160 een prima kaart en denk dat het mijn opvolger gaat worden van de 1060.
GTX 1660 Ti;3;0.5527681112289429;Ik heb nog even bij de buurman gekeken (guru3d) en met een goeie overclock heb je ongeveer GTX1080/ RTX2060 performance, mocht de kaart voor 275 euro te koop zijn, dan is het denk ik wel een interessante keuze, nu is overclocken in een laptop iets minder relevant, maar dat terzijde.. De 2060 voegt dan naar mijn mening niet veel meer toe en lijkt deze kaart inderdaad de betere keuze. De kans bestaat natuurlijk wel dat als er laptops uitkomen met de GTX1660 chip dat deze gedownclocked zijn, de vraag is dan hoeveel procent de kaart dan nog sneller is dan een 1060? is het dan nog financieel aantrekkelijk om je 'oude' laptop met GTX1060 te upgraden naar de 1660?
GTX 1660 Ti;2;0.4395063817501068;Die downgrade fase is eigenlijk niet meer aan de orde. Dat was nog in de tijd met de GTX960 en gtx960m, waarbij 'm' de mindere laptop versie was. De gtx1060 in mijn laptop is dezelfde als die ze in de pc's stoppen. Aangezien een 1660 bijna 50% sneller is dan een 1060, zelfs met een kleine downgrade blijft het een grote upgrade denk ik. Maar dan zijn er ook nog andere varianten dan de stock 1660. Msi en andere bedrijven kunnen de boel een beetje opschroeven dus moet die markt ook in de gaten blijven houden. Prijs van een 1660 is 279 dollar of 299 euro dacht ik.
GTX 1660 Ti;3;0.4963947832584381;Die chip mag wel hetzelfde zijn, afhankelijk van de fabrikant van de laptop kan de klokfrequentie nog steeds (flink) verschillen met de desktop modellen.
GTX 1660 Ti;4;0.4349662661552429;Het is dezelfde chip, maar dat wil niet zeggen dat hij op dezelfde kloksnelheid loopt. Vooral de koeling speelt ook een belangrijke rol. Genoeg laptops die het niet aankunnen waardoor de GPU vrij snel terug moet klokken (throttlen) om niet te heet te worden.
GTX 1660 Ti;5;0.5255282521247864;en met een goede overclock op een rtx2060 heb je ongeeer rtx2070 performance. De 2060 heeft 1900 cores vs de gtx1660 1500 cores, de 2060 kost 40 euro meer. 25% meer performance voor 15% meerprijs.
GTX 1660 Ti;5;0.522029459476471;Zelfde met de RTX 2070. Dat is nu wel de zweetspot kwa prijs en performance. Met een OC haal je zo stock 1080TI snelheid. Battefield V 1440p met alle toeters en bellen (ray tracing) 60fps. zelfde voor metro exodus helemaal met DLSS. En dat voor 500 euro terwijl de 1080TI veel meer kost en al die all die dingen niet heeft.
GTX 1660 Ti;3;0.3268807530403137;Nou ik heb een 970 en zie deze kaart opzich wel zitten! De 970 heeft het altijd prima gedaan maar hij mag toch echt wel vervangen worden ondertussen. Als ik de 970 voor +- 100 euro kan verkopen is het ineens niet zon dure upgrade meer.
GTX 1660 Ti;1;0.31837648153305054;"Dit lijkt inderdaad een goede koop voor gamers die elke tweede generatie een midrange kaart kopen. Dat is heel veel goedkoper dan elke generatie de topkaart kopen - niet alleen zijn die duurder, maar dan geef je ook veel vaker geld uit. De 970 is al weer 4 1/2 jaar oud; elke 50 maanden een kaart van 300 euro kopen is maar 6 euro per maand. Maar om die reden is je 100 euro vraagprijs voor eee 970 optimistisch. Misschien als je'm op heel korte termijn verkoopt, maar er gaan er dus veel op de tweedehands markt komen. En je hebt rond dat prijspunt concurrentie van 980's en 980Ti's, afkomstig van de nieuwe bezitters van RTX kaarten."
GTX 1660 Ti;3;0.4282785952091217;Ik denk dat het ook zal afhangen van de 2e hands markt. Ik denk dat een 2e hands 1070 of 1080 dan prijs/performantie gunstiger gaat zijn dan een nieuwe 1660ti
GTX 1660 Ti;1;0.6250147819519043;Op V&A is de goedkoopste 1080 al 55 euro duurder dan de goedkoopste nieuwe 1660. Zelfs voor een 1070Ti ben je 22 euro duurder uit, tweedehands. Pas als je naar de 1070 gaat kijken vind je goedkopere modellen. Inderdaad, 1070 < 1660 < 1070Ti is ook de volgorde in het artikel. Maar je hebt dus voor feitelijk geen meerprijs een nieuwe kaart. Als je toch voor een tweedehands kaart gaat: afdingen.
GTX 1660 Ti;3;0.4106059968471527;De prijzen waarvoor kaarten worden aangeboden tweedehands zijn vaak niet de uiteindelijke verkoopprijzen, die liggen lager. Een gtx1070 tweedehands koop je voor rond de 220 euro, toch dik 100 euro goedkoper dan een nieuwe 1660, plus je hebt 8gb vram wat de komende jaren nog een verschil kan maken tegenover de 6gb van de 1660
GTX 1660 Ti;2;0.28429320454597473;Ik heb zelf ook een 970 en zit dus ook te twijfelen om deze kaart te kopen. Maar hierboven is al aangegeven dat de RTX 2060 ook een goeie keus is omdat je voor 15% duurder 25% meer performance krijgt. Daar zit ik nu dus echt over te twijfelen. Iemand die mij daarover advies kan geven ?
GTX 1660 Ti;2;0.4627171754837036;uhh, maar de GTX1070 is NOOIT voor een prijspunt van 300 euro geweest, ding is amper ooit onder de prijs van 400 euro geweest (en dan waren het al extreme aanbiedingen of de minimale versies). En de reden dat je niet de 1070 hebt gekocht is niet omdat je vond dat de prestaties van de 1060 goed waren, maar omdat de prijs van de 1070 gewoon te hoog was, want was de 1070 de prijs van de 1160 dan had je zeker wel de 1070 gekocht. Het is maar wat je overbodig noemt mbt raytracing, de 2060 is ook nog steeds wat sneller, dus EN meer prestatie EN meer opties. Maarja wil je 4k gamen, dan zijn geen van deze kaarten echt interessant, tenzij je alles op low zet.
GTX 1660 Ti;1;0.3791603446006775;Feit is dat er vanaf begin dit jaar een GTX 1070 (Zotac mini) te koop is bij de mediamarkt voor 275 euro, helaas alleen bij de duitse fillialen van de Mediamarkt.
GTX 1660 Ti;1;0.3898893892765045;Staat er nu nog voor 289, je vraagt je af hoe het kan als die op andere plaatsen daarna minimaal 100 euro duurder is. Dat is dan ook een extreme uitzondering, weet niet of ze naar nl verschepen. Maarja, nu ben je dan toch eigenlijk wel beter af met een 1160 mits je die voor 300 kunt krijgen
GTX 1660 Ti;3;0.3744754195213318;Nieuw was een 1070 destijds inderdaad een stuk duurder, maar een tijdje terug was er al een 1070 voor 250 euro, (geen Ti) Ik doelde er meer op dat mijn upgrade vanaf een gtx1060 eerder een Rtx2070 zou zijn. Nu is deze inderdaad stukken duurder en ik dacht dat de 1660 rond de 350 euro zou kosten.. Maar als de prijs nog iets zakt word de 1660 toch wel een interessante upgrade vanaf mijn 1060.
GTX 1660 Ti;3;0.32850557565689087;Denk dat nvidia voornamelijk de 10xx kaarten wil uitfaseren.
GTX 1660 Ti;3;0.38353657722473145;+ je krijgt meer performance voor dezelfde prijs.
GTX 1660 Ti;2;0.3683938980102539;Ze moesten toch iets doen als de rtx reeks niet goed verkoopt.Toveren ze gewoon iets nieuwe uit de mouw laten gewoon terug gtx noemen dat lusten ze wel.
GTX 1660 Ti;1;0.49028244614601135;Nou deze kaart heeft een MSRP van $280 terwijl de 1070 volgens mij $380 ofzo was en de 1070ti nog hoger?
GTX 1660 Ti;5;0.40179529786109924;Wat dacht je van mensen met ongeveer 300 te besteden voor een videokaart? Dan is dit de beste keuze.
GTX 1660 Ti;1;0.4768024682998657;Nou deze kaart is <€295 en daar heb ik een GTX1070 nog nooit voor gezien, laat staan een 1070ti. Ik neem aan dat dit nog gaat dalen het komende jaar (mocht er geen mining revive komen). Dus als je 1440p wilt gamen maar niet €400 uit wilde geven aan een GTX1070 en niet wilt spelen met een Vega of de €450 uitgeven voor een goede Sapphire nitro is de 1660ti je kaart.
GTX 1660 Ti;4;0.38903379440307617;Ik was al een tijdje opzoek naar een opvolger voor mijn gtx970, deze lijkt een goede optie te zijn.
GTX 1660 Ti;4;0.6262498497962952;Goede informatieve review. Dank hiervoor Tweakers. Interessante kaart als upgrade voor mijn 970. Gemiddeld zo'n 40% betere prestaties voor -hopelijk- iets rond de 300 a 330 euro zie ik wel zitten. Ik hou de pricewatch extra goed in de gaten de komende weken
GTX 1660 Ti;1;0.601553201675415;De 1e die uit zijn gekomen zijn +- €340,- en voor 4 tientjes meer heb je een RTX2060
GTX 1660 Ti;3;0.4391312897205353;"Hier een lijst van de beschikbare 1660Ti kaarten bij Alternate. De goedkoopste is 299,- en de duurste 349,- Launch prijzen dus. Dat gaat zeer waarschijnlijk wel dalen na een tijdje. En ja, ik hou ook de RTX2060 in de gaten; Prima kaart inderdaad. Alleen met zo'n 380 Euro voor de goedkoopste vind ik dat net even te prijzig."
GTX 1660 Ti;3;0.37770161032676697;inmiddels zijn er meer zag ik net ja, alleen is PW niet bijgewerkt 1660Ti's bij Azerty ook mooie mini-itx. dan denk ik nog, voor 8 tientjes meer liever een RTX2060. maar als je budget €300,- is, heb je er een mooie kaart aan.
GTX 1660 Ti;3;0.4166874289512634;Zotac AMP is €369 op amazon, krijg je kleine compacte doch redelijk fabrieksoverclock.
GTX 1660 Ti;2;0.3747404217720032;precies, voor 4 tientjes meer heb je 25% procent meer performance, de 2060 is een betere koop.
GTX 1660 Ti;1;0.4525986313819885;Waar haal je die 25% vandaan? Uit de HWI prestatie index kom ik uit op gemiddeld 10%, tot maximaal 14% op 4K ultra.
GTX 1660 Ti;2;0.32996779680252075;25% meer cores
GTX 1660 Ti;3;0.5172696709632874;Ik vind het wel tegenvallen, marginaal sneller dan een gtx1070, Kan je dan niet veel beter een 2060 kopen?
GTX 1660 Ti;1;0.30229631066322327;Of gebruikte 1070 ti genoeg te koop voor mooi prijsje 250 euro meer performance.
GTX 1660 Ti;3;0.4961133599281311;Ik vind het ook een goede informatieve review, alleen ik kom tot een compleet andere conclusie... Namelijk dat deze kaart weinig bestaansrecht heeft. Alleen als je precies genoeg budget hebt voor de 1660 Ti, maar niet voor de 2060, dan is de 1660 Ti een goede optie. Tenzij het ding nog een stuk in prijs zakt kun je veel beter een 2060 kopen (zeg ik terwijl ik RTX weinig waard vind).
GTX 1660 Ti;2;0.48420506715774536;Sorry, maar als je over 'iets duurder' praat voor een Vega56... dan varieert dat met slechts een kleine ~11-15 euro bij meerdere winkels tot aan een edities die 200 extra kosten. Voor 15 euro vraag ik me zeer af of je dan niet gewoon even moeite moet doen met een Vega56 te zoeken of een generatie ouder te pakken. De conclusie is mij wat te positief gesteld voor een Nvidia kaart die bijzonder onnodig lijkt te zijn.
GTX 1660 Ti;2;0.3834894597530365;Waarom is die kaart onnodig? Een Vega56 is dus ruwweg vergelijkbaar in prijs, en zeer vergelijkbaar in prestaties. Waarom zou je dan per se die kopen, dan is het op het eerste gezicht toch gewoon een kwestie van voorkeur? Je krijgt dan wel weer 3 games bij een Vega56, maar zijn verbruik is ook echt serieus hoger. En even los van elektriciteitskosten (wat niet enorm is, maar wel optelt), je hebt dan ook een zwaardere voeding nodig, je kast moet beter zijn warmte kwijt kunnen, en waarschijnlijk is een 1660 gewoon stiller omdat hij minder hoeft te koelen.
GTX 1660 Ti;1;0.4040757119655609;kuch zwaardere voeding kuch 450-550W heel zwaar...... enkele eur verschil warmte heb je een punt, maar dan nog 1 enkele 80-92cm fan dezer dagen of een 120cm fan enkel in je voeding is meer dan voldoende om die boel buiten te trekken
GTX 1660 Ti;3;0.31365713477134705;"120cm ja? Dan moet jij wel een flinke kast hebben ;-)"
GTX 1660 Ti;2;0.46026673913002014;Ik hou niet zo van dat stofzuigergeluid in huis, daar moet je echt een liefhebber van zijn. Ook dat vervelende feit dat fans slijten is voor mij niet zo aantrekkelijk. Maar ja, dan heb je wel een AMD die verder... uhm... waaropm? Ik ben echt geen nVidia fan per se, maar je suggestie dat de 1160 onnodig is vanwege een bestaande kaart die objectief gezien gewoon aanzienlijk slechter is snap ik niet.
GTX 1660 Ti;1;0.6794577240943909;Niks met fan te maken... die Vega56 komt te vaak boven die 1660 uit in prestaties... en is vergelijkbaar in prijs. Jouw 'objectief aanzienlijk slechter' zie ik niet in die grafieken die Tweakers aanlevert, of je focust serieus op een paar grafieken die jij belangrijker vind. Meer prestaties en dus hitte en dus geluid tegen een paar cent stroom meer is logisch, ook daar zie ik voor deze kaart geen winst over een V56. En ik noem deze 1660 nog steeds onnodig als segment vuller, want dan kan je imo beter een 1070 kopen of die werelds euro's stroom overwegen voor een 1080.
GTX 1660 Ti;2;0.41638490557670593;Objectief is nou eenmaal dat hij warmer wordt, dus meer geluid maakt en de fans harder slijten. Je aanhalingstekens kan je dus gerust weglaten, Het is vrij simpel. of je wil snel maar staat meer herrie toe (en dat gaat in de loop der tijd bij elke kaart toenemen), of je wil dat niet. IK hou niet van de stofzuiger. Jij vindt dat prima. Dat is helemaal niet erg, daarom zinn er keuzes... en daarom is de 660 echt zo gek nog niet. Heel makkelijk te bepalen als je in plaats van emoties over een merk laat meespelen gewoon kokkt naar de waardes. Het is zoals het is, niet zoals we willen dat het is.
GTX 1660 Ti;3;0.5777661204338074;leuk verhaal, jammer van de meningen die je als feiten probeert af te doen. ten eerste slijten fans niet harder puur omdat ze een ander type werkpunt behandelen. dezelfde fan slijt harder als die harder moet draaien, maar verschillende soorten fans slijten niet harder puur omdat jij iets hoort qua toerental. gezien de hier besproken 1660 axiale fans heeft en de hier inmiddels 2 jaar oude reference vega56 een radiale fan gebruikt, zijn ze niet 1 op 1 te vergelijken in levensduur bij de verschillende toerentallen. daarbij komt nog kijken dat radiale ventilatoren beduidend hogere toerentallen kunnen leveren ( vooral ook omdat ze meer gericht zijn op luchtdruk ), dus die hogere snelheid levert niet zozeer minder levensduur op... de fan is ontworpen voor een ander toerental en slijt niet harder puur omdat het geluid van de koeler anders is. nog beter gesteld, er is geen enkele reden om aan te nemen dat een van de twee type fans sneller stuk is als ze allebei werken op het werkpunt waarvoor ze zijn ontworpen.... zowel de 1070 ( 345 ), die enkele 1080 ( 370, die versie wil jij niet, die maakt herrie ) en zelfs de 2060 ( 367 ) zijn gewoon betere kaarten die je beter kan kiezen dan deze 1660 kaart. daarbij heeft amd een vega56 ( 319, in de versie van axiale ventilatoren dus niet het geluid ) in dezelfde range en die is logischer voor dit bedrag dan deze 1660. ga je naar 50 euro duurder zit je op een 2060 als de betere keuze of als je wel met geluid kan leven zelfs een 1080, ga je naar 50 minder... dan zit er een gat die veel logischer was om op te vangen. beter dan de 590, iets minder goed dan de v56, maar goedkoper en zuiniger. dan positioneer je de kaart logisch. voor 300 euro is deze kaart nutteloos, hij concurreert hoofdzakelijk met de eigen producten en biedt amper iets over de amd concurrenten ( want de axiale versies zijn niet zo luidruchtig als jij stelt, hooguit groter om die warmte op te vangen... ).
GTX 1660 Ti;2;0.5867941379547119;Dat klinkt zo logisch, maar is misleidend. Welk design je ook kiest, het geluidsvolume binnen dat design is absoluut bepaald door de warmte van de kaart. En dat is voor de hele discussie direct relevant. Als een fabrikant een zelfde soort koelsysteem als dat van de Vega neemt is het resultaat dat de kaart stiller is en de fan minder toeren maakt. Ja, fans (lagers) die harder draaien en/of zwaarder belast worden slijten sneller, kwestie van slijtage leer. Wat uiteraard waar is dat lagers onderling wel weer van kwaliteit verschillen, maar tenzij AMD betere lagers gebruikt dan nVidia komen we weer terug op de hoeveelheid warmte die verplaatst moet worden. 1. Hij vervangt een eigen model. Dat heeft wellicht kostentechnische voordelen (een lijn chips die gebint worden?). Qua straatprijs is hij nu al relevant tegenover o.a. een 1070, dus te duur is een rare opmerking als je ziet dat je hier geld bespaart. 2. Uiteraard concurreert hij direct met een AMD kaart. Jij zelf geeft al aan dat de Vega superieur is, dus je impliceert dat hij concurreert Nogmaals, er is helemaal niks mis met jouw keuze voor de Vega, maar dat betekent niet dat deze kaart niet prima past in het gamma van nVidia. En of je het nou leuk vind of niet, de hogere temperatuur van de Vega is gewoon een nadeel (en een feit).
GTX 1660 Ti;2;0.4472360908985138;Je zoekt het maar uit hierna, dus heel kort: - geluidsvolume heb je geheel fout, het design van luchtdoorstroming bepaalt het geluidsvolume en kan gewoon aangepast worden naar een stil design. - een axiale fanlager vs. een radiale fanlager is niet vergelijkbaar in absolute toerentallen qua slijtage of zijn effecten. Heeft niets met 'betere lagers te maken', maar hoe de krachten slijtage in specifieke richtingen veroorzaken. Dat verschil is ook waarom de sneller draaiende radiale fan toch even lang mee kan gaan als de axiale fan, niks dat een radiale blower eerder stuk is omwille van het type fan. - Ik heb geen Vega56 en ook niet voor gekozen. Het nut van deze kaart staat los van wat ik zelf heb en wat ik over deze 1660 vind.
GTX 1660 Ti;2;0.4947710633277893;Vega is wel een stuk sneller en iemand die klaagt over een verbruik van een Vega kaart heeft er duidelijk geen ervaring mee. Volgens reviews wordt mijn VII heet en is hij zeer luid en gebruikt hij meer dan 300 watt, in realiteit met een simpele undervolt is hij koel en niet eens hoorbaar en verbruikt hij evenveel als een RTX 2080. Reviews, clickbate, maar weinig nuttigs... Iedereen die een Vega koopt doet een undervolt en overclock. Wie het niet doet, die heeft nog niet gehoord van google vrees ik.
GTX 1660 Ti;2;0.4951990246772766;Ik wist dat die gaat komen. Want het is altijd hetzelfde liedje bij AMD fanboys (en voor de duidelijkheid, ik heb zelf een AMD R9 Fury, en op één tijdelijke GTX960 na heb ik eigenlijk altijd AMD kaarten gehad). Ik durf wel te stellen dat iig bij gewone Vega kaarten (VII is zo exclusief dat het misschien anders is) veruit het merendeel hem gewoon zal draaien en niks meer. Volgens mij overschat je het aantal mensen dat direct core voltages gaat aanpassen enorm, fora en reddits laten altijd een scheef beeld zien. En het blijft altijd dus het verhaal dat een geundervolte en overklokte AMD moet vergeleken worden met een stock Nvidia. Dat is nou niet bepaald een eerlijke vergelijking. En als AMD meer te undervolten is als Nvidia, is dat alleen maar omdat Nvidia hun processen en chips beter onder controle heeft.
GTX 1660 Ti;1;0.3907064199447632;Klopt. Je moet Vega kopen als je het niet erg vind (of zelfs leuk) om te tweaken. Als je zover durft te gaan als bios flashen, kom je zelfs in de buurt van een reference 1080. Dat gaat je met een 1660 niet lukken
GTX 1660 Ti;1;0.27408522367477417;Eigenlijk is het helemaal niet eng, check in gpu-z of je Samsung memory hebt en dat is het eigenlijk. Je hebt dual bios dus je kan altijd terug naar een werkende bios. Er kan letterlijk niks mis gaan behalve als je dom genoeg bent om beide biosses te bricken
GTX 1660 Ti;1;0.5071073770523071;In het lage segment, volledig akkoord. Maar Vega? Komaan zeg, dat laat je toch nooit op stock draaien... Mensen die Vega kopen deden dat om te minen of om het rode alternatief van 1070 of 1080 te kopen. Die groep mensen, zijn tweakers... Vega is pas sinds een paar maaanden terug voor aanvaardbare prijs te kopen. Iedereen die er vroeger een gekocht heeft, wist wat te doen met de kaart. Want op stock zijn ze echt niet concurrerend met Nvidia. En Nvidia doet het beter met een veelvoud van het R&D budget. Shocker...
GTX 1660 Ti;2;0.45540255308151245;Sowieso is het argument dat je elke Vega moet tweaken heel vreemd. Waarom verkoopt AMD kaarten die met wat veranderde instellingen opeens gehakt maken van de concurrentie. Dat ligt niet aan R&D of nVidia die gemeen is... dat is AMD die gek is... of misschien... misschien... misschien is het verhaal hier ietsiepietsie overdreven en rooskleurig en zijn er wel degelijk nadeeltjes die we hier niet bespreken?
GTX 1660 Ti;1;0.5414338707923889;Ik begrijp ook niet waarom iets als hitte en geluid (effect van stroomverbruik) opeens tot zoveel geagiteerde gebruikers leidt. Alsof je maar een rare bent die dat minstens zo zwaar vind wegen dan een paar procent meer frames.
GTX 1660 Ti;2;0.5731959342956543;Niemand wil undervolten en de kans hebben dat de chip instabiel word. hoe klein ook het risico is er en dat vind ik onacceptabel. Daarbij is hij zelfs bij undervolten nog meer powerhungry en dus kost het op lange termijn meer.\ ook dat moet je meerekenen en dat maakt ze al snel minder interessant. De Slimme gamers willen gewoon plug en lay en werken zonder risicos en hogere lange termijn kosten. Dat voltage is zo hoog omdat de QQ veel minder is als bij nvidia en ze dus niet zeker wewten of iedere chip wel in staat is om op een laag voltage te draiien. dat betekend dus dat er chips zijn die op zijn gat gaan als het voltage laag is en je zal maar net een even dure of duurdere amd chip gekocht hebben met de hoop te kunnen undervolten en goedkoper uit te zijn blijkt dat je chip niet stabiel is met een undervolt en zit je mooi met een hoge stroomrekening bovenop de al te hoge prijs. undervolten is voor de masselaar tweakers die een risico acceptabel vinden niet voor de massa.
GTX 1660 Ti;1;0.6259698271751404;Die hoge stroomrekening is alleen werkelijk aanwezig als je aan minen doet. De werkelijke stroomverbruiken zijn simpelweg hilarisch en men heeft gewoon echt geen 'benul' van hoe laag die kosten zijn. 100W meer stroomverbruik op 2 uur gamen elke dag is slechts €16.79 op jaarbasis... Nu, hoeveel hier zijn werkelijk elke dag opnieuw 2 uur aan het gamen op maximaal vermogen en dan... 100W is dus heel ruim genomen om elke discussie weg te nemen. Dus, tenzij je 8760 uur per jaar aan het minen bent, is die paar Watt vermogen niet de discussie waard. Dan is hooguit je thermische ontwerp van je case relevanter.
GTX 1660 Ti;1;0.2825465202331543;16 3uro x 3 jaar maakt de gpu dus ecfectief 48 euro duurder dat is significant als je het mij vraagt. En gezien er velen zijn die meer als 2 uur per dag gemiddeld gamen als jij denkt.. Het dient dis gewoon meegenomen te worden in je aanschafprijs.
GTX 1660 Ti;2;0.40348073840141296;"Ja, 48 euro duurder, maar die V56 is ook iets beter met uitzondering van een paar specifieke titels. Op basis van die resultaten zou ik meer kijken naar de ""per game"" kaart en eigenlijk... Ik zou de 2060 zelfs nog eerder nemen dan zowel de V56 en dan pas de 1660...De 1070 lijkt me ook nog beter, maar dan kies ik persoonlijk eerder voor een nieuwer model dan een twee jaar oud model (en daarmee zowel de V56 als 1070 die wegvallen). Overigens, twee uur elke dag... is iets wat je alleen kan doen als je nog op school zit... Zelfs tijdens mijn universiteitsjaren was 2 uur veel voor elke dag en dan waren er nog brave 20 uur sessies in het weekend ter compensatie. Als het argument hier wordt gegooid ""dat undervolten iets is voor Tweakers"" dan is meer dan 2 uur ook iets voor gamers/Tweakers en niet terecht in een normaal vergelijk En als we het hier hebben over thuis-woners/scholieren/studenten dan mag die 'stroomkosten'-argumentatie ook vervallen want de meeste daarvan betalen de stroom niet. Maar voor +48 euro extra heb je een betere kaart... en nee als je die direct meeneemt en niet aan undervolten doet / de nogal debiele FE settings van 2 jaar geleden even corrigeert, dan kom ik ook niet uit op die V56 hoor - Goedkoopste 1080 voor 370 - Goedkoopste 2060 voor 367 - Goedkoopste 1070 voor 345 - Goedkoopste V56 voor 319 + 48 (op dit moment 19 ipv 15) - 1660 tussen 305 en 345 in pricewatch. Als je de keuze uit bovenstaande hebt... ga je dan werkelijk een 1660 overwegen? Tenzij je budget het inderdaad niet toelaat om hoger te gaan, is dit wel een extreem middenmoot kaart om uitsluitend de 1060 te vervangen... en dan ook alleen als je de goedkoopste versie pakt, want de duurste is 345 momenteel en dan is het 22 euro voor een 2060.... kom op! De kaart is te duur met 300-345 en had 250 moeten kosten, dan was het een nuttige kaart geweest."
GTX 1660 Ti;1;0.4402962923049927;Waarom zou ik moeite doen om mijn videokaart te gaan tweaken als de drivers dat al niet zelf doen, leuk voor tegen het einde van het leven. Ken maar weinig mensen die een videokaart kopen en dan dat ding gaan tweaken. En als die kaarten standaard fatsoenlijk draaien op die aangepaste instellingen, waarom levert AMD ze dan gewoon niet meteen met die instellingen?
GTX 1660 Ti;1;0.5170719027519226;amd heeft dat juist in de drivers zitten en komt gelijk met dergelijke voorstellen om deze optimalisaties te activeren... met 1 druk op de knop is het geregeld. kijk maar laatste video van adoredtv. in dat opzicht hebben ze dat wel goed geregeld, de kaart unvolt gewoon automatisch. waarna deze inderdaad een stuk zuiniger is met gelijke prestaties. bij amd activeren ze het alleen niet standaard al, en dat betekent dus ook dat alle reviews een hoog stroomverbruik laten zien terwijl dit veel beter is in werkelijkheid. alle kaarten komen dus uit de fabriek rollen met hoge voltages, om er zo zeker van te zijn dat ze allemaal stabiel draaien. dat wil dus ook zeggen dat het niet 100 % zeker is dat de kaart efficient undervolt, een slecht exemplaar zal het mogelijk moeten doen met die hoge voltages. nvidia bakt dit wel gelijk al in de gpu, daaorm klokt ook elke kaart ( ookal is het dezelfde videokaart ) anders omdat niet elke gpu exact even puur is. blijft er voor de tweaker wel heel weinig speelgoed over. dat maakt een amd kaart mogelijk leuker om mee te klooien, maar voor mij voorlopig even geen amd kaart meer, ik heb nogsteeds een 290x heethoofd en ga denk ik maar voor de 2060 of 2070 ofzoiets... ik gok tevens dat amd deze manier hanteert voor de radeon vii omdat het eigenlijk van origine helemaal geen gamers kaart is. bij een videokaart voor workstations zal het een worst zijn dat deze wat meer stroom verbruikt. als deze maar ten alle tijden stabiel is. de optie om de kaart zo makkelijk te undervolten zal er wel als last - resort voor ons gemak in gemikt zijn. wat tevens weer laat zien hoe alles gerushed is, want de kaart was een stuk beter uit de bus gekomen als ze de kaart standaard automatisch laten undervolten. dan was de kaart verrassend zuinig voor een amd kaart met dergelijke prestaties! maar nu denkt iedereen daar totaal anders over. slechte marketing zet weer van amd...
GTX 1660 Ti;2;0.30991503596305847;Ik moet als gebruiker dus nog steeds moeite doen om iets dat gewoon default zou moeten zijn aan te zetten. Dat jij dat via videootjes allemaal wilt gaan instellen is leuk voor je, maar ik heb geen zin om al die instellingen af te moeten gaan om te kijken wat nu het beste werkt, want dat betekent dus ook dat ik me moet gaan verdiepen wat die instellingen allemaal doen. Het zou dus gewoon goed moeten werken 'out of the box' zonder dat ik als gebruiker nog een hoop moet doen.
GTX 1660 Ti;2;0.5028122663497925;"Vergeet niet mee te rekenen dat de Vega56 wel ~100w extra aan stroom verbruikt onder belasting; dat is relatief best veel. Stel dat je dagelijks gemiddeld 2 uurtjes gamet per dag en de kaart ~3jaar gebruikt voor je een nieuwe koopt. Dan heb je het over die periode toch over 50 euro extra aan elektriciteitskosten alleen. Daanaast heb je ook een iets zwaardere (en dus duurdere) voeding nodig en mogelijk meer koeling in je kast. Neemt niet weg dat ik deze GTX1660ti ook erg vind tegenvallen (tenzij de prijs snel onder de 300 gaat zakken; maar dat verwacht ik niet); een GTX1070 is inmiddels bijna 3 jaar oud en deze word nog steeds niet verslagen door een nieuwere kaart één segmentje lager."
GTX 1660 Ti;3;0.39183270931243896;Volgens de review wordt de 1070 juist wel verslagen en tegen een lager prijspunt, snap niet helemaal welk punt je probeert te verdedigen?
GTX 1660 Ti;2;0.5258120894432068;"Prestatieverschillen van kleiner dan pak hem beet ~5% zie ik persoonlijk als gelijk binnen de foutmarge; al is dat een kwestie van persoonlijke interpretatie natuurlijk Maar ja ik vind het over 3jaar een vrij matige verbetering inderdaad. Als je terugkijkt naar de GTX470 vs GTX760 bijv. - ook één segment lager na 3 jaar - dan is laatstgenoemde gewoon 2x zo snel Maar misschien moet ik er maar aan wennen dat het qua ontwikkelingen niet zo snel maar gaat"
GTX 1660 Ti;2;0.4610568881034851;Dit dus, men loopt nu al een paar jaar er tegenaan dat het gewoon vrijwel onmogelijk wordt kleinere transistors te maken. Laten we niet vergeten dat de pentium 4 nog op 90nm zat en graka's uit die tijd op 110/130nm. Daarna hebben we elke 2 jaar shrinks gehad tot aan ~14nm en sindsdien gaat het trager, 1 die shrink in wat, 4 jaar?
GTX 1660 Ti;2;0.502408504486084;Vega is nog steeds niet lekker op vooraad, Asus model is nu 350 en ik heb hem geflashed naar een V64, draait nu 1050MHz HBM en moet 1,05V rond de 1570MHz ingame. Core 63gr, HBM 67 en hotspot VRM 94C op 2300RPM fan. Conclusie: Wil je niet kloten haal de 1660ti, je kan minder ermee spelen maar je verbruikt de helft van de stroom en is bijna even snel en veel stiller. Wil je wel kloten óf AMD iets meer ondersteunen haal een V56 of wacht nog 5 maanden op Navi (hopelijk). Je kan trouwens niet alle V56 flashen, volgens mij meestal alleen de Samsung geheugen chips. Ik denk trouwens dat de V56 wel iets toekomstbestendiger is met 8GB geheugen en in het verleden zijn AMD kaarten ook veel beter oud geworden (kijkt naar de GTX680 vs de HD7970).
GTX 1660 Ti;5;0.4844687879085541;"Met een Vega56 hoef je niet eens te ""kloten"" om die sneller te maken. De niet-referentie versies zijn standaard vaak al sneller dan stock. Daarnaast kan je ook zonder de bios te flashen, maar met undervolten al behoorlijk in de buurt komen van Vega64 prestaties."
GTX 1660 Ti;2;0.4560849070549011;Nou ja maar dan is die iets sneller met 3x t stroomverbruik. Zonder flinke hbm overclock is het niet waard imo.
GTX 1660 Ti;3;0.4791451692581177;De Vega56 is zelf goedkoper in de pricewatch, de conclusie is zeker te positief opgesteld. Hij had beter kunnen concluderen dat de GTX1660 een betere optie is mits die kaart onder de 315 euro zit. Qua stroomverbruik is het sowieso een betere kaart. Dat lijkt mij een objectievere conclusie. Wel grappig dat de vega64 niet is meegenomen, aangezien de 1080FE wel is meegenomen. Waarschijnlijk kan die kaart nu ook beter meekomen na de driver optimalisaties.
GTX 1660 Ti;2;0.255583256483078;Dat niet alleen wat me opvalt is met name in de hogere 4 k resolties dat de 1080 nog zo een 25 a 45% sneller is als de gtx 1660 ti.De waardes worden verwaarloosbaar in de full hd resolutie. De 1660 zit in het midden segment en volgens mij heeft Nvidia deze videokaart gemaakt om precies in dat segment te laten vallen.Om het midden segment te bestrijken met hun videokaarten lijn. Binnenkort van plan om een nieuwe computer te kopen deze kaart is misschien precies wat ik nog zocht. Nog net niet zo duur als een 1080 er.Petje af voor amd zij zitten er net boven.
GTX 1660 Ti;2;0.26732689142227173;Als een zware AMD fan, heb ik gekozen voor Nvidia. Puur omdat ik van de stroomrekening van mijn AMD kaart vorig jaar praktisch een 1080 kon kopen.
GTX 1660 Ti;1;0.6833256483078003;Dat kan alleen als je aan het minen was... 100W is 16,79 per jaar aan extra stroomkosten. Compleet onzinnig om een 1080 te suggereren, die niet eens 100W zuiniger is
GTX 1660 Ti;1;0.5012276768684387;Ik weet niet hoe jij aan die stroomkosten komt? Mijn PC heeft afgelopen jaar ongeveer 1400kwh verbruikt. Dat is nu meer dan gehalveerd. Met de huidige stroomprijzen komt dat neer op zo'n 322 euro (totaal), / 2 is 166 euro. Mijn vorige kaart gebruikte overigens 415W dus het verschil met een 1060 is nogal huge. De idle van mijn huidige 1060 is like 15w en van mijn vorige kaart was 90w... Load scheelt 250w
GTX 1660 Ti;1;0.4680773615837097;Referentie waarden staan in de tabel door Tweakers als 100W meer bij belasting. Die zegt 100W verschil bij maximale belasting in-game. Pure idle is flink discutabel welke bron je pakt... Tweakers komt anders uit dan Linus, Jay, Anand etc. en in alle gevallen praten we over 5-10 W meer verbruik tussen die V56 en de 1660 per uur. Normale mensen laten de pc niet de hele dag non-stop aan staan... Daar komt ook het euvel, wij zijn hier niet normaal. 100W met 2 uur gamen per dag (voor normale mensen), elke dag, komt je op 16.79 bij dezelfde 23ct. kWh prijs. Ga je nu suggereren dat je meer dan 2 uur per dag vol-belasting aan gamen doet op dergelijke titels, ga ik als tegenargument geven dat je dan geen gemiddeld persoon bent en wel degelijk kan undervolten/tweaken en lagere verbruiken zal realiseren. Het 12 jarige buurjongetje, wat op zijn klote krijgt als hij te lang gamed, kan het... Maar je maakt me wel nieuwsgierig, 415W op de kaart, 90W idle op de kaart??? Wat had je draaien? Net zoals, 1400 kWh betekent dat jij je pc nooit uitzet? en ook daar valt de discussie over te starten of je dat mag gebruiken als 'normaal gebruik'. Niet dat ik anders ben, maar daarom gebruik ik ook niet mijn uren als referentie
GTX 1660 Ti;1;0.38703715801239014;Ik gebruikte de sapphire r9 290 vapor-x editie, één van de eerdere uitgaves welke vervangen is ivm onstabiliteit door de hoge overclock. NIET de tri-oc editie welke je vaak vind als je er op zoekt. Ik heb een wattage meter tussen mijn setup en de muur hangen, en mijn wattage is nu tijdens lichtere gamen (denk rocket league) lager dan mijn setup met mijn vorige kaart idle draaide. Dat zijn gewoon harde zelf-tests en geen benches op het internet. Mijn vebruik is gedropt van idle 200-250w naar idle 90-120w en tijdens rocket league van 350-400w naar 170-200w. Met zwaardere games zoals space engineers, is mijn wattage gedropt van minimaal 550w naar ~350w. Hiervan pak ik (ivm verschillende loads mbt waar ik aan werk en/of ik game) het gemiddelde verbruik, dat bereken ik vervolgens op 16u per dag, 365 dagen per jaar (en het gebeurd ook dat ik mijn PC enkele dagen aan een stuk op load heb draaien) -- zo kom ik aan een ongevere schatting. Aan mijn eindejaarsafrekening te bekijken, zit mijn schatting in ieder geval binnen 10% van het daadwerkelijke verbruik. Dat zijn behoorlijk grote verschillen. Ik game geen 2 uur per dag, maar ik werk thuis waardoor mijn pc wel een uurtje of 16 (minimaal) per dag aan staat. Undervolten enz. zijn zaken waar ik niet alleen geen zin in had, maar omdat ik tussendoor ook grafische werkzaamheden verricht zou dat al helemaal veel moeite kosten... Overigens heb ik nooit mijn switch benoemd. Ik heb die 1080 nooit gekocht en dat was ook wel wat overdreven. Ik heb een tweedehands 1060 gekocht. Dus van de r9 290 vapor-x naar een 1060 msi armor. Dus je hebt natuurlijk wel gelijk dat ik er geen 1080 van kan betalen, maar die 16,79 is wat ik even aan wilde vechten. Het komt voor mij neer op een goede 100-150 euro per jaar. -- Edit: en met de leeftijd van mijn vorige kaart (2014 geloof ik) * 5 is dat zeker 500 euro!
GTX 1660 Ti;3;0.43328142166137695;Hehe, Mijn verhaal ging tussen die 1660 en een V56 en hoe mooi een R9-290 als monster is dat vergelijk telt natuurlijk niet helemaal hier. Wat dat betreft zijn de idle nummers beduidend belangrijker (helemaal als je zoals jij 16 uur per dag actief bent). Ja, AMD is niet bepaald zuinig, maar zo'n voorbeeld als jouw versie kom je niet vaak tegen. Ik vind trouwens je idle nu nog steeds hoog, maar dat terzijde. Wil wel zeggen dat een idle van 90W GPU wel een hele goede reden is om de kaart niet te nemen (kijkt half naar zijn oude HT-PC met een oude game-kaart die 24/7 aanstaat) en wat zuinigers te nemen als de pc vaak aanstaat. Mooi voorbeeld hoe we soms flink op de verkeerde info focussen.
GTX 1660 Ti;1;0.3439204692840576;Mijn idle wattages zijn gebaseerd op mijn complete setup (2 schermen, speakers, alles aangesloten) 50-60w hiervan gaat naar mijn beeldschermen dus komt mijn PC nu idle neer op zo'n 50-60w. Best reëeel met 6 hdd's en 8 case fans Maar inderdaad. Op het internet is het zo makkelijk om elkaar nét verkeerd te begrijpen!
GTX 1660 Ti;2;0.5070264339447021;Ahhh... dan vind ik het laag met monitoren en case-fans... Mijn huidige Threadripper komt op ~100 W idle exclusief monitor... Zelfs met M2 drives ipv HDD's en 'maar' 6 fans en 1 waterpomp, maar ja... de Intel oplossing kost het dubbele dus die EL rekening kunnen we wel opvangen ermee Nee, AMD raad ik niet aan als je enigszins op je stroomrekening wil letten.
GTX 1660 Ti;2;0.4969828426837921;Hahahaha ja, met een threadripper heb je het ook ergens over... Jeetje. Ik zit op een 4790K waarvan ik de turbo boost heb uitgezet... Tot deze 4790K heb ik altijd AMD gehad, maar toen betaalde ik de stroomrekening nog niet! Ben overgestapt naar Intel omdat mijn phenomX2 955 black edition (@3.6ghz geloof ik) een akelige bottleneck werd. Veroorzaakte allemaal micro-lags en dat verpestte voor mij de 'smooth' ervaring. De FX serie vond ik niet interessant... Met Ryzen overweeg ik echter AMD weer!
GTX 1660 Ti;2;0.44596922397613525;Hehe, Ik zat nog op een Q6600... mijn stroomrekening is al jaren niet relevant geweest. Na 10 jaar overgestapt en ach, dan maar gewoon knettergek er vol in met een TR1950x. Grappig feitje, de 1950x draait zuiniger als je de boost uitzet en alle cores fixed op 1 snelheid (en break-even was op 3.8 Ghz bij mij). Raar gedrag, lijkt wel een AMD dingetje om te graag veel stroom te verbruiken. Wat betreft Ryzen, wacht even op die 3de serie die 4790K kan best nog een half jaar mee en dan heb je straks wel een hele mooie 16core voor minder geld dan ik hier
GTX 1660 Ti;1;0.35439854860305786;Hahaha ja helemaal mee eens! Mijn 4790K kan denk ik nog wel 2 jaar mee. Ding draait nooit op 100%. Het is echt een beest.
GTX 1660 Ti;1;0.5192943215370178;LOL, kundig met financiën en die verder kijkt dan een paar euro stroomkosten. Of je ene Vega56 wil of een 1070 / 1070Ti haalt, mij om het even, maar een 1660 toont geen reden tot nut.
GTX 1660 Ti;1;0.4160274267196655;Adviesprijs van €299 Winkelprijs €250??? Dat maakt het aantrekkelijk, maar niet 8800GT waardig Gisteren heb ik voor mijn neefje&nichtje voor €185 een 2e hands 1070 GTX met garantie gekocht. En ja dat is een dooddoener, nieuw met oud vergelijken, maar wie een 1660 Ti koopt, koopt dit met een budget en niet zozeer futureproofing. €118 euro betalen voor 4% hogere prestaties gaat er bij mij niet in..bij een dergelijke budget/segment
GTX 1660 Ti;1;0.4925799071788788;Dat is ook wel een gekke prijs hoor, normaal gaan ze rond de €225 weg (zonder garantie).
GTX 1660 Ti;1;0.5590827465057373;Ik heb ook voor 160 een 160 gekocht en zag ook tussen de 180-200 1070's staan. Niet de TI versie.
GTX 1660 Ti;4;0.2781260013580322;Benieuwd of een variant van deze kaart de opvolger gaat zijn van de 1050ti. Die zit in de huidige Dell XPS15. Opvolger komt binnenkort uit. Modelletje met 1660 erin zou heel leuke all-purpose-laptop zijn :-) Ontwerper heeft al gezegd dat de RTX2060 te heet wordt, maar de 1660ti gebruikt beduidend minder stroom...
GTX 1660 Ti;1;0.4819567799568176;die gaat vast en zeker komen als ze genoeg chip hebben met meer defecte cores dan nodig is voor een 1660TI Dit is volgens mij gewoon dezelfde chip als in de 2080TI zit alleen dan met defecte ray trace units en dermate veel defecte cores dat er geen RTX2060 meer van te maken is.
GTX 1660 Ti;1;0.32381290197372437;Ja ik hou het wel bij mijn 1070. Wat een onnodige kaart.
GTX 1660 Ti;1;0.5185337066650391;De 1660TI is goedkoper dan de 1070, dus hoezo een onnodige kaartr? Kijk ik snap dat IEDEREEN al een 1070GTX thuis heeft, dus ja volstrekt onnodig. Oh wacht. Dat is natuurlijk niet het geval er zijn wel degelijk mensen met een oudere kaart. Goh dat Nvidia toch een punt heeft deze kaart in de markt te zetten.. verbluffend. Ik kan er haast niet met mijn hoofd bij hoe bizar dit is.
GTX 1660 Ti;1;0.564229428768158;Ik ben het helemaal met je eens hoor, de rtx2060 had gewoon een prijs moeten hebben van rond de €300,- We zijn nu een half jaar verder sinds de introductie van de rtx20xx kaarten. Deze kaarten zijn blijkbaar niet interessant genoeg en worden slecht verkocht. Het probleem hierbij is dat de lagere kaarten eigenlijk niet snel genoeg zijn voor raytracing en in verhouding gewoon veel te duur zijn. Hierbij blijft Nvidia zeggen dat de rtx geen opvolgers zijn van de gtx series, zodat ze de rtx serie kaarten nog hoger en duurder kunnen aanprijzen. Iedereen roept hier hoe mooi de prestatiewinst is en vergelijken het met een gtx1070, waarbij de gtx1660Ti het minder doet op Ultra kwaliteit. We praten hier over een kaart die geïntroduceerd is eind 2016, wat nu al weer 2,5 jaar geleden is.
GTX 1660 Ti;3;0.4285005033016205;het is maar wat je onnodig noemt, jij hebt dan een 1070, maar ik zit nog steeds met een 760 omdat ik alle 1060 modellen tot nu toe nog veel te duur vond voor wat de prestaties die ze leveren, nog steeds niet eens een midrange kaart maar wel heel lang 300+ gebleven (de 1060/6GB variant, de 1060/3GB variant is een stuk minder weer).
GTX 1660 Ti;5;0.3683094382286072;ja ik hou het ook wel bij mijn 1080 ti, dit is echt een onnodige kaart
GTX 1660 Ti;2;0.46764495968818665;"Was meer gericht aangaande de conclusie: ""Kijken we naar de volgende stap, de GTX 1070, dan is er nog steeds een voorsprong, maar die is gereduceerd tot een procent of vier op 1080p en 1440p. Op 4k is de 1070 vooral op de Ultra-settings iets sneller, maar veel scheelt het niet, want alle prestaties gemiddeld genomen is de 1660 Ti nog altijd ruim 4 procent sneller dan de 1070."" Ik snap gewoon niet waarom deze kaart er is."
GTX 1660 Ti;3;0.41094523668289185;Omdat dit een opvolger is van de 1060. Ik snap niet wat hier lastig aan is
GTX 1660 Ti;2;0.4466564357280731;"Nee u heeft gelijk hoor, ik was wat snel met concluderen. Het is gewoon mijn gevoel dat zegt dat een nieuwe kaart gewoon ""beter"" moet zijn dan wat al enkele jaren op de markt is. Dat is inderdaad wat voorbarig."
GTX 1660 Ti;1;0.4135827422142029;Deze kaart is er niet voor jou. Je hebt destijds meer betaald voor die gtx 1070 dan 300 euro. Dus waarom zou jij voor 4% extra performance een 1660 ti kopen? Deze kaart is er voor de 1050-1060 gebruikers. Je krijgt 40% extra performance voor de zelfde prijs (gtx 1060 vs gtx 1660ti) En dan niet te vergeten de mensen die de 1000 generatie hebben overgeslagen. Die krijgen ook 40% performance voor deze kaart dan als ze een 1060 hadden genomen voor dezelfde prijs.
GTX 1660 Ti;5;0.4866105318069458;Mooi, tijd om de gtx 970 met pensioen te sturen naar mijn zusje.
GTX 1660 Ti;2;0.348945677280426;is een 2060 niet een veel betere koop?
GTX 1660 Ti;3;0.40669184923171997;Ik snap de prijs net als velen hier niet echt. Je hebt voor de prijs van een premium kaart inclusief een strix (of vergelijkbaar label) letterlijk ook een GTX 2060. Ik zou wel weten waar mijn geld dan naartoe gaat hoor. Zal interessant zijn wie dit ding nou uiteindelijk gaat kopen. Maar dat zal de toekomst ons leren. De afdelingen van Nvidia die over de prijs beslissen moeten toch wel even iets beter hun best gaan doen om dit tot een echt interessant alternatief te maken. Maar wie weet willen ze juist dat we allemaal RTX kopen en doen ze het daarom zo? (Zet een aluminiumfolie hoed op.)
GTX 1660 Ti;4;0.31463316082954407;I las in de intro dat een RX 580 zou worden meegenomen, maar ik zie 'm nergens in de grafieken. Misschien even corrigeren? Hebben jullie 'm ook geprobeerd over te klokken? Voor de rest: heel mooie review Ik heb nu een duidelijk beeld van de 1660, hopelijk kan AMD er nog wat zuinigs tegenover zetten, bij 1 u per dag ben je er binnen 3 jaar zo 40 euro extra aan stroom kwijt.
GTX 1660 Ti;5;0.7311597466468811;Ik heb net een MSI GTX 1660 ti Ventus gekocht voor mijn Dell T5500 dual X5675 Xeon, is ruim voldoende voor mijn 1080p systeem. Ik heb mijn GTX 780 ti Windforce te koop, de vette versie van 450 watt koeler. En zo kan het ineens veranderen. Zodra hij binnen is ga ik een filmpje maken voor Youtube, hoe goed is ie met blender, zbrush, maya en andere software die ik gebruik. Krijg Fortnite erbij, kan ik ook kijken joe dat draait, ben heel benieuwd.
GTX 1660 Ti;1;0.38159897923469543;Krijg je fortnite er bij?
GTX 1660 Ti;3;0.312430739402771;Ja.
GTX 1660 Ti;3;0.5168956518173218;Dat is wel nice dan
GTX 1660 Ti;3;0.4702034890651703;"Goede nieuws is dat deze kaart het prestatie niveau van de GTX 1070 (2016) nu voor +- 165 euro minder beschikbaar maakt (tov 2016). Kost je wel 2 GB ram, maar dat is niet het einde van de wereld. Prestatie per watt tevens nog wat beter. Verder is positief dat AMD door deze launch de adviesprijs van de Vega 56 heeft verlaagd naar $279 dollar. Problematisch is wel dat ik toevallig zelf zo'n GTX 1070 heb en je merkt anno 2019 echt wel dat dit midrange is geworden, in ieder geval op 2560x1080. Momenteel vind ik de RTX 2060 eigenlijk interessanter; vanaf 369 euro en daarbij de game Anthem twv 42 euro en die is kaart dan +- 15% sneller. Voor mij zou dat in sommige games net wat meer comfort geven. Wat dat betreft is het hogere segment redelijk problematisch. Ray tracing en DLSS hebben mij niet overtuigd. Aan de kant van Nvidia moeten we het hier ongetwijfeld 1.5-2 jaar mee doen. Navi wordt pas over 8 maanden verwacht en ik daarbij verwacht ik persoonlijk dat dit meer Vega 56/64 performance voor polaris prijzen wordt en dat snellere kaarten nog 12 maanden langer duren. Dus tja wil ik upgraden naar 1440p dan vrees ik dat er niks anders opzit dan te upgraden naar een RTX 2070 a +- 540 euro, ook al win ik daar slechts 30% mee."
GTX 1660 Ti;3;0.3989778459072113;Ik blijf lekker op Full Hd en wat lagere settings gamen op mijn geforce 960 4Gb : Ik ga pas een nieuwe kaart kopen als dat mij voor dezelfde prijs een nieuwe kaart aangeboden wordt die op z'n minst 30% hogere prestaties levert op 1080p. (Met mijn oude ogen is een hogere resolutie sowieso niet erg merkbaar) Dit is weer pet : nog geen 10% verhoogde prestaties voor dezelfde prijs. Ergens moet ik dus blij zijn dat blijkbaar de hardware limieten bereikt zijn, maar upgraden is stiekem ook erg leuk. En ik had dus verwacht dat de 1660'Ti' gelijkwaardige prestaties zou hebben als de 2060 zolang je de nieuwe foefjes uit laat. Dat 'Ti' is dus helemaal flauwekul pffff Komt er wel een 1660? Of alleen 1650Ti? Die dan 4% meer prestaties als 1050Ti gaat leveren voor hetzelfde geld? Lijkt AMD wel.
GTX 1660 Ti;2;0.34724462032318115;Is een 1660ti niet meer dan 30% sneller dan een 960?
GTX 1660 Ti;2;0.5390866994857788;Die is niet voor dezelfde prijs verkrijgbaar. Ik heb 960 4 Gb voor onder de 250 euro gekocht en dat was dus voor een wat luxer MSI model met backplate. Kijk ik nu naar geizhals.de voor een indicatie waar de prijzen over een tijdje ook in Nederland den ik komen te liggen ben je voor een luxere MSI 1660ti kaart met backplate ca 320 euro kwijt. Qua prijs is als ik de geruchten mag geloven wordt de aankomende 1650 veel beter in prijs vergelijkbaar. Bij deze prijs vind ik het vooral teleurstellend dat 1660ti niet dezelfde prestaties haalt dan de RTX 2060 als die de nieuwe features niet benut en is het verschil in prijs dus te klein tussen die twee. Maar niets nieuws onder de zon : Het geschuif met nummertjes tov de prijs is al langer om toch nog de illusie van significante prestatie verbetering te geven.
GTX 1660 Ti;3;0.3175656199455261;Ja Nvidia is raar bezig, maar ik snap niet waarom ik mensen zie zeggen dat ze dachten dat de 1660ti een rtx2060 zou zijn zonder raytracing, de rtx2060 is ook een veel grotere gpu, die heeft 1900 cores, de 1660ti heeft er 1500, dat is een flink stuk minder, in games valt het minder op, maar in synthetic benchmarks is de rtx2060 al ruim 20% sneller dan de 1660ti, dat is niet weinig, kan ook niet anders met 1900 vs 1500 cores.
GTX 1660 Ti;2;0.39120689034461975;Het is wel enigszins te begrijpen door dat de hardware limieten er voor zorgen dat de ontwikkelingskosten ook flink stijgen, maar aan de andere kant ook wel erg teleurstellend. We hebben natuurlijk de laatste 20 jaar ook op GPU gebied ook wel een ongelovelijke ontwikkeling doorgemaakt. Maar wees kritisch over de nummering en de prijzen. Het zal wel aanslaan bij mensen want anders deden ze dit niet. Maar elke 2e generaties een nieuwe GPU in de computer wordt nu bij mij 3 of 4 generaties... ook wel zo goedkoop natuurlijk (zeker ook omdat het stroomverbruik ook niet meer lijkt te dalen) edit : Als ik er nu zo over nadenk is er nog een logische reden... Omdat als de 2060 de RTX features wel gebruikt eigenlijk deze kaart al gauw te licht bevonden wordt zou de aanschaf van die kaart misschien ook wel erg oninteressant zijn tov 1660ti als beide verder gelijkwaardige prestaties hadden.
GTX 1660 Ti;2;0.41437023878097534;Jongens een gtx1060 van meer dan 2 jaar oud. Op een gsync scherm op mein leben rond de 60fps in wolfenstein 2 gloedje vloeiend in 1440p. Waar lullen we met ze allen nog om dan? Echt ik wil met plezier een dikke kaart aanschaffen voor me 4k oled.Maar laat het blaffen dan! Zelfs metro ziet er zo geniaal uit op de 1060. Ik snap ECHT de meerwaarde niet meer, van high naar ultra/extreme. Het lijkt echt een neppe setting die gewoon je portomonee stretcht en niet de gfx. Kom maar op met de filmpjes heb er tientallen gezien, NIets laat me echt de meerwaarde zien. Digital foundry met metro exodus of bf 5. Ik mis het crysis momentje De half life 2. Het unreal tournament jammer joh je pc is butt momentje. Die je ff laat zien hoe ruk je kaart is, Ben bang dat we dit gewoon niet meer gaan mee maken aangezien de sprong kleiner en kleiner word.
GTX 1660 Ti;3;0.46928519010543823;Ik denk dat ze rek er aardig uit is, uit de hele technologie zelf ( silicon wafers) Met cpu's is het helemaal erg, een i7 2600k uit 2011 van 8 jaar oud kan nog hartstikke goed mee anno 2019, rek is eruit.
GTX 1660 Ti;2;0.43746620416641235;Een review van een budgetkaart Dus nog even wachten tot de 1670 en 1680 uitkomen. Ik dacht dat men het redelijk eens is over het feit dat een xx60 kaart altijd een slechte verhouding is tussen prestaties en aanschafprijs. Een budgetkaart is wel goedkoper, maar je moet je voorstellen dat voor dubbel de prijs krijg je meer dan dubbel de performance. Dus de 1660 maar beter even links laten liggen.
GTX 1660 Ti;3;0.34304505586624146;Laat het nou net dat raytracing niet vooruit te branden is op de 2060 (in de huidige titels) en dat DLSS enorm buggy/fuzzy is op dit moment. Meerdere outlets geven aan dat je beter de resolutie terug kunt zetten of display scaling (mits de game het ondersteund) terugschalen voor soortgelijke prestaties als DLSS met scherper beeld. Verder mooie kaart wel die 1660 Ti, klinkt als een leuke optie voor 1080p gaming. Iets voor de PC van vriendinlief. Dat, of ik trakteer mijzelf op een nieuwe GPU en zij krijgt de 980
GTX 1660 Ti;3;0.42778465151786804;Ja, maar verwacht niet dat de 1660Ti voor 300 EUR in de schappen ligt. Dat zullen waarschijnlijk alleen de reference design/sommige Gigabyte modellen zijn. De Strix versie zal met de Asus-tax waarschijnlijk eerder rond de 370 EUR uitkomen. Ja dan kan je net zo goed een RTX2060 van 400 EUR halen en de Anthem/Battlefield V code op marktplaats voor 30 EUR verkopen, heb je een betere kaart voor evenveel geld.
GTX 1660 Ti;2;0.36683374643325806;Lijkt me niet dat de strix/gaming X versies op 370 euro uitkomen dan heb je voor 50 euro extra de 2060 en dat is dan een nobrainer. Ik hoop/denk dat de strix/gaming x versies zo rond de 330 euro komen te liggen. EDIT: Gaming X 350 euro bij alternate, maar een GIGABYTE windforce voor bijvoorbeeld 310 euro. Dubbele edit: De meerprijs op basis van dezelfde versies (dus bijv. MSI Ventus 1160 en MSI ventus 2060) lijkt zo'n €75,- te zijn oftewel 25% (bij de RTX 2060 krijg je wel Battlevield V of Anthem er gratis bij)
GTX 1660 Ti;1;0.4892423450946808;Nou kijk daar heb je het dus al. De Gaming X 1660Ti is 350 terwijl MSI's eigen RTX2060 Ventus versie 375 EUR kost. Nou daar hoef je dan niet 2x over na te denken welke je dan gaat nemen.
GTX 1660 Ti;1;0.4078789949417114;Maar een Ventus 1660TI is dan weer 300 euro en dan is het prijsverschil opeens weer 25% met de 2060 Ventus.
GTX 1660 Ti;2;0.42230308055877686;Ja maar zo verkoop je geen premium modellen als de Gaming X en de Strix, dat is mijn punt. De premium modellen zijn te dicht op de prijs van de RTX2060.
GTX 1660 Ti;4;0.5668331980705261;Leuke grafische kaart voor 2560x1440 of 1080P 144Hz, behalve Metro Exodus, daar draait zelfs de Geforce GTX 2080 Ti niet hoger dan 95fps met alles op Ultra, zonder Ray-Tracing. Die spel vraagt enorm veel van je CPU/Grafische kaart. En de Geforce GTX 1660 Ti is ongeveer net zo snel als de Geforce GTX 1070, dus geen goede vervanging nog van de Geforce GTX 1080, dan moet je toch echt gaan voor de Geforce RTX 2080/Radeon VII.
GTX 1660 Ti;5;0.6177517771720886;Precies, geen appels met peren vegelijken. Al die grote merken hebben meerdere lijnen, en in al die lijnen zul je de 1660 én 2060 vinden. Je kunt ze dus altijd paarsgewijs vergelijken, helemaal omdat ze allebei dezelfde 192 bits bus naar 6GB GDDR hebben, Het prijsverschil is puur de GPU zelf.
GTX 1660 Ti;2;0.3800438642501831;De Strix RTX2060 kost ook 450 EUR, tegenover het veel goedkopere instapmodel van bijv. MSI voor 375 EUR. Als de goedkoopste 1660 Ti ronde de 300 EUR zit, zal de Strix-versie akelig dicht bij een RTX2060 in de buurt komen qua prijs.
GTX 1660 Ti;2;0.37670502066612244;Als je een goedkope 2060 met een dure 1160TI gaat vergelijken dan heb je gelijk. Maar op dit moment (als ik kijk naar de prijzen van de MSI ventus en de MSI Gaming X versies) is het verschil tussen dezelfde versies €75,- en dat is toch 25% meer wat ik een flink percentage vindt.
GTX 1660 Ti;1;0.4143553376197815;Kijk ik nou verkeerd? Op de site van Alternate zie ik nog helemaal geen 1660s... In de pricewatch zie ik twee 1660 Ti's met prijzen, twee MSI kaarten. Die zitten rond 330 en 350 euro (de goedkoopste winkel 5 euro er onder). Tegelijk zie ik heel wat 2060 opties vanaf zo'n 375 (wederom nog iets er onder zelfs als je voor de goedkoopste gaat). Dat is dus een verschil van 45 euro op de goedkoopste modellen. Op dit moment betaal je dus zo'n 14% meer voor de 2060... Dat krijg je ook terug in performance, maar daar bovenop krijg je dan nog wat extra functionaliteit. Ik heb ook weinig met RTX en DLSS, maar in dit geval zou ik bij dit prijsverschil toch voor de kaart die je meer functionaliteit biedt gaan. Uiteraard staat of valt alles met de prijs. Als de 2060 op dit prijspunt blijft ziten en de 1660 Ti wordt nog zo'n 50 euro goedkoper, ja dan is het weer een nobrainer de andere kant op. Maar dat dat gebeurt daar durf ik niet op te wedden.
GTX 1660 Ti;1;0.4568418264389038;Je kijkt verkeerd.
GTX 1660 Ti;5;0.5377195477485657;Dank je, verhelderend...
GTX 1660 Ti;1;0.4680953621864319;Ik ben inderdaad lui, maar die site heb ik gisteren wel gecheckt... De search zal niet goed gewerkt hebben of zo want op 1660 vond die gisteren niks. Ook de pricewatch bevatte gisteren nog maar twee 1660 Ti's met prijs, vandaag een stuk meer en inderdaad met lagere prijs. Verder heb ik jou nergens van beschuldigd, iets wat jij nu wel bij mij doet. Maar ga gerust door met overal het slechtste van mensen denken, lekker leven leid je dan...
GTX 1660 Ti;2;0.40207087993621826;Lijk me niet dat dit uiteindelijk de prijzen worden want dan concurreren ze met zichzelf(2060) en dat is niet handig. Edit: Verder denk ik dat als ze de prijsstelling goed neer zetten dit voor veel gamers interessant gaat zijn.
GTX 1660 Ti;2;0.48271700739860535;Nee dit zal niet veel veranderen, hoogstens twee tientjes maar de RTX2060 zal in die tijd ook een beetje meedalen. De prijs van de 1660Ti is gewoon te dicht in de buurt van de RTX2060. De premium versies van de 1660Ti mogen eigenlijk al niet meer dan 310 EUR kosten aangezien deze geen RT en lazytensor cores hebben zoals de RTX2060.
GTX 1660 Ti;3;0.32746097445487976;ray trace werkt volgens vele testers toch echt prima voor singleplayer op 1080p zoals geadverteerd... En laat nou net meer als 93-94.5% van de gamers 1080p of lager gamen. Dus voor menig gamer die ook de extra frafische prach wil is een 2060 gewoon de betere keuze.
GTX 1660 Ti;1;0.46802595257759094;Is dit een soort teken van dat nvidia zelf al niet meer helemaal geloofd in raytracing?
GTX 1660 Ti;1;0.5118201971054077;Neen... Dit is gewoon om ook nieuwe goedkoper geprijsde kaarten aan te bieden...
GTX 1660 Ti;5;0.5663452744483948;Ik hoop dat je gelijk hebt. Ik heb de rtx2080ti gekocht omdat ik enorm enthousiast ben over de potentie ervan. De 3dmark raytrace demo ziet er echt geweldig uit! Zelfs de Quake2 rtx versie maakt van zo een oude game bijna weer een modern uitziend iets. Ik hoop dat het niet nu al op een dood einde loopt.
GTX 1660 Ti;3;0.5907512903213501;Beetje speculatie: Ik denk niet dat RTX zomaar zal verdwijnen, ondanks de ruwe kantjes die er nu nog aan zitten. Mits RTX er daadwerkelijk beter uit ziet is het een van de weinige redenen waarom, voor budget en midrange, de kaarten niet vervangen kunnen worden door een paar gpu cores op 'n cpu package. De hoeveelheid ruimte die alle extra tensorcores ed innemen, samen met het bijkomende warmteproductie maakt dit een stuk lastiger dan zonder RTX. Aangezien Nvidia geen cpu afdeling heeft lijkt me het niet waarschijnlijk dat ze zitten te wachten op een toekomst waarin ze afhankelijk zijn van AMD of Intel, welke beide al eigen gpus maken. Wat dat betreft is een beetje meer stroomverbruik en die space alleen maar positief voor Nvidia, mits er een mooie reden voor is. Dat is nou net wat RTX doet. Daarnaast bied het een extra stukje onderscheidt tussen gamen op 'n pc of op 'n Playstation/Xbox.
GTX 1660 Ti;2;0.49291524291038513;hm lijkt mij een wat overbodige kaart gezien de alternatieve. Komt en dan ook een 1070ti en een 2060ti?
GTX 1660 Ti;2;0.29532864689826965;Volgens mij bestaat de 1070ti al een tijdje. Denk zelfs dat hij EOL is.
GTX 1660 Ti;1;0.37733194231987;Waarom is het niet de 1160 geworden ipv de 1660, wat is nu in hemelsnaam de benummering geworden ?
GTX 1660 Ti;2;0.40959805250167847;"Volgens de buren: Verwarrende naam om verwarring te voorkomen Een opvolger van de GTX 1060 zouden veel mensen vermoedelijk een GTX 1160 noemen. Nadat Nvidia met de RTX 20-serie kwam, was GTX 2060 ook nog een optie, net onder de RTX 2060 gepositioneerd. De fabrikant lijkt echter alle ideeën over de naam in een blender te hebben gegooid, en te zijn uitgekomen op GTX 1660 Ti. Navraag leert ons dat deze naam is gekozen om verwarring met de RTX 2060 te voorkomen. Waarom is er na een sprong van 1000 naar 2000-nummering gekozen voor een 1600-serie? Waarom is er het Ti-suffix aan toegevoegd als er nog helemaal geen zicht is op een langzamere versie? Antwoorden op deze vragen kregen wij helaas niet. Nvidia zelf gaf geen andere toelichting dan dat de nieuwe videokaart met deze naamgeving goed te onderscheiden is van de RTX 2060. Getuige de recente informatie over een 1650 is er in elk geval sprake van een consistente lijn; heel duidelijk zouden we het niet noemen."
GTX 1660 Ti;1;0.3310306966304779;Ja, hallo? Wij willen gewoon een Founders Edition hoor!
GTX 1660 Ti;5;0.27892810106277466;Ik vind het wel fijn dat Nvidia deze stappen neemt. Zelf heel goedkoop een 1060je kunnen scoren welke er meteen uit ging en deze betaalt zichzelf minimaal 1x per jaar terug in verhouding tot mijn vorige 3-na meest hongerige AMD kaart die bestaat. Mijn werkverbruik is van 200-250w gezakt naar 110-130w en onder games van zo'n 300-600w (verschilt enorm per game) naar zo'n 180-280w... en dat alleen vanwege de switch naar nvidia. Heerlijk!
GTX 1660 Ti;5;0.6053726077079773;Mijn rx470 verbruikt 100 watt, dus het kan ook met AMD
GTX 1660 Ti;3;0.40718865394592285;haha ja, maar dan zou het een dikke performance loss zijn geweest ben ik bang. Ik heb mijn kaart gebenched (in mijn build) tegenover een rx 480 en op directX 11 won mijn kaart met bijna 1.5x de performance. DX12 performance was ongeveer even sterk... de 1060 is nét ietsje sneller dan mijn oude kaart dus dat was wat beter te verantwoorden
GTX 1660 Ti;3;0.29998424649238586;Dikke performance loss nou nou
GTX 1660 Ti;1;0.5320050120353699;Ja? Als mijn kaart 1.5x zo snel was als een RX480, dan verlies ik al helemaal op een RX470, denk je niet? Ik begrijp niet waarom je er zo op reageert?
GTX 1660 Ti;2;0.4092707931995392;Omdat het performance verschil wel meevalt, dikke performance loss vond ik overdreven vandaar mijn reactie. In gta 5 is een 1060 een stuk sneller dan een rx480, maar welke games nog meer dan? Ik val je niet aan ofzo, gewoon een nette discussie, en er is niets mis met een 1060.
GTX 1660 Ti;2;0.42096856236457825;de 470 zou een performance loss zijn op mijn oude kaart, waar ik voor zou betalen. Dat is meer wat ik bedoelde. Ik zou dan betaald hebben voor een performance loss.
GTX 1660 Ti;1;0.42513978481292725;Ja dan snap ik dat dat geen goed e keus was dan.
GTX 1660 Ti;2;0.4425867199897766;Gezien stroom telkens weer terugkomt, zou het dan niet fair weze bij kaarten de advies instellen van hun software over te nemen ? bij AMD is dat paar muisklikken werk en bespaart het nodig op stroomgebruik. Undervolten en software instellen daar scoort nvidia een stuk minder in. Wellicht is dat te veel tweak werk voor tweakers, maar persoonlijk heb ik daar evenveel plezier uitgehaald als het gamen opzich, en vervolgens komt AMD er aanzienlijk rooskleuriger uit.
GTX 1660 Ti;2;0.24096710979938507;Ik heb net blenchmark gedraaid op Dell T5500 met dual X5670 Xeon's, heel vet, GTX 1660 ti net zo snel als GTX980ti SLI namelijk 37 seconden, als ik de samples verlaag kom ik op 26 seconden uit, 24 cores t.o.v 12 cores, geeft een vertraging van een halve seconde, dus het lijkt erop dat Blender beter werkt met 12 cores dan 24. . In Maxon cinebench r15 is de GTX 1660 ti vreemd genoeg langzamer dan GTX 780 ti in OpenGL. Respectievelijk 66 fps voor GTX 780 ti en 57 FPS voor GTX 1660. Geekbench 4 geeft nu 21483 dus ietsje sneller dan iMac 4K 2017. Daar ben ik heel tevreden mee. Moet de processors nog updaten in twee weken, zijn nog onderweg, maar in userbench krijg ik voor Gaming 84% en voor Workstation 82% Dat heeft onder andere te maken met de SATAII bus voor de Crucial MX500 500Gb. Moet nog uitzoeken of RAID 0 werkt voor een tweede MX500. Ik heb wat geëxperimenteerd, maar hij doet af en toe wat vreemde dingen, en wil steeds de andere HDD's in de RAID opnemen, weet iemand daar een oplossing voor? Verder heel tevreden met het oude beest.
GTX 1660 Ti;2;0.4001222550868988;Of misschien gewoon dat het duurder is om deze kaarten te produceren. Nieuwere technieken betekent niet altijd ook goedkoper te produceren hoor.
GTX 1660 Super;2;0.44987308979034424;De manier waarop Nvidia z'n producten op de markt brengt is echt zeer vaag voor een doorsnee consument. Als er 2 versies zijn van een kaart, dan zal een doorsnee consument het verschil waarschijnlijk niet zien mits hij/zij overtuigd is door een tech-savvy persoon die er wel alles van afweet. Ik houdt niet zo van deze praktijken die Nvidia de laatste tijd doet met de benamingen van de videokaarten. 1650, 1650 SUPER, 1650 ti, uiteindelijk zie je door de bomen het bos niet meer.
GTX 1660 Super;2;0.41027921438217163;"Eens. Volgens mij is het een beetje het supersize idee van de fastfood ketens. Voor een klein beetje meer geld krijg je opeens ""zoveel meer"".. Dus waarom dan voor het instapmodel gaan? Leuk psychologisch spelletje.. Als je dan een lekker ruim middensegment hebt met meerdere opties die elkaar niet veel ontlopen qua prijs, dan weet je in ieder geval dat mensen massaal voor de mid-end zullen gaan en de goedkoopste opties links laten liggen. Ergo; je kan dus wel stellen dat middensegment dan waarsch. te duur is naar verhouding. Daarbij, waarsch. zijn dit binned modellen. Schijnbaar is het voor Nvidia lastig om consistent High end te produceren waardoor er gewoonweg meer ""rejects"" zijn die dan als minder model verkocht worden?"
GTX 1660 Super;2;0.3662791848182678;Precies. Als het daarop aankomt doet AMD het goed. Die hebben geen Ti of Super benamingen bij hun producten. Het enige wat ze wel doen is hun kaarten vaak hergebruiken (denk aan rx480->rx580->rx590). Bij de klant is het op die manier veel duidelijker. Ik denk inderdaad ook dat het gewoon 'rejects' zijn, dat ze bijvoorbeeld een aantal stream processoren uitgeschakeld hebben die wel gewoon werken. Doen ze wel vaker in GPU land. Ik hoop in ieder geval dat Nvidia in de toekomst af gaat zien van hun Ti en Super benamingen. Super vervelend voor de doorsnee consument op dit moment om een goede videokaart te kiezen.
GTX 1660 Super;3;0.29035115242004395;AMD heeft XT.. is dat beter?
GTX 1660 Super;3;0.5754153728485107;Ik vind het eigenlijk wel beter, ze hebben de 5700 en de 5700xt en verder (nog) niets anders(laten we het maar niet hebben over de Radeon VII). Je hebt bij AMD op dit moment dus alleen de mogelijkheid om te gaan voor één van de twee terwijl Nvidia een hele lineup heeft met meer dan 10 soorten GPU's.
GTX 1660 Super;1;0.3771032392978668;nV had ook maar twee modellen per nummer. Twee 2060's, twee 2070's, etc. Bij de 1660 heb je er nu drie inderdaad.
GTX 1660 Super;2;0.4252573549747467;Dat klopt. Ik hoop in ieder geval dat nvidia snel stopt met meer dan twee versies van één GPU uit te brengen. Het maakt het de consument niet makkelijker op. Als ze het nou gewoon houden bij een gewone versie en een Ti versie blijft het tenminste nog een beetje overzichtelijk. Ik vind het allemaal super wonky op dit moment.
GTX 1660 Super;3;0.4118378758430481;"Alleen is er nauwelijks verschil tussen een 2060 Super en een normale 2070. Hetzelfde voor deze 1660 Super en de 1660Ti. Bij AMD is het een stuk duidelijker. Het was imho zelfs logischer geweest om die kaarten een nieuwe ""generatie"" te geven, dus 1760 of 1750. Bij AMD is het gewoon duidelijk dat een 5700 XT sneller is dan een 5700 en is er niet nog een 5700 Super of een 5600 Super die weer even snel is als het vorige model."
GTX 1660 Super;1;0.4093421995639801;"AMD heeft altijd al XT gehad, of anderzijds, ATI heeft altijd XT gehad. Ze zijn in feiten terug gegaan naar de oude benaming wat minder verwarrend is, wat dit betekend is uit gebreid verteld op het internet. XT moet je lezen als Nvidia heeft ene enorm lange tijd de ""TI"" benaming gebruikt wat staat voor ""TITAN"" Dus dan heb je de RTX 2080 TI(TAN) Nu Hebben ze ""Super"" wat qua performance vrijwel niet tot amper lager in performance is maar nergens kenbaar word gemaakt of ""Super"" de vervanger is voor ""TI(TAN)"" Verbaasd me niks als we straks een RTX 3080SU zien...."
GTX 1660 Super;2;0.4660113453865051;Dat is ook niet helemaal waar want de nvidia top kaarten staan er ook niet tussen. Er staat geen 1080 erbij en ook geen 2080 daarmee moeten die amd kaarten mee gaan concurreren. Wat her vergeleken word is gewoon het ietjes hoger segment van amd en nvidia. Buiten dat wat ik gelezen heb is dat er meer cuda cores inzitten en dat de nvidia kaarten gddr6 geheugen hebben gebruikt wat dus sneller is als het oudere gddr5 geheugen.Dat is het enigste verschil wat dus die prestatiewinst ietjes hoger uitpakt. Wat wel waar is dat amd het goed voor elkaar heeft mits ze ongeveer goedekoper of dezelfde prijzen hanteren als de nvidiia concurentie kaarten.De topkaat van amd staat er ook niet bij. Het is het ietjes hogere midden segment.Waardoor amd de winnaar is.
GTX 1660 Super;3;0.4595782458782196;Het is niet heel veel anders dan wat ze al heel lang doen met de normale en Ti versies, waarbij het altijd zo was dat de Ti beter was. Nu wordt het alleen verwarrend omdat ze de Super als refresh gebruiken, in plaats van een nieuw nummertje, in de GTX2xx reeks was dit de GTX265 geweest, niet de GTX260 Super En AMD doet het ook, de 5700/5700XT, of in het geval van de 560 dan weer andersom, een met 14CUs, de ander 16, maar precies dezelfde marketing naam.
GTX 1660 Super;3;0.48602816462516785;Misschien hadden ze beter net als AMD met haar refreshe van polaris gewoon een 30-series lineup moeten maken om zo de verwarring wat tegen te gaan. Dan zou het allemaal veel duidelijker zijn en had je ook de Super benaming achterwege kunnen laten.
GTX 1660 Super;3;0.4712369740009308;Dit klopt niet helemaal wat je zegt. AMD heeft voorheen altijd wel 2 kaarten geintroduceerd op mid en high end niveau, althans sinds ik me kan herinneren. 4850/4870 - 7850/7870 + 7950/7970 - rx570 - rx580 (rx590 is hier een outlier net als de R9 285 eigenlijk). Nividia gebruikte x70 en x80 om hun high end, en high end met mindere cuda's etc. aan te duiden (buiten de gehele GT, GS en al die onzin van de 8xxx en 9xxxx series). De Ti zat vaak in de midrange (x60 en x60 Ti of x50Ti). Later kreeg je de 780Ti. Toen kwam de titan. Nu heb je Ti en Super die overal gepast en ongepast gebruikt wordt er boven op. Wat is het nut van die derde kaart? Wat is het nut van de 2080S vergeleken met de 2080Ti? 2060s vs 2070? 2070s vs 2080? Die kaarten zitten zo dichtbij elkaar kwa performance, je kon ze beter als refresh uitbrengen in plaats van SUPER erachter plakken. Maar goed, het is wat het is. Nvidia wil de 3xxx series voor iets specialers gebruiken. En de 2160 vonden ze misschien niet ' cool' ofzo. En de 1665 Ti/S vonden ze ook niet cool ofzo (waarschijnlijk testen gedaan en slaagde minder bij mensen dan de SUPER variant). Het is wat het is, maar mooi is het niet alvast. AMD houd het wel wat consistenter met namen (rx290/rx390 etc.) met hier en daar outliers echter refresht AMD zijn kaarten vaker waardoor je dus soms een kaart krijgt die 3 generaties mee gaat (Pitcairn).
GTX 1660 Super;2;0.4165743291378021;Ach, tegenwoordig is het aantal kaarten gebaseerd op 1 GPU al niet meer op 1 hand te tellen.. Maar IMHO is de benaming toch redelijk duidelijk hoor, 1650< 1650 super < 1650 ti..
GTX 1660 Super;2;0.5753257870674133;Niet alleen vaag maar het voelt ook niet alsof er veel vertrouwen is in de RTX lijn. Echte uitleg is er nooit geweest - opeens zagen we een SUPER badge geteased worden en daar was het dan... meer midrange die we al drie jaar kennen, tegen gereduceerd tarief. Turing voelt voor mij een beetje aan als paniekvoetbal. Nvidia zag de koers zwaar gecorrigeerd worden en ging een tandje harder draaien.
GTX 1660 Super;5;0.2517869770526886;Ach nieuw is het niet hoor bij Nvidia, vroeger had je ook de Geforce 2 GTS. Geforce 2 MX (200/400), Geforce 2 Pro, Geforce 2 Ultra, Geforce 2 Ti. Nvidia heeft hier van oudsher al een handje van, om maar lukraak uit te breiden naar hun eigen dunkt in hoe de markt zicht beweegt.
GTX 1660 Super;1;0.6540591716766357;Ohja, die was ik helemaal vergeten
GTX 1660 Super;3;0.4113214313983917;Deze prestaties waren er al met de GTX1660Ti, dus er is niks nieuws voor de consument. Alleen is de prijs ligt wat lager.
GTX 1660 Super;1;0.28343671560287476;Dat is toch prima? De consument kan nu dezelfde performance in huis halen voor een paar euro minder, zo gaat dat altijd en zal het ook altijd blijven gaan.
GTX 1660 Super;1;0.4725850224494934;Dat klopt, maar het is niet inovatief, en in mijn ervaring maakt iedereen die een Gtx 1600 kaart koopt voor 1080p ultra een grote fout, want de 1660ti heeft nu al moeite met 60fps 1% lows op 1080p ultra in Gears 5 Het is leuk dat je een 1660ti goedkoper kan halen, maar het is een nutteloos product, zeker met de komst van de Rx 5500. Nvidia kan gewoon geen fatsoenlijke mid-range kaarten maken.
GTX 1660 Super;2;0.32268938422203064;Wat is tegenwoordig wel innovatief? En waarom moet alles per se innovatief zijn? Dit is gewoon meer performance voor hetzelfde geld.
GTX 1660 Super;2;0.43973103165626526;Klopt, maar het is niet een nieuwe kaart. Het is een 1660, met GDDR6 vram. Het is dus zeker leuk als je een nieuwe kaart gaat kopen, maar om van een 1660 naar een 1660 super te gaan is niet interessant
GTX 1660 Super;1;0.42553260922431946;nutteloos met de komst van de Rx5500? dat moeten we eerst nog maar eens zien. Zelf zit ik ook te kijken naar een nieuwe GPU, en denk dat het een RTX2060super gaat worden en niet de RX5700XT, tja in raw power misschien iets sneller, maar lees al genoeg over de brakker drivers, en laat dat nou net ook mijn negatieve ervaring zijn geweest met de AMD(ATI) kaarten die ik in het verleden heb gehad, die zijn allemaal kapot gegaan, terwijl ik met nvidia (knock on wood) daar nog geen problemen mee heb gehad, ja 1 keertje een fout in de driver, maar die was snel opgelost waarna IK in iedergeval geen problemen meer heb ondervonden.. Maar dat wil niet zeggen dat ik echt helemaal niet open sta voor AMD, die RTX2060super is mogelijk voor maximaal een jaar (terwijl ik nu al zo'n 6 jaar met een GTX760 doe), en tegen die tijd hoop ik dat er weer een nieuwe generatie is die veel meer power heeft, of dat nu van AMD is of van Nvidia, hell who knows, misschien zelfs wel van Intel (al betwijfel ik dat ten zeerste)..
GTX 1660 Super;3;0.4197853207588196;Ik heb zelf een Rx 5700 en nog geen problemen, alle games draaien gewoon zoals ze horen. De 2060super is zeker een leuke kaart, alleen ik ben tegen de bedrijfspractise van Nvida. De 20 serie lijn heeft een slechtere prijs/performance dan de 10 serie lijn. Terwijl je verwacht dat de prijs/performance beter zou worden
GTX 1660 Super;2;0.4758026897907257;Maar spelen veel mensen echt op Ultra? bij veel games is dat toch gewoon niet echt nodig wel? Ikzelf speel op 1440p maar ultra is vaak echt niet te doen.
GTX 1660 Super;2;0.4542804956436157;Is per game heel verschillend, de ene game voegt minder toe in Ultra dan de andere game. De ene game draait beter op Ultra dan de andere game. In het ene geval zie je het verschil alleen in stilstaande schreenshots en in de andere game kan je het verschil echt merken. Maar mensen benchmarken altijd games op Ultra settings om zo de GPU het meest te taxen.
GTX 1660 Super;3;0.6246205568313599;Sommige games kan ik beter op 1440p spelen dan andere. De nieuwe Modern Warfare gaat bij mij goed op een 1070 op ultra settings, maar andere games die meer gpu intensief zijn gaat het soms wel op medium settings 1440p.
GTX 1660 Super;1;0.4568428099155426;Dus met andere woorden, ga niet meer voor de Geforce GTX 1660 Ti, want de Geforce GTX 1660 Super is net zo snel en straks goedkoper, en waar is Nvidia mee bezig, gaan ze de Geforce GTX 1660 Super uitbrengen wanneer ze al de Geforce GTX 1660 Ti hebben, en de Geforce GTX 1660 Super nu net zo snel is als de Geforce GTX 1660 Ti, snap echt niet de logica van Nvidia.
GTX 1660 Super;1;0.3088538944721222;Nvidia komt straks met de Gtx 1660 Ti Super voor 10 euro meer
GTX 1660 Super;3;0.3478868007659912;1070 Ti. Borderlands 3 bijv. hangt vaak rond de 40/50. En ik heb niet eens alles op ultra.
GTX 1660 Super;1;0.6260664463043213;dus jij wilt beweren dat deze kaarten beter dan 1070ti zijn? het moet niet gekker worden hier. dan heb je echt andere problemen of jouw 1070 ti is kapot.
GTX 1660 Super;1;0.46596068143844604;Spring jij altijd naar conclusies? het ging mij er meer om dat gamen op Ultra onrealistisch is, je kunt veel beter wat settings naar beneden schalen. Ik vind jou reactie echt heel erg vreemd.
GTX 1660 Super;3;0.5110663771629333;Ultra is inderdaad per game erg verschillend. De ene game draait wel goed en dan zie je ook een verschil. En de andere game zuipt veel performance voor iets wat je alleen ziet in een screenshot.
GTX 1660 Super;4;0.233290433883667;Het staat toch ook (meermaals) in het artikel, dat deze Super voornamelijk is om het (mogelijk) tekort van GDDR5 af te vangen, evolutie dus, geen revolutie. Vooruit plannen is zeker in technologie vereist om ervoor te zorgen dat je niet met tekorten = naamsverlies /winstverlies komt te zitten. (Dat de RX5500 (mogelijk) de betere kaart is, is irrelevant, want Nvidia produceert geen RX5500's, en daar gaat dit artikel ook niet over, het is slechts een upgrade van Nvidia's productlijn).
GTX 1660 Super;5;0.49364951252937317;De RTX supers doen weinig anders, een 2060s is zo goed als 2070 performance voor een lagere prijs, idem voor de 2070s
GTX 1660 Super;2;0.41507601737976074;en nu is het wachten op het antwoord van AMD met een 5500-5600 want de 590 is een maatje te klein voor deze super en te hoog verbruik.
GTX 1660 Super;5;0.2940806746482849;nieuws: AMD presenteert Radeon RX 5500 en 5500M met kleinere Navi-gpu Dit kwartaal dus nog.
GTX 1660 Super;5;0.464608758687973;Deze prestaties waren er al met een GTX 980, 5 jaar geleden...
GTX 1660 Super;2;0.40524306893348694;Er worden veel kaarten getest maar ik mis eigenlijk de gtx 1070. Waarschijnlijk loont een upgrade niet is mijn verwachting maar dat is op basis van gevoel
GTX 1660 Super;2;0.38961923122406006;Uit mijn hoofd presteert de 1070 ongeveer op rtx 2060 (normaal) niveau. Deze kaarten presteren weer iets minder dan de 2060 dus is de 1660 in welke uitvoering dan ook geen upgrade.
GTX 1660 Super;3;0.7344937324523926;Correct. Het meest significante verschil tussen 1070 en 1660ti is het GDDR5 tov het GDDR6 geheugen. Maar qua prestaties kijk je in de praktijk dan naar quasi hetzelfde.
GTX 1660 Super;3;0.291864275932312;Waar ik nou altijd naar benieuwd ben is hoe deze kaarten presteren in VR? Weet iemand hier een site die dat meeneemt? Momenteel heb ik nog een GTX970 en zou graag willen upgraden maar dan wil ik wel weten hoe ze presteren in VR.
GTX 1660 Super;4;0.4113668203353882;Ik heb een GTX 1050 Ti in combinatie met een Oculus Rift. Ik kan alles wat ik uit de Oculus Store haal prima draaien zonder performance issues. (met 3 camera room scale setup) Ik verwacht dus dat een 1660 Super of 1660 Ti dat nog beter zal doen. Ik kan me voorstellen dat een headset met meer pixels wel wat issues zou leveren met een 1050 Ti, maar gezien deze nieuwe kaarten zo'n 100% sneller draaien zou dat prima moeten werken.
GTX 1660 Super;2;0.41845524311065674;Ligt er helemaal aan welke headset je wil gaan gebruiken. De 1060's zijn toch wel de geadviseerde kaarten voor de originele Vive/Rift (ofwel waar je fatsoenlijk mee kunt spelen). Maar de Index heeft dan weer de 1070 (of 1660ti/2060) als geadviseerde. Hoe hoger de resolutie, en hoe hoger de framerate, des te beter moet de videokaart zijn om fatsoenlijk aan te sturen (foveated rendering is nu nog niet echt beschikbaar). Overigens wordt de GTX970 als minimale aangegeven voor de originele Vive/Rift, dus als je voor die headset wil gaan, zou je nog niet meteen hoeven te upgraden (maar is wel verstandig). Zou van een GTX970 in iedergeval niet overgaan naar een GTX1050(ti), welke ook als minimale voor die 2 headsets wordt geadviseerd. Helaas is de jump tussen de 1060(super) en de 2060 wat betreft prijs wel behoorlijk en naar een 2070 en hoger al helemaal.
GTX 1660 Super;3;0.569837749004364;Ik heb momenteel de Vive 1st Gen en heb de 970 overclocked. Merk inderdaad dat het tegen het minimum aanzit en ik zoek dus een goede kaart voor de prijs kwaliteit verhouding. Alleen is dat dus erg lastig te vinden online. Lees namelijk ook dat meer werkgeheugen beter is voor in VR. Heb een tijd terug zitten kijken naar de GTX 1070ti met 11GB erop.
GTX 1660 Super;3;0.3972821831703186;Dat zou natuurlijk een behoorlijke vooruitgang zijn, maar ook een behoorlijke prijsklasse hoger (afhankelijk natuurlijk of je alleen naar nieuwe kaarten kijkt of ook 2e hands). Weet je zeker dat het een 1070ti was met die 11GB, want ik zie alleen maar ti's met 8GB, die 11GB is pas bij de 1080.. Maar zoals ik zie lijkt de nieuwe RTX2060Super een betere koop op dit moment dan een nieuwe 1070ti. Jammer dat een beetje fatsoenlijke GPU's voor VR zo verrekte duur zijn..
GTX 1660 Super;4;0.46515947580337524;Sinds de eerste geruchten al vind ik de positionering gek. Vervangt dit nu de GTX 1660? Hij is een stuk sneller, maar ook iets duurder, dus niet helemaal. Vervangt ie dan de GTX 1660 Ti? Misschien, hij is wel een stuk goedkoper, maar nog steeds een fractie langzamer. Valt ie er tussen in? Dan zitten er wel heel veel kaarten in dit segment. Gaan we dan ook nog een GTX 1660 Ti Super (of Super Ti?) zien met 14Gbit/s GDDR6 en 1536 shaders? Daar moet dan ook nog zo'n 10% performance te halen vallen. De GTX 1650 Super is dan weer een hele serieus upgrade. 43% meer shaders, 3% hogere frequentie, anderhalf maal de geheugenbandbreedte. En gebaseerd op TU116 dus ook de NVENC-encoder uit de Turing generatie. Als die onder de €200 blijft zeker interessante kaart!
GTX 1660 Super;4;0.5966041684150696;Precies, mogelijk wel de ideale kaart om de games voor weinig geld op high settings @ 1080p60fps te kunnen draaien. Gezien de 1650 prestaties moet dat mogelijk zijn met de betere specs van de 1650 super.
GTX 1660 Super;3;0.5632697343826294;Ik mag hopen op wel wat meer om eerlijk te zijn. 60fps haal ik met mijn gtx 970 @ stock nog wel in de meeste games op directx 11.. Games als BF en battlefront 2 draaien op Ultra nog wel op 60 fps.
GTX 1660 Super;5;0.2665494382381439;Ja en nee, de 1660ti begint in Tripple A titels moeite te hebben om 60fps lows te halen in 1080p ultra. Dit is dus een kaart die nog maar een paar maandjes oud is. De 1660ti is een stukje sneller dan de 1650 of 1650 super. Deze 2 zullen dus al helemaal moeite hebben met 60fps in 2020. Echt interessant is de Rx 5500 voor de mid-range.
GTX 1660 Super;1;0.4878065288066864;Maar die RX5500 is toch helemaal niet verkrijgbaar?
GTX 1660 Super;1;0.32116806507110596;Juist, en over de performance/prijs van dat ding is nog helemaal niets bekend.. Hoop voor de AMD liefhebbers natuurlijk dat deze een goede performance kan leveren, is misschien dan weer wat hete adem in de nek van Nvidia..
GTX 1660 Super;3;0.42269599437713623;Nog niet, maar we weten al een aantal dingetjes en datjes, zoals hoeveel Vram, hoeveel cores dat soort dingen en aangezien we weten hoe de Rx 5700(xt) performen kunnen we een gok doen naar de performance.
GTX 1660 Super;1;0.3985719382762909;We hoeven niet te gokken. Volgens AMD zelf ongeveer even snel als een 590, maar met een veel prettiger energieverbruik. Qua prestaties absoluut geen concurrentie voor deze 1660.
GTX 1660 Super;3;0.4300682246685028;Klopt. maar dat is dan de Rx 5500 we verwachten ook een XT variant van de Rx 5500, dus die zal dan misschien rond Vega 56 liggen, misschien net onder de Vega 56. Maar ja sws betere energie verbruik en de XT zal zeker conqureren met de 1660 super, dat is waarschijnelijk ook de reden waarom deze kaart bestaat.
GTX 1660 Super;2;0.45401495695114136;De 5550XT gaat ook niet veel meer doen. Competitie voor de GTX1650, niet de 1660-lijn.
GTX 1660 Super;3;0.4439184367656708;Nee de Rx 5500 is competitie met de Gtx 1650 aangezien een Gtx 1650 de performance heeft van een Rx 570... De Gtx 1650 super, rond de ~Rx 590 give or take a bit of performance. De Xt zal wel zeker competitie zijn voor de Gtx 1660(super) De Rx 5500XT zal sws 8gb Vram hebben, dat is al 2gb meer dan de 1660 en grote kans dat de Rx 5500XT hogere clocks heeft
GTX 1660 Super;3;0.38369762897491455;low/medium bedoel je
GTX 1660 Super;2;0.40284135937690735;Ga er maar van uit dat de GTX1060's die nu nog te koop zijn, puur de voorraad is en dat de fabrikanten nu allemaal overschakelen op de Super voor nieuwe kaarten.. Alleen bij de GTX1650 zou het kunnen zijn dat die nog wel langer geproduceerd zal worden omdat die nog wel zonder extra powerconnector kan draaien en daar dus best een aardige groep voor is..
GTX 1660 Super;1;0.45546114444732666;Is het handig om Breakpoint mee te nemen in de tests? De game is flink geflopt en Ubisoft heeft aangegeven er nogal wat aan te gaan veranderen: Ik zou dan eerder wildlands erin laten, die game is onderhand wel uitontwikkeld. edit: Nu ook op tweakers: nieuws: Ubisoft komt met roadmap voor nodige verbeteringen aan Ghost Recon Br...
GTX 1660 Super;5;0.6336345672607422;Dus gewoon een press release en practically een rebrand. Het werkt ook erg goed voor ze, alle tech websites doen mee.
GTX 1660 Super;5;0.5836004614830017;Kan niet wachten tot de GT 1630 TI Super uit komt.
GTX 1660 Super;4;0.3848831057548523;De super heeft een aparte power connector nodig itt de normale. Voor mij een reden om normaal te blijven.
GTX 1660 Super;1;0.511878490447998;Spijtig dat Nvidia zich schuldig maakt aan productie van dit soort “kwantiteitsmodellen”. Het wordt er op deze manier voor de gemiddelde consument niet duidelijker op.
GTX 1660 Super;2;0.40902164578437805;Ik zit al een tijd te overwegen een computer te kopen met een videokaart ipv een laptop. Wat voor mij het meeste interesse heeft is de performance van ML/NN. Ik zou dus echt wel interesse hebben in een benchmark voor het trainen van ML/NN-algoritmes. Mocht het buiten het interesse gebied van de doelgroep van tweakers vallen dan zou er misschien via een apart artikel naar gekeken kunnen worden.
GTX 1660 Super;4;0.6211382746696472;Ben wel verbaasd dat je nog hele nette frames haalt met een kaart voor rond de €250. ook op 1440P Ultra. Soms zal je wat moeten downscalen qua graphics maar toch wel netjes. Ik keek altijd naar de high end kaarten maar deze doen het best goed. Dat is toch een mooie ontwikkeling.
GTX 1660 Super;1;0.33063769340515137;Ik denk dat de meeste concurentie voor deze kaarten uit het 2e hands AMD kamp komen met een RX 580 voor ong. €100 of een Vega voor ong. €200
GTX 1660 Super;3;0.49791109561920166;toch jammer dat ze geen fully-enabled TU117 in LP uitvoering willen uitbrengen.
GTX 1660 Super;5;0.25380176305770874;Wederom nog meer marktverzadiging van Nvidia. Meer keus is niet altijd beter. Vooral niet voor leken..
GTX 1660 Super;5;0.4942231774330139;Zolang de winkels hun rekken vol leggen met nVidia varianten zodat er nauwelijks AMD te zien is, is nVidia tevreden. Een veel gebruikte marketing techniek die zeer goed werkt. Hetzelfde zie je in de webwinkels: hele pagina's met nVidia kaarten en een paar AMD kaarten er tussen.
GTX 1660 Super;2;0.45498546957969666;Maarja, de keuze bij AMD is IMHO behoorlijk verwarrend met hun nietszeggende namen. daar moet je nog veel beter eerst onderzoek doen naar welk type nu beter is..
GTX 1660 Super;1;0.47965753078460693;Dat is inderdaad een probleem bij AMD. Eigenlijk gaat dat probleem al helemaal terug naar de ATI dagen en later de Radeon series.
GTX 1660 Super;2;0.4229259788990021;Juist, en dat is zo jammer want ik gun AMD zeker ook een goed aandeel in de GPU markt. En het probleem is eigenlijk simpel op te lossen door even een goede brainstorm sessie te houden met het oog ook op de toekomst.
GTX 1660 Super;2;0.3426973521709442;Waarom ontbreken er kaarten in de PrestatieIndex? de RX 560 en 570 hebben wel 1440p Ultra resultaten in de games, maar komen niet meer terug in de Prestatie Index.
GTX 1660;1;0.5033681392669678;Een GTX1660 heeft GDDR5 geheugen GEEN GDDR6...
GTX 1660;5;0.548733651638031;Klopt helemaal, is aangepast
GTX 1660;1;0.601173996925354;Ook de specificaties van de GTX 1060 kaarten kloppen niet wat betreft geheugensnelheid, geheugenbus en geheugenbandbreedte. Moet voor beide kaarten respectievelijk 8Gbps, 192 bit en 192GB/s zijn. Bron: staat ook verkeerd in
GTX 1660;1;0.41371291875839233;Waarom vindt ik geheugen bandbreedte altijd zo bar weinig toevoegen in deze context, ik bedoel ik heb hier nog twee haast antieke GTX-760's liggen welke al op 199,3 GB per seconde zitten. Deze worden al lang niet meer gebruikt of zo, maar toch. Maakt dit nu dat deze twee destijds nog lang niet deze doorvoer konden vol krijgen of hoe moet ik dat zien... ik bedoel, waarom zit er zo'n doorvoer op, als je het toch nog lang niet nodig bent, als ik hier lees dat de GTX-1660's een geheugen bandbreedte van 192 GB per seconden hebben, en spiksplinternieuw zijn, waar de GTX-760 haast antiek zijn.
GTX 1660;2;0.3892897665500641;Ik ben geen expert, maar er zijn zeker algoritmen die vooral geheugenbandbreedte gelimiteerd zijn. Als je een beetje met CUDA C/C++ hebt gespeeld, wordt dit snel duidelijk. Vooral de simpele algoritmen zijn erg bandbreedte gelimiteerd. Denk aan het elementsgewijs optellen of vermenigvuldigen van matrices. Hierbij wordt data uit het videoram gehaald, wat relatief lang duurt, worden alle elementen heel snel vermenigvuldigd, wat in een fractie van die tijd klaar is en wordt het resultaat teruggeschreven naar het videoram. Dit gaat echter nog heel snel in vergelijking met het lezen en schrijven van en naar host memory, wat via PCIe (3.0) gaat. Met PCIe 3.0 x16 is de theoretisch maximaal haalbare communicatiesnelheid slecht 16GB/s, niet rekening houdend met overhead. Om deze reden wil je dat je algoritme zo min mogelijk hoeft te communiceren met videoram en zoveel mogelijk gebruik kan maken van het on chip cache geheugen. En je wilt al helemaal beperken hoeveel er gecommuniceerd moet worden met host memory. Dit is pure speculatie, maar wat ik denk is dat geheugenbandbreedte tot nu toe minder belangrijk was dan geheugengrootte. Meer geheugen kan ervoor zorgen dat er minder hoeft te worden gecommuniceerd met host memory. En waarschijnlijk satureerden oude kaarten als de GTX 760 inderdaad de geheugenbandbreedte niet voor de meeste workloads. Nu de GPUs weer een stuk sneller zijn en geheugengrootte weer een beetje op orde is, wordt er weer gewerkt aan geheugenbandbreedte en als er straks kaarten uitkomen met PCIe 4.0 (~2GB/s per lane) en kort daarna PCIe 5.0 (~4GB/s per lane) interface, zal ook communicatiesnelheid met host memory weer op orde zijn.
GTX 1660;1;0.22228197753429413;nvm
GTX 1660;3;0.3017619252204895;Deze GTX1660 lijkt een redelijk hoge 'Bang for Buck' radio te hebben. Hoe presteren twee van deze GTX1660 kaarten (van 230 euro elk) in SLI in vergelijking met één kaart van zo'n 450-500 euro? (zoals een 2070, 1070Ti of Vega64)
GTX 1660;3;0.4616735875606537;Dat zal moeilijk gaan want de 1660 heeft geen SLI ondersteuning. Enkel de high end kaarten van Nvidia hebben nog SLI support.
GTX 1660;4;0.3970279097557068;En maar goed ook want het enige wat verbeterd is het verbruik voor de energie leveranciers
GTX 1660;3;0.4434550106525421;Beter voor het klimaat
GTX 1660;3;0.6389563679695129;Ik vind het anders vrij karig.. Ze lijken wel hun best te doen om steeds net wat minder bang for the buck te geven als wat AMD te bieden heeft, want ik kan me niet voorstellen dat ze echt niet meer in huis hebben dan dit.
GTX 1660;1;0.47055575251579285;SLI is al lang op het punt dat het als 'dood' kan worden beschreven, vrij nutteloos om daar nog benchmarks mee te doen. Weinig games ondersteunen het nog en de games die het ondersteunen hebben nog altijd de nodige problemen... Hooguit met de absolute high-end kaart zou SLI nog een (klein) nut hebben, twee mid-range kaarten in SLI vergelijken met een hogere kaart met dezelfde prijs is nutteloos.
GTX 1660;2;0.47358208894729614;"Ik weet eerlijk gezegd niet hoe dood SLI is gezien het anderzijds ook een beetje het enige is dat RTX nog enigszins bruikbaar kan houden boven 1080p; iets mi ook wat broodnodig is om het bestaan van RTX in de huidige vorm een hand boven het hoofd te kunnen houden. Gelukkig heeft deze kaart niks met RTX te maken, anderszijds dus is het idd niet heel belangrijk te noemen (m.i.)"
GTX 1660;4;0.34444862604141235;Command & Conquer First Decade, ik blij dat ik SLI kan gebruiken voor Red Alert 1.
GTX 1660;1;0.4299652874469757;Draai je nog op een Gtx 7900 dan ?Best eens upgrade doen naar recent kaartje dan. Anders voor Apex Legends een profile uit . SLI is al lang op het punt dat het als 'dood' kan worden beschreven, Zo dood is dus toch niet.
GTX 1660;1;0.6574541926383972;Kon je niet linken naar die lijst ipv iedereen zijn scrollwiel extra te slijten?
GTX 1660;3;0.3092774450778961;Of je moet perse een nieuwe kaart willen hebben, maar voor hetzelfde geld heb je dus een 2e hands GTX 1070 die een stukje sneller is (op het niveau van een GTX 1660Ti)..
GTX 1660;1;0.43009674549102783;Door al het mining de afgelopen jaren zie ik mezelf niet zo snel een 2e hands grafische kaart kopen. Straks is de garantie eraf en zit je ineens met een defecte kaart. Dan liever iets meer betalen voor een kaart waar je gewoon 2 jaar garantie op hebt.
GTX 1660;2;0.3543398976325989;Niks mis mee hoor mining kaartjes meestal geen overclock en lage voltage. De fan ziet meestal het ergst af gewoon vervangen waneer nodig.
GTX 1660;2;0.3703535795211792;DIY reparaties is niet voor iedereen. sommigen willen gewoon iets wat werkt en geen omkijken naar is. als je graag zelf een beetje rondprutst is het natuurlijk geen probleem.
GTX 1660;1;0.4991261661052704;Ik zeg niet dat een mining kaat moet kopen die defect is. Maar als er een koopt achter een jaartje de kans groot is dat de fan het begeeft die is te vervangen voor 15 euro per blade.
GTX 1660;3;0.4087737500667572;Voor een kaart die iets sneller is maar 2 jaar ouder dezelfde prijs betalen is waanzin... Een tweedehands kaart van 2 jaar oud zou maximaal 2/3 (bij een afschrijving van 6 jaar en dat is nog ruim) de prijs-performance verhouding moeten hebben van een nieuwe. Een oudere tweedehands kaart verkoopt over een paar jaar nou eenmaal een stuk lastiger dan eentje die nu gloednieuw wordt gekocht. Maar tja dit soort gemekker over tweedehands kaarten blijft hier toch wel door gaan zucht.
GTX 1660;2;0.4590155780315399;Ik vind zo'n 20% sneller niet iets sneller, dat is duidelijk sneller.. Een GTX 1070 is vergelijkbaar met een GTX 1660Ti qua prestaties.. En wat afschrijving betreft, ach, over 3-4 jaar zal de restwaarde van beide kaarten elkaar niet veel ontlopen denk ik.. Een 1660 biedt qua features niks nieuws over een 1070.. Enige echte argument is de garantie op een nieuwe kaart, dus de keuze is tussen 20% meer prestaties voor hetzelfde geld of de zekerheid van garantie..
GTX 1660;2;0.4066306948661804;Je kunt wel meer denken... maar een kaart van 3 jaar oud of eentje van 5 jaar oud zou verschil in prijs moeten hebben. Dat raadt iig de kopers van die dingen aan. Ik zou nu ook meer over hebben voor een tweedehands GTX 1070 dan een tweedehands 980. (Maar ik hoef geen andere)
GTX 1660;2;0.36287328600883484;Een 1070 is vergelijkbaar met een 980Ti qua prestaties.. Ga nu eens het aanbod van beiden bekijken op deze website bij vraag en aanbod.. Dan begrijp je misschien waarom ik het denk.. 'Ik denk' dat prestaties meer bepalend zijn voor de restwaarde van een videokaart dan de ouderdom..
GTX 1660;1;0.41254106163978577;Ik wil juist niet met verkopers van tweedehands spul worden lastig gevallen bij reviews van nieuwe(!) kaarten. Zoals ik al zei : geen interesse in een andere videokaart. Ik ben in geinteresseerd wat er nieuw op de markt komt.. niet geinteresseerd in het nutteloze aangeprijs hier van tweede hands kaarten bij reviews van nieuwe modellen. Duidelijk dat de vraag naar tweedehands modellen weer eens tegenvalt? Verlaag de prijs naar een realistisch nivo en er is vast meer interesse. De leeftijd van een videokaart maakt dus wel uit. Maar ik kan alle valide argumenten die hier al zo vaak op tweakers zijn gegeven zijn blijven herhalen... De verkopers van tweedehands spul zullen toch door blijven zeuren hier. (en om dan nog even terug te komen bij die 2 jaar oude 1070 : Als die al 20% sneller is dan de 1660 zou je dus er 120% * 230 (nieuwprijs 1660) * 2/3 = rond de 180 euro voor kunnen vragen.. een 4 jaar oude 980ti kaart zou nog rond de 100 euro moeten kunnen opbrengen)
GTX 1660;1;0.6409565210342407;Als het je niet interesseert prima, maar reageer dan gewoon niet i.p.v. complete onzin te lopen verkondigen zoals je nu doet..
GTX 1660;1;0.6149981617927551;Dat willen verkopers van oude rommel maar al te graag horen dat mijn verhaal onzin is Prima joh... probeer jullie je antieke vid kaart voor veel te veel geld te slijten op vraag en aanbod en succes met het 'tweedehands videokaart' gespam bij elk videokaart-bericht.
GTX 1660;3;0.33377960324287415;Liever een nieuwe kaart met volledige garantie als het prijs/prestatieverschil zo gering is.
GTX 1660;3;0.43105968832969666;Voor hetzelfde geld haal je een 1070Ti die nog maanden (soms jaren, een enkele fabrikant geeft 3 jaar) garantie heeft. Recent zelf gedaan, heb wel even getwijfeld om toch de 1660Ti nieuw te halen. Ik had geen haast dus heb eerst rustig een paar weken de prijzen in de gaten gehouden, tweedehands verkoop sites staan er echt vol mee en ook hier op Tweakers Vraag&Aanbod zijn ze regelmatig te vinden. Dan wordt het prestatieverschil toch al wel het overwegen waard (vond ik). Het argument van dat je niet zeker weet hoe de kaart behandeld is blijft natuurlijk staan.
GTX 1660;2;0.4926430583000183;Ja, dat zie je wel vaker op de 2e hand markt. Vervelende is dat je niet weet hoe die kaart behandeld is. Als ie al 2.5 jaar flink overklokt, flink wat uren per dag werd gebruikt, is ie een stuk sneller verouderd dan eentje die standaard geklokt is.
GTX 1660;2;0.4599108397960663;Ik vind het jammer dat de RTX 2060 niet in de benchmarks is meegenomen. Dit was (imo) juist een leuke vergelijking geweest tussen de GTX 1660 (ti) en de RTX Kaart. Jullie nemen de RTX 2060 wel mee in de eerste pagina, maar daarna komt die ook niet meer voor in de review.
GTX 1660;3;0.25482356548309326;Staat er nu bij Edit: komt eraan bedoel ik.. -hij is ondertussen verschenen.
GTX 1660;2;0.40655359625816345;Is dan eventueel ook niet handig/mooi om de 2 kaarten van AMD erbij te zetten, aangezien daar ook naar gekeken wordt? Of pas dit niet in tabel dan meer?
GTX 1660;2;0.3488222658634186;Wordt inderdaad een beetje proppen en niet overzichtelijker, in deze review staan de 580 en 590 naast de 1060 reviews: AMD RX 590-videokaart - Een goedkope, opgevoerde RX 580?
GTX 1660;5;0.5969251990318298;cool thanks!
GTX 1660;4;0.4449390769004822;"Msi Rtx 2060 gaming z / F1 2018 benchmarks (zelf net gedraaid); 1440P; AA=TAA AF= 8x Ultra high preset min =75 / avg = 91 / max = 114 4K; AA=TAA AF= 2x Ultra high preset min =46 / avg = 55 / max = 68 Maar is zonder al teveel moeite makkelijk op 60fps te krijgen."
GTX 1660;1;0.2786700427532196;TDP van de 1060 is tevens 120 watt, niet 165 (ook terug te zien in verbruik). Zie ook website van nVidia.
GTX 1660;1;0.4050055146217346;Klopt, copy/paste-foutje, is aangepast.
GTX 1660;1;0.7353376150131226;Ook deze cijfers slaan nergens op. Het is of 220 dollar of 159 en 193 euro. In dit geval moet het zo te zien 220 dollar zijn.
GTX 1660;1;0.6543272137641907;Klopt, de adviesprijs van nvidia is 219usd, ik heb het stukje over de dollarprijs weggehaald, want daar kan je in Europa toch niet mee betalen
GTX 1660;2;0.41403746604919434;Nog één, het geheugen is juist hoger geklokt, van 1500MHz naar 2000MHz, alleen de bandbreedte is minder geworden. En gelijk daar onder wordt Mega Transfers per seconden (MT/s) behandeld als een frequentie in plaats van de bandbreedte.
GTX 1660;3;0.31009212136268616;Beide aangepast.
GTX 1660;3;0.6928421258926392;Vindt eerlijk gezegd de reacties op de GTX 1660 nogal overdreven, als je naar de Index 1080p avg. framerate kijkt is 'ie maar 7% sneller dan de RX590. Die op dit moment misschien duurder is, maar in principe een veel oudere architectuur is. De kosten zijn hoogst waarschijnlijk al terug verdiend. AMD hoeft alleen de prijzen te verlagen naar 200-190 euro en de RX590 is weer midrange king. Het verbruik van de kaart echter wel indrukwekkend. Maar qua prestaties een beetje teleurstellend, vond de resultaten van de 2060 t/m 2080ti een stuk overtuigender dat AMD aan de slag moet.
GTX 1660;2;0.3531234562397003;Deze reactie snijdt past hout als AMD de prijs van de RX590 daadwerkelijk verlaagt. Ik wacht met smart! En ik wacht dan ook weer met smart op de reactie van nVidia. Ik vind overigens verbruik van de onderdelen in mijn PC best belangrijk, belangrijk genoeg om een paar euro meer te willen betalen. Maar dat is offtopic en persoonlijk.
GTX 1660;2;0.3735654950141907;Ja, klopt in Duitsland gaat de RX590 voor 227,- euro over de toonbank Het is maar de vraag of AMD de prijs wil verlagen, maar om van evt. overstock af te komen lijkt dit wel een logische stap. Maar gezien het verbruik van de GTX1660 zou mijn voorkeur ook uitgaan naar de GTX1660, mits 'ie voor 220 euro over de toonbank gaat.
GTX 1660;1;0.4465288519859314;Die kans is vrij klein. Fabrikanten hebben al aangegeven dat dat vrij onmogelijk voor ze wordt omdat zij dan ook geen winst meer maken
GTX 1660;3;0.36409085988998413;Het is geen concurrent voor 'huidige' AMD, daar denkt Nvidia niet eens aan. een GTX1060 6GB is/was al in veel gevallen beter. Nu maar afwachten wat AMD met de RX3000 op de markt brengt. De RX3080 moet goedkoper zijn, maar 15% beter presteren dan Vega64
GTX 1660;2;0.3400619626045227;Of nvidia er aan denkt maakt niet eens uit. Hij is iets sneller en iets goedkoper, valt dus in dezelfde range, dat betekent automatisch dat het een concurerend bordje is. Zodra mensen tussen deze twee borden gaan kiezen, en dat gaan ze doen, zijn het concurrenten.
GTX 1660;1;0.4393802881240845;RX580 is uitgerangeerd en vervangen door RX590(het is niet voor niets dat AMD een veredelde RX580 uitbrengt). De GTX1660 komt uit en is goedkoper dan RX590 en presteert beter. zoals ik al zei, een GTX1060 was/is een concurrent van RX580/90. RX3000 komt nog uit e daar kan een concurrerend uit komen. (dat moet nog blijken) AMD loopt constant achter de feiten aan qua GPU's. @EMR77 Wat wil je nu zeggen met je quote. heb je mijn eerdere post niet goed gelezen, of wil je gewoon een reactie geven ? nadere uitleg: AMD RX3060: 4GB GDDR6, vervangt RX580 - $130.- AMD RX3070: 8GB GDDR6, vervangt Vega56 - $200.- AMD RX3080: 8GB GDDR6, vervangt Vega64 + 15% - $250.- meer info op Computex 7 juli
GTX 1660;1;0.44083061814308167;Ik zal het duidelijk voor jouw opschrijven @Hackus, dat AMD niet zó achterloopt qua architectuur. De gtx1060 was geen concurrent vd RX580/590, misschien op papier maar niet in de praktijk. Zie nogmaals Is het zo moeilijk om benchmarks te interpreteren ?????
GTX 1660;3;0.2668111324310303;kijk hier de test van GTX1660 en lees het verschil met RX580/590 ! Je ziet in je eigen link toch duidelijk dt de RX 580/50 achterblijft bij een GTX1660 ?
GTX 1660;2;0.45229393243789673;Bij de cost per frame loopt de gtx1660 al bijna achter de voorgespiegelde feiten aan.. De adviesprijs van $220,- zal hier al snel €250,- worden. Al bij al toch een vrij magere kaart aangezien de AMD kaartjes al vrij oud zijn ondertussen.
GTX 1660;3;0.48862501978874207;Kijk eens op techspot hebben zij de vega VII, 64 en 56 meegenomen kunnen toch best aardig meekomen. Als jij naar de benchmarks voor oa resident evil 2, forza, just cause 4, shadow of the tombraider kijkt, dan kan die oude architectuur aardig meekomen en concurreren met de RTX2070. De prijzen én verbruik zijn ongunstig. Maar qua architectuur niets mis mee, gezien de leeftijd.
GTX 1660;5;0.552085816860199;Ben echt benieuwd wanneer er een nieuwe videokaart uit komt die low profile is en zijn voeding uit de pci slot kan halen. Dit kan alleen nog de 1050 TI en die is al 2,5 jaar oud.
GTX 1660;3;0.4268980920314789;Zou niet raar opkijken als we nog een GTX 1650 zien die aan je eisen voldoet. De vraag is natuurlijk wanneer die komt. Denk dat we voorlopig wel even voorzien zijn wat aanbod vanuit nVidia betreft.
GTX 1660;3;0.4465091824531555;Vind de prijs voor de GTX 1660 nog erg redelijk voor de prestaties die die levert. Erg mooi kaartje waar niks op aan te merken valt.
GTX 1660;1;0.4643961191177368;Bij amd krijg je gratis games, veel betere deal..
GTX 1660;1;0.40987515449523926;Als de GTX 11gb heeft en een AMD maar 8 maar wel een gratis game erbij, waarvoor zou jij dan gaan? Persoonlijk zou ik dan liever laten we zeggen 50 euro bijleggen voor 11GB ipv voor de 8GB te gaan met gratis game.
GTX 1660;5;0.29882022738456726;Firestrike score Korea GTX 1660 6GB
GTX 1660;3;0.549490749835968;Ik vind het fijn dat de 960 nog meegenomen wordt in het advies, aangezien ik het daar nog steeds meedoe. Prima kaart, maar nu toch wel aan vervanging toe. Ik vond de ti al een aanlokkelijk alternatief, maar deze klinkt voor mijn portemonnee nog beter.
GTX 1660;3;0.46030905842781067;Kaart met een mooie prijs/prestatie zo lijkt het. Gezien de flink beperkte bandbreedte tov de GTX1660Ti vermoed ik dat ie best goed een stuk sneller te maken is door de geheugensnelheid wat op te krikken. Guru3d heeft van drie versies de kaart overgeklokt en kwam uit op 9600MHz, 9800MHz en 10000MHz: De kaart die op 10GHz geheugen draait: Dat is dus 20-25% overklok op het geheugen! Het lijkt me interessant om te kijken wat het met de prestaties doet als je de core stock laat draaien, zo kun je zien of er een bottleneck zit op het geheugen.
GTX 1660;1;0.36222779750823975;Wanneer komt deze kaart in de winkel?
GTX 1660;3;0.35671281814575195;Op verschillende sites las ik dat je voor 1440p gaming minimaal een RTX 2060 of zelfs 2070 nodig heb. Uit deze review blijkt dat een 1660 het al kan op Medium! Dat is wel heel goed nieuws!
GTX 1660;1;0.3839170038700104;Dat is omdat sommige (veel?) gamers denken dat het lager instellen van een game dan ultra direct betekent dat ze minder man zijn of zo... Vandaar ook die verhalen van dat je elk jaar een nieuwe videokaart zou moeten kopen als PC gamer, of dat je per se een van 600+ euro nodig zou hebben.
GTX 1660;5;0.3297053873538971;Dit inderdaad. De RTX 2060 wordt gezien als de 1440p kaart omdat die in bijna alle (recente) games op Ultra / max settings de 60 FPS weet te halen. Zit je op 1440p en vind je wat concessies op settings en / of zit je op een sync scherm en vind je 40-50 FPS ook prima, kun je ook prima uit de voeten met een GTX 1660 (Ti).
GTX 1660;3;0.4195723831653595;Dat hangt er vanaf wat je eisen zijn op grafisch vlak. De ene wilt meer fps en lagere instellingen, de andere juist andersom. Ik zit in het tweede kamp. Een game moet er grafisch goed uit zien voor mij. Als je MP speelt wil je ontegensprekelijk hoge fps en lagere grafische instellingen of je koopt een 2070 die beide kan.
GTX 1660;5;0.30874642729759216;Jow, mijn 1060 6GB draait nog steeds veel games rond de 100 FPS op mijn gsync monitor waar ik alles kan spelen met muis en keyboard en volop mods kan installeren... Heb ik al genoeg redenen genoemd waarom ik dat ding heb, ook al heb ik geen achterlijk dure videokaart? Ik heb zelfs óók een Xbox One X én een PS4 Pro, maar ik ben dan ook een beetje gek (maar iets serieuzer, het is gewoon mijn hobby dus waarom niet?)
GTX 1660;1;0.5647376775741577;Behalve dan dat je Steam mist, geen mods kunt draaien, niet makkelijk met muis en toetsenbord speelt en dat veel goede games PC only zijn...
GTX 1660;1;0.5517725944519043;Wat heeft dat met op QHD spelen te maken? Mijn monitor is rond de 400 euro, QHD met gsync... VR heeft niemand het over gehad (heb ik ook, daar niet van, sterker nog ik zit uit te puffen van een goed half uur beat saber... draait ook prima op mijn 1060... koste overigens ook maar 450 euro).
GTX 1660;5;0.7052136063575745;Ja minecraft loopt ook top op mijn oude gtx960 4gb. Het is gewoon een wonder kaart die 1060 ik versta zelfs gewoon niet waarom ze nog andere op de martk brengen!1070,1070ti allemaal ruk 1060 doet het gewoon allemaal!
GTX 1660;1;0.5708226561546326;Waar heb je het toch over man? Ik zeg helemaal nergens dat er geen reden is voor snellere kaarten. Ik heb zelf ook laatst overwogen om een 2070 te kopen (maar ik hou het geld voor nu even apart voor de nieuwe ryzen). Natuurlijk is het beter met een snellere videokaart... Maar waar ik op reageerde was de stelling dat je MINIMAAL een 2060 of zelfs 2070 nodig had voor qhd gaming en dat is gewoon niet waar tenzij je er ook bepaalde (hoge) kwaliteitseisen bij stelt.
GTX 1660;2;0.3336451053619385;Tenzij je er ook bepaalde (hoge) kwaliteitseisen bij stelt.We gaan scherm koopen van 400 euro en dan gaan spelen op medium low of zo?Neem dan gewoon 1080p scherm met een gepast kaartje 1060 klaar.
GTX 1660;3;0.3665224015712738;Sorry, maar ik accepteer jou niet als god Dus ik bepaal zelf wel wat ik belangrijk vind.
GTX 1660;2;0.4052544832229614;Als ze beginnen over god en jesus ja dan stopt het inderdaad.
GTX 1660;2;0.3723551630973816;Net zoals jouw discussie skills... Ik heb het niet over religie, ik bedoel dat jij niet voor iedereen kunt bepalen wat ze belangrijk vinden (en dat zei ik door te zeggen dat je geen god bent maar blijkbaar ben je zo allergisch voor religie dat je me meteen tot Jezus freak bombardeert). Wat betreft de monitor en videokaart overigens... Die monitor was 430 euro toen ik hem kocht... De monitor die ik daarvoor had heeft 13 jaar trouwe dienst gedaan als primair scherm en is nu nog in dienst als secundair scherm. Ik koop een monitor dus niet maar voor een paar jaar. Mijn videokaart, die 1060, koste 300 euro (en was toen een hele goede deal, bijna 2 jaar geleden). Ergens in het komende jaar zal hij wel vervangen gaan worden. Die aanschaf gaat dus veel minder lang mee. Zo bezien is het juist absurd om veel meer aan de videokaart uit te geven dan aan de monitor. Maar uiteindelijk heeft iedereen zijn eigen smaak en eigen prioriteiten. Maar dat lijkt iets te zijn dat erg complex is voor jou.
GTX 1660;2;0.39021873474121094;Voor een paar tientjes meer heb je al een al een RTX2060, dus lijkt me niet echt een toegevoegde waarde de 1660ti
GTX 1660;1;0.38594627380371094;Snelle blik in de pricewatch levert een laagste prijs voor de 1660 Ti van €280, de 2060 voor €360. Da's een €80 en daarmee iets meer dan een paar tientjes...ruim 25% meer.
GTX 1660;5;0.6421440839767456;Ik vind de 1660Ti prijs/prestatie een vd beste kaarten die er momenteel te koop zijn.
GTX 1660;1;0.3714299201965332;Welke producent komt er met de GTX1666, met extreme rode LED-verlichting (en liefst een speakertje die helse geluiden maakt)?
GTX 1660;2;0.39562371373176575;Weet iemand wat de machine learning (Tensorflow) performance is van de 1660 (ti)? Deze kaarten hebben natuurlijk geen tensor cores maar ze hebben wel de 2x FP32 performance in FP16 als ik het internet mag geloven. Zelf heb ik al even gezocht op internet maar ik kon er weinig over vinden helaas .
GTX 1660;5;0.5707443952560425;Het is voor het eerst in historie dat je goed kunt gamen op 1080p met een el cheapo kaarten. Vroeger zat je dan al gauw aan een hele lage resolutie... Ik geniet zelf van een high end kaart. Als je ouder wordt heb je natuurlijk 10x meer geld dan dat je jong bent en op school zit.
GTX 1660;3;0.36021310091018677;Gamen op 1080p gaat toch behoorlijk goed met een GTX 1060. Dat je niet alles op high/ultra kan zetten, soit. Ik koop zelf liever om de 2-3 jaar een kaart van 2-300 euro dan er een van 1000 euro te kopen. Verder is er natuurlijk niks mis met een high-end graka.
GTX 1660;5;0.7391685843467712;NVIDIA heeft hiermee Polaris ten grave gedragen. Prijs, prestaties en verbuik, allemaal ten gunste van NVIDIA. Your move, AMD!
GTX 1660;1;0.3968099355697632;de GTX1660 snap ik op zich nog wel maar de TI versie niet, net zo duur maar sneller dan de 1070, Nvidia beconcurreerd zijn eigen kaarten ?
GTX 1660;5;0.5644052028656006;1070 tweedehands is een veel beter keus. Voor 220/230 heb je een 1070, en daar speel ik met High settings heerlijk soepel GTA V in 4k op.
GTX 1660;3;0.35722416639328003;de 1660ti is gewoon een monster voor 1080p. de 1070 is nog altijd beter voor 1440p mits hele hoge settings
GTX 1660;4;0.4723430275917053;Goede review! Ik ben benieuwd of mijn I7 980 (@4ghz) niet de bottleneck zou zijn als ik deze GPU koop. Ben nog aan het twijfelen. Heb nu momenteell de GTX 960
GTX 1660;3;0.35862404108047485;Game je op 1440p of 1080p? Dat maakt het verschil denk ik tussen geen bottleneck of wel.
GTX 1660;4;0.2280166894197464;1080p
GTX 1660;2;0.429730087518692;Als je door blijft gamen op 1080p dan krijg je sowieso last van een bottleneck. 1440p zou net moeten kunnen denk ik al hou je niet veel ruimte over.
GTX 1660;3;0.43500423431396484;Ik hoop een beetje dat de nieuwe generatie van de Dell XPS 15 deze chip aan boord heeft. Dat zou nog eens een fijne alleskunner zijn.
GTX 1660;3;0.3442600965499878;Met wat voor kaart is een GTX970 te vergelijken? Heb de mijne overclocked en presteert nu op 1080p op GTX980 niveau. Wat voor kaart uit de 10xx serie komt daar het meest bij in de buurt?
GTX 1660;3;0.5316565036773682;980 = 1060, en de 970 zit daar een stukje onder. Niet zo laag als een 1050ti echter.
GTX 1660;3;0.3585069179534912;Ongeveer een 1060 3GB prestaties. Dus als je een 970 hebt dan ben je waarschijnlijk wel in de prijsklasse van 300-400 euro. Ik moet dan zeggen dat als je wil upgraden de 2060 een zeer goede kaart lijkt, 1070ti/1080 prestaties. Dus al gauw 50-60% meer dan de 970 en als je hem een beetje overclockt kom je daar nog wel boven ook
GTX 1660;2;0.49229636788368225;Dus niet echt de moeite. Ik heb dat ook betaald voor mijn GTX970 en verwacht 2 generaties verder wel op +100% te zitten. Helaas.
GTX 1660;1;0.4697931110858917;Gewoon wachten tot oktober is maar 6 gb
GTX 1660;2;0.44116562604904175;Jammer van die 21% BTW.. 192 euro klinkt toch een stuk fijner dan 232 euro.. 40 euro belasting dus op zo'n videokaart..
GTX 1660;1;0.4836398959159851;"""192 euro klinkt toch een stuk fijner dan 232 euro.. 40 euro belasting dus op zo'n videokaart.."" Tsja, en op elk ander ding van 192 euro zit ook 40 euro belasting."
GTX 1660;5;0.4607385993003845;Mooi, en nu de GTX1180ti
GTX 1660;1;0.6440080404281616;Wat een aanfluiting om hem te vergelijken met de rx 590, wat ook een wanproduct is voor een te hoge prijs. Vergelijk eens met de rx 580, maar marginaal sneller en minstens 50 euro goedkoper.
GTX 1660;3;0.470663845539093;Dan is de vraag koop je een TI of non TI. Ik heb nog een 960 gtx en was van plan om die eens te upgraden. Niet dat ik zoveel spellen speel (Wow, HearthStone, Diablo en nu ook wat Apex Legends). Dus een midrange kaart is goed genoeg voor mij. Maarja de TI versie is met moeite al te verkrijgen, dus de 1660 zal wel nog wat duren
GTX 1660;3;0.6617268919944763;Voor jou gebruik is het besparen van 60-80 euro redelijk wat. Dus dan klinkt de non-ti een stuk beter. Je verliest overigens maar ongeveer 15% voor een 40% lagere prijs
GTX 1660;2;0.32545211911201477;Ik koop zowieso altijd een midrange kaart. Dacht eigenlijk eerst een 2660 te kopen. Maar dan zag ik dat die 1660ti uitkwam dusja Recent geupgrade naar 2x Dell ultra hd 27 inch monitoren en je merkt toch wel dat er hier en daar een hapering is. ZOwieso speel ik nooit op full details, want dan wordt ik gewoon gek. Maar Die gtx960 moet eens vervangen worden. Het zal een moeilijke keuze worden maar is wel mooi dat er nu toch alternatieven komen
GTX 1660;3;0.5060055255889893;Zou wel eens willen weten hoe hij dan presteert tegenover een 970. Ikzelf zit nog steeds op de 970, en voor mij is dit prima. Maar waarom de vergelijking met een 960?
GTX 1660;2;0.44532668590545654;"Tja met deze kaart zal Nvidia vooral tegen zichzelf concurreren. An sich lukt dat redelijk goed. De GTX 1060 3GB had een vergelijkbare prijs. Deze keer krijg je 6GB. Natuurlijk stiekem een sigaar uit eigen doos want er wordt GDDR5 in plaats van het snellere GDDR6 gebruikt. Maar onder de streep is deze kaart toch 10 tot 20% sneller. Dus voor wie nu een midrange kaart nodig heeft is er weinig reden tot klagen. Zeker met de absurde prijzen van vorig jaar in het achterhoofd. Waar we wel over kunnen klagen is dat het prestatie verschil ten aanzien van de vorige architectuur dus zeer beperkt is te noemen. Dit soort beperkte winsten zagen we vroeger vaak tijdens de 'rebrands'. Maar dit is geen rebrand, die is een splinternieuwe architectuur! Nvidia had dit ding makkelijk op +30% tov de GTX 1060 kunnen zetten en gewoon GDDR6 kunnen gebruiken. De verklaring voor deze beperkte stijging zien we in deze test al; dit ding is sneller dan elke midrange kaart die AMD te bieden heeft. Als Nvidia een nog snellere kaart op dit prijspunt zet dan kan AMD met zijn RX 580 kaarten net zo goed door de shredder halen. Vergis je niet, een RX 580 is niet alleen +- 20% trager, het verschil tussen 198W en 108W in gaming load is ook enorm. Dus AMD Radeon krijgt het nu al heel zwaar."
GTX 1660;1;0.7328792214393616;Amd Moet 50 euro gaan zakken. Probleem opgelost
GTX 1660;2;0.3994448184967041;Dus... Zie ik het nou goed dat de 1060 een die size had van 200mm2, de 1660(ti) 284mm2, dus dat is 42% meer, en daarvoor krijg je (bij een 1660TI) minder dan 40% extra prestaties... Dus per vierkante millimeter zijn de prestaties gezakt, terwijl er ook nog duurder geheugen gebruikt wordt. Toch vreemd.
GTX 1660;1;0.5599380731582642;Jullie hadden in de conclusie ook nog wel even mogen noemen dat het stroomverbruik ingame van de concurrent gewoon dubbel zo hoog ligt. De 1660 is dus niet alleen sneller en goedkoper, maar ook nog eens véél zuiniger. Eigenlijk is er met deze prijzen 0 reden om voor de 590 te gaan tenzij je echt enorm merkgebonden bent.
GTX 1660;4;0.5522365570068359;@PiweD Van deze review word ik toch wel weer een beetje enthousiast. En dan niet over de kaart zelf want die is niet heel best (heeft al VRAM bottleneck op 1080p ultra in sommige gevallen en valt dan beduidend slechter uit tov andere kaarten), maar wel over de hoeveelheid tests en data en de gekozen games. Top!
GTX 1660;2;0.3760170638561249;De vorige keer dat ik het zag dacht ik dat het een typfout was, maar nu zie ik het weer: De 1660 heeft net zo veel transistoren als de 1060. Op een kleiner procedé, nu 12nm ipv 16nm, zou dat resulteren in een kleinere die toch? Echter is de die size zelfs gegroeid! En niet zo een beetje ook niet, van 200mm2 naar 284mm2! Kan iemand hier iets zinnigs over uitleggen? Het licht in ieder geval niet aan de architectuur verandering rondom de tensor of RT cores want die heeft hij niet...
GTX 1660;3;0.45240458846092224;Bedankt voor de review, hoewel ik het gevoel heb dat niet relevante games worden getest. Ik zie geen hyped games zoals APEX, maar meer het oude vertrouwde.
GTX 1660;3;0.3940369784832001;Is het mogelijk om de prestatieindex uit te breiden met de 1070 bijv ?
GTX 1660;1;0.4198538661003113;Is het mogelijk om de Tweakers.net benchmarks methodiek/scripts/etc. openbaar te maken? Zou ter vergelijk ook zelf jullie tests willen uitvoeren op mijn eigen pc FarCry 5 heeft ook een eigen benchmark bijvoorbeeld. Waarom is niet voor deze benchmark en data gekozen?
GTX 1660;4;0.5699841976165771;Prima concurrent voor de RX590 kaarten. AVG FPS is meestal wel goed maar die min fps is dan wel veel lager in sommige games. AMD kan de RX590 ook makkelijk voor 200-210 euro gaan verkopen wat het is eigenlijk een RX580 qua PCB alleen een zit er een nieuwere chip revisie op 12nm op gesoldeerd. En voor +- 200 euro is de RX590 een prima alternatief. De RX580 was al de betere keuzen tov de RX580 @ 180 euro nieuw in de winkel.
GTX 1660;3;0.5291327238082886;In dat opzicht is het verschil heel groot idd. Verbruik is gelijk aan een Vega 56 alleen een stuk trager. Ze zouden er goed aan doen de 570 en 580 opnieuw uit te brengen op 12nm met lagere voltages. Dan hoeven de prestaties niet achteruit maar verbruiken ze wel aanzienlijk minder. Ze hebben de 590 veel te ver gepushed amd moet daar echt mee stoppen. Vega hetzelfde er kan zo 30-40 watt van af zonder prestatie verlies en nog genoeg oc ruimte. Als je maximaal optimaliseert kan ze zo 60-75 watt lager uitkomen. Soms nog meer.
GTX 1660;5;0.5188400149345398;En gebruiksvriendelijker en stabieler.
GTX 1650;1;0.4717958867549896;ben ik de enige die de 1060 in dit rijtje mist als vergelijkings matteriaal, alle recente 50 en 60 kaaarten staan er in behalve de 1060.
GTX 1650;1;0.42553257942199707;Ik heb de GTX 1060 6GB toegevoegd (iets oudere driver: 418.91) met stock clocks. De 1070/1080 staat er niet bij, da's een compleet ander segment, de 1070 begint bij 350 euro, meeste 400 euro.
GTX 1650;2;0.2715725302696228;Hm ik heb wel een feature request voor T.Net: zelf hardware toevoegen bij vergelijkingen (in het hele artikel). Uiteraard besef ik me dat je dan appels met peren vergelijkt met drivers en andere hardware enzo... Maar als dat transparant gebeurt lijkt me dat geen issue. Ik wil bij dit soort dingen nl vooral weten hoe het scoort tov. mijn huidige systeem, om te kunnen beoordelen of ik niet een keer moet upgraden.
GTX 1650;5;0.34541311860084534;die feature request heb ik al sinds de nieuwe benchdb is ingevoerd, een jaar of 4-5 geleden
GTX 1650;2;0.41775161027908325;Mja, dan krijg je zoiets als bij SolidWorks Performance Score, waar iedereen zijn score kan indienen. Je zoekt je een ongeluk naar een systeem met bepaalde specs, als die al bestaat. Grappig, maar ik vind het een rommeltje.
GTX 1650;2;0.3936445116996765;zijn er voor de rx570 en 580 4gb of 8gb kaarten gebruikt? Kon het zo snel niet vinden in de review.
GTX 1650;3;0.2986482083797455;4GB voor 570, 8GB voor 580, ik edit de namen in de grafieken even
GTX 1650;4;0.3252203166484833;blijkbaar is de RX570 8GB nog weer een stukje sneller en zelf die kaart heb je voor minder dan de GTX1650
GTX 1650;5;0.3257123827934265;Staat op de lijst voor een test
GTX 1650;5;0.7059711217880249;Top! Bedankt, dan weet ik nu wat m'n upgrade gaat worden
GTX 1650;1;0.6157963275909424;Een geundervolte 480 voor 100 euro van marktplaats?
GTX 1650;1;0.40455499291419983;Nope, een RX570 4GB ITX uit Duitsland
GTX 1650;1;0.3859596252441406;Ik zie hem er (nog) niet bij staan onder kopje 17 en bijvoorbeeld 19. Maar misschien kijk ik verkeerd?
GTX 1650;1;0.2844192087650299;Dat zijn de onderlinge vergelijkingen van de 1650-kaarten, daar staan uberhaupt geen andere videokaarten bij.
GTX 1650;3;0.647267758846283;"Okee, da's logisch. Maar waar moet ik dan op klikken om dit te zien wat je hebt toegevoegd? Welke URL? ""Ik heb de GTX 1060 6GB toegevoegd (iets oudere driver: 418.91) met stock clocks."""
GTX 1650;1;0.32518860697746277;p5 tm p16 reviews: De GTX 1650 - Vijf instapkaarten van fabrikanten tot reviews: De GTX 1650 - Vijf instapkaarten van fabrikanten
GTX 1650;5;0.5028185248374939;Oh pfff Dank je, ik was nog niet wakker.
GTX 1650;4;0.2846148908138275;
GTX 1650;3;0.5223747491836548;Nieuw wel maar er zijn denk ik genoeg sloebers als ik die graag een tweedehandsje op te kop tikken.
GTX 1650;3;0.32817304134368896;"Bij Guru3d zijn een aantal 1650's ook getest en daar staan de 1060's wel ter vergelijking genoemd: link; de 1060 is 10-35% sneller afhankelijk van met welke editie je vergelijkt (3gb/6gb/oc) Edit: Was even heel snel uit het hoofd gerekend op basis van de eerste kaarten die ik zag, nu aangepast. Thx lasty."
GTX 1650;3;0.432149738073349;"Apart, als ik reken met de guru3d link, kom ik uit op een verschil van tussen de 10-26 procent, nogal andere waarde. Kijk je bij de samenvattingen van techpowerup (MSI GTX1650 Gaming X, een van de snelste) dan zie je nog grotere verschillen (over de gehele benchmark data); - een standaard geclockte 1060 3GB is tussen de 10-15% sneller dan een aftermarket 1650 - een standaard geclockte 1060 6GB is tussen de 25-35% sneller dan een aftermarket 1650. zie hier voor de link;"
GTX 1650;3;0.2843410074710846;Niet alleen de 1060, ook zou ik in dit soort testen wel een 1070/1080 willen zien.
GTX 1650;3;0.5341857075691223;Ja maar dat is een heel ander segment dat kan ik nog begrijpen dat je geen lowend en highend gaat moxen maar de top next gen lowend vergelijk je meestal wel met de previous gen mid range.
GTX 1650;3;0.5089452862739563;Inderdaad. Het kan interessant zijn omdat je mogelijk twijfelt tussen een nieuwe 1650 of 2e hands 1070, zitten namelijk nu wel in eenzelfde prijsklasse
GTX 1650;1;0.29240673780441284;Serieus? kan me niet voorstellen dat iemand ooit zal twijfelen tussen die 2 kaarten. De 1070 is echt vele malen sneller als de 1650 en zijn nu, en op moment van uitkomen totaal andere klasse kaarten
GTX 1650;2;0.34001344442367554;Er zijn mensen die liever niet 2e hands kopen met korte of geen garantietermijn, maar wel gebonden zijn aan een max budget. Zo gek is dat toch niet?
GTX 1650;3;0.3607305884361267;Nee maar het is wel gek om te twijfelen tussen een 1070 en een 1650.. de reden heb ik al uitgelegd
GTX 1650;2;0.430009663105011;Dan snap ik je nog steeds niet. Als ik de keuze heb voor een 2e hands BMW 1M serie of een nieuwe BMW 118i Ze kosten bijna evenveel. Van de nieuwe 118i serie ben ik 1ste eigenaar, volledige garantie. Een paar nieuwe technische snufjes, stukken zuiniger. Wel langzamer en oogt wat casual. De 1M is een paar jaar ouder, heeft bijna 100K erop zitten. Geen garantie. Ik weet niet 100% zeker wat de staat is, misschien is er wel volop mee geracet. Daarentegen is dit echt wel een veel snellere sportievere auto. Beide auto's brengen mij echter van a naar b Ik zou dit een lastige keuze vinden en twijfelen.
GTX 1650;3;0.3062785267829895;ik probeer te zeggen dat een 1070 een midrange kaart is waar je makkelijk op 1080p alles op Ultra kan spelen en de 1650 is een budget kaart die al moeite met 1080p Ultra kan hebben Jouw autovergelijking kan je beter doen dat je twijfelt tussen een 2e hand BWM 1M of een nieuwe Lada die toevallig even veel kosten Totaal ander type en klasse auto's, waardoor ik me niet kan voorstellen dat mensen tussen die 2 kaarten zullen twijfelen Als je goed wilt eten twijfel je toch ook niet tussen MC Donalds en een 5 sterren restaurant als die toevallig het zelfde kosten Ja ze vullen allebei je maag, maar de keuze lijkt mij super simpel
GTX 1650;1;0.3416351079940796;Ah ja, ik zie nu dat er een performance verschil is van +/-78% Bizar, wie koopt er dan nog een nieuwe Lada? Nog erger dat fabrikant AvtoVAZ er brood in ziet om lada's te produceren...
GTX 1650;2;0.3433377742767334;Een 1070 is 2dehans ook niet in dezelfde prijs klasse hoor.. 250+ is toch wel gangbaar.
GTX 1650;1;0.25178512930870056;Ik heb december 2018, een 2e hands gigabyte 1070 windforce OC voor €200 (225 vraagprijs). Weliswaar geen garantie meer, maar vooralsnog helemaal in orde.
GTX 1650;2;0.42620307207107544;Gefeliciteerd, maar als je nu kijkt zijn ze bijna allemaal duurder.
GTX 1650;3;0.4201807677745819;"Ik ook. Kunnen we niet zelf die grafiek maken o.i.d.? Tweakers slaat dat toch tegenwoordig op met een ""nieuw"" systeem enzo? Zou misschien een fijne feature zijn, een eigen apparaat toevoegen. Edit: Verder dan dit kom ik niet. Helaas zitten daar geen benchmarks bij: Edit2: Ontopic: Voor onder de 200 pak je echter wel een mooie kaart uit de rij die precies overal qua kwaliteit en prestaties goed zit. Ideaal voor een casual gamer of een beginner met weinig budget. 200 euro wegboeken is dan nog eens te doen. Nu was de 1060 al een prima kaart, maar ik weet dus niet of er veel verschil in prijs en kwaliteit zit wat dat betreft."
GTX 1650;1;0.43036705255508423;Nee, niet de enige. Ik mis hem ook! Had wat mij betreft erbij gemogen als tegenhanger van de RX570/580.
GTX 1650;3;0.4123065173625946;Ik mis ook de 960. Maar goed, die lijkt erg op de 1050ti.
GTX 1650;3;0.3566533625125885;Conclusie, koop AMD, tenzij je gelimiteerd zit aan 75 watt/geen koeling hebt. Best jammer, maar ik had dan ook niet verwacht dat de kaart een betere prijs/performance ging hebben
GTX 1650;1;0.37762174010276794;Conclusie koop NVIDIA, als je in Nederland woont. Ik ben zeker van de prijs/kwaliteit maar stroom in Nederland is zo duur. Zeker vanaf dit jaar dat je wss 22 cent per kWH betaalt door de hogere belastingen. De rx570 gebruikt 75 watt meer, neem aan dat je 10h gamet per week. Dat is 0.75kWH, in 1 jaar is dat besparing van ~8,50 en in 2 jaar heb je winst... Nou weet ik dat de kids geen stroom betalen maar in principe ben je echt wel duurder uit.
GTX 1650;1;0.3399747610092163;Of je gebruikt AMD Wattman en begrenst het aantal frames op bijvoorbeeld 60fps als je monitor bijvoorbeeld maar 60hz is. Zo produceert de videokaart geen overbodige frames en bespaard hij veel elektriciteit....
GTX 1650;1;0.4000914692878723;En hoeveel euro bespaar je er op jaarbasis mee denk je door voor een Nvidia te kiezen tov AMD? Dat zal hooguit een tientje zijn tenzij je je GPU 24/7 vol laat stampen.
GTX 1650;2;0.2931571304798126;"Ik heb veel budget systemen gebouwd op basis van AMD. RX570 en RX580 kan je flink undervolten zonder enig merkbaar prestatie verlies; ik doe dat standaard. Daarmee gaan stroomverbruik en noiselevels omlaag. Met een undervolt ben je 4-5 jaar verder voor je de meerprijs van een 1650 eruit hebt en in de tussentijd heb je slechtere prestaties."
GTX 1650;1;0.5885800123214722;Het verschil in aanschaf is 25 euro, dus na 3 jaar ben je quitte, na 4 jaar heb je 8,50 bespaart.
GTX 1650;1;0.30069687962532043;En dan heb je 4 jaar op onspeelbaar lage fps gegamed
GTX 1650;1;0.47713223099708557;Alsof je echt naar dat kijkt en niet naar de prestaties van de kaart zelf.
GTX 1650;5;0.4512835144996643;Stroomverbruik, zwaardere voeding, ventilators die harder moeten werken en meer lawaai maken.... Genoeg redenen om voor zuinig te gaan.
GTX 1650;2;0.37022292613983154;Zwaardere voeding is een beetje een non argument. GTX1650 heeft over het algemeen alsnog een 6pin nodig en de RX570 draait ook op elke normale 400W voeding. Ik had een BeQuiet 400W van €40 die een RX470 prima draaide. Net of je nóg goedkoper gaat met een GTX1650...
GTX 1650;3;0.4484255909919739;Als de ene kaart 100 watt meer gebruikt dan de andere kaart, dan lijkt mij toch dat de pc met de zuinige kaart met een 100 watt lichtere voeding uit kan.
GTX 1650;2;0.41484832763671875;Maar die bestaan bijna niet of worden niet verkocht. 400W is voldoende voor de rx570 en 300W voedingen die een beetje kwaliteit hebben zijn zeldzaam.
GTX 1650;3;0.557641863822937;Ze bestaan wel. Maar belangrijker is dat je bij gelijke prestaties natuurlijk altijd voor de zuinigste zou moeten gaan. Afgezien van dat je het op termijn terugverdient.
GTX 1650;3;0.4356992840766907;Echter even duur als de 400W variant: pricewatch: be quiet! Pure Power 11 400W Dus ze bestaan wel maar zijn niet goedkoper. Dan hou je alleen nog besparing op het verbruik zelf over.
GTX 1650;3;0.3805983066558838;ja en de 1650 en RX570 zijn dus niet gelijkwaardig want de RX570 verslaat de 1650 op elk vlak
GTX 1650;3;0.6215667724609375;Ok, da's een goed argument om de andere te nemen.
GTX 1650;2;0.39456626772880554;Alleen de oude Dell/HP/lenovo systemen hebben meestal nog 250W/300W voedingen erin zitten. Als je dan een €180/200 videokaart gaat stoppen in een systeem dat misschien nog net €100 waard is weet ik niet wat mensen in hun hoofd halen. Heb je zo'n soort systeem uit 2011 en later zit er vaak gewoon een 400/450W voeding in met een 6pin aansluiting en kun je beter voor de RX570 kiezen.
GTX 1650;2;0.44050300121307373;als je wat vaker een OEM workstation openmaakt zul je zien dat er niks extra's in zit dus ook niet een 6 pin als deze niet gebruikt wordt. vaak heeft het moederbord afwijkende stekkers \ aansluitingen , waardoor je de voeding niet zomaar kunt vervangen door een aftermarket model. ik heb hier een gekregen xeon 1280 workstation staan voor de kids (o.a. fortnite) waar nu een 1050 inzit ,die binnenkort wordt vervangen door een 1650 (zonder 6pin aansluiting) als de prijs wat zakt \ dan wel een open doos c.q. 2e hands model mijn pad kruist. Als je een pc hebt met 6 pin voedingaansluitingen zou ik zeker geen 1650 nemen dan heb je idd betere alternatieven.
GTX 1650;2;0.32279929518699646;"Laat ik dat nou net wel eens gedaan hebben Wat ik zeg, op de wat latere modellen zit vaak wel een 6 pins aansluiting op de voeding. Voor de afwijkende kabels zijn gewoon verloop stekkers te bestellen voor spotgoedkoop en heb dit dan al vaker gedaan. Als je gewoon een tower model hebt van een oem pc kun je de voeding dus makkelijk vervangen, dit namelijk ook wel een aantal keren gedaan. Er is gewoon geen goede reden om een 1650 boven de rx470 te verkiezen want zelfs de beter geklokte 1650 hebben ""TADA"" ook nog steeds een 6 pins aansluiting nodig"
GTX 1650;2;0.2940974235534668;Ik heb hier nog een 6870 in Mn game bak hangen - aangezien ik weinig game - dat ding slurpt ook behoorlijk maar Mn fans hoeven er echt niet harder on te draaien.
GTX 1650;2;0.4067697823047638;Ook niet de fans op de videokaart zelf? Volgens mij moet 100 watt verschil in verbruik van de videokaart toch wel invloed hebben. Het zijn ook nog vaak kleine fans die veel geluid maken.
GTX 1650;1;0.7102046608924866;Compleet “stil” systeem, komt niet boven een optiplexje van Dell uit (die malen meer geluid nog), en dat met 2 fans op de GPU, en 5 in de kast.
GTX 1650;4;0.30411630868911743;Netjes.
GTX 1650;3;0.29634854197502136;De conclusie is dat je veel beter een Radeon RX 570 kan kopen, zijn nu goedkoper en sneller. Zo lang er geen Low Profile versies van de Geforce GTX 1650 uitkomen zie ik geen echte nut om voor de Geforce GTX 1650 te gaan. Als je kijk naar stroom gebruik, en je gamed vaak en je wil zuiniger gaan is de Geforce GTX 1650 een betere keuze misschien, aangezien hij 56% zuiniger is dan de Radeon RX 570. Mooie review van Tweakers deze keer.
GTX 1650;1;0.354422926902771;Zelfs toen ik mijn R9 290X met geluidsproductie niveautje turbine motor nog had. Had ik zeer weinig last van de herrie onder het gamen. Meeste mensen game namelijk met geluid aan
GTX 1650;3;0.308269739151001;Ja precies 390x hier. Maar met mijn headset op hoor ik echt niet meer of die kaart een kabaal maakt. Staat onder mijn bureau wat ook wel wat mee helpt tegen geluid.
GTX 1650;4;0.542039692401886;Precies! En wat extra been verwarming in de koudere wintermaanden is wel plezant Benen medium rare in de zomer is dan weer wat minder.. maar oké
GTX 1650;1;0.4064754247665405;Nu zit de 1650 al bijna aan het 1080p limiet, laat staan 1440p. Over één jaar moet je op minimaal gaan gamen, over twee jaar heb je een dia slide, Kun je beter nu een 570/580 kopen (je zal toch niet 24/7 gamen) of even door sparen voor een 1660. Die kaarten zijn veel beter 1080 en 1440 future proof.
GTX 1650;1;0.5742590427398682;8,50 per jaar is mijn inziens verwaarloosbaar. Over 2 jaar kan je deze kaart misschien al afschrijven, omdat hij niet meer meekomt met de nieuwste games. Ik moet bij de AMD kaarten zelf steeds vooral denken aan de zwaardere voeding doe je moet kopen.
GTX 1650;3;0.47128334641456604;Leuk, alleen een rx 570 is al voor 130 euro te krijgen, een 8gb model zelfs voor 138. Succes met je jarenlange mindere prestaties voor een paar euro minder na 3 jaar gebruik.
GTX 1650;2;0.434831440448761;Op een gemiddeld stroom verbruik van 4 personen in ons land (meer dan 4000 kWh) is dat een verhoging van welgeteld 0,25% aan verbruik. Dan redt je het niet om het verschil in 2 jaar tijd goed te maken, Maar dan moet jij wel goed rekenen... 0,75 * 52 is geen 8,5 , maar 39 kWh. dan kom je uit op 1% verhoging van het verbruik. Besef dat de grootste kosten liggen in de netwerkkosten. Niet in de kosten voor de energie zelf.
GTX 1650;2;0.49748966097831726;In andere woorden totaal niet de moeite waard dus, en al helemaal niet waard om 2 jaar jang met mindere performance te gaan spelen ook leuk om te zien
GTX 1650;1;0.44815900921821594;Ook als je CEMU gebruikt kan je beter Nvidia nemen eigenlijk werkt het niet op AMD, sowieso ontwikkelen zeker obscure software makers eerder voor de grote groep Nvidia gebruikers.
GTX 1650;1;0.369642049074173;Alleen interessant als je voeding een RX570 niet trekt. Een maandje geleden voor €150 een 8gb RX570 aangeschaft inclusief the Division 2 & Devil May Cry, daar gaan geen 1650 tegenop.
GTX 1650;1;0.5843870043754578;Wat een lauwe review. Deze kaart zou moeten worden afgebrand. De rx 570 is voor een mooi model al voor 130 euro te koop, er staan er meerdere in de pw. En dan wordt hier gesproken over 'hetzelfde of minder'. Het verschil is minimaal 25 euro en dan heb je een heel simpele 1650 als je geluk hebt. Sterker nog, er staan meerdere 8gb varianten van de rx 570 te koop voor rond de 139 euro! Test die maar eens tegenover de 1650 op 1440p. En dan het stroomverbruik, jaaa! als er niets meer is voor fanboys om over te hebben dan halen we dat aan. Je kunt 3 jaar gamen met dit prijsverschil om ongeveer hetzelfde kwijt te zijn en dan heb je jarenlang een stuk betere prestaties gehad. En waar kies je een videokaart voor? Game je om stroom te besparen? Wie doet dat? Het noemen is prima, maar kom op, dit gaat nergens over met de rx 570 kun je in bijna alle games prima gamen op 1440p, met de 1650 gaat je dat gewoon niet lukken. Wat een verschil! Mijn respect richting Tweakers is weer een stukje minder en het was al zo hoog. Ik vind het oprecht jammer.
GTX 1650;3;0.35025522112846375;De eindconclusie is toch duidelijk: Kortom, hoewel de prestaties van de GTX 1650 een flinke verbetering van de 1050 en 1050 Ti vormen, koop je voor net zoveel of minder geld een snellere RX 570 of zelfs 580, waarbij je dan wel het veel hogere energiegebruik voor lief moet nemen. We kunnen ons dan ook eigenlijk moeilijk voorstellen dat je een GTX 1650 koopt die in het premiumsegment zit. Hou het bij de goedkoopste varianten als je om een slot-powered kaart verlegen zit, maar koop een snellere, even dure kaart zoals de 1660 of RX 5x0-kaarten als je die kunt voeden. Als je daar niet uithaalt dat deze kaart niks toevoegt ten opzichte van concurrerende kaarten, denk ik dat Tweakers nog meer Jip en Janneke taal zal moeten gaan gebruiken...
GTX 1650;1;0.5811756253242493;Als je het stroomverbruik dan zo aanhaalt, wat veel meer lijkt tegenover prijs en prestaties dan moet men hier ook echt transparant zijn. Ik zie geen enkele techsite die inzichtelijk maakt wat het verbruik je daadwerkelijk kost. En dat het verschil dan na 3 jaar een paar euro is terwijl je inlevert op prestaties, soms zelfs flink, in vergelijking met een 2 jaar (3jaar eigenlijk) oudere kaart die ook nog eens een stuk goedkoper is.. Je zou er zelfs een makkelijke tool bij kunnen zetten waarbij men zelf kan invullen wat het verbruik en de kWh prijs is om tot een eigen conclusie te komen. Maar alleen roepen dat het zo veel meer is, gaat nergens over. Wat in andere reacties terecht wordt opgemerkt is dat je bij koude dagen minder hoeft te stoken omdat je pc iets meer warmte uitstraalt, klinkt dom maar het is wel zo. Dit is de meest slechte Nvidia kaart die ze op de markt hebben gebracht, het voegt niets toe. Daar moet je Nvidia op afstraffen, het is niet dat ze geen beter product of een lagere prijs kunnen bieden. Ik ken geen mensen die zich daadwerkelijk druk maken om een paar euro na jaren gebruik als ze al die jaren meer prestaties hebben gehad. En als je meer gamed dan hoef je waarschijnlijk niet de rekening te betalen omdat je bij je ouders woont of inclusief huurt als student. En als je je er wel druk om maakt dan ben je een groene oliebol.
GTX 1650;2;0.4717313349246979;Omdat dat echt een triviale berekening is? runeazn in 'reviews: De GTX 1650 - Vijf instapkaarten van fabrikanten' rekent het voor, maar dat wordt door de AMD fanboys weggemod. (En dat zeg ik als eigenaar van een AMD GPU). Daar komt bovenop nog dat je een zwaardere voeding nodig hebt, en dat bij verder gelijke omstandigheden extra warmte altijd extra geluid betekent om het weer te koelen. Het klinkt dom omdat het dom is. En voor iedereen met airco moet hij in warme dagen nog harder draaien. Wat betreft koude dagen: Rechtstreeks verwarmen met gas is enorm veel goedkoper dan elektrisch, daarom gebruiken we gas. Elektrisch verwarmen kan wel, maar dan gebruik je een warmtepomp, die zo een factor 4 efficienter is dan je GPU om te verwarmen.
GTX 1650;1;0.49410566687583923;Dat komt omdat die reactie onjuist is zoals veel reacties er op aangeven. Een rx 570 is namelijk 25 euro goedkoper, dus na 2 jaar heb je helemaal geen winst. Dat duurt 3 jaar of langer en tegen die tijd koop je weer een nieuwe kaart. Tijdens al die jaren heb je dan ook een hoop meer performance gehad. Er zijn zelfs 8gb modellen te koop voor rond de 139 euro, wat een koopje. Een zwaardere voeding? 400 watt is meer dan genoeg, waarschijnlijk is 350 watt ook voldoende. Voor 35 euro heb je al een voeding van een goed merk. Als je minder vermogen hebt dan is het waarschijnlijk een hele discutabele kwaliteit die je liever sowieso een update geeft. En dan nog, die situatie zal bijna niet voorkomen. Gelijke omstandigheden heb je niet omdat de rx 570 modellen met een prima koeler komen, daar zitten goede modellen tussen ipv de goedkoopste 1650 modellen die met een simpel mini koelertje komen, mogelijk maken die zelfs meer herrie. Extra geluid is gewoon echt complete onzin. En het gaat er niet om dat je electra of gas gebruikt om te verwarmen, dan snap je het niet. Het gaat er om dat de verwarming niet of veel minder aan hoeft tijdens de winter. Dat is vooral om aan te geven hoe dom het gezeur over een klein beetje meer stoom is. Maar goed, als je dit plaats valt er waarschijnlijk eindeloos te discussiëren en heeft dat toch geen zin. Dan ben je meer van de fictie dan de feiten en helaas zijn er daar genoeg van en gaat de 1650 het nog goed doen ook. Dé reden dat Nvidia die belachelijke prijsverhogingen kan doorvoeren zonder dat er consequenties zijn, het wordt toch wel gekocht. Zelfs een gedrocht van een product als dit wordt nog de hemel in geprezen. Oh, ik zie nu dat je ook nog 2 gratis games krijgt bij een 8 GB rx 570 van 139 euro. Ok doei.
GTX 1650;3;0.33023571968078613;Verder redelijk eens wel, maar stroomverbruik is ook hitteproductie. Wil je een HTPC of mini game pc bouwen dan kan dat daarvoor wel echt technische winst zijn. Een pagina in het artikel daarover zou voor die use case niet misstaan ook, als het energieverbruik al zo groot benoemd moet. Dan zou je er al meer mee kunnen. Iets voor een best buy thema lijkt me.
GTX 1650;3;0.4447095990180969;Mee eens, dan zou het goed onderbouwd moeten zijn. Hoewel ik niet denk dat de Rx 570 erg heet wordt. Ik heb de kaart zelf niet, maar las ervaringen van mensen die hem wel hebben dat deze kaart niet perse heet hoeft te worden. Zou in een erg kleine case wel kunnen natuurlijk. 150 watt geeft echt niet veel warmte. Je hebt het dan over hele specifieke gevallen waar het misschien een mogelijkheid is, maar dan nog heb je vaak nog betere opties dan deze kaart.
GTX 1650;2;0.4700433611869812;Gezien het huidige prijspeil is de 1650 zoals de conclusie al aangeeft niet interessant. In het slechtste geval verbruikt een 580(of570)100Watt meer in load... dat is 2 cent per uur. Om een prijsverschil van 10 euro te overbruggen moet je dan al 500 uur load draaien, los van de extra warmte en geluid.
GTX 1650;4;0.34233203530311584;En in de winter is de extra warmte geen verlies omdat je verwarming dit dan niet hoeft te leveren
GTX 1650;2;0.3885733485221863;Mits je PC natuurlijk in dezelfde ruimte staat als je thermostaat, anders wordt het dubbel zo warm
GTX 1650;1;0.3017426133155823;Ik heb zelfs geen thermostaat in huis. Bij stofzuigen, strijken, koken moet je dus een deur/venster openzetten .
GTX 1650;5;0.5930207967758179;Is echt handig hoor! Heb nog een AMD FX systeem staan als kachel voor de winter
GTX 1650;1;0.5433063507080078;"""De RX 570 is namelijk zo rond de 150 euro te koop, met soms een kaart voor een tientje minder."" De RX570 is al lange tijd te koop voor 129,-"
GTX 1650;2;0.4422673285007477;Dat is bij Max ICT, een bedrijf dat er om bekend staat te lage prijzen neer te zetten en niet te leveren wanneer je een bestelling plaatst. Ik zou Max ICT prijzen niet direct serieus nemen. Wat is de prijs bij de eerstvolgende vermelding? Volgens mij €150+, wat dan weer redelijk in lijn is met het artikel. Bij alle artikelen die je linkt is er steeds een aanbieder veel goedkoper dan de rest. Daardoor kun je niet echt stellen dat de kaarten standaard op dat prijspunt zitten.
GTX 1650;1;0.421330988407135;Als dat zo is, waarom staat Max ICT dan nog als site genoteerd in pricewatch ?
GTX 1650;2;0.2889312207698822;Dat durf ik niet te zeggen. Daar is wel een hele discussie over op het forum.
GTX 1650;3;0.5180268287658691;De RX 570 kan prima undervolted worden. Dat scheelt nogal in opgenomen vermogen. Bij mij is het zonder prestatieverlies van 1,15v naar 1,02v gegaan. Natuurlijk voorkomt dit niet dat de voeding alsnog een pci-e stroomstekker vereist, dus een krachtiger voeding dan in de gtx1650 situatie blijft een eis. Toch zal een voeding voor de gtx1650 ook van goede kwaliteit moeten zijn, het is volledig afhankelijk van een volbelast pci-e slot.
GTX 1650;5;0.44430264830589294;En de AMD driver worden over de jaren wel steeds beter. Extra % snelheid gratis erbij
GTX 1650;2;0.38062772154808044;Veelste duur voor deze prestaties, beter een 1660TI kopen dan heb je er tenminste meer jaren plezier mee voor deze prijsklasse.
GTX 1650;2;0.46318739652633667;1660ti, dat dacht ik ook en toen er van het weekend een voorbij kwam in de aanbieding voor 235.- (kleine itx versie) was wat mij betreft het 1650 verhaal voorbij. Jammer want de 1050ti was wel een leuk kaartje de afgelopen jaren, weinig stroom/geluid en klein.
GTX 1650;2;0.44435861706733704;Voor een beetje tweaker zou het hoge stroomverbruik van een AMD kaart indien een issue geen struikelblok hoeven zijn. Met Wattman is een RX570 eenvoudig te undervolten naar een verbruik van ongeveer 100 watt en de prestaties van de kaart liggen dan nog steeds hoger dan die van een 1650.
GTX 1650;2;0.3681890368461609;Maarja, dan moet men weer extra handelingen verrichten en de vergelijking gaat dan ook niet op, dit soort testen moeten uitgevoerd worden met de standaard versies zoals ze geleverd worden.
GTX 1650;1;0.7411246299743652;Belachelijke bedragen voor de prestaties die de kaart geeft. Ik keek hier toch wel stiekem naar uit om in mijn media pc te stoppen. Maar dat is het geld niet waard. Het zal gewoon lekker een rx 570 worden. Die €10 euro per jaar extra aan stroom zal me een worst wezen. Voor die mensen die daar over klagen heb ik een tip, ga 1x per jaar minder naar de mac dan heb je het er al uit.
GTX 1650;2;0.6119256019592285;Tja de 1050ti is een stuk langzamer dan een r9 290(die ongeveer dezelfde snelheid heeft als een gtx 1060). Deze kan je nu tweedehands krijgen voor rond de 90 euro. Ik snap dus niet helemaal waar nvidia mee bezig is. Lijkt wel intel praktijken, geen concurrentie dus dan maar minimale upgrades uitbrengen voor maximale prijs. Erg jammer, gezien de ouderdom van vorige generatie.
GTX 1650;3;0.41784703731536865;Als ik de 1050Ti vergelijk met de 1650 dan is naast de architectuur enkel het volgende dat verschilt: rekenkernen: +16.6% kloksnelheid: +15,0% (boost: +19,6%) geheugensnelheid: + 14,1% Vervolgens heb ik me bezig gehouden met het gemiddelde te berekenen van alle benchmarks in deze review. Conclusie: De 1650 presteert gemiddeld 25,8% beter dan de 1050Ti. Kort door de bocht kan je stellen dat 15% bekomen wordt door de hogere snelheden en meer rekenkernen, terwijl de nieuwe Turing architectuur voor de overige 10% prestatiewinst zorgt. In de benchmarks vond ik wel een aantal eigenaardigheden:In Wolfenstein II - 1440p - Mein Leben presteert de 1050 beter dan de 1050Ti.In Shadow of the Tomb Raider - 4K - Medium presteert de 1050Ti beter dan de 1650.
GTX 1650;3;0.5189582705497742;Deze kaart is alleen echt interessant als je om wat voor reden dan ook vastzit aan een laag stroomverbruik. Voor een snelle ultrabook - de XPS15 bv - is het een prima - of eigenlijk het enige - alternatief. Als je gewoon een low budget gaming pc'tje wilt is een AMD-kaart een betere keuze.
GTX 1650;3;0.4524153172969818;Teleurstellende GPU , maar door het lage stroomverbruik kan een OEM-builder met een veel goedkopere voeding af en dan verdwijnt voor hen het voordeel van de RX570 en zal men denk ik mede door het nog steeds sterke NVIDIA brand toch massaal voor de 1650 kiezen ? De goede verkoop van de AMD 2200G&2400G zal er voor zorgen dat dat nog wel een tijdje het minimum blijft voor 1080p low settings in games (wel op 30 fps, maar geen 'slideshow') Tov van die performance is de 1650 toch wel heel wat sneller en zal dus voor de niet veeleisende gamer lang genoeg zijn. NVIDIA weet nog steeds wat ze doen en zijn net als Intel vooral erg voorzichtig om niet te hard de oude voorraden te beconcurreren… Zo houden ze een goede relatie met webstores en andere leveranciers. Vervelend voor de Tweakers maar in deze slecht functionerende olichargie wel begrijpelijk van NVIDIA
GTX 1650;1;0.39416009187698364;Dus, 2e hands RX570 kopen 8 tientjes en je bent de man
GTX 1650;1;0.5389660596847534;"Misschien heb ik iets gemist hoor (het is een tijd geleden dat ik zelf een systeem gebouwd heb), maar een kaart van minstens 180 euro, die twee (of drie) slots inneemt telt tegenwoordig als ""instap""!? Zijn de ""ingebouwde GPUs"" (de APUs) tegenwoordig echt dusdanig snel dat single-slot kaarten geen bestaansrecht meer hebben?"
GTX 1650;2;0.4381220042705536;ik vind dat julie nog aardig zijn over deze kaart, dit is tog gewoon een wanproduct, die alleen op de markt is geplaatst om de recentelijke prijsverhogingen van nvidia te justificeren door te zeggen dat ze ook een budget-optie hebben in het turing assortiment (een hele slechte) die niet eens kan concureren (in prijs of performance) met 2 jaar oude tech (gtx570)
GTX 1650;4;0.5138792991638184;Ik vind dit een mooie review. Het geeft mij de info die ik graag wil hebben. Geluid, vermogen, prijsprestatie en vergelijk met andere producten. Ben een simpele gamer, 1080p. Wat ik ook plezierig vind van Nvidia is de makkelijke bijbehorende software.
GTX 1650;3;0.35943660140037537;Geen VR gaming vergelijking? Want er zijn mensen die benieuwd zijn of nieuwere chip betere VR functies / streaming encoding heeft. Dat telt namelijk wel mee voor budget VR gaming. En ermee vergelijken met GTX 1060 op gebied van VR gaming. Eigenlijk kun je alle kaarten in deze review checken op VR gaming met aantal populaire games op performance. Op gebied van gewone games kan je altijd instellingen bijstellen, wat geen probleem is. Maar bij VR gaming kun je eigenlijk niet veel instellen en dat is toch wel een reden om ook even ernaar te kijken: heeft GTX1650 wel zin voor VR gaming?
GTX 1650;3;0.2937874495983124;Wat je aan een goedkopere kaart (AMD) bespaart kan je aan een duurdere voeding (en hogere stroomrekening) weer uitgeven. Bijna 100W meer is een flink verschil.
GTX 1650;5;0.3772962987422943;standaard voeding in elke budget behuizing die je vandaag de dag koopt is 350W-400W. Niks duurder dus.
GTX 1080 Ti;1;0.31507354974746704;Tjeetje ik ben verbaasd om te zien hoe goed een 3x CF rx480 het doet! 3x een rx 480 heb je voor ongeveer 700 euro dat scheelt toch 150 euro met de 1080 ti. Maar ja als je een 3x CF setup een jaartje draait ben je aan stroom die 150 euro zo kwijt
GTX 1080 Ti;1;0.6062716245651245;De review is veel te kort door de bocht om dat te kunnen zeggen. Deze review is hoe men 10 jaar geleden reviews deed, met gemiddelde framerate en minimale framerate, maar beide vertellen eigenlijk niks over hoe soepel een game loopt. Voor dat moet je namelijk frametimes gaan meten. Gemiddelde kan immers wel hoog liggen als de frametimes ruk zijn dan loopt het alsnog niet soepel, zelfs bij 120 frame per seconde kan een game verschrikkelijk slecht lopen. Gebeurt eigenlijk nooit, maar is niet onmogelijk. Bij twee of meerdere videokaarten in SLI of CrossFire is het zelfs dood normaal dat frametimes stukken slechter zijn. Overigens heeft minimale framerate meten ook heel weinig nut. Als dit namelijk maar één maal in het uur voor komt en de rest van de tijd de minima veel hoger licht dan merk je er alsnog niks van. Is de minima dan een stuk hoger, maar dropt die wel vaak dan kan dit dus ook zorgen voor veel slechtere game ervaring. Daarom frametimes, frametimes, frametimes. Framerate metingen zijn iets van 5 jaar geleden. En metingen ertussen van een GTX Titan X Pascal met een oudere driver is natuurlijk not done. Zorg dan dat je de videokaart van iemand kunt lenen.
GTX 1080 Ti;3;0.39139503240585327;Ik weet het niet zeker, maar van frametimes heb je toch geen last met een G-SYNC display? Ik gebruik al bijna 2 jaar G-SYNC monitoren, schaamte dat ik dat niet eigenlijk weet
GTX 1080 Ti;1;0.5778065919876099;Nee G-Sync of ook FreeSync lost dat niet op. Bij beide technieken bepaald de videokaart wanneer de monitor een nieuwe frame weergeeft. Als de videokaart dan onregelmatig frames eruit gooit. Dus bijvoorbeeld drie frames binnen 30 milliseconden, dan maar één frame in 100 milliseconden, dan weer drie binnen 30 milliseconden en ga zo maar door. Dan geeft het scherm dit ook zo weer waardoor de game alsnog dus niet soepel loopt.
GTX 1080 Ti;3;0.3445851504802704;heb je een voorbeeldje van een site die wel goede reviews doet?
GTX 1080 Ti;4;0.3341897130012512;PC Perspective en Tech Report bijvoorbeeld.
GTX 1080 Ti;4;0.5260087847709656;PC Perspective is goed, net zoals Digital Foundry via YT of Eurogamer.
GTX 1080 Ti;3;0.382980078458786;Bij Hardware.info doen ze dit daarentegen wel (de frametimes meten):
GTX 1080 Ti;2;0.457645446062088;Alleen klopt dat maar half, met software metingen is het bekend dat SLI en CrossFire nogal eens afwijkt in vergelijking met de praktijk door bijvoorbeeld framepacing bij AMD. Frame pacing wordt echter later toegepast in de GPU dan dat de software meet. Wil je dus goede metingen doen dan moet dat met hardware. Overigens test HWI eigenlijk altijd alle kaarten met allemaal verschillende drivers waardoor onderling vergelijken niet eens goed mogelijk is.
GTX 1080 Ti;1;0.5336713194847107;Was frametimes meten anno 2016 al niet of bullshit(want onrealistisch beeld door software) of niet te doen voor middelgrote reviewers (want veel specialistische hardware en modificaties voor wel correcte cijfers)?
GTX 1080 Ti;3;0.4102073907852173;Ah, dat wist ik niet, dankje! Dat is wel balen, dat ze met oude drivers testen. Is niet helemaal eerlijk nee, maar kost wel veel tijd denk ik.
GTX 1080 Ti;2;0.40063878893852234;Ik begrijp je punt inderdaad. Reviews schrijven is echt een kunst, dat merk je wel weer. De framerates laten wel zien wat een kaart of in dit geval kaarten kunnen. Dus in dat opzicht blijft t opmerkelijk hoe goed 3x CF rx 480 presteert. Is het aan te raden in een systeem waarschijnlijk niet. Waarschijnlijk zullen de frametimes niet al te best zijn t.o.v. de 1080ti.
GTX 1080 Ti;2;0.40800946950912476;Dit heeft weinig met de schrijfkunst van de reviewer te maken, maar des te meer met de testers bij Tweakers.
GTX 1080 Ti;1;0.2615334093570709;Dat nog niet alleen die titan x hoe lang is deze al op de markt terwijl die 1080 ti net uit is. Als ze van die 1080 ti een titanx maken met de zelfde techniek dan zal die titan wel weer gaan winnen. Maarja als je 500 euro gaat besparen kan je dit gewoon gaan doen. Het is gewoonweg te duur om een sli ti te maken. Daarom zullen veel mensen een singel 1080 ti kopen en hebben ze echt waar voor hun geld. En sommige kaarten in crossfire of sli zijn nog niet sneller. Wat een snelle kaart is dit.
GTX 1080 Ti;3;0.41805392503738403;Wut? De 1080ti IS een Titan X. En een SLI TI 'maken' begrijp ik ook niet helemaal. Die is meegenomen in de review. Misschien ben je in de war met dual-GPU kaarten zoals de GTX 690 of de Pro Duo. Ook geeft de review aan dat het prijspunt van de 1080ti uit verhouding ligt tov de 1080 daaronder: 30% meer perf voor 50% meer centen. Dus 'waar voor je geld' valt ook wel mee. Ja het is de snelste, maar perf/dollar ligt lager.
GTX 1080 Ti;2;0.47440868616104126;er is wel meer af te dingen op de review. het enige wat tweakers hier doet is een paar benchmarks draaien en kennelijk heeft iemand opgepikt dat er een fantastische nvidia driver aankwam, en ' that ' s all she wrote '. het is wel duidelijk dat er geen vast format ligt voor de wijze van reviewen nu... - geen diepgang. de oc resultaten bijvoorbeeld : wat vage tekst over kloksnelheden en een zinloze temperatuurmeting op max fanspeed. wat moeten we hiermee? we hebben een boel informatie waar geen enkel inzicht bij zit : zo is het algemeen bekend dat de stock blowers van nvidia zorgen dat de kaart flink gaat throttlen, en pascal is zeker qua clocks super gevoelig voor de load temps. ga je boven de 50 c dan verlies je al boost bins. deze kaart gaat op max fanspeed naar de 70 c... maar geen enkel grafiekje over het verloop van de gpu clocks. iets met een klok en klepel... - de genoemde driver versies. wel de nieuwste nvidia driver naast een andere zetten, maar niet alles opnieuw benchen. half werk, wederom bevestiging dat er maar lukraak wat benches gedraaid worden. - frametimes, en dan vooral 99p en 95p grafiekjes, ontbreken. - verouderde benchmark suite. nieuwere titels laten een volledig ander beeld zien dat sterke verschillen tussen engines blootlegt. het boeit niemand nog een kont wat dragon age inquisition nu doet, toch? als je dan frostbite wil zien, pak dan bf1. - dx11 versus dx12? misschien? anno 2017? - wel 3cf rx480 in de grafiek, maar geen 2cf. geen hond draait 3cf tegenwoordig, maar toch bedankt. het houdt weer niet over... als we hier dan een hdtv review naast zetten vind ik het verschil in kwaliteit schrikbarend. stop er maar mee, want de gpu reviews zijn bijna een slecht aankoopadvies te noemen onderhand. het argument ' ja maar benchen kost zoveel tijd ' is geen argument. als je er de tijd niet voor wil nemen, doe het dan gewoon niet.
GTX 1080 Ti;3;0.5504127740859985;Ik was ook verbaast! Ik had niet verwacht dat deze games allemaal zo degelijk mee zouden scalen naar 3x crossfire. Maar het werkt nog best wel aardig blijkbaar. Hoe dan ook, die 1080 Ti is alsnog sneller dan de 3 480's bij elkaar. Dus het zou vanuit een prijs-prestatie oogpunt een vreemde keuze zijn. Aan de andere kant, wel stoer om 3 videokaarten onder elkaar te hebben hangen in een case met een window!
GTX 1080 Ti;3;0.3957027494907379;Ik denk dat je energie leverancier het ook wel gaaf vind dat jij 3 van zulke kaarten onder elkaar hebt hangen.
GTX 1080 Ti;1;0.42710304260253906;Ach, die 300 watt die de setup vraagt? We zijn veel te verwend tegenwoordig, vroegah nam een enkele kaart al zoveel: (Mijn RX480 GTR Black Edition komt nauwelijks boven de 100 Watt uit) Denk dat de energieleveranciers nog meer staan te springen om oude hardware
GTX 1080 Ti;3;0.40517061948776245;400W veel? Check deze Maar goed dat is oude techniek zeer beefy en krachtig. Een X3 480x kan je nog tweaken qua prestaties (wattman, vsync of freesync) maar de vraag is of je dat eigenlijk wel wilt. Een single kaart doet het qua frametimes en latency's allemaal veel beter.
GTX 1080 Ti;3;0.38348639011383057;Je bespaard dan wel weer op verwarming als het wat kouder is
GTX 1080 Ti;2;0.5909885764122009;Mijn ervaring met CrossFireX/SLI: doe het niet. CrossFireX en SLI hebben alleen nut als je de meerdere exemplaren van de krachtigste kaarten hebt. Voor kaarten daaronder zijn de compatibiliteitsproblemen het niet waard. Zo veel spellen werken er gewoon niet lekker mee. Zeker bij de launch, maar er zijn ook spellen waarbij de problemen nooit opgelost zijn. Dan staat een enkele videokaart te bakken terwijl de ander als dood gewicht dient. Koop liever van de prijs van 2-3 mid-end kaarten een goede high-end kaart, of spaar nog even dat kleine laatste beetje door. Overigens wel goed dat ze het hebben meegenomen in de benchmarks.
GTX 1080 Ti;2;0.5162633061408997;Ik had echt gehoopt dan DX12 en SFR voor nieuw leven in Multi-GPU systemen zou zorgen, maar tot nu toe vind ik de ondersteuning echt bedroevend... reviews: AMD Radeon RX 480: CrossFire, VR, DX12 en overklok getest Voor komende games hoop ik echt dat de ondersteuning beter wordt: Tweedehands kaartje erbij kopen na ~1 jaar is scheelt flink wat geld, en de meeste systemen kunnen de hitte wel kwijt en qua voeding lukt het ook wel. (Hoera voor zuinigere architecturen )
GTX 1080 Ti;3;0.4129118025302887;Rise of the Tomb Raider laat wel een flinke fps winst zien met SLI GTX 1080 Ti op ultra HD in DX12: 80% (en bij de buren zelfs 92%). Maar dat is dan wel direct de enige game waar je zo'n sterk verschil ziet. Maar in theorie zouden game devs met DX12 best goede schaling moeten kunnen halen met een tweede kaart. Wat natuurlijk altijd een nadeel blijft van later nog op SLI/CF overgaan is dat je natuurlijk ook je huidige kaart nog kan verkopen en gewoon een betere kan kopen zodat je geen SLI/CF hoeft te gebruiken. Ook is de laatste tijd VRAM wel wat limiterend, als het al goed zou werken. (Neem bijvoorbeeld de 3.5GB van de gtx 970.) Maar het blijft inderdaad jammer dat het nooit echt goed heeft gewerkt.
GTX 1080 Ti;3;0.3466201722621918;Crossfire alleen voor de krachtigste? Mijn 2xR9 270x( schaalt perfect en presteerd identiek zoniet beter dan een enkele 290x. De meeste slechte verhalen die ik ken blijken een x16+x4 oplossing te draaien ipv 8x+8x of 16x+16x, het duurdere moederbord is iets wat je wel vooraf moet bepalen bij het bouwen van je systeem. Voor de enkele game die niet werkt switch je naar een alfternatief crossfire profiel, en desnoods schakel ik de 2e kaart gewoon uit voor die game. De kaarten beginnen nu hun leeftijd te tonen, maar ik rek het hier wel even mee tot ik een Ryzen/Vega systeem bouw.
GTX 1080 Ti;3;0.4446660280227661;Ik mis vooral een benchmark met 2x de 480,dit lijkt me doorgaans toch een iets veelvoorkomende scenario, en daarmee had je ook direct een een completer beeld kunnen schetsen
GTX 1080 Ti;5;0.47627994418144226;Helemaal waar. Dan zet je ook de 2x480x tegenover een enkele 1070, zoals AMD bedoeld had.
GTX 1080 Ti;5;0.30127155780792236;Dan liever 150 Euro meer en altijd werkend. Als een Game Multi-GPU niet goed ondersteund dan zit je met je 3x CrossFire.
GTX 1080 Ti;2;0.3474554121494293;> Maar ja als je een 3x CF setup een jaartje draait ben je aan stroom die 150 euro zo kwijt Op maximale load trekt 3x rx480 zo'n 450 watt1, een 1080ti ongeveer 220 2. Dus je fikt er per uur gamen 230 watt extra doorheen, dat kost ongeveer 5 cent per uur meer (€0.22/kWh). Wil je 150 euro op die manier in een jaar halen moet je elke dag, 8 uur per dag continue gamen. Dit is wel de upper bound, de 480s gaan zichzelf throttlen als ze teveel verbruiken. Als ze wel throttlen en ~20 watt per kaart besparen, dan zit je al gouw op 12-16 uur per dag gamen. Trouwens zal het ook leuk voor je CPU zijn dat er 230 watt extra warmte in de kast gedumpt word.
GTX 1080 Ti;3;0.3392174541950226;Jammer dat er geen 980ti is mee genomen in de vergelijking. Weet nu niet of ik mijn kaart moet vervangen of niet. edit: zie net de review op hardwareinfo.nl
GTX 1080 Ti;3;0.3296124041080475;Op HWI.nl staat de 980Ti wel meegenomen. % sneller tov 980Ti. 1080p Medium: 130% 1080p Ultra: 148% 1440p medium: 151% 1440p ultra: 168% 2160p medium: 156% 2160p ultra: 176% Met 10 games gemeten, gemiddeld als enige kaart over de magische 60 fps grens op 4k Ultra trouwens. Mooi dat die mijlpaal bereikt is.
GTX 1080 Ti;5;0.3834347128868103;Vooral als je op 1440 of 4K speelt met hoge/ultra settings (wat de doelgroep volgens mij wel al doet) een behoorlijke upgrade dus.. Voor 1080p is deze kaart natuurlijk complete overkill, tenzij je een 120+ Hz monitor gebruikt en ook echt al die frames zoveel mogelijk wil halen constant..
GTX 1080 Ti;1;0.5751562118530273;Wichard van digital foundries kwam tot de conclusie dat het op 1080p totaal geen zin heeft. Je krijgt amper meer frames als een GTX 1080 op die resolutie omdat een overgeklokte i7 6700k de GPU bottleneckte. Op 1440p haal je bijna dezelfde framrates als op 1080p in hun tests. Dus niet alleen overkill. Gewoon geen toegevoegde waarde over de goedkopere en zuinigere GTX 1080 op 1080p.
GTX 1080 Ti;2;0.49922817945480347;Hangt van je game af, dit is veel te kort door de bocht. Als algemeen statement is het zeker waar, maar er is tegenwoordig ook een 7700K die makkelijk op 5 Ghz loopt en er zijn ook zeker games die 120 fps steady niet halen op een Titan X @ 1080p.
GTX 1080 Ti;2;0.2984648644924164;"""130% sneller"" betekent: 2.3 keer zo snel Dus, 1080Ti ten opzichte van 980Ti = 1080p medium: +30% | ultra: +48% 1440p medium: +51% | ultra: +68% 2160p medium: +56% | ultra: +76%"
GTX 1080 Ti;2;0.4156535565853119;Nee , dat is ten opzichte van de 100% van de 980ti. Dus geen 2.3 keer sneller maar 1.3 keer sneller. Als de 980ti 100% is op Ultra is de 1080Ti 148%....
GTX 1080 Ti;1;0.3637157082557678;"= +130% = 100% + 130% = 230% = 2.3 * 100% = ""2.3 zo snel"" en dat is niet zo. 1080Ti op 1080p medium = 980Ti + 30% Edit: volgens mij bedoel je hetzelfde, maar zeg je: ""nee""."
GTX 1080 Ti;2;0.4374462366104126;Ligt eraan wat je van de kaart verwacht, toch ? Ik game sowieso de komende 1 a 2 jaar nog in 1080P en zie niet direct noodzaak om mijn 980Ti (Gigabyte OC) weg te doen. Gemiddeld nog steeds een stukje sneller dan de gemiddelde 1070.
GTX 1080 Ti;5;0.41342803835868835;Linus heeft deze intussen ook getest, mét de 980Ti erbij:
GTX 1080 Ti;4;0.28553879261016846;Op deze website wordt de 980Ti wel meegenomen:
GTX 1080 Ti;3;0.2542897164821625;Check hier even, van de buurtjes:
GTX 1080 Ti;3;0.23361286520957947;Lees 1070
GTX 1080 Ti;3;0.5138267874717712;De DVI aansluiting mogen ze van mij wel vaker weglaten! Waarom de -1's? Het zorgt voor betere cooling en uiteindelijk gaan we toch allemaal naar DP.
GTX 1080 Ti;4;0.37761905789375305;Neem een aftermarket koeler en hij kan er op blijven. Betere koeling en stiller. Waarom zou je dan nog met een FE kaart nemen? Er zijn enkele scenario's te bedenken maar zijn op 1 hand te tellen.
GTX 1080 Ti;2;0.4044463038444519;- Reference kaarten zijn net zo breed als de PCI sleuf, dus hij past gegarandeerd. - Onvoldoende airflow in de kast om de hitte van een open-air design af te voeren. - SSF systeem dat een inlaat bij de GPU heeft. - Warmte wordt de kast uitgeblazen, heeft dus geen invloed op CPU/GPU2 temps. - Hebben vaak minder stroomconnectors nodig dan aftermarket ontwerpen. - Makkelijker om waterblocks e.d. voor te vinden. - Uiterlijk, iedereen weet meteen welk merk chipfabrikant je hebt gekozen. Voor Tweakers/Powerusers is een STRIX/TwinFrozr/Windforce de betere optie, maar Jan A. met zijn Dell/HP/Lenovo desktop heeft meer baat bij een reference ontwerp
GTX 1080 Ti;3;0.4268544316291809;punt 1 is natuurlijk gewoon even meten. Maar 99% van de kaarten past gewoon in de breedte. Punt 2 intake bij de GPU is geen probleem. De open air koeler gooien het immers gewoon schuin achter hun PCB er uit. Punt 3 Klopt. Alleen dat heeft een marginaal effect op de CPU temp. Punt 4 Juist daarom is het een nadeel. Alleen crap voedingen hebben te weinig aansluitingen. Kijk wat de rx 480 reference modelen heeft gedaan bij genoeg Mobo. Punt 5. Alphacooling heeft voor elke GPU wel een blok(alleen de Extreem OC hebben er niet genoeg aan). En ook daar kan je prima aftermarker versie met stock PCB. Punt 6 tja als je E-peen zo groot is kan je natuurlijk ook een custom backplate maken/bestellen. Ziet er meteen mooi uit. En je noemt een hele reeks met punten op die vrijwel nooit van toepassing zijn bij Jan A. En alleen bij Tweakers/computer fanaten spelen. Jan A. Wil of een kleine game PC. Waar je 9/10 keer een angepaste PCB voor nodig hebt.(ala Zotac 1080 mini. Of willen stiller. En ook daar kan je beter geen FE voor gebruiken.
GTX 1080 Ti;5;0.29453718662261963;Ik was ook heel benieuwd naar die doelgroep 'Jan A' die SFF gaat bouwen met reference blower 1080ti's in SLI, maar geen poweruser is.
GTX 1080 Ti;3;0.48968714475631714;Omdat ik de FE mooier vind dan anderen. Ik weet dat ik meer betaal en iets minder prestaties krijg, ik vind een FE gewoon mooier in m'n kast. Misschien beetje een fanboy (alleen zonder AMD bashing)
GTX 1080 Ti;2;0.56428062915802;'Iets minder' Zeg maar gerust 15-20% minder. De FE's zijn bedroevend slecht en Pascal is ontzettend temperature-limited. Kun je beter een GTX 1080 kopen
GTX 1080 Ti;2;0.40223678946495056;Ik meen dat sommige mensen monitoren uit Korea halen, welke goedkoop zijn en met bepaalde mods/instellingen 144hz kunnen draaien. Nadeel van die monitoren is dat ze dit truukje alleen via DVI kunnen doen. Gebruikers van die monitoren kunnen dit nu niet meer doen, maar AFAIK levert Nvidia ook een adapter naar DVI mee, dus snap het probleem inderdaad niet.
GTX 1080 Ti;4;0.36564770340919495;Zo'n adapter werkt mogelijk niet voor dual link dvi. Maar goed, de dvi sporen zitten nog steeds op de pcb, die identiek is aan de titan x (pascal). Custom kaarten hoeven dus niet eens een custom pcb te hebben om nog steeds dvi te kunnen hebben.
GTX 1080 Ti;4;0.4745357036590576;Mooi om te zien hoe een kaart zo dik als een GTX 1070 hier zowaar als mid-range prut ding neergezet lijkt te worden, niet normaal die 1080 Ti. Geen gebrek aan echte winst, mooi spul, duidelijk verhaal ook
GTX 1080 Ti;5;0.6773800253868103;Laat me raden: jij hebt de 1080Ti al binnen en komt binnenkort met een review? Erg goed werk zeg, die reviews die je maar elke keer in elkaar knutselt voor ons! Super mooie aanvulling.
GTX 1080 Ti;3;0.31321483850479126;If only Maar zodra ik er één te pakken krijg gaan we draaien natuurlijk, al is het maar om de custom kaarten in actie te zien. De dag dat ik een 'originele' in handen krijg geef ik wel een feestje. #reviewergoals.
GTX 1080 Ti;4;0.2846148908138275;
GTX 1080 Ti;5;0.2942139506340027;Dat feestje zal ik je aan houden. Zolang er ook bier is.. ik bedoel, spa rood
GTX 1080 Ti;2;0.31406497955322266;Dat klopt toch ook ? De 1080Ti is high end, de 1070 is mid-range.
GTX 1080 Ti;2;0.5240771174430847;Naja, tot een week of 2-3 geleden (pricedrop 1080) was de 1070 een uitstekende subtopper, zo ver lag de 1080 er niet boven, enkel in prijs. Vervang 1070 in bovenstaande zin overigens ook gerust door 1080, tis nogal een verschil met die Ti.. pff.
GTX 1080 Ti;3;0.27917128801345825;De buren hebben 1 minuut voordat Tweakers de review dropte, ook online gezet: Toch een stukje uitgebreider naar mijn mening. Is ook nog eens met meer videokaarten getest, én ze meten de frametimes.
GTX 1080 Ti;2;0.5335204601287842;De reden dat er meer gpu's in de vergelijking staan bij HWI is omdat ze oude resultaten meepakken in de laatste grafieken. Maar die zijn dus ook met oudere drivers getest, en vaak niet representatief.
GTX 1080 Ti;1;0.5802563428878784;"Bovendien is de tweakers.net titel onlogisch. ""Op eenzame hoogte met de Titan X"". Hoe kan je nu eenzaam met z'n tweeen zijn?"
GTX 1080 Ti;3;0.4625934660434723;"""De dvi-aansluiting, die op de GTX 1080 en Titan X nog wel aanwezig was, is weggelaten. Daardoor is de uitstroomopening voor warme lucht groter en zou de kaart makkelijker zijn warmte kwijt kunnen, waardoor de ventilator minder druk hoeft te genereren en dus minder hard hoeft te draaien."" Nu mis ik alleen de gemeten temperatuur in deze review. Anders kan Nvidia namelijk ook de target temp hoger hebben gezet, waardoor de kaart minder lawaai maakt maar warmer loopt."
GTX 1080 Ti;2;0.48948541283607483;Vergeet niet dat dit een videokaart is met een blower koeler, wat sowieso lawaaierig is, én minder goed koelt.
GTX 1080 Ti;3;0.34876540303230286;Dat sowieso, de meesten van ons zullen ook sneller voor de open-air designs gaan dan reference stofzuigers. Maar ik ben gewoon benieuwd of die extra uitsparing ook daadwerkelijk verschil maakt (Anders kan ik het hier ook testen met m'n oude GTX260, maar daar zet ik liever niet de slijptol in )
GTX 1080 Ti;3;0.4846911132335663;Wel een 390x en Fury X in de vergelijking maar geen 980ti Dit zou voor de 980ti eigenaren een logische overstap zijn lijkt mij.
GTX 1080 Ti;3;0.46038374304771423;Maar hier wel:
GTX 1080 Ti;5;0.41930851340293884;Hulde
GTX 1080 Ti;2;0.4112209975719452;Moet je alleen wel al die DX12 benchmarks van hardware.info negeren, want niemand gaat natuurlijk z'n games in DX12 draaien als DX11 ook beschikbaar is en DX12 alleen maar mindere prestaties oplevert en grafisch precies hetzelfde eruit ziet.. En hoe een GTX1070 30% sneller kan zijn dan een GTX980Ti in sommige DX11 games zoals GTA-V begrijp ik ook niet helemaal, dat verschil hoort niet zo groot te zijn.. Misschien gaat men uit van stock, aangezien de meeste custom 980Ti's standaard al zo'n 40% zijn overclocked.. Maar het is duidelijk dat een 1000 serie kaart wel veel beter futureproof (zover dat bestaat) is dan een 900 serie, wat logisch is, want nieuwer.. DX12 is de toekomst (of het nou wel of niet een toegevoegde waarde heeft, tot nu toe niet dus)
GTX 1080 Ti;2;0.4088885486125946;Ik vraag me toch altijd weer af hoe dit getest wordt bij hardware.info... Ze komen bij alles structureel hoger uit. Met de tweakers benchmarks kom ik altijd wel redelijk op hetzelfde uit qua FPS (in bijvoorbeeld doom zowat 1 op 1), maar mijn 1070 draait battlefield echt niet op 113 FPS zoals in hun benchmark (dx12 ultra). Linus heeft trouwens de vergelijking met de 980ti ook netjes neergezet met de nieuwe Ryzen cpu's erbij :
GTX 1080 Ti;4;0.35941487550735474;Beetje het 8800 GTX verhaal qua performance/prijs..... =).. Veel performance voor veel geld... Komende 3 jaar kan je prima 2k gamen met een 1080ti, dan is 850 euro een prima investering!
GTX 1080 Ti;2;0.3959285616874695;Ik dacht eerst, 2K? 4K bedoel je! Maar dat zal idd tegen vallen. Vroeger was 2GB ook ruim genoeg voor 1080p en de games van nu.. Denk idd op 1440p dat je met een GTX1070 over 1.5 jaar wel moet gaan upgraden, dan is de GTX1080ti wel mooi. Zelf blijf ik liever een stapje achter (nu op 980ti) en als bijvoorbeeld de GTX1180 komt over ~1-1.5 jaar dat ik dan een GTX1080ti voor een redelijk prijs over kan nemen ^^
GTX 1080 Ti;2;0.43678855895996094;Sorry dat ik het zeg maar ik vind het tamelijk duf van tweakers dat de AMD RX 480 alleen in CF3-opstelling bij de testresultaten staat. Waarom niet de resultaten een enkele RX 480 erbij dan? Dat lijkt me veel relevantere info dan een CF3 configuratie. Niet dat die CF3-opstelling niet een nuttige aanvulling kan zijn maar het is inmiddels wel bekend dat CF en SLI nou niet bepaald lineair schalen. Bij de NVidia-kaarten vermelden jullie tenslotte ook de resultaten van beide opstellingen. Zonder de resultaten van de enkele opstelling te vermelden zegt het dus geen drol over de prestaties t.o.v. de RX 480.
GTX 1080 Ti;2;0.4492988884449005;Fijn dat iemand anders dit ook aankaart. Ik krijg steeds meer het gevoel dat Tweakers zich focust op intel en nvidia en een zekere anti-AMD houding aanhoud. Voor tech reviews is dit dan ook zeker niet meer mijn eerste keuze.
GTX 1080 Ti;2;0.3381582498550415;Heb je nog andere sites die naar jou betere informatie geven? Ik lees meer de gebruikers ervaringen dan tech_reviews op tweakers tegenwoordig.
GTX 1080 Ti;2;0.44121086597442627;Gebruikerservaring is altijd een goede, maar dat helpt niet bij een nieuw product. Het liefste kijk ik op meerdere plekken om een gebalanceerd beeld te krijgen, hier een paar voorbeelden: techreport.com overclockersclub.com guru3d.com hardware.info
GTX 1080 Ti;4;0.4425106942653656;volgens de benchmarks zou ik eerder zeggen dat een Titan X bijna een 1080-TI is want overall scoort de TI toch net iets beter. hij staat in ieder geval op mijn wishlist. ik wacht nog wel even tot er meer benchmarks vrij komen van de andere merken. het wil nog weleens verschillen in prestatie! al in al ziet dit er wel uit als een top kaart waar je voorlopig klaar bent voor de toekomst!
GTX 1080 Ti;3;0.35727715492248535;"""We lieten de koeler op volle snelheid draaien en wisten de kaart daarmee rond de 70°C te houden."" Vind dat nog erg hoog en lawaaiig trouwens. Als ik ga overclocken zoek ik altijd een fijne balans tussen lawaai en prestatie. Meestal ligt dat rond de 80-90% fanspeed want anders krijg je van die stofzuigers. Custom fanprofiel die zich opbouwt naarmate de chip warmer word vind ik een stuk verfijnder dan volle bak en gas d'r op."
GTX 1080 Ti;3;0.5083259344100952;Dat zullen velen ook doen, een goede custom profiel, echter het is wel handig te weten wat de temps zijn op volle snelheid icm. decibel metingen. Het geeft juist het nadeel aan van deze reference design + blower cooler kaarten.
GTX 1080 Ti;1;0.5896275043487549;Ik vraag me vooral af hoe ze 800+ euro voor een kaart kunnen vragen waar het geluid uberhaupt een probleem is. Zet er een fatsoenlijke koeloplossing op met meerdere goede, stille fans.Als ik 800€ uitgeef voor een videokaart verwacht ik dat ik niet eerst de hele koeling moet vervangen voor wat anders.
GTX 1080 Ti;5;0.27286621928215027;Wat een kaart! Als consument heel benieuwd naar Vega (ik hou wel van concurentie). Maar ben bang dat ze niet in de buurt gaan komen.
GTX 1080 Ti;2;0.40507107973098755;volgens vage''bronnen'' 10% sneller dan de TITAN X.... Mischien dat daarom Nvidia de de 1080ti bijna gelijk aan een TITAN heeft gemaakt. Waarom ze uberhaupt de 1080ti releasen het is ''nog' geeneens nodig.
GTX 1080 Ti;2;0.45509201288223267;Hoezo ineens? Ze zijn juist relatief laat met de Ti dit keer. Meestal komt de Ti een half jaar na de gewone.
GTX 1080 Ti;1;0.5155960917472839;"""zoals de DirectX 12-versie van Total War: Warhammer, of de Vulkan-versie van Doom, is er helemaal geen ondersteuning voor het gebruik van meer dan één gpu"" Eerlijk, dit is de eerste keer dat ik dit lees, en dus helemaal niets vanaf wist, en dan met name de laatste game die hier genoemd wordt, DOOM. Aangezien ik deze zelf in mijn bezit heb, en nu ik dit weet, er eigenlijk best van sta te kijken, dat in mijn geval, een AMD 8350 inclusief één GTX760 (waarvan er overigens twee van aanwezig zijn in mijn systeem) deze game toch nog constant op 60fps weet te houden. En al helemaal nu ik weet dat i er maar één van gebruikt"
GTX 1080 Ti;2;0.42409104108810425;Grafisch dan ook een geen echt monster game.... BF1 is een ander verhaal.
GTX 1080 Ti;2;0.4397630989551544;De 980 was een slechte koop vergeleken met de 980ti. De 1080 lijkt nu toch een betere koop dan de 1080ti. Begrijp me niet verkeerd, natuurlijk is de 1080ti sneller. Maar kijk je naar hoeveel fps je haalt per euro, dan legt de 1080ti het toch af tegen zijn kleine broertje. Zoals aangegeven in de conclusie. Benieuwd wat AMD gaat doen. Hoewel ik niet verwacht dat ze veel terrein winnen ten opzichte van Nvidia. Zoals ze dat, ondanks het geklaag, met de cpu's wel gedaan hebben. De 1800x is velen malen goedkoper (€550 ipv €1100), verbruikt minder en is zelfs sneller in multi-core rendering dan de 6900k. De 8 core performance chip van Intel. Waag het eens over de 7700k te beginnen Maar waar Intel grotendeels stil heeft gestaan, is Nvidia door gegaan met flinke performance en tdp verbeteringen. Ik vermoed dat het weer eenzelfde verhaal word als de 'fury' launch. Leuk dat HBM2 maar eerst zien dan geloven.
GTX 1080 Ti;2;0.38089287281036377;1080's TI revieuwen in 1080p.... dan pak je toch minimaal 1440p en uiteraard 4k. Als je degelijke kaarten gaat kopen dan mag ik hopen dat je geen 1080p monitor meer hebt staan.
GTX 1080 Ti;3;0.5683689713478088;Zou mooi zijn als ze ook de winst in VR meepakken in de reviews. Volgens mij schaalt dat niet hetzelfde als op een 'plat' beeldscherm. Ik heb zelf een 1080p scherm, maar voor de Oculus is wel wat meer power nodig.
GTX 1080 Ti;3;0.4882276952266693;Dat zeg je, maar voor 1080p/120 is het wel degelijk relevant die resolutie mee te nemen. Voor 120 fps *gemiddeld!* moet je hier al terug naar Very High ipv ultra. Hoezo snel genoeg? Verder is er nog wel het een en ander af te dingen op een gigantisch paneel voor je snufferd, namelijk de ideale kijkhoek versus kijkafstand, de PPI die je kunt waarnemen van die afstand enzovoort. Ik zie mezelf nog niet zo gauw naar 1440p gaan voorlopig, doe mij maar 1080p/120hz op 24 inch. Prima monitor formaat, prima PPI, ongekend smooth. PC gaming betekent wat mij betreft toch echt dat iedereen de voor hem/haar ideale opstelling bouwt en daar hoort 1080p/120 zeer zeker bij tegenwoordig, net zoals ultrawide en 4K.
GTX 1080 Ti;1;0.5522102117538452;Spijtig dat Octane of een andere gpu renderer niet meegenomen wordt. Er is veel interesse naar.
GTX 1080 Ti;3;0.4260711371898651;"De pech is wel dat octane bench nog niet op pascal draait.. Ik zou ook wel wat compute testen willen zien. luxmark en blenchmark zijn wel makkelijk te draaien testjes. Al is de 1080ti weinig anders dan de titan x; dus stuk sneller dan de 1070/1080 maar de 1070 beste price/performance. Nog wat compute bij anandtech:"
GTX 1080 Ti;4;0.2747512459754944;<nostalgie>Ik herinner me nog de heroïsche discussies hier tussen de believers van de NVidia architectuur en de non-believers, die met quasi zekerheid het einde van NVidia aankondigden. Waren vaak boeiende stukjes om te lezen. NVidia schijnt het al bij al toch goed voor mekaar te hebben intussen.</nostalgie>
GTX 1080 Ti;2;0.5031871795654297;Bizar. Daar waar SLI scaling een paar jaar heel erg goed was (heb zelf een SLI setup) merk ik nu dat de performance zelfs achteruit gaat. iig op mijn resolutie van 1080p. Dat is kak. Volgende videokaart wordt dus gewoon een enkele kaart. Maar ik denk dat ik nog even op de volgende generatie wacht.
GTX 1080 Ti;2;0.470464825630188;Volgens mij is sli niet echt populair behalve bij mensen die echt 2 van de snelste kaarten in sli draaien. Games worden er gewoon niet meer voor gemaakt, alleen Tomb Raider heeft echt goede scaling.
GTX 1080 Ti;5;0.40667349100112915;Battlefield had altijd hele goede scaling. Battlefield 4 ging echt een klap sneller met twee kaartjes. Daarvoor had ik altijd SLI. Zodat ik 144hz kon draaien, 144 FPS ten alle tijden op Ultra.
GTX 1080 Ti;2;0.39443838596343994;ik mis in de test een opstelling van de GTX 1080 in SLI ? Dat staat op de wensenlijst nu de prijzen dalen.
GTX 1080 Ti;3;0.38267040252685547;Duur, maar wat een beest weer.
GTX 1080 Ti;2;0.32807400822639465;Als je dan toch een gemiddelde PC uitgeeft voor een enkele videokaart, zou ik voor die schamele 500 euro meer toch gaan voor de Titan X. Puur voor de naam
GTX 1080 Ti;1;0.45293188095092773;Bakbeest, man man wat gaat het hardt zeg, ik moet verdomme bijna een 2e baan nemen Ik wacht nog ff 2 maandjes en tik er dan één op de kop. Ik ben erg benieuwd naar de watergekoelde edities!!
GTX 1080 Ti;3;0.6471503973007202;Prima review, maar ik had graag een meting van de temps gezien. Hopelijk zijn jullie dat van plan als er 1080ti's uitkomen met 3rd party coolers.
GTX 1080 Ti;3;0.553676962852478;Ik blijf lekker nog door gamen met mijn GTX970 ik sla een generatie videokaarten over en dan wordt het denk ik wel interessant. Zolang 4k niet geïmplementeerd is bij games en reguliere tv is het niet interessant.
GTX 1080 Ti;3;0.5180239677429199;wel leuk voor degene die binnenkort een 1080 voor 300 euro willen scoren, zal niet lang meer duren denk ik
GTX 1080 Ti;5;0.42102664709091187;Eindelijk een kaart die single gta 5 op 4k ultra redelijk aankan
GTX 1080 Ti;2;0.4050218164920807;Inderdaad, voor 3% meer fps hoef je echt niet te upgraden, misschien dat ze over een jaar of 3 keertje met iets komen wat de moeite waard is Is eigenlijk ongelooflijk dat cpus in 6 jaar tijd nauwlijks sneller zijn geworden, weet nog toen Sandy Bridge uitkwam, dat was pas een upgrade.
GTX 1080 Ti;4;0.4336121082305908;Leuk om te zien dat Nvidea met zichzelf concurreert, nu nog even wachten op Vega (knock some wood)
GTX 1080 Ti;3;0.2964613735675812;Ik denk dat Nivea dit zeker doet met verschillende zalfjes
GTX 1080 Ti;1;0.42856737971305847;ikke toch
GTX 1080 Ti;3;0.5029168725013733;Toch zou ik eerder voor Twee GTX1070 gaan dan voor de GTX180 Ti. Prijs zal gelijk zijn aan elkaar, afhankelijk waar je hem haald. Evga kost 1 GTX1070 426 euro. Plus je bent sneller dan de GTX1080 Ti.
GTX 1080 Ti;4;0.34741923213005066;Die kleine extra'a dat na het verwijderen van de DVI poort om zo meer warmte kwijt te kunnen, er toch een kabeltje gegeven wordt om een DVI monitor aan te kunnen sluiten geeft voor mij aan dat Nvidia wel goed om zijn klanten denkt, terwijl hierover op de forums voldoende geklaagd wordt. Ik heb zelf een AMD kaart in mijn systeem, maar moet stiekem wel toegeven dat dit bij ons niet altijd zo het geval is.
GTX 1080 Ti;3;0.3723708391189575;Ja, dat kan zeker. Als je twijfelt zou ik even een topic aanmaken in Componenten Aankoopadvies.
GTX 1080 Ti;2;0.31581661105155945;Had juist mijn twijfels bij de voeding of deze zwaar genoeg zou zijn
GTX 1080 Ti;2;0.3416871726512909;550 watt is ruim zat. Je houdt dan nog circa 150 watt over zelfs .
GTX 1080 Ti;1;0.3757387697696686;uhm.... ja?
GTX 1080 Ti;3;0.312430739402771;Ja.
GTX 1080 Ti;5;0.5158780813217163;Uiteraard!
GTX 1080 Ti;1;0.35380449891090393;Waarom denk je van niet? Als je echt zo twijfelt bezoek het CAA. Ik zou eerder vragen welke voeding. Merk/type/watt.
GTX 1080 Ti;2;0.29811134934425354;Een 6700k zal geen bottleneck zijn voor deze kaart. Als je er twee wilt halen misschien wel.
GTX 1080 Ti;3;0.5361591577529907;Zeker weten. Ik moet zeggen dat de vraag wat karig is aangezien ik voor de rest niks weet van je systeem. Er vanuit gaande dat je systeem nu redelijk high end is denk ik dat het moet lukken.
GTX 1080 Ti;1;0.255984365940094;Als je een PCI-E slot hebt, een voeding met de aansluitingen en het vermogen, een kast waar hij in past, en een UEFI waar geen hardware whitelist op zit: ja.
GTX 1080 Ti;2;0.28988897800445557;Natuurlijk werkt die met een i7 6700K.. Dat is samen met de i7 7700K nog steeds de beste CPU voor gaming.. Want de snelste cores die te koop zijn.. Het zal daarom nog steeds beter draaien dan al die 6+ core Broadwell-E (of Ryzen) CPU's die men in de benchmarks gebruikt op websites als deze.. Verreweg de meeste games hebben namelijk nog altijd meer aan 4 snelle cores dan aan meer minder snelle cores.. Die i7 6700K hoef je voorlopig echt niet te upgraden..
GTX 1080 Ti;4;0.5528794527053833;handige site om een eventuele bottleneck te zoeken
GTX 1080 Ti;4;0.4859132468700409;Nog handigere site om eventuele bottlenecks te vinden, je krijgt er zelfs feedback en adviezen bij! Componenten Aankoopadvies
GTX 1080 Ti;5;0.5071222186088562;Dat is altijd het beste inderdaad Richardus27! Als ik deze en de review van de buren zo lees twijfel ik toch wel over het upgraden van mn 980ti..
GTX 1080 Ti;2;0.48280513286590576;"Die afweging moet je zelf maken, maar mijn advies zou zijn: Draai je games zonder FPS counter op de instellingen die je wilt. Heb je geen ""last"" van framedrops of andere ongein, dan gewoon de kaart lekker houden. Die 1080TI maakt het beeld er niet mooier op, en als je nergens last van hebt is het weggegooid geld. Of je nou 100 FPS of 130 FPS haalt maakt niet uit zolang je geen verschil ziet. (Je hebt natuurlijk wel een grotere E-peen met een 1080TI, dus als dat belangrijk is zou ik zeggen ga je gang)"
GTX 1080 Ti;3;0.5134840607643127;"voor de initiele check is het prima ;-) Overigens bedankt voor je volwassen reactie. Zullen we het gezellig houden in de reacties?"
GTX 1080 Ti;2;0.419199675321579;De gebruikte 6950X is in deze benchmarks de bottleneck. Dit is te zien aan de minimum FPS wat gezien de GPU's sowieso ver boven de 60-70FPS zullen zitten. Als je plannen hebt om op hoog FPS te spelen zul je moeten investeren in de allersnelste CPU. Als je af kan met spelen rond de 60-70 FPS, dan kan je meestal prima af met de 60 euro kostende pentium G of een Core i3. Voor meer stabiliteit is een echte quadcore aan te raden (blijkt uit praktijk tests). De testwebsite klopt dus. Een klein praktijkvoorbeeld voor je: Een i7 2600k + GTX980Ti levert een gigantische bottleneck in bijv. Battlefield 4 of Battlefield 1. Toen ik naar de i7 6700k had geüpgrade zag ik mijn FPS omhoog gaan met rond de 100. Mijn minimum FPS stoog van rond 90 tot 160 FPS. Een i7 3770 is veel en ook veels te traag voor een Titan X op 1080p (en mogelijk met lagere settings voor meer FPS).
GTX 1080 Ti;1;0.4867297410964966;Hoe zou een overklokte 4930 @4.0 zich gedragen met 2 van deze broodjes?
GTX 1080 Ti;2;0.4245865046977997;een 6700k is maar iets sneller in games als een 2600k, hooguit 10 procent, 100 fps meer, weet niet hoe je daarbij komt, liep je 2600k op stockspeeds ofzo?. Mijn 2600k draaid op 4.8 ghz en als ik zou upgraden naar een 6700k zou weggegooid geld wezen voor die 10% meer fps
GTX 1080 Ti;1;0.44359177350997925;Zelfde hier, heb mijn Intel Core i7-5775C OC naar 4.0GHz en heb 16GB DDR3 2400MHz CL11 RAM en MB is ASRock Z97M Killer, en mijn Geforce GTX 1080 loopt ongeveer net zo snel als een computer met een Intel Core i7-7700K en 16GB DDR4 2400MHz+ met een Z270 MB. Zonde voor mijn geld om een Z270 MB te nemen met Intel Core i7-7700K en 16GB DDR4 2400MHz+ je haalt zo goed als niks extra FPS in spellen op 3440x1440. Ontopic: Jammer dat Tweakers maar 3 DX12 spellen test, Guru3D doet 7 DX12 spellen, en daar zie je dat de Geforce GTX 1080 Ti tussen de 11 en 17 fps sneller is met DX 12 spellen dan de Geforce GTX 1080 op 4k, verders wel een nette review.
GTX 1080 Ti;1;0.5123883485794067;De 5775C is hele andere koek! Dat is een van de zeldzame CPU's van Intel die specifiek gemaakt zijn met gaming in het achterhoofd. Dus dat kan ik dan niet mee tellen.
GTX 1080 Ti;2;0.3653213083744049;Ik draai BF4 op low-medium settings met mijn GTX980TI. De 2600k draaide op 4.5GHz. Ik had constant last van framedrops. Ik speel op een 144Hz paneel met FPS lock op 144FPS. De 2600k kon de minimum FPS niet boven de 144 houden. Het was te instabiel. De 6700k, kompleet stock doet dit op zijn sloffen. Mijn maximum FPS is ook door het dak gegaan (iets over de 400 op pieken in tegenstelling tot maximaal 180 met de 2600k). Hierbij heb ik dan ook mijn ram van DDR3 naar DDR4 geüpgrade. Weet niet of dat laatste veel uit heeft gemaakt.
GTX 1080;2;0.32457172870635986;"En waarom is de 970 niet meegenomen in de vergelijkende testen? De 1070 is toch immer de directe opvolger van de 970. De 1070 zal waarschijnlijk een veel groter publiek bereiken dan de 1080. Dus is wat mijns inziens dé belangrijkste vergelijking niet gemaakt. Huidige ""enthousiast"" kaart versus de nieuwe generatie ""enthousiast"" kaart."
GTX 1080;3;0.37810835242271423;Qua naamgeving is de 1070 inderdaad de direct opvolger van de 970, maar qua prijs is 'ie op dit moment meer een opvolger van de 980. Die 980 hebben we wel opnieuw getest en zit dus ook in de review en als je weet dat die (afhankelijk van je kloksnelheden) ongeveer tien procent sneller is dan een GTX 970 kun je wel een beeld krijgen van de verhoudingen. Kleine pleister op de wonde: testresultaten hebben we sinds kort ook bij de specificaties in de pricewatch staan. Die van de Inno3D GTX 970 staat hier: pricewatch: Inno3D GeForce GTX 970 OC 4GB (maar is wel voor het laatst volledig getest met driver 358.91)
GTX 1080;2;0.3795861601829529;Op dit moment niet, maar (veel) meer gamers hebben een 970 dan een 980. (Bron Zo heb ik ook een 970 in mijn pc zitten, en voor mij is dus op een gegeven moment een relatief simpele vraag: tik een goedkope 970 op de kop voor sli, of ga ik over naar de 1070. Toen ik mijn 970 kocht was die ook 380 euro, dus dat ontloopt de 1070 amper. edit: Het mag dan inderdaad 100-150 euro zijn maar op een totaal pc is dat te verwaarlozen )
GTX 1080;1;0.4192299544811249;Amper? Dat is 35% duurder! Het is niet handig om de naamgevingen van de fabrikant blindelings te volgen. Anders zit je straks met de 1270 ineens op 900,- euro, maar ja -- het is wel de opvolger.. De 1070 valt in dezelfde 'price bracket' als de 980 en is daarmee for all intents and purposes z'n opvolger. Of schaalt jouw budget ook mee met de msrp's van nvidia? De echte opvolger is er nu nog niet, maar waarschijnlijk zal dat de 1060(ti) worden. Een andere mogelijkheid is dat nvidia nu nog zoveel mogelijk probeert te melken en tegen het einde van het jaar de 1070 naar 350 ~ 400,- verlaagt. Tot die tijd heeft de 1070 helemaal niks (m.u.v. de naam) te maken met de 970. Dat staat niet los van het feit dat in iedere benchmark die momenteel gepubliceerd wordt volledig incompleet is zonder een 970 mee te nemen. Puur vanwege het feit dat dat de meest gebruikte kaart op het moment is (en vrij evenredig presteert als de 290/390). Zonder de 970 (met huidige drivers .e.d) mee te nemen wordt iedere benchmark al snel nutteloos.
GTX 1080;3;0.31679871678352356;Inderdaad. Ik ben best benieuwd hoe deze kaart in vergelijking met mijn huidige GTX970 presteert. Die conclusie valt nu niet te trekken.
GTX 1080;3;0.3953971862792969;Omdat de 1070 in een compleet andere prijsklasse valt dan de 970 misschien? Behalve het nummertje 70 is er niet veel te vergelijken aan deze kaart, en qua prijs valt hij in de 980TI klasse dus hij zal hiermee moeten gaan concurreren.
GTX 1080;2;0.3651469945907593;Momenteel wel ja. Wacht 6 maandjes als er genoeg aanbod is. En ze zijn 400 euro. Wat ongeveer de zelfde prijs is als toen de 970 uit was. En wacht dan nog eens 6-12 maanden en je kan hem ook voor 350 kopen.
GTX 1080;2;0.47878456115722656;Ze zijn zeker geen 400 euro. 550 + is meer de regio waar je het gaat moeten zoeken. over 6 a 12 maanden voor 350 gaat hem helaas ook zeker niet worden. en zo ook andere sites (zie pricecheck)
GTX 1080;3;0.3993755877017975;Dit worden, als de prijzen een beetje stabieler worden, de 'hogere' prijzen: EVGA verkoopt haar kaarten direct via de website voor vaak ongeveer 10% meer dan bij de meeste webshops namelijk
GTX 1080;2;0.45133674144744873;De advies prijs is 399$ voor de 1070. Momenteel is de vraag er groot en het aanbod karig. Dus gaat de prijs omhoog. Vandaar dat ik zij wacht 6 maanden. zodat er meer aanbod is als vraag. En de prijs zal zakken. pricewatch: MSI GeForce GTX 970 GAMING 4G Puur kijkend naar de 970 zie je ook dat hij een hele tijd 375-400 euro heeft gekost(gemiddeld). Dus komt de prijs redelijk overeen wat Nvidia als advies prijst geeft. Zoals je hier ook kan lezen. nieuws: Nvidia publiceert specificaties van GTX 1070 met 1920 cudacores Maar je kan de huidige prijzen niet vergelijken met wat je moeten gaan kosten. pricewatch: Sapphire R9 290 4GB GDDR5 OC Vapor-X Dit is een mooi voorbeeld van hoe het kan gaan Begon bij 425ish en is op een bepaalt moment 6/7 maanden later te koop voor 275 ish. Dat is een prijs drop van 150 euro. Dus het kan prima. Maar om de huidige prijzen als draad te nemen van wat de 1070 moet gaan kosten als ze op voorraad zijn vind ik overdreven.
GTX 1080;1;0.6148936748504639;399 is sowieso al zonder 21% BTW en dan moeten ze ook nog geïmporteerd worden.
GTX 1080;1;0.6321238875389099;Wat na dollar>euro + btw eigenlijk altijd rond het zelfde uitkomt. De 1070 blijft echt niet rond de 529-599 steken hoor.
GTX 1080;1;0.6479958891868591;Op newegg ook allemaal in Back order voor 430 - 470 dollar. Hier 530+ euro. Hoe werkt dat dan?
GTX 1080;2;0.4097636938095093;Waarschijnlijk omdat de VS er genoeg van nvidia kan krijgen. pricewatch: MSI R7 370 Gaming 4GB Prijs is zelfs iets goedkoper. Dus kom je op vraag en aanbod verhaal. Als nvidia er 50.000 naar de vs stuurt en maar 500 naar de EU(aantallen zijn niet echt). Dan kom je vanzelf uit dat de prijs bij ons vele malen hoger zou zijn. Daarom zij ik ook even 6 maanden wachten(dit kan ook bv over 1 maand zijn) maar als er genoeg aanbod is en minder vraag daalt de prijs vanzelf naar normale waarde. En zit het ineens weer rond de 970.
GTX 1080;1;0.7910454869270325;Ik hoop dat je snapt dat import voor een bedrijf bijna niks kost? Invoer rechten is bijna 80% btw wat een bedrijf weer terug krijgt van de belasting. Vervoer kosten is niks meer als dat de Amerikaanse markt moet betalen. Die kaarten worden immers zoiezo in China of een ander Aziatisch land gemaakt en vanaf daar verscheept.
GTX 1080;1;0.4390033185482025;Klopt. En in china krijgen de bedrijven het wereldwijd verschepen bijna(al niet helemaal). gratis.
GTX 1080;1;0.8505843281745911;Tuurlijk...dat een aliexpress voor een paar euro een pakketje verstuurd wil niet zeggen dat de containers van Xiamen/Ningbo etc. naar het Westen niks kosten. Kost je gewoon duizenden euro's afhankelijk van een hoop factoren.
GTX 1080;2;0.42506444454193115;Paar duizend euro op een container waar duizenden videokaarten in zitten dan valt het wel mee. Wat ik er mee bedoelde dat vervoer kosten voor Europa niks hoger zijn als voor Amerika. Dus het is raar om dat als argument te noemen dat de kaarten op de Europese markt duurder zouden zijn door vervoer.
GTX 1080;1;0.5170173048973083;Mijn reactie was op Loki maar wat je zegt is inderdaad waar. De videokaart is een product met veel marge dus die transportkosten van 2 euro per kaart zal ze een worst zijn. Die vervoerskosten worden pas interessant wanneer het product dat je verkoopt 2 euro kost en de vervoerskosten 0,25 of 0,5 euro per stuk zijn.
GTX 1080;1;0.7197301983833313;399 dollar is bij mij geen 600 euro.....dus weer puur oplichterrij
GTX 1080;1;0.6116737723350525;Waarom zou je het vergelijken met hoeveel het over een jaar gaat kosten? Dat is het meest belachelijke wat je maar kan doen. De huidige prijs-kwaliteitsverhouding is wat het nu kost. Tevens is er geneens stock van de videokaart, het gaat nog een lange tijd duren voordat de prijs zo laag wordt.
GTX 1080;3;0.42778757214546204;Dat kan. Maar puur advies prijs van nvidia uitgaant(het enige wat wij hebben) zal hij ongeveer even duur zijn als de 970 in het begin was. Maar we zullen zien.
GTX 1080;1;0.5883432626724243;Hahaha de adviesprijs van Nvidia. Als je echt de marketing van die hufters gaat geloven is de 1080 een geweldige overclocker en is de 1070 beter dan een 980TI. Nvidia heeft hun adviesprijs 100$ lager gezet en vervolgens een Founders Edition uitgebracht, vervolgens welke zelfs ook niet in stock is waardoor de prijs NOG verder opgedreven wordt . Het allerlaatste wat je wil gaan doen is Nvidia geloven op hun bullshit waar alles er honderd keer leuker uitziet dan in de realiteit. We moeten kijken naar de echte prijzen, ook niet voor Amerika, maar hier in Nederland, omdat dit de echte relevante resultaten zijn voor ons.
GTX 1080;1;0.42709195613861084;Je vergelijkt de adviesprijs en die is 399 dollar en volgens mij was de 970 destijds 359 MSRP Simpel...
GTX 1080;1;0.4273013770580292;Ja man een advies prijs is echt een werkelijke uitbeelding van de realiteit. Vooral als hij in alle winkels anderhalf keer zo duur is
GTX 1080;3;0.5439761281013489;Vraag en aanbod, zoals door anderen is aangegeven. Ja de prijs zal wat hoger liggen, maar niet zo veel hoger als nu, ik verwacht dat we tegen augustus een prijs hebben rond de 425 euro.
GTX 1080;2;0.36949941515922546;De releaseprijs van de 970 was een stuk lager dan die van de 1070. De 970 was de goedkoopste 70 serie bij release.... Dus mensen die hopen dat hij onder de 400 euro gaat uitkomen verwacht ik dat ze bedrogen uitkomen, tenzij AMD er echt iets goeds tegenover weet te zetten.
GTX 1080;2;0.42081376910209656;niet onder maar wel rond de 400. 420 dollar. En als je kijkt naar pricewatch: MSI R7 370 Gaming 4GB scheelt het je 20 euro. Dus kan het wel. Alleen is de voorraad voor europa waarschijndelijk erg laag. En schiet de prijs omhoog.
GTX 1080;2;0.41566720604896545;Dan mag de euro wel een stuk sterker worden tov de dollar Dollar=Euro gaat al even niet meer op namelijk. Ik verwacht ze voor December niet onder de 450 te zien, zeker de custom designs niet.
GTX 1080;1;0.4886019825935364;vertel mij dan maar is warrom de 370 duurder is bij n ewegg als euro>dollar niet opgaat.
GTX 1080;1;0.3709820806980133;Vraag en aanbodverhaal gewoon. Die kaart is ook al een hele tijd op de markt, plus dat je nu de goedkoopste uit de pricewatch vergelijkt met een random winkel (newegg). De 370 wordt ook nog maar door 2 winkels verkocht volgens de Hardware USA prijsvergelijker. Hier door veel meer winkel dus meer concurrentie. De vraag/aanbod is gewoon heel anders dan voor de GTX1070. De prijs, met de huidige waarde van de euro tov de dollar, is altijd hoger in euro's dan in dollars. Pas als de euro sterker wordt (1,21 dollar per euro) gaat dat misschien omdraaien.
GTX 1080;1;0.8165620565414429;Dat roept iedereen maar echt veel merken doen we er niet van. En misschien gaat hij 10 euro duurder worden.(wat ik betwijfel maar daar zullen we nooit achter komen). Dan nog zit hij ver onder wat de NL en hoogstwaarschijndelijk heel europa er voor vragen. Tevens zou de foundereddition over 100 dollar duurder zijn als die met aangepaste PCB. En de foundereddition is voor 490 te koop. Haal daar 88 van af. En hij komt verdomde dicht bij de 400..
GTX 1080;2;0.5081929564476013;"Helaas dat er ook al tussen neus en lippen door is verteld door insiders dat er geen bedrijf is dat de 1070 winstgevend kan leveren voor de prijs die Nvidia opgaf voor de goedkoopste custom kaarten. Nvidia heeft gewoon een leuke streek geleverd met hun ""De founders editie is duur, maar de customs worden veel goedkoper"". Waarom zouden fabrikanten de customs goedkoper in de markt gaan zetten? Ze moeten voor de custom kaarten R&D draaien en ontwikkelen. Voor de Founders Edition zit veel minder R&D voor de partners. Ze maken dus minder winst op een product wat meer geld kost om te maken. We gaan zien wat de prijzen doen, maar met de huidige dollarkoers gaan ze het komende jaar gaat 400 dollar (zonder btw) niet gelijk zijn aan 400 euro (inclusief btw)"
GTX 1080;3;0.43782752752304077;Nu vallen ze nog in een andere prijsklasse, voornamelijk door de schaarste, en het is even afwachten hoe de Polaris RX 480 het gaat doen. Als die qua performance in de buurt van de 1070 komt, zullen de prijzen sneller gaan zakken. De prijs van de 970 is over een langere periode niet echt gezakt en redelijk constant gebleven..
GTX 1080;2;0.3969030976295471;waarom de 290X wel dan, die is een krappe 50e duurder, de logica van de 970 als een de meest favoriete gpu's op dit moment niet mee te nemen is telefoons vergelijken zonder een samsung galaxy s6 in die vergelijking mee te nemen, omdat die nu wat goedkoper is dan de nieuwste telefoons. Als je echter wat vaker de reviews leest kan je op zich ook wel zelf bedenken dat een 970 tussen de 980 en 290x in zit qua prestaties, soms wat er onder soms gelijk etc. gem. gezien ong 10-15% minder snel dan een 980.
GTX 1080;3;0.4754980802536011;Interessante review maar mijn conclusie is dat je je 970/980/980ti/290/290x/390 etc niet in hoeft te ruilen. De nieuwe kaarten maken nergens het verschil tussen speelbaar en onspeelbaar (behalve mss in VR). Als je eenmaal boven de 60fps zit maakt het voor de meeste mensen niets meer uit (tenzij je een scherm net een hele hoge refreshrate hebt). Wel mooie kaarten op zich maar ook behoorlijk prijzig. Hoop dat AMD ook speelbare framerates neer kan zetten voor een lagere prijs.
GTX 1080;3;0.5897031426429749;Totdat het volgende spel met dikke graphics uitkomt. Ik vind het de laatste jaren wel maar er langzaam opschieten hoe veel mooier spellen zijn geworden. Er zijn na gta v zat spellen uitgekomen die een stuk minder mooi waren waarbij dat echt wel beter had gekund.
GTX 1080;4;0.5141400098800659;Fijn weer een review die de prullenbak in kan. Jullie gebruiken een aftermarket 1070 die boost snelheid kan houden tot 2 GHz maar een reference 980ti? Gebruik dan ook een MSI 980ti of een nog betere versie. Uit andere revieuws is gebleken dat een 980ti en 1070 even snel zijn, maar de 980ti is eerder sneller, mits je een editie hebt die ver turbo'd. Mijne gaat al naar 1430MHz core zonder handmatige overclock.
GTX 1080;2;0.46838533878326416;Wat bvk zegt: we hebben hier van de meeste videokaarten alleen het reference-model liggen en Nvidia heeft ons geen reference-versie van de GTX 1070 doen toekomen om te vergelijken met de reference 980Ti die we al hadden. Sowieso kunnen we niet alle kaarten die op de markt zijn testen, en liggen de prestaties van de 1070 en 980Ti inderdaad dicht bij elkaar, waardoor je al kunt concluderen dat een overgeklokte 980Ti het beter gaat doen tegenover een 1070 op standaard kloks. Als ik persoonlijk vijfhonderd euro voor een videokaart over zou hebben zou ik overigens wel voor de 1070 gaan gezien de verbeteringen en nieuwe features van Pascal ten opzichte van Maxwell.
GTX 1080;2;0.43338683247566223;dat is helemaal niet zo duidelijk in de review eerlijk gezegd. pas op de pagina ' overklokken ' komt naar voren dat de overklokruimte door nvidia ' s gpu boost 3. 0 zelf wordt weggesnoept en dus gewoon onderdeel van de marketing is, terwijl dat bij de 980ti niet het geval is. het is nogal een verschil, dat de 980ti met simpele oc zo ' n 15 - 18 % sneller wordt terwijl er bij de 1070 nog maar 5 - 7 % in zit. moet de lezer ' maar concluderen ' dat een 980ti dus in sommige gevallen zelfs sneller is, terwijl de 1070 in elke bench het lijstje aanvoert? dat is fijn reviewen zeg! zeker na de introductie over hoe pascal verschilt van maxwell, en dat je eigenlijk moet concluderen daaruit dat er weinig is veranderd behalve wat ' features ' en extra compressie. als je weet dat pascal zelf al zwaar op de oc leunt, dan moet je daar oc versies van de 980ti naast zetten om dit weer te geven, en niet met twee halve zinnen iets kenbaar maken. verder, de fe modellen zijn constant aan het throttlen en de msi gaming x, de asus strix en gigabyte ' s g1 koelers doen dat niet. nogal een verschil, want het levert een gemiddeld klok verschil op van zeker 50 - 80 mhz ( constant, dus zelfs met de licht fluctuerende clocks van pascal ) en de variatie is ook stukken kleiner. dat zegt nogal wat over hoever nvidia pascal zelf al heeft gepusht - zelfs op 16nm is de oude nvttm shroud niet genoeg om pascal in de hand te houden. voor een dergelijk rijkelijk late review van pascal had ik meer verwacht. er zijn al verschillende aib ' s op de markt en gereviewed, maar ik zie hier alleen msi. daardoor ontbreekt het ook aan de mogelijkheid om een scherpe conclusie te trekken, want elke aib koeler doet het vooralsnog stukken beter dan de fe. het enige dat eigenlijk over de fe wordt gezegd in de review, is dat hij luidruchtiger is. maar eigenlijk is het gewoon een slappe rotkoeler waar je 100 piek extra voor mag neerleggen.
GTX 1080;4;0.3965507447719574;Om aan je verhaal toe te voegen:
GTX 1080;1;0.7126539349555969;Oeps Nvidia heeft ons een zwaar gebooste kaart t.o.v. de reference gegeven om de resultaten er beter uit te laten komen en dit hebben we zonder hem gewoon terug te clocken naar stock speeds maar geaccepteerd en niet duidelijk in de review vermeld want ookal is dit een complete vervalsing van test resultaten we hebben niks anders aanwezig haha Dit is hoe je nu klinkt.
GTX 1080;1;0.2687590718269348;Nvidia heeft ons een GTX1080 doen toekomen. De GTX 1070 en 1080 waren afkomstig van MSI en zijn ondertussen ook weer terug naar MSI. Zomaar terugklokken naar stock speeds ligt nog niet zo simpel, melden verschillende mensen in de reacties hierboven al. En de kloksnelheden staan wel duidelijk in de review vermeld, te weten op pagina vijf.
GTX 1080;1;0.6566672921180725;De gemiddelde lezer heeft weinig interesse in de losse kloksnelheden, en dit zegt vele mensen ook helemaal niks.We willen kijken naar de grafieken en op basis daarvan een visuele vergelijking kunnen maken die representatief is aan de werkelijkheid. De reden dat mensen reviews lezen is omdat vele mensen niet goed genoeg geinformeerd zijn over wat alle nummers precies betekenen, en gewoon willen kijken naar wat sneller is en meer waar voor hun geld. De 980TI werd altijd geprezen als een super overclocker, en om dan ineens in de test zo uit te voeren dat de 1080 erbovenuit komt is gewoon visueel bedrog. Tevens lijkt het me echt niet moeilijk voor jullie om aan een goede 980TI te komen, er is vast wel iemand in de staff die er een thuis heeft liggen die je evt zou kunnen gebruiken.
GTX 1080;2;0.5373885631561279;Ehm... de GTX 1080 IS ook gewoon sneller dan een 980ti hoor. Het is de 1070 waarmee het stuivertje wisselen is met de 980ti. Je maakt een boel geluid maar er klopt niet heel veel van... Het gaat ook niet om 'zwaar gebooste' kaarten. FinFet klokt gewoon hoger en Nvidia heeft met GPU Boost 3.0 die ruimte heel goed weten te benutten. Dat is met Maxwell nog niet zo, het overklokken werkt nu anders, maar Pascal klokt vrijwel altijd tussen de 2050-2114 mhz, dus 'gebooste kaarten gegeven' is 100% complete onzin want daar gaat dit hele verhaal niet over - ze klokken allemaal ongeveer hetzelfde.
GTX 1080;1;0.5165359377861023;Mijn fout moest 1070 typen maar was een typfoutje.
GTX 1080;4;0.32010525465011597;"Er onstaat een coherent verband met MSI en Asus de GTX 1080 ""review"" kaarten die ze aan de persleden hebben gegeven en zelf hebben aangepast zodat ze (iets) hogere kloksnelheden hebben, waardoor ze beter uit de test komen. Bron: link, en link."
GTX 1080;3;0.5950523018836975;2e hands 980ti zijn wel voor een stuk minder te krijgen Daarbij snap ik het argument wel, maar er ontstaat wel een kromme vergelijking. Nu lijkt een 980ti -> GTX1070/1080 een grote stap, terwijl het in de praktijk -5 tot +0% en 15-20% verschillen zijn.
GTX 1080;3;0.2835366725921631;Ik snap je punt en ik zal eens kijken hoe we dat de volgende keer beter kunnen doen, bijvoorbeeld door een kaart te onderklokken zodat 'ie op dezelfde speeds loopt als de reference kaart zodat je iig reference vs reference-snelheden hebt.
GTX 1080;2;0.36619359254837036;"Kan, maar 75% van de mensen koopt in mijn ervaring edities met non-reference coolers. nVidia heeft de reference edities opvallend hoog geclocked t.o.v. de non-reference kaarten, waardoor het gat reference 980ti-1080 veel groter is (~30%) dan het gat non-reference 980ti-1080 (15-20%) zou zijn. Ook is ""stock clocks"" een raar begrip, want gpu 3.0 clockt eenmaal zo ver als de koeling het toe laat. HWI haar 1080 reference ging tot 1880MHz geloof ik, maar als je dat ding onder een waterblock gooit zal die automatisch verder clocken. Persoonlijk zou ik graag zien dat een vendor gpu tegenover andere vendor gpus wordt gezet, hopelijk kan dat vanaf nu ook. Neem bijvoorbeeld de MSI gamer editie, zeer populair zoals jullie ongetwijfeld ook zullen weten. MSI 980ti vs MSI 1070 vs MSI 1080 lijkt mij een leuke vergelijking waar men ook echt iets aan heeft."
GTX 1080;3;0.5589324831962585;Ze zijn afhankelijk van wat ze opgestuurd krijgen. Dus ook merk vergelijken wordt moeilijk. Daarnaast zal een MSI meerdere versies uitbrengen...
GTX 1080;2;0.2940291464328766;Dit onderklokken zou bij GTX500 serie kaarten nog werken, maar sinds GPU-Boost er is. Is geen enkel scenario met eenzelfde geklokte kaart hetzelfde. Zo zal een MSI GTX1080 op stock snelheden onderklokt, nog steeds beter scoren dan de FE op stock clocks. Dit te maken met de power target die standaard hoger is, en de temperatuur die lager blijft door de betere koeling, hierdoor zal de GPU Boost hoger clocken dan met een FE editie kaart. Het enige wat je wel gewoon kunt doen is FE kaarten met FE kaarten vergelijken en aftermarket kaarten met andere aftermarket kaarten.
GTX 1080;1;0.2606719732284546;Zoiets als deze reviewer bij HWI heeft gedaan:
GTX 1080;3;0.3159683048725128;of misschien kunnen jullie een kaart lenen van een tweaker ik zou bijv er geen problemen mee hebben als jullie iets willen lenen voor bijv een benchmark om de community te laten zien wat het kan bijv: jullie starten een topic op het forum als jullie iets willen lenen lijkt mij wel een mooi plan
GTX 1080;2;0.5415335297584534;Underclocken van de nieuwe kaarten is niet representatief, de reference kaarten gaan namelijk eerder throttelen dankzij de brakke koeler en brak pcb, eigenlijk iedere custom kaart zoals een asus of msi blaast de reference compleet uit het water en is geen vergelijking. Als je een koude kaart op een testbench 3D mark laat draaien dan is het verschil merkbaar maar niet enorm, als je de kaart in een systeem inbouwd en de hele dag laat draaien dan is het verschil een stuk groter in het voordeel van de custom kaarten.
GTX 1080;3;0.3908804655075073;Jammer dat jullie geen reference GTX 1070 hebben gekregen van Nvidia Zeker aangezien de conclusie is dat de 1070 eigenlijk de beste bang-for-the-buck geeft: ik vermoed dat er dus veel mensen in dat model geinteresseerd zullen zijn, wat het deste interessanter maakt om te zien hoe de reference 1070 t.o.v. van de aftermarket presteert. Dat zou ik althans erg graag willen weten vóór ik één van de twee zou aanschaffen... Maar goed, niet jullie 'fout' natuurlijk, je moet roeien met de riemen die je hebt. Ik zat me wel af te vragen: zijn jullie helemaal van Nvidia afhankelijk voor deze keuze? Ik meen dat er ook wel eens webshops zijn die review samples uitlenen toch? Overigens, ik zie een hoop negatieve commentaren weer op de review, dus wilde ook even een positief geluid laten horen. Als multi-monitor gamer klinkt de smp feature mij veelbelovend in de oren en dat wordt toch mooi even besproken. Natuurlijk is het zo dat er websites zijn die nóg meer 'in-depth' de architectuur bespreken, maar mijns inziens heeft Tweakers altijd een prettige balans: de technische kant wordt niet geschuwd, maar het blijft beknopt genoeg zodat het artikel niet al teveel leestijd kost. Zo heb ik in een kwartiertje een aardig beeld van wat de nieuwe kaarten te bieden hebben. Dus wat mij betreft: dank voor weer een mooi overzicht
GTX 1080;3;0.3746863603591919;Hardware.info heeft een review waarin ze wel de referentiekaarten meenemen. Zie onderaan dit bericht. Verder wilde ik opmerken dat hardware.info vrijwel zonder uitzondering de betere reviewsite is voor computer onderdelen. Ze testen bijvoorbeeld ook uitgebreid voedingen, koelers, moederborden en behuizingen. Sowieso testen ze veel meer onderdelen en varianten van hetzelfde onderdeel. Daarnaast zijn de tests vaak net uitgebreider. Tweakers komt wat dit betreft niet eens in de buurt. Echter, ik heb ook niet het idee dat dit doel van Tweakers is. Ik vraag me eigenlijk af of Tweakers zelfs wel weet waarom ze dit soort reviews publiceren, terwijl ze duidelijk niet focussen op het uitgebreid reviewen van componenten. Blijft het dan niet altijd net niet? Daarentegen heeft Tweakers ook erg sterke kanten. Tweakers wint het ruim met de pricewatch, de reacties (als je door de onzin heenkijkt zit er veel waardevols in) en het nieuws. Wat mij betreft heb je met de sites samen een goed overzicht. Review van de gtx 1070 Onderlinge vergelijking verschillende 1080 modellen:
GTX 1080;1;0.5040481090545654;geen van beide is bang for the buck, ze zijn alle 2 rete duur
GTX 1080;2;0.3992059528827667;nee dat zeg ik niet, het is eerder bij launch en zonder concurrentie momenteel dat kaarten te duur zijn.... meestal bij invoer van lagere reeksen en competitie gaat de prijs zich ook beter plaatsen. AMD fanboy, lol ik denk dat je bril te groen ziet typische auto spam reactie
GTX 1080;1;0.6761013865470886;Volledig met je eens. De prijzen bij Nvidia vind ik gewoon te belachelijk gestegen. Rond de 500 euro voor de top versie zou het uitgangspunt moeten blijven en niet richting de 1000 euro. Ik zou anders de 980 weer verkocht hebben en opgewaardeerd. Nu doe ik dat gewoon niet.
GTX 1080;3;0.42629173398017883;Je kan natuurlijk wel downclocken naar stockniveau.
GTX 1080;3;0.5019149780273438;Bedenk wel dat Tweakers alleen de kaarten kan testen die ze aangeleverd krijgen of wat ze op de plank hebben liggen. Nou liggen er op het HQ best aardig wat kaarten, maar de genoemde zullen daar niet tussen gezeten hebben. Want anders hadden ze die wel getest natuurlijk...
GTX 1080;1;0.5618345737457275;Ze zijn normaal gesproken ook niet te berierd on cijfers uit vorige testen te hergebruiken. Ik heb het gevoel dat NVidia hierachter zit en dat ze dat soort kaarten niet mogen gebruiken. ( zal wel onderdeel zijn van de overeenkomst om de kaarten te mogen testen ) Zelfde met de SLI testen, ineens zijn alle 980(ti) / 970 kaarten afwezig.
GTX 1080;1;0.7183535099029541;Dat is niet waar. Het is zelfs zo dat Tweakers in eerste instantie niets van nvidia gekregen heeft... Wat dan wel weer waar is is dat deze review gewoon zonde is, omdat uit de vergelijkingen geen conclusies getrokken kunnen worden door de gebruikte kaarten.
GTX 1080;3;0.2378612756729126;Ja,maar ik vind dat een site ,zo groot als deze ,desnoods ,gewoon ,moet kopen. .Dat is onafhankelijk ,objectief ,en professioneel . Ze hebben een verdien model en groeien nog steeds.Het is allang geen hobbysite meer ,maar onderdeel van een bedrijf.
GTX 1080;2;0.49926769733428955;Dat is leuk en aardig dat kopen, maar dan moet alles wel beschikbaar zijn en dat waren ze eerst niet. Begrijp me goed, ik verdedig tweakers niet, want het tweak gehalte is behoorlijk afgenomen (zelfs voor een niet tweak-er als ik duidelijk).
GTX 1080;1;0.4348539710044861;Tip: spaties zet je achter je komma's, niet ervoor.
GTX 1080;1;0.4466451108455658;Zeker mee eens hoor. Maar los van het kopen of niet, dit is gewoon onacceptabel ook al is Tweakers niet echt op de hardcore pc-bouwer gericht.
GTX 1080;1;0.6874906420707703;"Wij gebruiken juist vrijwel nooit cijfers uit vorige tests omdat drivers zo veel invloed kunnen hebben op de prestaties van videokaarten in bepaalde games. Nvidia heeft hier niks mee te maken; in de gevallen dat ze ons een sample leveren hebben ze verder niets te zeggen over het vergelijkingsmateriaal."
GTX 1080;1;0.4599454402923584;Denk je dat men probeert reviews te sturen door bepaalde sites bepaalde kaarten niet te sturen?
GTX 1080;3;0.5340694189071655;Dat is wel een beetje overdreven, niet? De aftermarket kaarten zijn dan wel iets sneller geklokt, maar groter dan enkele procenten is het verschil meestal niet. Het is en blijft dezelfde GPU. Trek van de cijfers van de aftermarket kaart 2 a 5 fps af, en je hebt de reference cijfers. Het grootste verschil tussen de aftermarket en reference kaarten is de koeler. De goed ontworpen aftermarket kaarten zijn normaliter stiller en koeler. EDIT: Of toch niet. Volgens deze benchmarks is de aftermarket 980ti wel degelijk een pak sneller dan de reference:
GTX 1080;3;0.4548732042312622;Refferenced 980 Ti kaarten hadden heel veel op headroom. Aftermarket kaarten speelde daar aardig op in het verschil was erg groot. En die kon je vaak ook nog eens een stuk verder overclocken. Een refferenced kaart kan je ook ver overclocken maar stock draait die op bescheiden snelheden.
GTX 1080;3;0.3537595868110657;Ik zou ook graag het vergelijk zien tussen een 980Ti op circa 1450Mhz. Dat doen vrijwel alle 980Ti's namelijk. Dus het is wel appels met peren vergelijken nu: non-reference 1070/1080 met reference 980Ti. Eerdere reviews laten zien dat een non-reference 980Ti zo maar 25-30% sneller is Oc'd.
GTX 1080;3;0.24768629670143127;Hier heb je een vergelijking op 1450 Mhz tov een GTX 1070 op 2050 Mhz. Beide referentie modellen.
GTX 1080;2;0.4897327721118927;"Precies, ik wachtte op deze review om de vergelijking te maken tussen een MSI GTX 1070 met een aftermarket GTX 980 Ti. Nou ben ik weer niks wijzer geworden. Het lijkt bijna marketing van Nvidia om ""early reviewers"" om te kopen om de kaarten met een reference kaart te vergelijken. Natuurlijk presteert de nieuwe kaart dan beter... ik verwacht dat hogere geklokte Gigabyte 980 Ti OC of KFA2 Hof beter presteren dan een MSI GTX 1070 of andere aftermarket cooler. De 1070 zit met zijn locked BIOS al bijna tegen de max. clock aan. Misschien wint een aftermarket 980 Ti niet op TDP efficiëntie maar wel op een verantwoordelijke boost clock."
GTX 1080;2;0.41275474429130554;Dat klopt, voor de echte vergelijking moet je vrijwel nooit bij tweakers.net zijn. Je kunt hoogstens zien wat de fps van de individuele kaart is en andere info zoals geluid/temperatuur is interessant. Persoonlijk moet ik altijd heel wat verschillende websites af om een goede relevante vergelijking te vinden. En vaak kom je dan tot andere conclusies.
GTX 1080;1;0.41066262125968933;Een overclockte 980ti is veel sneller nog zelfs, volgens dit filmpje tenminste
GTX 1080;1;0.41202089190483093;Dit klopt dus echt niet. Een GTX 1070 OC op 2050 Mhz is sneller dan een GTX 980 Ti OC op 1450 Mhz. Ja er zijn snellere 980 Ti's te vinden maar dat geldt ook voor de GTX 1070. De GTX 1070 is gemiddeld genomen een snellere kaart dan de 980 Ti en het verschil zal met verdere driver optimalisaties en DX12 ondersteuning alleen maar gaan toenemen.
GTX 1080;1;0.4795638620853424;Dit is niet waar. Een 980TI heeft enorme overclocking headroom, terwijl een 1070 al bijna op zijn limiet zit. Een 980TI met een goede overclock is beter dan een 1070 met max overclock.
GTX 1080;2;0.4216087758541107;Sommige 980 Ti modellen kunnen overklokken tot 1500 Mhz of iets hoger nog maar dat is bij lange na niet het geval voor elke 980 Ti. Een 1070 kan overklokken tot 2100+ maar dat is ook niet voor elk model het geval vandaar dat ik de vergelijking post van een 1070 op 2050 en een 980 Ti op 1450. Dat zijn voor de meeste kaarten haalbare kloksnelheden en dan is de 1070 gewoon beter. Maar feiten zijn al lang niet meer relevant op tweakers.net. Vandaar dat deze reactie ook gedownmod zal worden.
GTX 1080;1;0.5847102999687195;Wat een onzin over CPU het verschil tussen deze en volgende generatie CPU is 1 a 2 fps met de meeste games.
GTX 1080;1;0.4564380347728729;Vind de prijzen belachelijk hoog wacht eerst review af van de rx480 koop daar liever 2 van mochten ze sneller zijn dan de 1080
GTX 1080;3;0.4109923243522644;"Ondanks dat ik 200 eur voor zo'n unieke kaart sowieso wel over heb; stel dat crossfire ruk blijkt te zijn (met name voor VR.. iemand hier ervaring mee?) en die lekkere performancecijfers die we tot nu toe zien de uitzonderingen blijven.. dan toch maar de reviews afwachten? misschien is zo'n 1070 achteraf toch geen slecht idee."
GTX 1080;5;0.7199713587760925;Dual GPU wordt juist behoorlijk gepromoot voor VR, ene GPU is voor je ene oog, andere voor je ander, waardoor microstutter bijv. geen probleem meer is. En in DX12 hoort erg goede dual gpu ondersteuning mogelijk te zijn, veel beter dan DX11 het heeft.
GTX 1080;1;0.5073012113571167;"Mwa, door SMP kunnen de 1070/80-kaarten straks zonder performanceverlies beide ogen tegelijk renderen, dus dat haalt 't nut van 2 kaarten daarvoor al weg.. en ben zelf zo'n sukkel geweest die meteen 2 6800gt's in SLI zette in 2003, en er pas echt wat aan had tegen de tijd dat de 8800gts er was nu heb ik een rift liggen, en ik ga het me niet laten gebeuren dat ik straks met 2 kaarten weer loop te vloeken dan maar 1 beest van een kaart. [EDIT] OT Vraag: Konden we met DX12 niet zo'n 1070 en RX480 samen laten spelen? Antwoord: ""DirectX 12 explicit asynchronous multi-GPU"" ..shit, dan maar opnieuw de mist in, moet ik proberen"
GTX 1080;3;0.27786973118782043;Dat kon nog wel eens een heus gevecht worden..
GTX 1080;2;0.5048021674156189;De Radeon RX480 moet het doen van zijn prijs, niet zijn snelheid, hij zal ongeveer rond de Geforce GTX 980 zitten en tussen de Radeon R9 390 en R9 390x. En ik heb het eigenlijk ook niet zo op Crossfire en SLI, omdat er genoeg spellen zijn die er niet goed mee werken, en je problemen kan ondervinden, en dat het een stuk meer energie kost. En erg jammer dat Tweakers 4,5 weken later pas komt met een review van de Geforce GTX 1080, dat is gewoon triest, en daardoor was het voor mij niet nodig om nog de hele review te lezen, alleen de VR stukje was nog interessant, maar dat waren maar 2 spellen. En vind het erg jammer dat bij 4K jullie de spellen op High of Medium zette, had leuker geweest als jullie die ook gewoon dan op Ultra draaide, om te laten zien dat veel spellen niet goed draait op 4K op Ultra en 60fps op zelfs een Geforce GTX 1080, dat is ook een van de rede dat ik een 21:9 monitor koop met een resolutie van 3440x1440, dan kan je ruim 90% van alle spellen op 3440x1440 met Ultra draaien op 60+ fps. TECHGAGE heeft een hele mooie review van 4K & Ultra-wide Gaming. En dat het er beter uit ziet dan een 4K monitor omdat je link en rechts VEEL meer ziet, sinds ik een 21:9 2560x1080 monitor heb wil ik echt niet terug naar een 16:9 2560x1440 monitor, 21:9 geeft gewoon veel meer diepte in je spellen, het is gewoon mooier, en de meeste spellen werken gewoon op 21:9, al moet je met sommige wat tweaken om op 21:9 te draaien.
GTX 1080;1;0.4304434657096863;Er is meerdere malen vermeld op tweakers, in ieder geval in de comments, dat tweakers geen 1080 heeft ontvangen voor review .
GTX 1080;2;0.3761254549026489;Dat kan, maar dan ga ja er toch achter aan, dan hadden ze het al 3 weken terug kunnen plaatsen hier op Tweakers, nu is het 4,5 weken later, nu zijn er niet veel mensen meer die nog niet een review van de Geforce GTX 1080 gezien hebben, voor de meeste mensen was deze review niet zo interessant meer omdat all de andere allang meerdere reviews gemaakt hadden van verschillende Geforce GTX 1080.
GTX 1080;1;0.4605395793914795;Het ging mij hier om de AMD en Nvidia in 1 systeem samen laten werken, weet niet waarop jij reageert?
GTX 1080;1;0.488450288772583;Begin er maar niet aan, want dat is een SLI verhaal in het kwadraat met alle lasten en bijzonder weinig lusten. Er is tot nu toe maar één game met support voor MGPU en dat is Ashes, en daar is stutter eerder regel dan uitzondering. Bij DX12 is multi GPU gewoon per definitie overgeleverd aan de grillen van elke game developer, dus reken er maar op dat het niet beter wordt dan nu - eerder slechter door de veelheid aan mogelijkheden ipv 'alleen twee identieke GPU's'.
GTX 1080;3;0.2759261429309845;"Ondanks dat de DX12-manier van SLI nog niet in gebruik is of breed voor ontwikkeld wordt, zou het nog wel nut kunnen hebben ""Linked mode is available when there are multiple near-identical GPUs in the system ... to be combined into one larger addressable unit. So if let’s say we have two R9 390 cards ... twice the memory and twice as many GCN shader units. Essentially creating a 16GB, 5160 GCN core behemoth."" Gaat dan niet over mixed vendor maar klinkt toch echt als iets gaafs dat tot nu toe in SLI/CF nog niet is gedaan"
GTX 1080;2;0.3987670838832855;Theoretisch heel mooi, en de dagelijkse praktijk van nu en de afgelopen 10-15 jaar wijst uit dat de praktijk altijd bijzonder tegenvalt en het geheel staat of valt met een dikke investering van zowel GPU fabrikant als developer/publisher. Plus de nazorg om alles ook na release mooi draaiende te houden. Dat is bij heel veel games niet het geval. Allemaal dus heel erg gaaf, maar zéker met DX12 en VR ben je het beste af met één sterke GPU. Dat gaat echt sterker gelden dan ooit tevoren, want ook onze eisen met betrekking tot minimale FPS gaan omhoog. Een console draait op 30 fps, of soms zelfs minder, maar VR vraagt om 90 of meer. Kleine variaties in frame pacing worden daardoor veel sterker voelbaar - zowel op een monitor als in VR - en linked of multi GPU is daar bijzonder gevoelig voor. Nog een kleine tip: neem WCCFtech met een paar kilo zout. 95% is clickbait.
GTX 1080;5;0.2467714101076126;Duimen en hopen dan maar dat met tijd, doorontwikkeling en dev-adoptatie het waarmaakt wat ermee mogelijk schijnt te zijn.. ik zou alvast 2 rx480s linked willen zien draaien
GTX 1080;3;0.41406628489494324;Prijzen zullen eventueel nog wel zakken doordat de beschikbaarheid op het moment schaars is
GTX 1080;3;0.4019579589366913;Heb zelf nu 2e hands heel goedkoop een GTX 970 op de kop kunnen tikken. Daarmee zing ik het wel een jaartje uit en zie dan wel wat de prijzen doen van deze kaarten. Vooralsnog is meer dan 500,- voor een 1070 veel te veel. Daar kan nog wat vanaf
GTX 1080;3;0.3903186023235321;330 zou een nette prijs zijn
GTX 1080;1;0.3687099814414978;Je zou daarom ook je kans kunnen grijpen en nog een oude Maxwell-videokaart op de kop kunnen tikken, want daarvan heeft Nvidia de prijzen behoorlijk verlaagd. Een 980 Ti heb je al voor 500 euro, terwijl de goedkoopste GTX 980 380 euro doet. Deel je die prijs door de prestatie-index, dan kunnen we zien welke videokaart de meeste waar voor je geld biedt. MSI's GTX 1070 kost een euro of 500, maar de GTX 1080 kost meer dan 800 euro, terwijl er al GTX 1080-videokaarten voor ruim 100 euro minder te bestellen zijn. Geen hint naar Polaris hier?...
GTX 1080;1;0.46506232023239136;Waarom zouden ze? Polaris is nog niet op de markt, en afgezien van AMD's eigen benchmarks hebben we nog geen (geloofwaardige) prestatie indicatie. Zou in mijn ogen niet goed zijn als Tweakers hints of adviezen zou geven op basis van geruchten.
GTX 1080;1;0.2894507348537445;Tsja, copy pasta. Zie de reactie naar PiweD
GTX 1080;2;0.48665693402290344;De hint naar Polaris staat in de conclusie , maar daarvan vind ik het altijd riskant om alvast conclusies te trekken op basis van benchmarks van AMD zelf, want die hebben er belang bij om de zaken zo rooskleurig mogelijk voor te stellen, en aan de andere kant van leaks waarmee getest wordt met een oude driver, die zeker niet de launch-driver van die videokaart wordt.
GTX 1080;2;0.40175917744636536;Dat is niet echt waar ik op doel. Het gaat mij om het stukje waar ze een advies geven aan de koper, hier staat niet vermeldt dat AMD right around the corner is. Benchmarks terzijde, dat zal allemaal wel
GTX 1080;1;0.719986617565155;"Zo lang er geen 16NM 250-300w kaarten zijn slaat het allemaal nergens op. Er wordt weer gemikt op 1-2k en NIET op 4k en minimaal 60fps. Wachten dus tot einde van het jaar. Wat moet je in godesnaam met een kaart die gemiddeld amper 2x sneller is dan de 3 jaar ;;stokoude;; 290x AKA de 1080 Tja, denk daar maar eens goed over na! 16nm leuk!! maar wat moet je er mee als de kaarten niet minimaal 60fps draaien bij 4k En geld besparen op stroom??? Als jeje auto pakt pomp je ook zo 10 euro weg als je een klein stukje rijdt. Verbruik is geen excuus, je gaat geen 800 euro lappen omdat je videokaart zo lekker mileuvriendelijk is Vooralsnog zie ik geen 4k games die 60-144fps fps draaien op een enkelle 1070 of 1080. Waar betaal je dan die 800 euro voor?"
GTX 1080;2;0.5069958567619324;Twijfel zelf ook heel erg, heb nu 2 x 290x en ik kan nergens een echt goede vergelijking vinden tussen 2 x 290x vs GTX1080.. Ik wacht nog rustig af en hoop dat AMD met de Vega iets moois neerzet. Tegen die tijd zal de 1080TI er ook wel zijn. En 4K is voor een hoop mensen helemaal niet van toepassing, heel af en toe start ik wel eens een spelletje dat Eyefinity gebruikt en dat 3 schermen benut en dan komt een 290x CFX setup nogsteeds redelijk mee. Zolang ik nog op 1080p zit zie ik niet veel nut. 1 x GTX1080 lijkt me een redelijke vervanger voor 2 x 290x ben benieuwd..
GTX 1080;4;0.2662082016468048;Goed punt. Goeie kans dat die kaart er al ligt, maar dat ze deze even op de planken laten liggen tot de markt ernaar vraagt, zodat jij en ik over 2 jaar opnieuw een paar honderd euro aan een upgrade uitgeven
GTX 1080;5;0.3765402138233185;Die kaart ligt er al, en heet GP100. Een paar aanpassingen, memory controller van de GP104 erop, NVlink en HBM er vanaf, GDDR5X ertegen aan, twee SMX'en eruit en presto, een gehandicapte GP100 voor zo'n 900-1100 euro. Mark my words...
GTX 1080;1;0.43147507309913635;Goeie kans dat die kaart er al ligt, maar dat ze deze even op de planken laten liggen tot de markt ernaar vraagt. AMD in de buurt dreigt te komen... FTFY
GTX 1080;3;0.28139516711235046;Klopt, het is toch ook gewoon het standaard spelletje van Nvidia/AMD. Elk jaar is hetzelfde, er komt een 'fantastische' nieuwe gpu reeks uit die je moet hebben! In de praktijk is de snelheidswinst maximaal 30% maar er word wel de hoofdprijs voor betaald. Een echte vorm van concurrentie is er natuurlijk al lang niet meer bij deze bedrijven. Dit is gewoon het verdienmodel, elk jaar een paar stapjes sneller. Ik durf te wedden dat ze prima een kaart kunnen uitbrengen die met gemak minimaal 2x sneller is. Maar waarom zouden ze dat doen? Dat maakt het werk alleen maar lastiger en ze verdienen dan minder.
GTX 1080;3;0.5959058403968811;Met sommige dingen ben ik het zeker eens. Maar het laatste ben ik nog niet zo zeker van. We kunnen pas weten wat amd doet als de kaarten gereleased zijn. Ja ze mikken op het midden segment 150-300 euro, maar ik denk dat hier de beste prijs/prestatie verhouding uit voorkomt. Ook vind ik de winst die het bied kwa frames voor die gigantische prijs niet echt heel goed. Als je een 980TI OCed kom je heel dicht bij die 1070. Ook de temperaturen vallen mij zeker tegen voor deze nieuwe generatie. Ook vind ik het raar dat er voor DX12 deze titels word gekozen terwijl bekent is dat DX12 kapot is op Rise of the tomb raider. Wil je echt het verschil zien kies dan ashes of the singulirty op DX12, want deze titel laat echt zien wat DX12 gaat betekenen later.
GTX 1080;3;0.281554251909256;Er wordt ook enkel geconcludeerd dat je voor de snelste kaart bij de 1080 van NVidia moet zijn:
GTX 1080;1;0.41240498423576355;Vind ook dat tweakers moet laten weten dat een GTX 970 of een r9 390 kopen nu dom is, omdat de rx480 voor hetzeflde geld 1.5 keer de performance gaat leveren.
GTX 1080;3;0.3614090383052826;Enige benchmark die ik lag legde de RX 480 tussen de 970 en 980 in en ik verwacht dat hij ook niet sneller zal zijn dan een 390x tbh, alleen lager verbruik en ~240 euro.
GTX 1080;4;0.37930014729499817;Ik zag enkele benchmarks waar hij rond de 300puntjes hoger zit in FireStrike dan een 390x. Dus dat is zeer interessant.
GTX 1080;3;0.3184392750263214;Op het niveau van een 980 dus, dat is wel interesting voor 240 euro
GTX 1080;1;0.40996721386909485;laten we dan eerst maar eens echte onafhankelijke benchmarks kijken voordat je uberhaupt gaat beweren dat de rx480 1.5 keer de performance gaat leveren...
GTX 1080;2;0.322421669960022;"totdat er een review is mag Tweakers dat naturlijk niet zo direct zeggen ze kunnen natuurlijk wel zeggen dat het erop lijkt dat binnenkort AMD met een beter ""budget"" alternatief gaat komen. (welke ik inderdaad wss persoonlijk ga halen)"
GTX 1080;2;0.2976754605770111;Ik denk dat AMD wel een paar goede kaarten kan neerzetten. Ze zijn daar hard bezig, nu nog iedereen proberen te overtuigen. Ik zou bijvoorbeeld liever geen AMD kopen. Nvidia lijkt nog altijd meer en beter te zijn, hoor bij AMD systemen ook nog wel eens gezeur.
GTX 1080;2;0.4072946012020111;Dit is het probleem. Marketing, vraag de menige casual welke grafische kaarten zijn beter zeggen ze nvidida.... De r9 390 is nu beter dan de 970 ivm vram en word beter voor geopitmaliseerd en inkomende dx12. Amd moet gewoon heel hard aan zijn imago werken om ons als gamer te overtuigen. Eind dit jaar komen hun concurrente voor de 1080 aan en nu focussen op de mainstream
GTX 1080;2;0.37256669998168945;"""Ook onder belasting zijn de MSI-koelers buitengewoon stil, in tegenstelling tot de standaardkoeler van Nvidia, die aardig wat geluid produceert."" Hier had wel wat meer over gezegd mogen worden en 30 en 40 db zegt niet zoveel alleen dat 40 drie keer zo hard als 30 is. Iets van het klonk wel harder dan de MSI kaart maar was geen moment storend onder het browsen. Of onder gamen was het wel te doen en viel het geluid niet op maar je wilt deze kaart in een pc op je bureau op 50cm afstand hebben terwijl je browsed."
GTX 1080;1;0.5229246616363525;"40dB is 10 keer zo ""hard"" als 30 dB. dB is een logaritmische schaal. (Op basis van 10)"
GTX 1080;1;0.47817960381507874;Zeker weten? Stond me bij dat elke 3 db erbij een verdubbeling is van het geluid. Dus van 30 naar 40 is dan drie keer een verdubbeling. 10 x zo hard is dan idd een wereld van verschil wat een lezer er niet zo snel uit zou halen zonder kennis vandaar ook mijn opmerking om niet alleen grafieken te tonen.
GTX 1080;3;0.41918298602104187;Het is 10x de geluidsenergie en 3x de amplitude van de geluidsgolf bron dus het is eigenlijk allebei waar Maar vergeet niet dat onze oren ook logaritmisch werken dus van 30 naar 40 klinkt niet 10x zo hard. Verder is dB een relatieve schaal alleen bedoeld voor vergelijkingen tussen twee waarden, dus hoewel het inderdaad betekent dat 40dB 10x zo veel energie is dan 30dB, zegt het niks over de absolute waarde. Daarom wordt voor geluid (net zoals hier) meestal dB(A) gebruikt, dat ook nog eens speciaal is aangepast aan het logaritmische gedrag van onze oren. En voor radio heb je bijvoorbeeld dBm, waarbij 0 dB gelijk staat aan 1 milliwatt. Door de 0-waarde vast te leggen, kan je toch absolute waarden meten. Maar inderdaad zijn de stock koelers vrij luidruchtig. Ik heb ook een stock 970GTX in mijn Game PC (Alienware X51) omdat er geen grotere in passen met uitstekende heatpipes, dus de stock was de veiligste keuze omdat Dell die zelf ook levert. Maar inderdaad, je hoort hem wel. En ik game toch altijd met een hoofdtelefoon op dus #care
GTX 1080;3;0.2514682114124298;Ik weet niet hoe betrouwbaar deze bron is, maar van wat ik zie is 40dB nog steeds super stil
GTX 1080;3;0.39632123708724976;Hangt ervan af wat de reference was. Maar daar er dB(a) genoemd wordt zal er gewoon goed gemeten zijn. (hopen we dan)
GTX 1080;3;0.3019765019416809;Dat een 1070 tot bijna een factor 3 sneller is dan een 970 in de VR test van Project Cars verbaast me wel enorm. Wat een topkaart voor VR blijkt het te zijn! Nu ga ik toch twijfelen om te upgraden, racing games zijn totnogtoe gewoon geen optie in VR met een 970.
GTX 1080;2;0.31632307171821594;Ze hebben de 1070 al 2x zo efficient gemaakt met VR door te zorgen dat hetzelfde beeld niet 2x hoeft worden uitgerekend te worden (is mijn vage begrip ervan)
GTX 1080;1;0.4390335977077484;Die techniek heet SMP maar wordt, zoals in het artikel staat, nog niet gebruikt in games. Dus dat houdt in dat die fps-verdubbeling voor een hoop VR-games nog aan zit te komen ook! ik ga maar weer sparen
GTX 1080;2;0.26905500888824463;Mwah, VR staat nog in kinderschoenen, voordat er echte games voor zijn ben je zo 1/2 jaar verder en zijn de 1070/1080 kaarten alweer vervangen of veel goedkoper. Nu zie ik het nut voor een upgrade iig nog niet.
GTX 1080;3;0.3844093978404999;Maar zelfs al zou de RX480 niet in de buurt komen van 1080, voor 200 dollar verwacht ik dat ook niet. Dat is minder dan een kwart van de verkoopprijs van de 1080 dus als deze de helft zo goed is vind ik het al een goed product.
GTX 1080;3;0.5586281418800354;Hopelijk zorgt het voor wat prijsverlaging bij het groene team, tegen die tijd zullen die kaarten ook beter beschikbaar zijn.
GTX 1080;4;0.2937898337841034;Hij zal waarschijnlijk (ff afkloppen op hout) tussen de 970 en 980 inkomen, als je hem met vorige generatie Nvidias wil vergelijken. Mss kom je met twee 480s in Xfire ongeveer op het niveau van de 1070, en dat zou qua prijs dan nog aardig kunnen kloppen ook.
GTX 1080;2;0.40182244777679443;Als je de leaks in de gaten hebt gehouden zou een rx 480 over 1,5ghz kunnen oc'en. Dit tegenover een boost/bassisclock (ben ff vergeten welke het was) van 1,266ghz. Met deze oc zou de kaart dicht in de buurt komen van een 1070. Maar het blijven natuurlijk leaks dus die kan je met een heel groot korreltje zout nemen. Deze performance voor een price van 300$ (dat is wat ze verwachten voor deze superclocked kaarten) zal de price van de 1070 waarschijnlijk laten zakken naar MRSP voor sommige kaarten. De 1080 zal denk ik niet zo heel veel in prijs zakken omdat die kwa single card performance nogsteeds een stuk hoger ligt. Maar het blijven leaks dus wie weet. (gebed aan GaKa goden )
GTX 1080;2;0.3625115156173706;"""Uit de prestatie-index bleek al dat de gemiddelde framerate van de GTX 1080 in de door ons geteste games ongeveer een derde hoger ligt dan die van de GTX 980 Ti. "" Waar dan? ik zie dat niet hoor"
GTX 1080;3;0.35413631796836853;"Dat zal afhangen vanaf welke kant je rekent, maar in mijn interpretatie is de hier genoemde formulering wel juist 1000/746 = 1,34 -> 34% betekent min of meer ""gtx1080 is 34% sneller dan gtx980ti"" 746/1000 = 0,746 -> -25% betekent min of meer ""gtx980ti is 25% trager dan gtx1080"""
GTX 1080;1;0.2774759829044342;Ja, dan gebruik je een OC 1080 vs een stock 980ti.....
GTX 1080;3;0.6159079670906067;Het verschil is goed en wel 20% met een OC op beide kaarten, varieert wat per game. Met name games die op een redelijk zware engine draaien zoals CryEngine laten een kleiner gat zien tussen de 980ti en de 1080, soms zitten ze zelfs tot 12-15% van elkaar af. Dat komt met name doordat de 980ti meer resources ter beschikking heeft, op lagere clocks - bredere bus, meer shaders. Over het algemeen dus weinig reden om naar een 1080 te gaan als je nu een 980ti hebt die aardig klokt.
GTX 1080;4;0.4592796564102173;Zeer nette review, waarvoor dank. Ik had gehoopt dat er inmiddels enkele reviews zouden komen welke de verschillende custom 1080s en 1070s tegenover elkaar zetten in een vergelijking, doch begrijp ik dat het - net zoals de gemiddelde tweaker - het ook voor Tweakers.net lastig zal zijn om voldoende customs 1080s en 1070s in handen te krijgen voor een fatsoenlijke review. Gezien de huidige afwezigheid van voorraden bij de webshops (en het feit dat ik niet onnodig 100-150 euro extra wil neerleggen bij de webshops welke ze wel voorradig hebben) zal het waarschijnlijk nog een paar weken duren voordat ik mijn 1080 heb gevonden.
GTX 1080;2;0.40389421582221985;We hebben nog getwijfeld om de review later te plaatsen en te wachten op meer custom kaarten, maar omdat veel fabrikanten geen levertijden konden beloven hebben we dat toch niet gedaan.
GTX 1080;1;0.6250255107879639;Schiet inderdaad niet op met die custom kaarten. Founders editions zijn pas net beschikbaar en die partner cards staan zo'n beetje allemaal op levertijd >12 dagen. Binnenkort heb ik vakantie en zou dan graag een 1080 in m'n PC droppen maarja...
GTX 1080;1;0.35673102736473083;waar zijn de 4k benchmarks ? ik kan wijnig wijzer worden van deze review of de kaart nou echt veel beter is. en waarom staat de 290x in de lijst maar niet de 390x
GTX 1080;1;0.23943354189395905;De grafiekjes bestaan uit tabs, het is steeds voor elke game 1920, 2560 en 4k.
GTX 1080;3;0.5173630118370056;ok thankz
GTX 1080;3;0.3039552569389343;Ik vind dit aan de koper zelf om te beslissen of een stille kaart aantrekkelijker is.blindstaren op prijs/fps <> rust in huis
GTX 1080;3;0.4566956162452698;"Klopt. Je ziet wel vaker in reviews dat ze (voornamelijk) redeneren vanuit één perspectief; bij videokaarten wordt dus meer gekeken naar prestatie, en soms naar prijs/prestatie-verhouding, dan vanuit stilte of stroomverbruik. Op zich ook wel logisch, aangezien die prestatie toch het primaire doel is van een videokaart. Zolang ze maar wel duidelijk maken inzichtelijk maken voor de lezer wát die afweging is, en de moeite nemen om andere kengetallen (zoals stroomverbruik of Decibel) ergens te noemen, al is het maar in een tabelletje of feitenlijstje onderaan het artikel. Dan kan een consument zoals jij die daar wél naar kijkt altijd zelf zijn afweging nog maken."
GTX 1080;2;0.31563040614128113;Ik mis een partner board van een 980ti bv Gigabyte 980Ti Gaming 6G Windforce, nu zie ik eigenlijk nog steeds niet of het een waardige upgrade is voor 1440p @60fps.
GTX 1080;3;0.47200170159339905;Nette review alleen mis toch wel de vergelijkingen met de GTX 970 in de benchmarks. Heel jammer.
GTX 1080;3;0.3976786732673645;Ik vind het voorbarig om de RX480 nu al the neglecten zonder dat we hiervan benches gezien hebben. Voor mij betekent de RX480 een mogelijkheid om de tijd te overbruggen tot aan de volgende upgrades. Ook al zijn deze kaarten met name geschikt voor VR, ik vind het toch fijner om niet gelijk toe te happen in generatie-1 met deze VR-voordelen. Ik wil een buffertje hebben voor VR als ik hier een kaart voor koop, de ti-versies lijken mij pas echt de voordelen te gaan brengen van deze eerste 16nm's.
GTX 1080;1;0.5599218606948853;Waarom denk je dat. Omdat AMD het belooft of omdat iedereen het zegt. De drivers van AMD zin klassiek slecht bij introduktie van nieuwe modellen. Het is dus echt wel afwachten en wat er niet is kun je ook geen rekening mee houden.
GTX 1080;2;0.4143582284450531;Omdat €200,- voor een 6,5 beter is om risico's te vermijden dan een 8 voor €500, en waar bij de laatste je zeker weet dat er nog niet alles uit gehaald is. Ik koop liever de AMD nu en dan verdien ik later die €200 wel weer terug nadat de 1080 ti uit is. En tegen die tijd zijn de brillen ook waarschijnlijk weer een paar honderd goedkoper.
GTX 1080;3;0.5903084874153137;Jammer dat jullie geen 980SLI setup in de benchmark hebben meegenomen. Wel interessant om te zien hoe die prestaties zich verhouden ten opzichte van de 1080.
GTX 1080;2;0.3904440999031067;Mooi dat jullie eindelijk ook de kaarten gereviewt hebben. Enige wat ik jammer vind aan deze test is dat de 980ti die is meegenomen een stock edition is die hier tegenover een custom kaart wordt gezet. (daar ga ik van uit op basis van de geluidsproductie). En we weten van custom 980ti's dat ze enorm goede overclockers waren, 1400-1500 mhz was meer de regel dan de uitzondering. Ik had wel eens willen zien hoe de 1070 het daar tegen zou doen. Daarnaast vind ik het gewoon eeuwig zonde dat AMD heeft besloten om voor de rest van het jaar geen high end kaart uit te brengen (het wachten is op Vega). Dit is niet goed voor de concurrentie en ik denk daarom dat de prijzen van de Pascal kaarten een stuk langer dan anders hoog zullen blijven. Wellicht dat ze ook nog langer zullen wachten met het uitbrengen van de 1080ti, als je immers al de snelste kaart op de markt hebt is het stom om de 1080ti daaroverheen te gooien. Dan kannibaliseer je, je eigen verkopen. Een suggestie voor toekomstige reviews: Een lijst met alle kaarten die je in je grafiek toont, en dan precies het type zodat we weten of het custom of reference kaarten zijn waarmee de kaart vergeleken wordt.
GTX 1080;4;0.4913327693939209;Goed punt appels en peren.
GTX 1080;2;0.43704143166542053;Ze zouden deze keer écht aandacht spenderen aan hun review van hun GPUs. Want tja, die waren in het verleden nou eenmaal niet zo sterk. Oh, en er is uiteraard sprake van een gigantische vertraging. En om dan te lezen dat ze reference kaarten met aftermarket ontwerpen vergelijken... Tja daar word ik dan toch een beetje verdrietig van. En kun je nagaan, ik had mijn verwachten al niet hoog staan. Triest.
GTX 1080;2;0.4608789384365082;Wat is er aan de hand met de Project Cars benchmarks? Vreemde resultaten.. Trouwens geinige kaarten maar voor de huidige prijs minder rendabel.. Zeker gezien de prijs die beloofd werd.
GTX 1080;4;0.48425546288490295;Fijn om de VR testresultaten te zien! De GTX1070 is een mooie teamspeler voor de Oculus Rift :-) Nu nog even duimen dat de GTX1070 kaarten leverbaar en daardoor ook betaalbaarder worden en dat de Rift preorder eindelijk geleverd gaat worden.
GTX 1080;3;0.6576923131942749;Beetje jammer dat de sli opstelling niet op high of ultra is getest op 4k. Had graag willen zien of de gtx 1080 in sli ook op 4k in high of zelf ultra settings ook gewoon 60fps uit de kaarten weet de persen.
GTX 1080;2;0.4332916736602783;In de review staat: AMD heeft de nieuwe RX480 al wel aangekondigd, maar het is Nvidia dat aan het langste eind trekt en zijn nieuwe videokaarten, de GTX 1070 en 1080, daadwerkelijk in de schappen heeft liggen. Dat vind ik echt niet zo, de gtx 1000 lijn is niet daadwerkelijk in de schappen, je kan ze amper krijgen nog. En gene die het wel leveren, is het allemaal veel te duur.. Bij de meeste shops staat nog steeds pre-order of levertijd 3 weken
GTX 1080;2;0.48257899284362793;Erg jammer dat onder andere The Witcher en GTA 5 niet in ultra settings wordt getest. Ook mis ik de GTX 970 wel een beetje in de benchmarks, denk dat er veel mensen met een 970 zijn die duidelijk willen zien of een 1070 kopen voor hun zin heeft.
GTX 1080;5;0.29419371485710144;Gezien 3D printers en 3 tot 5 assig CNC freesbankjes steeds meer een normaal huis vinden vraag ik mij ook af hoe programma's zoals Autocad en Solid works profijt hebben van deze kaart tov een nVidia Quadro. Natuurlijk is het een gamerskaart maar 3D CAD is ook steeds meer gebruikt.
GTX 1080;1;0.586798906326294;hoe brak kan je een vergelijking tussen resoluties maken ?? Zelfs al is het onspeelbaar traag op 4K, laat dat dan tenminste uit het cijfermateriaal blijken ipv naar medium terug te schroeven om dan plots een HOGERE framerate te noteren dan voor 2.5K waardeloos vergelijingsmateriaal, dus maar weer uitwijken naar een andere site waar ze wel een rechte lijn trekken in hun benchmarks onderling
GTX 1080;5;0.33302727341651917;Mee eens. Onbegrijpelijk
GTX 1080;1;0.42375633120536804;De toon is gezet Afhankelijk is een groot woord. AMD heeft zelf meeontwikkeld aan zijn eigen 14nm tech met afgescheiden dochter GloFo (en diens BFF Samsung). Heeft ook een flinke duit in het zakje gedaan bij TSMC's 16nm tech. Mag er wel bij vertellen dat Nvidia met een fabrieksgemiddelde communiceert, AMD communiceert met een technische max. Voordat je straks in de Polaris review gaat roeptoeteren dat de Polaris meer vermogen voorschrijft. Benoemd ff vlug tussen neus en lippen dat SLI softwarelocked op 2 is. Komt verder niet terug in de review of conclusie. Ook niet wat dit betekend voor de multi-gpu features van DX12... Ow wacht, DX12 support is nog steeds waardeloos. Dan maar weinig woorden er aan vuil maken en op naar de DX9(DX11) games. Opvallend ook dat er geen Fury kaarten in multi-setup voortkomen, maar wel de 1080 in SLI. Schijnbaar mag een Nvidia kaart niet slecht presteren, dus dan maar het spel schrappen (?). Geen idee ook waar AoS niet als gewoon spel is meegenomen. Vergeet er voor het gemak ff bij te vermelden dat AMD nog niet genoodzaakt is geweest om oude voorraden weg te doen door prijzen drastisch te verlagen. Prijzen van 1070/1080 worden ook uiterst gunstig aangehouden, vrij letterlijk in de sidebar voor 518/779, 525/750 lijkt me een realistischer gemiddelde. Geen idee waar je een FuryX voor 580 haalt (als je m uberhaubt nog ergens kan halen). Van te voren werd er trouwens nog hard geroepen dat de FE kaarten duurder zouden zijn dan die met vendor-specific koelers. Dat is duidelijk niet het geval. Was ook wel het noemen waard geweest. Hilarisch.. Is daar ook niet voor bedoeld. Dump 'm straks maar eens in een 3weg opstelling, ben benieuwd wat dat gaat doen tegen de 1080.
GTX 1080;5;0.3189561665058136;Topreactie! Ik ergerde mij precies aan dit soort punten. Twee dure kaarten tussen neus en lippen met een kaart van 200 dollar vergelijken om het goedkope kaartje als een misgeboorte af te doen. Ik verwacht geen totale objectiviteit maar dit artikel is mij veel te subjectief geschreven.
GTX 1080;2;0.35193192958831787;"Citaat: ""Ga je voor de snelste kaart van dit moment, dan zul je daar dus aardig wat voor moeten neerleggen en we vermoeden dat Nvidia zich voorlopig niet gedwongen voelt om de prijs te laten zakken."" Dat laatste lijkt me dan wat optimisch van Nvidia. Want verreweg de meeste klanten zullen wel degelijk naar price/performance verhouding kijken, en dan wordt de RX 480 toch een behoorlijke concurrent... die gaat véél beter presteren wat dat betreft, gebaseerd op de nu gepubliceerde benchmarks en prijzen. En er is ook nog zoiets als crossfire. Duw er 2x RX 480 in en je bent nog steeds goedkoper uit. Dus hoeveel mensen gaan daadwerkelijk € 700 voor een videokaart betalen? Ja, die zijn er best wel wat tegenwoordig, maar ik neem aan dat Nividia er veeeel wil verkopen."
GTX 1080;2;0.3687935471534729;ik zei de gek XD. OT: je weet dat 1x 1070 ook bijna aan dat bedrag zit en dan nog minder stroom verbruikt ook met zga de zelfde prestaties als 2x RX480? uiteindelijk is de 1070 dan wel goedkoper maar dan puur door de stroom kosten uit eindelijk. tenzei de RX480 ook de helft aan de stroom gebruikt van een 1070
GTX 1080;3;0.3981418311595917;"De 1070 is inderdaad wel wat goedkoper ja. Ik zie in de Pricewatch diverse modellen staan variërend van (minimaal) € 470 tot (minimaal) € 520 bij de goedkoopste aanbieders. Er van uitgaande dat de Rx 480 daadwerkelijk rond de € 200 gaat kosten (x 2 = rond de € 400), is dat nog wel ietsje goedkoper dan een Gtx 1070, maar goed ik besef me dat ""rond X euro"" een breed begrip is. Het is dus ook even afwachten wat de 480 echt gaat kosten. Hoe dan ook, mijn punt is inderdaad, dat voor het overgrote gedeelte van de consumenten de afweging van prijs/kwaliteit, inclusief de vragen ""hoeveel ga IK er daadwerkelijk van merken en hoe essentieel vind ik dat?"" nog wel zeker een rol zullen gaan spelen. Zeker voor de Gtx 1080 zal dat betekenen dat verreweg de meeste consumenten voor een prijskaartje van € 700 nog wel 2x zullen nadenken, en andere alternatieven gaan afwegen... inclusief de 1070 dus die toch alweer een flink stuk goedkoper is inderdaad. Voor mezelf sprekend, ik heb nu een HD 4890 waar ik nog redelijk wat games behoorlijk op kan spelen. Het begint nu een beetje te nijpen, maar goed, ik heb de kaart ook al sinds mei 2009 (= 7 jaar dus!!). Ik zit er dus nu over te denken om te gaan upgraden, maar dat zal eerder een upgrade worden van € 200 á € 250. Een prijs van € 700 heb ik gewoon echt niet over voor een videokaart, zeker niet als je ziet hoeveel stappen ik voor pakweg € 200 vooruit kan zetten! De Rx 480 is dus zeker ook een kandidaat die ik even afwacht. Moet je eens kijken hoeveel meer performance ik dan heb ten opzichte van een HD 4890 die me ook tot op de dag van vandaag nog goed genoeg voor mijn gaming-maatstaven heeft kunnen bedienen. Ik ben (dus) niet het type gamer dat nog eens € 400 á € 500 EXTRA gaat neertellen voor net die paar fps of resolutie méér."
GTX 1080;3;0.4165019392967224;voor mij was dat dus wel het geval. met basis hardware van 8 jaar oud was ik toe aan een nieuwe top of the line computer. daar heb ik dus de nieuwste I7 in gezet met (foutje van de winkel) 64gb aan ram geheugen, en de Videokaart was een GTX 780. echter had ik al een nieuw scherm gekocht en ik heb benchmarks bekeken en 3440x1440 kan de GTX 1070 wel aan maar de 1080 was voor mij gewoon de keuze om er zeker van te zijn dat ik niet zo snel een 2e kaart erbij hoef te zetten. en ja 2x een GTX1070 zal meer performance hebben dan de 1080 en ook voor ongeveer de zelfde prijs. maar ik zit net wat confortabeler met maar 1 vid in dat geval. vandaar dat dat mijn keuze was maar als ik op 1080P had blijven gamen was de 1070 meer dan voldoende geweest.
GTX 1080;3;0.4075542688369751;Maar met 2x480, ga je over de 1070 heen. Waarschijnlijk tipt het aan de 1080. En dat is erg gunstig. Ik denk dat we ff de release van amd moeten afwachten, maar ik denk dat de 480 de winnaar gaat worden qua prijs/speed.
GTX 1080;2;0.4345880150794983;"Voor mij persoonlijk gaat de Rx 480 zoals het er nu naar uitziet sowieso winnen, omdat die een stuk betaalbaarder is, voor mij qua prestaties méér dan genoeg stappen vooruit maakt, ten opzichte van mijn huidige HD 4890 En mede daardoor dus qua prijs/kwaliteit verhouding voor mij niet te overtreffen is, door een kaart van 500 á 700 Euro (wat ik er gewoon niet voor over heb). Iets waar iemand anders mij echter wel terecht op wees ten aanzien van 2 kaarten in Crossfire/SLI plaatsen: het is altijd maar afwachten hoe goed dat ondersteund wordt per game. Bij de ene game haal je bijna 2x zoveel prestatie als met 1 kaart, bij de andere game krijg je helemaal geen beeld totdat je de Crossfire/SLI uitschakelt en dus noodgedwongen terug gaat naar 1 kaart. Dus de redenering ""met 2x Rx 480 ben ik nog steeds goedkoper uit, en presteer ik misschien wel bijna net zo goed of beter als een 1070"", gaat maar gedeeltelijk op, voornamelijk dat tweede gedeelte van die zin."
GTX 1080;2;0.3301245868206024;"""Dus de redenering ""met 2x Rx 480 ben ik nog steeds goedkoper uit, en presteer ik misschien wel bijna net zo goed of beter als een 1070"", gaat maar gedeeltelijk op, voornamelijk dat tweede gedeelte van die zin. :P"" Het is mogelijk. Vandaar ook mijn zin: ""Ik denk dat we ff de release van amd moeten afwachten, maar ik denk dat de 480 de winnaar gaat worden qua prijs/speed."""
GTX 1080;2;0.4613456726074219;Ja afwachten tot de release van de Rx 480 is sowieso een goede idee, maar mijn punt dat Crossfire/SLI maar gedeeltelijk (niet door alle games, of niet even goed) ondersteund wordt, is een algemene. Die zal niet ineens veranderen met de release van de Rx 480 denk ik?
GTX 1080;2;0.4952441453933716;Nja, volgens mij is het een algemene dat de meeste games vrij goed schalen. Winsten tussen de 130 en 190% zijn al gauw mogelijk. Maar er zijn zeker ook games waarbij schaling weinig tot niets doet. Echter moet ik die spellen nog spelen. De games die ik zelf draai, draaien stuk voor stuk beter in CFX. Het is dus voor de gamer altijd een afweging of zijn/haar gebruikte opstelling past bij de interesse in de games denk ik.
GTX 1080;5;0.26110929250717163;Heb nog nooit zo'n stelletje zuurpruimen massaal zien verzamelen onder een review die ze zelf niet beter doen. Ga ff iets leuks doen ofzo, vrolijk jezelf op aub. Zou hier ook geen moeite meer in steken met zulk publiek, dan is de lol om moeite in je artikel te steken er wel snel vanaf.
GTX 1080;2;0.3961543142795563;"Ach ja, het geeft natuurlijk ook een vertekend beeld, want wie neemt er voornamelijk de moeite om onder een artikel te reageren? Voornamelijk NIET de mensen die denken ""goede review, bedankt voor de nuttige info"" of die denken ""ik heb eigenlijk niets toe te voegen""."
GTX 1080;3;0.5028167366981506;Armando Ferreira heeft de kaart ook gereviewd, zijn conclusie was dat het niet een grote upgrade is van de vorige versie. Uiteraard wel een goede aanschaf als je een veel oudere kaart hebt of een nieuw systeem aanschaft. Echter had ik het idee dat ze er meer van hadden verwacht.
GTX 1080;3;0.6032195687294006;Dit leest inderdaad als een genuanceerd verhaal, zoals Tweakers in de volgende paragrafen ook aangeeft. De pre-emption wordt fijnmaziger uitgevoerd, waardoor als gevolg van dit pre-emption proces kleinere correcties hoeven te worden uitgevoerd op al berekende output. Keerzijde is dat het proces zelf veel actiever moet zijn om zo fijnmazig te kunnen ingrijpen. Zoals ik het lees zal het huidige proces meer een continue load voor de kaart betekenen, terwijl het voorgaande proces door de grove correctie meer met bursts zal hebben gewerkt. Voor de overall performance een beetje koffiedik kijken. Wat mij aan de verandering wel aanspreekt is dat ik verwacht dat de fijnmazigheid kan helpen beeldafhandeling vloeiender te laten verlopen, met kwalitatief betere rendering tot gevolg. Zelf vind ik dat wel belangrijk, omdat mijn bias meer bij kwalitatieve rendering ligt. Fps-sen zijn ook belangrijk, maar in het verleden is de renderkwaliteit daaraan nog wel eens ondergeschikt gemaakt.
GTX 1080;5;0.3457888662815094;Super dat jullie ook vr benchmarken tweakers! Vraag nee af hoe dat dan werkt want hoofdbewegingen hebben toch effect op wat er gerendert moet worden hoe standaardiseren jullie die dan? In de toekomst stel ik voor dat jullie ook elite dangerous in VR meenemen. Met hoge settings zou die denk zelfs een 1080 op de knietjes moeten kunnen krijgen.
GTX 1080;1;0.7533189058303833;Even een domme vraag misschien, maar waarom zijn de kaarten zo slecht leverbaar? Was dat ook zo bij de launch van de 980 en 970? Of zijn het eigenlijk nog steeds de eerste paar exemplaren en zijn ze nog niet echt verkrijgbaar voor de customers?
GTX 1080;3;0.5150008201599121;Redelijke review, meer kan ik er niet van maken. Test zou wat uitgebreider kunnen zijn, zeker na zo lang wachten, aan de andere kant, wat kan je dan nog schrijven wat al niet door andere sites gemeld is? Voor de toekomst hoop ik dat Tweakers iets sneller is met reviews zonder dat het ten koste van de kwaliteit gaat. Kom op, even de banden met Nvidia en AMD versterken Tweakers! Of doe zoals voorgesteld, een beroep op de community. Alles beter dan hoe het nu gegaan is.
GTX 1080;4;0.5171955227851868;Mooie review en testen. Leuke uitslagen. Is het de moeite waard om over te stappen op een GTX 1070? vanaf een Radeon R9 290. Ik heb een bod via marktplaats voor €175 momenteel. Ik speel op Full-HD
GTX 1080;2;0.31878072023391724;Hoe kan het nou dat er zo een klein verschil is in performance met de VR benchmark tussen de 1070 en 1080? Project Cars VR, heeft 4 fps verschil tussen msi 1070 en msi 1080? Adr1ft, heeft 1 fps verschil tussen msi 1070 en msi 1080? Ik voel me best genaaid met mijn msi gtx 1080, sinds ik het voor VR gekocht heb
GTX 1080;2;0.44982093572616577;"Ik vind het erg jammer dat de reviewer AMDs nieuwe kaarten naast die van NVIDIA legt, terwijl het om een compleet ander marktsegment gaat. Daarmee lijkt de reviewer te zeggen dat AMD ondermaats heeft gescoord terwijl het appels met peren vergelijken is. Wel wordt er geschreven dat de 1070 een betere keuze is dan de 1080 gezien de meerprijs van de 1080 niet evenredig performance oplevert. Tja, dat zou je dan toch ook door kunnen trekken naar de AMD 480 met een veel kleiner prijskaartje? Hoewel AMD en NVIDIA aartsrivalen zijn, en de stoten die links en rechts worden gegeven iedereen doet lekkerbekken, hoort AMD niet thuis in deze review; in ieder geval niet zoals deze nu wordt weergegeven."
GTX 1080;2;0.5623285174369812;Ik ben persoonlijk wat teleurgesteld met de GTX1080. Het is weinig meer dan een overklokte Maxwell (GTX980). Kijkende naar puur de Cuda cores en de kloksnelheid. de kloksnelheid van de 980 was 1216MHz, die van de 1080 is 1733MHz. 1733/1216= (ongeveer) 1.4, dus 40 hoger. De 980 had 2048 Cuda cores, de 1080 heeft er 2560. 2560/2048= 1.25 (25% meer). Vervolgens kun je het simple sommetje maken, 1.4*1.25=1.75, ofwel, puur op cuda cores en kloksnelheid moet de kaart 75% beter presteren dan de 980, en laat dat nou ongeveer zijn wat ie doet. Dan kijken we naar de prestatie index van Tweakers, de 980 staat op 0.564, 0.564*1.75= 0.987. De 1080FE haalt 0.953, net iets minder dan je zou verwachten. Ik zie niets anders dan een overklokte Maxwell GPU hier en dat stelt wat teleur.
GTX 1080;3;0.42729315161705017;Leuke review om te lezen en handig dat de testresultaten per resolutie een eigen tabblad hebben, top! Op dit moment staat de line up van AMD er triest bij, niet de snelste kaart, niet de beste prijs prestatie verhouding. Hoop voor AMD en voor ons dat de RX480 daar snel verandering in gaat brengen want die prijzen voor de GTX 1070 en 1080 vind ik te hoog.
GTX 1080;2;0.34985145926475525;Zouden de Non-reference edities niet goedkoper worden dan de FE edities??
GTX 1080;2;0.41390886902809143;"Je bedoelt dat de non-reference kaarten goedkoper zouden zijn dan de reference kaarten (tegenwoordig dus founders edition), en dat klopt helemaal. Maar ik denk dat Nvidia niet had begrepen hoe veel geld mensen over hebben voor een kaart die er helemaal ""gaming"" uitziet. Andere voordelen zijn dus geluidsproductie, verlichting en een hogere standaardklok."
GTX 1080;1;0.6558144092559814;"Het stomme is dus nu dat al dat ""extra"" geld allemaal naar MSI, ASUS etc. etc. gaat ipv dat het goedkoper wordt"
GTX 1080;3;0.31141459941864014;FE is toch Reference?
GTX 1080;1;0.5417328476905823;Sorry, ik bedoelde de after market edities
GTX 1080;5;0.4479990303516388;Founders Edition == Reference Edition
GTX 1080;4;0.31524255871772766;Zie mijn reactie hierboven
GTX 1080;3;0.2778182923793793;En watvoor voeding heb je daarbij nodig, is 700 genoeg of meer? wordt Eneco blij van
GTX 1080;3;0.35750943422317505;De nieuwe kaarten zijn juist een stuk zuiniger dan de vorige generaties, vanwege het 16nm proces.
GTX 1080;5;0.42896974086761475;Staat duidelijk hier. 400W voeding is genoeg.
GTX 1080;1;0.384357750415802;SLI heeft minder performance dan 1x MSI gaming versie? Edit: Hoezo is dit nou weer -1, ik ben gewoon verbaasd.
GTX 1080;2;0.41863036155700684;SLI is eigenlijk overbodig op 1080p, zoals je kunt zien bij 2K/4K waar de GTX1080 SLI ineens vleugels krijgt als je b.v. bij Project Cars kijkt zie je ineens dat de FPS op 4K hoger ligt dan bij 1080p, daarbij is het ook weer erg afhankelijk van de engines die gebruikt zijn voor spellen. De Witcher 3 (REDengine) engine schaalt zeer goed met SLI (Crossfire ook) met de laatste drivers/game patches. Ook had ik wel willen weten bij de SLI tests of de 'game experience' niet lijd onder de enorme verschillen tussen min/max fps bij sommige benchmarks is het verschil tussen min/max fps bijna 50%. Bij bepaalde titels is het toch heel fijn om niet alleen een 100+ fps te hebben maar ook te houden (als voorbeeld) bij BF4 merk ik wel als mijn FPS niet op ~100 zit maar ineens op 60 zit dan voelt het 'stroef' aan. Nou vind ik het ook jammer dat er geen G-Sync of Freesync monitor gebruikt is. En DX12 heeft (nog) geen SLI/CFX support dus heeft het weinig zin om te testen.
GTX 1080;1;0.45448678731918335;Hmmm,,, Geen 980TI in SLI Geen 980 in SLI Geen TitanX (ook niet in SLI) Geen FuryX in CF Geen 290X in CF Geen 380 in CF Het rijtje met de geteste kaarten is erg schaars wel. Bovenstaande lijken me erg leuke tegenhangers, of referentie testmodellen/configuraties. Zou zomaar kunnen dat een 290X in CF tot aan de 1070 komt, of er zelfs overheen gaat. En dan kun je je dus afvragen of Nvidia echt de snelste is, voor het geld........ik vraag het me af.
GTX 1080;3;0.3997938930988312;Klopt ik heb zelf ook een 290x CFX setup en kan bar weinig _recente_ benchmarks vinden die dat dus vergelijken. Bij een upgrade verwacht ik minimaal dat 1 GTX1080 mijn 290x CFX overtreft. Daarbij komt ook dat de meeste benchmarks al ouder zijn, oudere drivers etc. Zelf zie ik ook wel dat bepaalde titels een boost hebben gekregen (of CFX support) met latere drivers. Zo ook is het 'min/max fps' verhaal best belangrijk.
GTX 1080;1;0.5412827730178833;Wat ik me eigenlijk afvraag is of nvidia nog kaarten uit gaat brengen die rond de 50 Watt verbruiken? Voor de mensen die niet de intel GPU willen gebruiken maar ook geen rekenmonster als GPU nodig hebben. Wellicht zelfs nog passief gekoeld. In het verleden had je nog de 710, 720, 730 en 740 series, maar hierna heeft nvidia nooit meer deze lage series uitgebracht.
GTX 1080;5;0.3669167160987854;Wow wat een zure reacties zeg.
GTX 1080;2;0.35714176297187805;Inderdaad zeg, mijn god. Klagen dat Tweakers afzakt naar een niveautje Nu.nl, maar de gemiddelde reageerder hier zit ook zo'n beetje op het niveau NuJij
GTX 1080;2;0.581623911857605;Begin me ook per dag meer te ergeren aan de reacties hier naar de redactie toe. Er is bar weinig respect meer en dat is jammer. Dat ze soms steken laten vallen is duidelijk maar is dat nu een reden om maar onbeleefd te worden?
GTX 1080;3;0.4658755958080292;"Soms? O, wel. Dat laat ik even in het midden. Wat ik o.a. wel mis is een uitgebreide conclusie; dit is nu nogal summier. Zoals ik niets lees in de conclusie over de SLI performance en/of het gebrek eraan. Zomaar een voorbeeld."
GTX 1080;1;0.5921130776405334;Voor een SLI performance hebben ze 2 kaarten nodig brein... En die hebben ze niet. Dus kunnen ze geen SLI performance geven. My god zeg... Ook het licht niet uitgevonden he??
GTX 1080;1;0.4303373098373413;"Sorry, maar ik zie toch echt SLI-scores van de GTX 1080. Wie is er nou blind hier Zal je ff op weg helpen: ""GeForce GTX 1080 SLI"" staat er bij."
GTX 1080;2;0.5188214182853699;Als de concurrentie consistent sneller en meer inhoudelijk is dan is het duidelijk dat je daar meer aan hebt. Dat de 10x0 nu nog zo duur zijn heeft grotendeels met het beperkte aanbod te maken. Gek genoeg zijn de 1070 OEM nog duurder dan de FE terwijl je zou verwachten dat die ong €50 goedkoper zouden zijn. Er zijn een paar prijsvechters zoals een KFA2 die de laagste prijzen hebben en waarschijnlijk andere merken ook met hun prijzen moeten gaan zakken om marktaandeel te houden. Wil je minder betalen zul je nog wel even moeten wachten totdat de prijzen naar ong. €400/€600 zakken. De enthousiasts die niet willen wachten hebben de hoofdprijs betaald. Daar hoor ik dan ook bij.
GTX 1080;1;0.40111401677131653;Ik zit niet echt meer in de gaming, maar deze kaart is wel een beest! Dikke Macboe wacht op antwoord (van AMD)!
GTX 1080;3;0.40000125765800476;Ik ben benieuwd hoe de 1070 in Star Citizen performt. Dat zou voor mij toch wel de grootste reden zijn over te gaan op een nieuwe kaart.
GTX 1080;2;0.368935227394104;?huh? Das gek? Hier staat dat de 1080 project cars draait op 80-90 fps high sett.. Op 1920x1080 Ik draai project cars op ultra met 2xmxaa op 1920x1080 op 70 fos met mijn gtx 960 (ge oced naar 1420mhz core 1485 boost) Ik had gedacht dat de 1080 veel sneller zou zijn?
GTX 1080;3;0.6621522307395935;Er is iets mis met de balkjes van jullie charts. Bv. Alien: Isolation - 3840x2160 - GTX 980 Ti en Fury X zijn even snel en toch is het balkje van de GTX 980 Ti langer. De GTX 1070 is dan weer sneller dan de GTX 980 Ti maar het balkje vrijwel even lang. Bv. Project Cars - 1920x1080 - Twee R9 kaarten en de GTX 1070 hebben ongeveer 74fps maar toch is de GTX 1070 balk net iets langer. Duidelijk is dat er iets verkeerd gaat bij het maken van de balk in relatie met de minimum FPS.
GTX 1080;5;0.5083492994308472;dan is mijn geforce 4 ti4600 toch eens aan vervanging toe denk ik, maarja doe t dan ook nog met een pentium 4 computer met agp aansluiting haha. win 8.1 draait toppie.
GTX 1080;1;0.4005744755268097;Geen test met de vive? wtf?
GTX 1080;1;0.5112941265106201;Geen Asus strix 1070 getest?
GTX 1080;1;0.3271648585796356;Ik denk AMD wacht tot volgend jaar om nvdia en intel pijn gaat doen met hun sterke producten en ook nog goedkoper dan hun en meeste mensen die nu zonder geduld dit spullen gaat kopen gaan ook de pijn voelen,AMD weet dat hun processor is snel en hun gpu, 2017 wordt spannende jaar voor AMD...
GTX 1080;1;0.3181041479110718;GTX 1080 van MSI? Hmmm....
GTX 1080;1;0.29719629883766174;"Ik lees nu net op verschillende sites dat MSI en Asus de GTX 1080 ""review"" kaarten die ze aan de persleden hebben gegeven, zelf hebben aangepast zodat ze (iets) hogere kloksnelheden hebben, waardoor ze beter uit de test komen. Bron: link, en link. Is dit te achterhalen bij jullie?"
GTX 1080;3;0.33455589413642883;"GTX 980 TI is via Amazon Deutschland al voor 450 euro te krijgen, goedkoper dan de prijs die hier in de prestatie index gehanteerd wordt. Daarbij is de GTX 1070 niet voor 500 euro te krijgen; tel er minstens 50 bij op. Dit maakt de prestatie/prijs verhouding met de GTX1070 nagenoeg gelijk. Voor mij een makkelijke keus: met de GTX 980 TI krijg je bovendien veel meer cuda's (in een lagere frequentie weliswaar). En niet onbelangrijk de Pascal architectuur moet zich nog bewijzen, daar waar de vorige generatie goed is uitontwikkeld."
GTX 1080;2;0.5437920093536377;Terwijl iedereen loopt te twisten over wat nou sneller is of wat de beste prijskwaliteitverhouding is, blijft het aanbod laag van alle nieuwe gtx kaarten en is de prijs belachelijk hoog.
GTX 1080;3;0.31197795271873474;Ik denk dat in heel wat benchmarks bij online reviews dx11 wordt gebruikt. Weet iemand of er ergens Battlefield benchmarks zijn gemaakt met Mantle (op 4K)?
GTX 1080;1;0.5646752119064331;Het gemiddelde intellect van een hoop tweakers is aan deze reacties te zien bedroevend laag. Kunnen mensen nou niet op een normale manier reageren? Een vraag stellen van ''goh, had dit misschien nog handig geweest''? Het is echt om te janken wat een ontzettend kinderachtige reacties er worden gegeven. Er is gewoon een degelijke review gegeven, waarin ik als persoon die op zoek is naar een nieuwe videokaart, wederom bevestiging krijg dat de 1070 een goede keuze is als je budget dat toe laat. Al die 980ti fans die maar lopen te blazen dat die sneller is, en beter... Nou en dat die misschien even snel is in sommige benchmarks wanneer die overgeclocked wordt. De 1070 is goedkoper. Wat wil je nou? Ik zeg: grow up!
GTX 1080;1;0.687520444393158;Eensch. Het is niet de inhoud maar vooral de vorm wat in dit draadje nogal lomp en arrogant is. Het feit dat je -denkt dat je- een punt hebt geeft je niet het recht om je manieren links te laten liggen en eigenlijk te gaan zeiken en afkraken. Daarbij snap ik ook inhoudelijk niet wat er nou zo vreselijk mis is aan deze review. Dus andere reviews hebben andere of betere info... Soi. Dan deel je die info. Punt.
GTX 1080;1;0.5003765821456909;De 1070 is helemaal niet goedkoper.
GTX 1080;1;0.4310351610183716;Dat is ie wel De prijs van een 980ti zit op ongeveer €525 / €550 gemiddeld. €550 voor de betere. De prijs van een GTX1070 zit nu op €520 gemiddeld. Dus goedkoper. Daarnaast is de consumentenadvies prijs van die kaart €430 (afgerond). Dat ze nu duurder zijn is omdat ze net uit zijn. Maar zullen snel genoeg een stukje zakken. Kortom, nu al goedkoper en over een enkele maand een paar tientjes goedkoper.
GTX 1080;1;0.4000546336174011;"Ja, meer mensen zouden moeten denken: deze review bevat informatie, ik doe daar (al dan niet) mijn voordeel mee, that's it. Maar sommige mensen reageren alsof de review een politiek stuk is dat voornamelijk om een mening draait, en waar dus persé op gereageerd moet worden om dingen recht te zetten! Het is gewoon consumenteninformatie, niet meer, niet minder, waar je (ook als je het niet met iedere zin in het artikel eens bent) je voordeel mee kunt doen om je aankoopkeuze al dan niet te beïnvloeden. ""Ja maar een andere kaart presteert op dit of dat onderdeel, of door die en die bril bekeken veel beter, en dat had hier vermeld moeten worden!"" Joh... als die informatie die jij mist nou bij jou wél in je hoofd zit, dan weeg je die toch mee bij jouw keuze? Good for you, succes zou ik zeggen! Waarom hebben mensen toch zo'n behoefte om hun eigen voorkeur zwart op wit bevestigd te zien in elke review die er over schrijft? Mensen reageren en doen daarbij alsof ze in feite in het belang van anderen reageren, zo van: die info die ik noem is nuttig en had in het artikel gemoeten zodat anderen ook kennis van die info kunnen nemen. ""Niet voor mijzelf, want ik ben mega slim en wist dit natuurlijk zelf al, daarom reageer ik ook! Wat ben ik, en mijn inbreng, toch nuttig en belangrijk..."" Terwijl het af en toe gewoon een verkapte fanboy-discussie wordt, vooral de manier waarop de discussie gevoerd wordt. Dat gaat er helemaal niet meer om medelezers te helpen, het gaat dan gewoon om je gelijk te halen."
GTX 1080;1;0.4078507721424103;"@JustTweak & RagingR2. Herken ook wel wat jullie zeggen. Op zich kunnen in de review conclusies getrokken zijn waarop wat is af te dingen. Of zijn er in een review aspecten niet meegenomen. Ik had graag de 970GTX willen zien in de grafieken, toch een algemene kaart op dit moment. Toch snap ik ook wel dat keuzes gemaakt moeten worden. En uit andere reviews is die info nog wel te destilleren. Nog altijd zijn conclusies in een review een resultaat van een interpretatie die (naar ik hoop) direct resultaat zijn van wat is gemeten. Dat een review hiermee een afwijkend geluid laat horen van andere reviews....het zij zo. Hellend vlak wordt het wel wanneer uitspraken gedaan worden over onderwerpen waarover de reviewer zelf geen eerste hand informatie heeft. Ook in een review kan dat dan nog wel gedeeld worden (als het relevant is als kader) maar het moet dan wel duidelijk zijn waar de informatie vandaan komt. Een valkuil van meetgegevens is dat ze suggereren absoluut zijn, terwijl ze slechts een facet geven van een hele complexe meetwerkelijkheid. Roepen velen ""onwaar"" neig ik eerder om te denken ""interessant"". Dat een conclusie in een review soms een bocht afsnijdt kan in de discussie door de tweakers wel worden gepareerd. Droog argumenteren is hiervoor wat mij betreft ruim voldoende om een reviewer te nuanceren. De hoge toon voegt hierin wat mij betreft niets toe. edit: typo's"
GTX 1080;1;0.5003035068511963;"Inderdaad, zolang de reviewer maar duidelijk (expliciet) maakt waaróp hij zijn conclusie baseert, en bij meetgegevens: wat hij precies gemeten heeft, dan weet je als lezer in elk geval de eerlijke informatie. Als je het niet eens bent met de conclusie op basis van de gegevens... is dat geen probleem, en eerder het bewijs dat de reviewer bewust inzage heeft gegeven in die gegevens! Je snapt ook meteen dat dat voor reviewers dus een spagaat is; geef je de achterliggende gegevens niet, of zeg je niet duidelijk genoeg wat je precies gemeten hebt, dan is het niet goed want dan is de review ondoorzichtig en waardeloos. Geef je de achterliggende gegevens wel, dan heb je altijd mensen die zeggen, je had iets anders moeten meten, of die het dan niet eens zijn met de conclusie die daar volgens jou uit voortvloeit."
GTX 1080;3;0.26912549138069153;Amen. Voor mijzelf sprekend dan toch maar de voorkeur gevend aan het geven van (grinding) veel informatie over meetopstelling, randverschijnselen, en disclaimers om als reviewer in ingedekt te zijn tegen de commentaren die je noemt. Is trouwens een commentaar op hoe is gereviewed terecht dan biedt het ook een leermoment. Al verwacht ik van diegene die reageert wel dat die de moeite heeft genomen om te kijken of het commentaar al ergens in de review wordt genoemd. Dat kan met een lange review nog best een weekje zijn. Ik realiseer mij dat het bovenstaande politiek correct klinkt. Misschien moet gewoon niet teveel gefocussed worden op de toon van de reacties. mensen de moeite nemen om commentaar te geven dan is de review in ieder geval in bepaalde zin van belang. Dat kan door de reviewer als een compliment worden uitgelegd . En er is een levendige discussie, en dat is altijd winst.
GTX 1080;1;0.3058907985687256;Ik zat hier al zo lang op te wachten. Het duurde wel heel lang voordat jullie deze review hebben geschreven!
GTX 1080;4;0.30636167526245117;Gaat tweakers ook nog een review doen van alle aftermarket GTX 1080's? Ben benieuwd of the ASUS 1080 ook écht de snelste is vergeleken met de EVGA FTW edition, of misschien is de een wel koeler of stiller dan de ander.
GTX 1080;1;0.47379475831985474;Er werd bij een vorig artikel door de redactie gesteld dat jullie nog geen review hadden omdat jullie geen kaart hadden gehad, maar dat jullie deze inmiddels wel ontvangen hadden. We moesten wel een beetje geduld hebben voor de review want aangezien jullie toch niet bij introductie konden reviewen wilden jullie het goed doen en wat extra doen dan de andere sites. Nu, ik heb deze review gelezen en ik heb niets, maar dan ook niets gelezen wat ook niet al op andere sites te lezen was, weken geleden. Dus wat hebben jullie precies extra gedaan om een betere en uitgebreidere review te bieden in en de extra tijd die jullie er voor genomen hebben? Deze reactie zal wel weer weggemod worden, maar jullie bieden weinig tot niets meer voor de tweaker. Jammer. Ik loop hier al rond sinds 2001 maar er is vrijwel niets meer wat het waard is om te komen. Admin-edit:Spelfouten en/of ander commentaar m.b.t. nieuwsposts horen thuis in Geachte Redactie. Gebruik de reacties hier om de review te bespreken, te verduidelijken, producten en resultaten te vergelijken, te speculeren, etcetera, maar de discussie over de toegevoegde waarde van deze review hoort op het forum, hier staat hij de ontopic discussie in de weg.
GTX 1080;2;0.5338374376296997;Couldn't agree more. Maar hoeveel uitgebreider kun je gaan als die van Guru3D? Die warmtecamera's gebruikt voor gedetailleerde warmtebeelden en FCAT apparatuur voor frametimes. Het is dan ook lastig om daarmee te concurreren. Tweakers heeft zich nu eenmaal meer gefocust op de normale mens. Reviews zoals deze zullen in-depth genoeg zijn voor veel mensen. Echter niet voor veel tweakers. Daarom dat ik eigenlijk ook nooit meer echt een review van tweakers nog lees. Ze gaan niet meer de diepte en dat is toch wel jammer. Tweakers was toch echt mijn go-to site voor al het tech-news en reviews. Maar is langzamerhand erg overbodig aan t worden
GTX 1080;3;0.609060525894165;Niet zo vreemd.. tweakers is het tech broertje van nu.nl geworden. Tech voor Henk en Ingrid. Sanoma gaat voor het grote advertentiegeld, kwaliteit komt op de 2e plek.
GTX 1080;3;0.43554335832595825;Wel erg grappig dat je Nu.nl in je profiel als homepage hebt ingevuld
GTX 1080;3;0.3208273649215698;Inderdaad oud grapje
GTX 1080;2;0.381702184677124;Tweakers hoort bij De Persgroep en niet bij Sanoma Ik denk dat het voor Tweakers gewoon niet mogelijk is om bestaansrecht te hebben voor een kleine groep die hard Tweakers. Willen ze blijven bestaan dan moeten ze wel voor de massa (=kassa) gaan schrijven. Gelukkig vind je op het forum vaak genoeg info voor iemand die echt iets wilt weten. Dus nieuws voor het volk, en het forum voor de Die Hards.
GTX 1080;1;0.3039318025112152;"Sorry, maar ik sluit me volledig aan bij deze comment. Maar goed, ik heb een paar jaar geleden al de hoop opgeven voor deze site om ook daadwerkelijk wat nieuws te verslaggeven. Ik kom hier nu meer voor de amusement om eerlijk te zijn en niet zo zeer om de hotste nieuwtjes over bepaalde categoriën te weten te komen. Ze hebben namelijk al vaak genoeg laten zien dat dit niet bijgebeend kan worden t.o.v. de internationale concurrentie. Hoe je het ook wend of keert, de competitie is op dit gebied internationaal en als je wilt bijbenen moet je je aanpassen. Nogmaals, ik adviseer je om dit ook te doen en bij iedere nieuwsitem het best even naar de comments te gaan. Daar zit vaak het echte nieuws, waarop de ""redactie"" weer een update gaat plaatsen bij het nieuwsitem, omdat er ""iemand"" weer te lui was om goed journalistiek werk te verrichten. Dit kritiek mag dan wel hard gevonden worden, maar het mag ook wel een paar keer gezegd worden. De richting waar deze site naartoe gaat is lachwekkend. Maar goed, als dat het doel is, dan doen jullie het goed? Edit: Lol admin-edit voor damagecontrol? Give me a break. Zijn reactie is een toegevoegde waarde aan deze review aangezien zijn reactie aantoont wat nu de echte nieuwswaarde van dit artikel is. Dit kan mensen een hoop tijd besparen alvorens ze al die pagina's gaan lezen. Zijn comment was meer informatief, dan de conclusie op dit artikel. Leuk dat alle vormen van kritiek achter een forummuur verscholen moet worden, waar er vervolgens niets mee gedaan wordt (lees: afgelopen 5 jaar). @oef, lees de eerste zin van mijn laatste alinea."
GTX 1080;1;0.4781274199485779;"Ik kom hier nu meer voor de amusement om eerlijk te zijn Daar zit vaak het echte nieuws, waarop de ""redactie"" weer een update gaat plaatsen bij het nieuwsitem, omdat er ""iemand"" weer te lui was De richting waar deze site naartoe gaat is lachwekkend. Maar goed, als dat het doel is, dan doen jullie het goed? Je post is op zo veel manieren denigrerend, ondankbaar en egocentrisch dat ik mijn afkeer van je schrijfsel niet eens onder woorden kan brengen."
GTX 1080;2;0.4298621714115143;Niet iedereen neust op andere tech-websites. Ik kijk bijvoorbeeld altijd op Tweakers voor de snelle hap op dagelijkse basis. Zo af en toe in depth vind ik prima, en als het hier niet goed genoeg is kan ik altijd nog uitwijken. Is Tweakers wat meer richting de massa gegaan? Ja, uiteraard. Maar als ze dat niet hadden gedaan was het doek waarschijnlijk al lang gevallen. We leven hier in Nederland en de markt die Tweakers aanboord met technische artikelen is niet zo gek groot. In ieder geval lang niet groot genoeg om alles te doen wat er van ze verwacht wordt. Een internationale website heeft een vele malen groter bereik, meer middelen, meer budget en betere connecties binnen de wereld. Als je Tweakers niet meer waard vindt om te komen, dan kun je ook gewoon vertrekken. Daarbij kom ik hier persoonlijk meer voor de community dan voor het nieuws want dát is wat Tweakers leuk maakt. Het nieuws is gewoon een aardige bijkomstigheid.
GTX 1070 Ti;3;0.36350563168525696;Ik heb stiekem het vermoeden dat we even moeten wachten met een mening over de prijs/prestatie verhouding. Alle kaarten zullen de komende maand (zeker richting kerst etc.) rechtgetrokken worden qua prijs. Zoiets heeft tijd nodig Daarna heb je pas een goed overzicht. Mensen die bijvoorbeeld al een tijdje hebben zitten wachten (en een oude kaart hebben) op AMD's aanbod en teleurgesteld zijn zullen denk ik wel overstappen. Waarom? Lagere prijs en bijna dezelfde prestaties als een GTX 1080.
GTX 1070 Ti;2;0.31672021746635437;Pretty much. GPU land is een zooitje atm, en anderzijds zijn er genoeg mensen die letterlijk vandaag nog dat nieuwe ding bestellen zonder verder te kijken dan hun neus lang is. Neem een webshop of Nvidia dan eens kwalijk dat de prijzen bij launch hoger liggen, we vragen er zelf om en vergeet niet dat GPU's in de regel geen super marge producten zijn voor de winkels. Oftewel, gewoon even aankijken hoe het er over een paar dagen voor staat. Zodra de rush-kopers voorzien zijn kan niet anders dan dat de prijzen ergens tussen de 1070 en 1080 gaan belanden, tig 1070 Ti's op de plank gaat een shop of nvidia niet laten gebeuren. Of we kunnen de alu-hoedje approach nemen: wellicht hebben ze helemaal niet zo veel van die dingen op de plank en gaan alleen mensen die graag teveel betalen voor een GTX 1070 Ti dat ding halen. Zijn ze toch door hun 95%-volledige-GTX1080 chips heen terwijl ze ook weer boven Vega 56 in de grafieken staan. Win-Win! Maar goed, even aankijken lijkt dan wel het logische advies, maar bij welke GPU launch was dat niet het geval?
GTX 1070 Ti;2;0.38100868463516235;Wat doen deze kaarten in termen van tflops btw? Over ik kijk helemaal scheel, maar ik zie dat zelden in specificatie overzichten terug. Of is die waarde gewoon niet relevant? MS schreeuwde van de daken dat de One X er 6 heeft. Maar wat doen de betere kaarten in PC land dan?
GTX 1070 Ti;2;0.4671116769313812;"Het is inderdaad niet relevant, omdat GPU's heel anders werken dan CPU's. Dan wordt het appels met peren vergelijken. FLOPs worden echter wel eens weergegeven, maar dan zie je in de kleine lettertjes staat dat het bijvoorbeeld om ""half precision floating point"" gaat, ofwel 16 bits. CPU's werken met double precision floating point, ofwel 64 bits. Pas als je GPU's gaat toepassen voor DSP gaan dit soort waardes tellen. Maar dan gebruik je GPUs ook op een heel andere manier."
GTX 1070 Ti;2;0.3318477272987366;Floating point performance? Dat is iets van 8 Gflops geloof ik? Of hebben we het over wat anders.
GTX 1070 Ti;5;0.4413069188594818;ben benieuwd of dat nu nog zo is. Met double floating point precision zijn ze veel beter dan nVidia, dus voor mij interessant omdat ik 3d animatie doe.
GTX 1070 Ti;5;0.30888453125953674;Weet jij welke benchmark voor jouw taken representatief zou zijn? Ik wil met alle plezier wat nieuws toevoegen.
GTX 1070 Ti;3;0.3698888123035431;Maya, Cinema 4DXL, Blender Cycles zou mooi zijn.
GTX 1070 Ti;3;0.33747243881225586;Blender graag. Het is inmiddels bekend dat amd het een stuk beter doet in cycles dan nv Had ik eigenlijk ook wel in deze review verwacht
GTX 1070 Ti;4;0.5737719535827637;Beetje jammer alleen dat er nauwelijks render engines zijn die het ondersteunen. Ik doe ook 3D animatie, en de toekomst ligt toch echt bij octane/redshift etc. Overigens echt geweldig want nu kunnen wij als kleine studio high end renderen op relatief goedkope hardware. Lees 3x 1080ti, 2x 1070, 2x 970.
GTX 1070 Ti;3;0.6936228275299072;Ja precies. Naja, enige verschil is tera en giga, maar komt op hetzelfde neer in termen van wat de kaarten kunnen.
GTX 1070 Ti;2;0.4399886131286621;Gtx 1080 ti doet er bijna 11. Maar het is niet echt relevant. Het is niet één-op-één om te zetten naar framerates.
GTX 1070 Ti;3;0.7931650280952454;Ok thx. Eerder las je er veel meer over. Microsoft zal het vooral gebruiken omdat het marketingwize wel lekker bekt en beter te communiceren dan *x frames per second bij Wicher 3 op ultra settings!* Klinkt als lekker geek materiaal, maar is dus niet zo heel boeiend meer.
GTX 1070 Ti;3;0.4600362777709961;Het klinkt inderdaad leuk voor de marketing ja, waarschijnlijk doen ze het daarvoor .
GTX 1070 Ti;2;0.4395397901535034;Kijkend naar het verleden, gaan deze kaarten echt niet zo snel goedkoper worden, zou mij niks verbazen als dit gewoon de prijzen blijven voor lange tijd. Als je naar de grafieken kijkt van bijvoorbeeld de 1080 is die in maanden amper goedkoper geworden
GTX 1070 Ti;2;0.35272860527038574;Ik snap het niet helemaal, uit jullie review van de vega's, is de 56 sneller dan de 1070 in BF1, terwijl hier de 1070 (non ti) sneller is dan de 56. in BF1 1080p ultra. reviews: AMD RX Vega: de Vega 64 en Vega 56 getest Zijn er nieuwe tests? Of waar ligt het aan?
GTX 1070 Ti;2;0.435744047164917;Dan lees je niet goed of ze hebben het aangepast.
GTX 1070 Ti;5;0.2474057376384735;Zie het ook niet meer
GTX 1070 Ti;2;0.31740710139274597;Die scores zitten toch in een database dus zullen overal gelijke score weergeven lijkt mij? (ik zie inderdaad zoals Joeri zegt niets raars)
GTX 1070 Ti;1;0.3101617395877838;Ik ook niet meer, gek.
GTX 1070 Ti;1;0.5201201438903809;Is er een speciale reden waarom prijzen van fabrikanten niet worden meegenomen? Bij eu.evga.com kun je de kaarten gewoon voor de adviesprijs krijgen. Dezelfde kaart bij mindfactory kost dan 100eu meer op het moment. Sowieso zijn de kaarten van EVGA in zowat 100% van de gevallen goedkoper bij EVGA zelf dan welke webshop dan ook.
GTX 1070 Ti;1;0.4497554302215576;"Wat bedoel je met ""Alle kaarten zullen de komende maand (zeker richting kerst etc.) rechtgetrokken worden qua prijs.""?? Dat de Nvidia GPU kaarten NOG duurder gaan worden? door een verhoogde vraag&aanbod verhouding, omwille van het kerst seizoen. Echt.. die hele mining hype, begint me toch serieus tegen te steken. Zelfde met de RAM prijzen, hoewel dit komt door de smartphone makers & databases."
GTX 1070 Ti;3;0.48446571826934814;Toch zie je veelal met kerst prijzen omlaag gaan. Ik zie het ook nog niet sterk dalen, ik denk dat winkeliers je dan liever 2 extra game codes geven dan er 100 euro af te halen.
GTX 1070 Ti;2;0.4749339818954468;Doe eens een gok De GTX 1070 en 1060 zit een redelijk prijsverschil tussen, ruimte zat om te zakken dus. Dan blijft er ruimte over voor de GTX 1070 Ti en ga zo maar door. En uiteraard worden ze niet duurder, dat zou niet bepaald slim zijn. Als je ze juist iets goedkoper maakt rond die tijd met een actie haal je redelijk wat mensen over de streep om toch zoiets te halen (zeker met een 13e maand). Overigens is geheugen inderdaad schofterig duur, laatste offerte moest ik meerdere keren bijstellen qua prijs
GTX 1070 Ti;1;0.5265358686447144;Bijna iedereen lijkt al overgestapt te zijn. Ik heb net de Steam Hardware Survey erbij gepakt, en wéér is het aandeel AMD gezakt. En hard: Nog maar 10.84% van de steam-gebruikers heeft een AMD-videokaart. En maar liefst 81.4% van de steam-gebruikers heeft een Nvidia-videokaart. Het lijkt erop dat de Chinezen die PUBG spelen via Steam allemaal een Windows 7-bak met een Intel CPU en een Nvidia GPU hebben. En vrijwel geen van hen gebruikt AMD hardware. Dat is ten minste mijn verklaring voor de shift. Maar goed, China behoort ook tot de wereld, en nog maar 10.84% aandeel voor AMD GPU's nu China wat beter vertegenwoordigt is op Steam... Niet goed AMD, niet goed.
GTX 1070 Ti;1;0.5323843955993652;Buiten dat er natuurlijk echt wel dingen af te zien zijn aan de Steam Surveys, vraag ik mij toch echt wel af of ze nou wel zo accuraat zijn. Deze maand b.v. een verdubbeling (!!!) van GTX960's. Hoezo?! De GPU die daaraan ten grondslag ligt wordt al zeker 2 jaar niet meer gemaakt? Het is neem ik aan ook geen chip die net uit een zeer groot aantal zakelijke systemen is komen rollen de afgelopen tijd? En of er nou werkelijk net zoveel X1900's nog in gebruik zijn als dat er GTX980 Ti's zijn, mwah, dat weet ik ook nog niet. Daarnaast smijten mensen ook op systemen met Intel 3000 chipsets massaal Steam, wat opzich natuurlijk niet verkeerd is, maar het vertekend natuurlijk ook vanalles en nog wat, zeker omdat die GPU vaak naast Nvidia / Radeon kaarten is gezet. De introductie van Intel's CL welke massaal 6c zijn zie je ook niet terug, 6 cores is juist omlaag gegaan in deze percentages, net als 8c. Ik geloof er weinig van dat er nu opeens meer quads zijn gesampled. Ryzen is daarnaast steevast topseller in de grote landen van de wereld, USA, Canada, Rusland, India, Indonesië, Maleisië... Misschien niet allemaal even gewichtig, maar AMD CPU's kunnen haast niet zo in de put zitten als Steam HW Surveys hier laten zien. Kijk, dat er meer GTX1060's gemaakt zijn dan VEGA & Fury chips bij elkaar is gewoon een feit. En dat VEGA nu al niet meer echt lekker verkoopt ook. En dat is dan ook denk ik de reden dat Nvidia deze kaart op de markt brengt. Ze hoeven de strijd niet aan te gaan en kunnen zo toch aantrekkelijker hun GDDR verkopen.
GTX 1070 Ti;3;0.2796899974346161;Heb je daar bronnen bij?
GTX 1070 Ti;1;0.5327409505844116;Er is een hele bak over te lezen geweest. AMD zat niet voor niets nét geen 20% in de lift. Hier zie je b.v. wat er bij de buren gebeurde, maar NewEgg & Amazon hadden ook Ryzen steevast in de top-sales zitten, vooral in Mei tot Aug. Er schijnen ook in China gigantische hoeveelheden Ryzen 3 series verkocht te zijn, en nu gaan laptops & Apple er nog een stapje bovenop zetten in het volgende kwartaal. Consensus was dat in de periode sinds introductie R5 en tot Coffee Lake launch >50% cpu verkoop van de grotere e-tailers Ryzen chips / systemen betrof. Daarnaast was er nog de ophef over het feit dat niemand de Ryzen's (CPU's only) zo goedkoop op de markt had als Rusland. Franse importeurs gingen ze zelfs daar vandaan halen om zo ermee te kunnen stunten. De 1700 was daar in Maart een goede €120 goedkoper dan in de Benelux / Duitsland / USA. Ook de R5's waren slechts 75-80% van de prijs.
GTX 1070 Ti;3;0.46324774622917175;Ryzen is zeker positief voor AMD, en de mining rage ook, maarre, is dat de beste bron die je hebt? Dat zijn wel hele specifieke cijfers (alleen bepaalde modellen, kleine tijdsrange, één duitse winkel). Kom je serieus hiermee om de cijfers van Steam in twijfel te trekken? Dit zegt meer, is impressive en steunt wel jou punt, ondanks dat je formulering nog steeds een stuk stelliger is dan de bronnen ondersteunen. Edit: overigens zijn er wel veel berichten over te vinden, maar die zijn allemaal gebaseerd op dezelfde (behoorlijk beperkte) bron.
GTX 1070 Ti;2;0.4410168528556824;"Ja, maar WCCF is nou niet bepaald een betrouwbare bron, het is een beetje roddel en achterklap. Niet dat er nooit waarheden staan hoor, maar er heeft wel degelijk rommel tussen gezeten, dus ik durf ze niet meer zo te citeren Ondanks dat geloof ik nog altijd niet in wat er in de Steam Surveys naar boven komt. Ik denk van de 3 gaming rigs, dat vooral m'n NIET gaming Indie game draaiende Linux box het meeste gesurveyed is. Jup, weer een Sandy i3 mobile met 3000 graphics in de lijst, want de Survey ziet de 530MX niet eens. M'n Ryzen + 1080Ti is nooit gesampled, m'n Intel 4770K + 1080 is nooit gesampled, m'n inmiddels weggegeven 3570K + 1080 misschien één keer en m'n verse installatie 6850K + 1080Ti is ook niet gesurveyed. Daarbij hebben AMD gebruikers de moeite genomen op de private ""RealAMD"" reddit (voornamelijk reviewers enzo) aangespoord om meer AMD de surveys in te forceren. Hoe dan ook; weer wat meer ontopic; Ik denk dat deze 1070Ti weinig tractie zal krijgen, tot deze ook in prijs gaat zakken, wellicht pas nadat de nieuwe generatie aangekondigd gaat worden. Leuk voor de oorspronkelijk waarschijnlijk 1070 kopers die pas heel laat in wilde stappen in een dergelijke kaart. Persoonlijk had ik het logischer gevonden als de OUDE 1080 met 8gbps en slechte binning in een soort 1080 SE (slechte naam I know) terecht was gekomen, naast de normale 1080 die inmiddels altijd 11gbps memory mee krijgt. Maar wie weet gaat de 1080 wel van de markt verdwijnen?"
GTX 1070 Ti;3;0.5318731665611267;Da's leuk voor AMD, en een knappe prestatie. Maar gezien de totale aantallen CPU's die geleverd worden (zelfs als we alleen naar X86 kijken) stelt het veel minder voor. AMD was verworden tot kleine speler op de CPU markt, er was dan ook sinds de Phenom X6 weinig interessants uit het rode kamp gekomen. Ryzen is met recht een goede CPU, maar het is voor AMD nog niet genoeg momenteel. Zeker omdat een piek aan verkopen te verwachten was doordat er voor AMD fans eindelijk weer eens wat te upgraden viel, en die zijn nu als eerste overgestapt, ik verwacht eerder stabilisatie van verkopen dan een grote stijging vanaf nu. Voordat er écht weer concurrentie is op X86 zullen ze toch een marktaandeel van ca. 30% moeten gaan krijgen, anders zou het toch op de langere termijn wel eens afgelopen kunnen zijn voor AMD. En misschien zelfs voor het hele X86 platform, als Windows on ARM aanslaat en de emulatie vlot genoeg is zullen Intel en AMD ook ten opzichte van Qualcomm moeten gaan concurreren.
GTX 1070 Ti;1;0.4096461534500122;Dan nog... 3 tientjes meer en je heb een 1080 met gddrx te pakken. Waarom zou je nu voor een 1070 TI met gddr gaan?
GTX 1070 Ti;3;0.4339176118373871;Dat is momenteel zo, je weet niet wat het over twee tot drie maanden doet. De eerste tijd zijn ze altijd wat duurder.
GTX 1070 Ti;3;0.5353839993476868;Ok we wachten het af
GTX 1070 Ti;2;0.4794735908508301;Ik snap je reactie overigens wel hoor, daar niet van. Met de 1xxx serie zijn de prijzen ook heel lang hetzelfde gebleven of gestegen. Uiteraard kwam dat duidelijk door een gebrek aan concurrentie.
GTX 1070 Ti;1;0.3126126229763031;Goedkoopste 1080 in de pricewatch is €520, EVGA GeForce GTX 1070 Ti SC GAMING is 479 op de webshop van EVGA zelf en EVGA's ref is 469.. is toch een behoorlijk groter verschil dan 3 tientjes..
GTX 1070 Ti;1;0.4832894802093506;Hey homme2204 ik hou me niet bezig met een paar tientjes extra ben namelijk geen mierenneu**** Overigens is de reactie al 3 dagen oud
GTX 1070 Ti;3;0.5516142249107361;Ik weet het nog niet helemaal. Er zit nog wel een kleine kloof tussen de GTX 1070 Ti en GTX 1080, in veel gevallen is dit 'pret FPS', gezien het aantal FPS al enorm gunstig is, afhankelijk van je resolutie et cetera. De prijs nog iets wat zeker gaat veranderen, maar ik verwacht echt niet dat dit de komende dagen zal zijn. Ook moet de prijs een acceptabel verschil zijn met de GTX 1080. Een prijsverschil van circa 70 tot 80 euro kan ik acceptabel noemen.
GTX 1070 Ti;2;0.4177606403827667;"Is toch allemaal niet zo heel vreemd? In de basis is dit vergelijkbaar met wat we normaal als refresh zouden krijgen; circa 1.5 jaar later een iets snellere kaart meestal een procent of 15%. Nvidia verlaagt tegelijkertijd de adviesprijs van de 'gewone' GTX 1070. De chips die beter gelukt zijn kunnen nu worden verkocht zonder ze opzettelijk te 'nerfen' in de vorm van de 1070 Ti. Beter voor Nvidia, die compenseren de lagere prijs van de GTX 1070. Ook beter voor de gamer, die niet een chip krijgt die het eigenlijk veel sneller zou kunnen doen maar in de fabriek is gesaboteerd OF je betaalt gewoon wat minder voor de 1070. Voor de gamer dus meer keuze en een goedkopere instap in high-end. Iedereen wordt hier dus beter van. Ondertussen lijkt de gekte ook weer uit de markt. De EVGA FTW die ik voor 465 euro kocht in de zomer van 2016 is nu weer voor 482 leverbaar. Zal nog even duren voor de aangekondigde prijsverlaging doorwerkt en deze nieuwe Ti zijn 'normale' prijs krijgt. Daarna is het allemaal ineens erg logisch . Overigens is dit niet de eerste refresh van Nvidia dit jaar. We hebben rond april al de snellere 1080 gekregen in de vorm van GTX 1080 11 GBPS. Deze was een procent of 10 sneller dan de originele kaart. Ik zie het nog wel gebeuren dat Nvidia stopt met de productie van de oude 'trage' GTX 1080. De line-up wordt dan de GTX 1070 (afgeprijsd), GTX 1070 Ti (refresh) en GTX 1080 11 Gbps (refresh). Nu de 1070 Ti tegenover de 1080 FE zetten is daarom niet zo handig. Immers custom kaarten waren al flink sneller en de 11 Gbps gaat daar nog dik overheen. Het gat is daarmee groter dan deze review doet vermoeden. Maar goed, het had voor iedereen makkelijker geweest als Nvidia gewoon even een rebranding had gedaan naar de GTX 1170 en de GTX 1180 en de oude 1070 noem je dan de 1160 Ti."
GTX 1070 Ti;2;0.4032256007194519;"Mja, ik mis ook de ""nieuwe"" GTX1080 een beetje in deze review. Net als hoeveel het nu eigenlijk scheelt als je de 1070, 1070 Ti, 1080 8gpbs & 1080 10-11gpbs versie overclockt. Ja tuurlijk, dat soort dingen zijn gewoon te vinden, maar niet in één systeem in een gecontroleerde setting zoals deze review. Ik vraag mij nog af of een 1070 FE met water setje op 2100mhz met ruime mem-OC niet een kwaliteits 1070 Ti aftermarket kaart eruit trekt, zeker gezien de 1070 FE's juist wel mooie gecherrypickte GPU's en mem setjes zijn. Goed, stock met normale boost parameters komt de 1070Ti nooit echt boven de 1080FE uit. Je 1080 vervangen voor deze nieuwe kaart doe je dus nooit. Maar kan je je oude 1070 non-Ti ook niet al zo ver krijgen? Ik weet nog wel dat het verschil bij top OC 1070's ook al 95% van een ""stock-boost3.0"" 1080 kon bereiken met bijna dezelfde TDP tussen beide kaarten van ~185W op dat punt."
GTX 1070 Ti;1;0.6988127827644348;Call me stupid, maar ik snap het nut van deze kaart niet. Kost evenveel als een 1080 en levert ongeveer dezelfde prestaties.
GTX 1070 Ti;2;0.4416293799877167;Er gaat even overheen voordat die prijs op het juiste niveau stabiliseerd. Dat zie je praktisch altijd bij nieuwe producten. Het is alleen niet zo gebruikelijk dat een losstaand product in een bestaande productlijn wordt gereleased, daarom valt het nu op. Uiteindelijk kan je de adviesprijs (of de winkelprijs zo kort na aankondiging) van de 1070ti niet vergelijken met wat een 1080 nu in de winkel kost. Dit wetende is de eerste alinea van de conclusie ook een beetje vreemd, en staat de echte conclusie in de laatste alinea:
GTX 1070 Ti;3;0.5188023447990417;Dat klopt, dan nog is het gat in prijs tussen de 1070 en 1080 niet heel groot, en de prestatie amper verschillend.
GTX 1070 Ti;3;0.4923262298107147;Maw, als de prijs straks normaliseert wordt dit waarschijnlijk een heel aantrekkelijke kaart. Enige is dat ie dat misschien ook wordt voor de miners, waardoor dat normaliseren wel eens wat langer kan duren.
GTX 1070 Ti;3;0.3236573040485382;Precies wat ik dacht (en verwacht had), het is een 1080 die net iets trager is. Zou dit een marketing truc zijn omdat Ti wat beter klinkt?, maar niet zo duur lijkt als een 1080?. Had de 1080 gewoon wat goedkoper gemaakt denk ik dan......
GTX 1070 Ti;2;0.42677968740463257;TI klinkt juist lame... Ultra klinkt veel luxer imo.. TI vind ik zon naampje voor een low end kaartje
GTX 1070 Ti;2;0.4944405257701874;Ja de Geforce GTX 1070 Ti kost net zo veel en is gemiddeld maar 6 fps langzamer, snap niet de nut van de Geforce GTX 1070 Ti, waarom zou je een Geforce GTX 1070 Ti kopen als een Geforce GTX 1080 net zo veel kost iets sneller is en net zo veel stroom gebruikt.
GTX 1070 Ti;1;0.4055764079093933;Gewoon overclocken en hij is sneller als een 1080 of niet?
GTX 1070 Ti;3;0.2711787819862366;Waarom zou je dat doen, als je een Geforce GTX 1080 voor de zelfde prijs kan kopen, en ook met gemak kan OC?
GTX 1070 Ti;2;0.3247974216938019;Ja, dat staat ook in het artikel. Naar mijn idee zullen ze de prijs van de 1070 naar beneden moeten smijten want in het huidige aanbod is er geen ruimte voor een TI.
GTX 1070 Ti;1;0.4753018915653229;Nee, dat klopt niet: de 1080 is goedkoper dan de 1070ti.
GTX 1070 Ti;2;0.45305824279785156;Bedrijven zoals asus msi enz mogen van nvidia niet de base clock, overclocken(factory overclock) daarnaast mogen ze ook geen gpu s meer binnen als de prestaties wat minder zijn.... Ik weet niet wat nvidia doet met de 1070 ti maar ik vind het raar.
GTX 1070 Ti;1;0.45273932814598083;Zocht naar dit. Vind het een naaistreek tegenover 3e partijen
GTX 1070 Ti;1;0.8414918780326843;Absoluut vond het eigenlijk al raar dat er niks van vermeld werdt in dit artikel.
GTX 1070 Ti;1;0.6222979426383972;Inderdaad, k snap ook niet waarom Nvidia deze kaart heeft gereleased
GTX 1070 Ti;5;0.3740883469581604;mss om geen gaten te hebben in de prijsklassen tussen de 1070 en de 1080
GTX 1070 Ti;3;0.2408483922481537;Oh dat kan ja
GTX 1070 Ti;2;0.4594864547252655;Qua prijs kan Nvidia rustig wachten op wat de Vega56 gaat doen. Gaat de Vega56 zakken dan zullen ze de 1070ti ook wel laten zakken. Ik vermoed dat Nvidia de prijs rond die van de Vega56 wil houden en daarmee maken ze het AMD erg moeilijk. Het is eigenlijk ongelooflijk dat ze die ruimte hebben maar soit. AMD is nu aan zet en ze hebben een videokaart die iets onder presteert aan de 1070ti maar wel meer stroom verbruikt en meer lawaai maakt, voor iets goedkoper dan de 1070ti (tot nu toe, ik verwacht dat Nvidia de prijzen snel op dezelfde hoogte brengt). Ik vraag mij af of AMD een prijsverlaging net zo goed kan opvangen financieel gezien als dat Nvidia dat kan
GTX 1070 Ti;1;0.5100536942481995;Vega is makkelijk goedkoper te krijgen hoor. Ik heb mijn Vega 56 voor 415 Euro via Duitsland (is nu 470) en de Vega 64 is daar nu 520 Euro. En mijn Vega 56 is trouwens sneller dan welke 1070ti dan ook Watergekoeld, Bios van Vega 64 en overgeklokt. Geen enkele 1070ti die de mijne verslaat aangezien die van mij net zo snel is als een 1080 of Vega 64. Met ander woorden: Nvidia had VEEL beter de 1070 en 1080 in prijs moeten laten zakken. De 1070 goedkoper dan de 56 en de 1080 goedkoper dan de 64. Totaal overbodig dus deze 1070ti. De snelste kaart (1080ti) blijft toch in hun handen.....Daar komt Vega niet aan.
GTX 1070 Ti;1;0.5998364090919495;Tja, waarom geen GTX 1060 ti? Of verkopen ze dan geen 1070 kaarten meer?
GTX 1070 Ti;5;0.5430187582969666;Precies GTX 1060Ti was de way to go geweest!
GTX 1070 Ti;2;0.3825693130493164;Omdat de GTX 1060 GP106 gebaseerd is (volledige chip actief). Een 1060 Ti zou dan een super-oc versie moeten zijn, maar we weten dat Pascals niet veel verder komen dan de 2000-2100 waar oc 1060's al tegenaan hikken, of het zou een nog verder generfde GP104 (GTX1070/1070Ti/1080) moeten zijn, maar waarom zouden ze dat doen, die zijn allemaal capabel genoeg om in GTX 1070 en hoger kaarten te zitten. Of ze stapelen zwaar afgekeurde 1080's op totdat ze er een 1060 Ti van kunnen bakken, maar ik heb daar een hard hoofd in, of ze zouden een hele nieuwe chip moeten ontwikkelen, maar dat is ook dom ruim een jaar in Pascal-lifecycle.
GTX 1070 Ti;3;0.5383812189102173;Daar zit ook wat in. Ik laat momenteel de upgrades lopen. Geheugen, SSD & videokaarten. Ik vind de prijzen te hoog. Mijn kaarten zijn nu wel oud aan het worden, maar op Full HD doen ze het nog steeds uitstekend. Het liefste wil ik zo zuinig mogelijk, dus als ik iets doe dan wordt het een GTX1050 ti of een 1060 3Gb voor mijn tweede PC. Ook om iets te dioen voor het mileu door minder stroom te verbruiken en toch performance te houden.
GTX 1070 Ti;2;0.32660937309265137;De website die je linkt heeft één RX 56 en die kost 479 euro. Duitsland een lager btw tarief dus dat is appels met peren vergelijken. De 1070 ti is daar nu al te krijgen voor 469 euro. Daarnaast zijn de Vega's op die website blower kaarten en dat is geen gehoor. Daar moet je de kosten die je maakt aan het 'upgrade' naar water koeling er ook nog even bij optellen. De adviesprijs van de GTX 1070 is overigens al verlaagd. De verklaring van deze 1070 Ti zal je niet bij AMD vinden maar eerder in de hoge yields. De 1070 is echt populair en het is natuurlijk zonde als er steeds units worden uitgeschakeld die blijkbaar perfect functioneren.
GTX 1070 Ti;2;0.48668771982192993;Onzin van dat appels met peren vergelijken. De prijs die ik ervoor betaal om hem hier thuis te krijgen is wat telt. Waar je hem vandaan hebt is totaal niet van belang.
GTX 1070 Ti;1;0.5373599529266357;"ik wacht nog steeds op een VEGA56 ""custom"" kaart..... terwijl nvidia 1070ti direct tekoop is."
GTX 1070 Ti;1;0.6150692701339722;€399 is wat ze bij AMD beloofd hadden toch? En rond €470 kost de Vega 56 nu ook in nl.
GTX 1070 Ti;3;0.5407664179801941;Viel me wel op dat vega 64 water cooled in battlefield 1 op 4k resolutie even goed presteerd als de gtx 1080ti. gebeurd niet veel. En total war warhammer is een beetje in de war volgensmij
GTX 1070 Ti;3;0.280386358499527;Ja dat viel me ook al op. Alle fps zijn gelijk aan die uit de Vega review behalve GTA V en Total war:Warhammer. In de Vega review gaat de 1070 zelfs 91.7fps en de 1080 72.2fps in warhammer Bij GTA V zijn de Vega's gelijk gebleven, maar is Nvidia nog harder gaan lopen. Zou wel willen weten waar die verschillen vandaan komen.
GTX 1070 Ti;4;0.4422779381275177;Ik ben blij met de komst van de 1070Ti. Hierdoor zal de 1070 zakken in prijs en kan ik upgraden van mijn 1060 die het op 1440p niet altijd goed doet
GTX 1070 Ti;3;0.45738837122917175;Jah het is goed voor de low end systemen, maar de GTX 1070 & 1080 eigenaren maken een klap... Had bovendien een GTX1060Ti beter gevonden met een prijs drop van €50 op de GTX 1070 en GTX 1080.
GTX 1070 Ti;1;0.339503675699234;Waarom heb je uberhaupt de 1060 gekocht als je op 1440p speelt? lijkt me kwestie van beter research doen voor je iets aanschaft.. lijkt er op dat je nu blind de 1070 wil kopen, maar voldoet die wel op 1440p op de games die jij speelt? Geen aanval naar jouw, maar meer een advies
GTX 1070 Ti;1;0.35424965620040894;Omdat ik eerst een 1060 had en dan een 1440p scherm. De kaart doet het goed op 1440p buiten de nieuwste games niet
GTX 1070 Ti;2;0.269112765789032;"Nvidia zag marktruimte om deze 1070Ti te plaatsen door de matige ontvangst van de nieuwe AMD kaarten. Er is geen haast om Volta generatie voor consumenten uit te brengen, dus kon men nog enkele maanden verder met het verkopen van Pascal generatie. Op zich niets mis mee, maar deze 1070Ti voelt dan ook terecht als een vreemde ""opvuller"". Het zijn waarschijnlijk onderpresterende 1080-chips met een nieuwe sticker op."
GTX 1070 Ti;1;0.43036362528800964;wat heb je aan VOLTA??? zo lang er geen 144hz 4k schermen zijn.. Volta kan wel 2x sneller zijn dan 1080ti. Op dit moment zeker geen last van een te trage 1080ti hier, Zou niet weten waarom ik zou upgraden naar volta.... daarnaast is de roadmap sowieso 2018 en vega heeft er niet aan veranderd denk ik.. 1440p 240hz gaat geniaal woirden voor quake etc
GTX 1070 Ti;3;0.3808976709842682;Laten we inderdaad maar geen snellere opvolger uitbrengen, want wat heb je daar aan, jij bent immers tevreden met je huidige kaart die anno nu snel genoeg is voor jou en je setup...
GTX 1070 Ti;1;0.3960111439228058;Nvidia heeft al aangegeven pas in 2018 met volta te komen. ze melken eerst de pascal helemaal uit. ze zijn al veel verder dan volta... zelfs 1-2 generaties al
GTX 1070 Ti;3;0.27933722734451294;4K schermen met 144Hz worden in 2018 al verwacht, dus een videokaart die dit kan hendelen zou wel mooi zijn. Ik zit zelf ook op een opvolger van de GTX 1080 Ti te wachten om in de komende jaren nog goed in 4K te kunnen gamen rond de 60fps. Bron:
GTX 1070 Ti;2;0.5074010491371155;Ze hebben al de technologie om 4k op 200 FPS te draaien maar houden dit achter tot de markt verzadigd is met 2k 144hz en 4k 60....etcccc 2018 en later ja.. had zelf ook liever al een 144hz 4k scherm met complete volta setup staan.. helaas maar 2k 144hz en 7700k gehaald met 1080ti .. jammer.. wilde wel wat snellers hebben met meer reso
GTX 1070 Ti;1;0.6257636547088623;Volta kost Nvidia 1000 euro per kaart om te maken? Wil jij €1200 voor 1 gpu betalen, als ze uitkomen. Want zo hoog zal de vraag/aanbod scheefgetrokken prijs zijn. Mark my words. bron: Heel misschien eind 2018 een release van de founder edition.
GTX 1070 Ti;2;0.5235799551010132;800 900 1200 als de performance in verhouding goed is... Titan is bagger voor de prijs weinig performance voor prijs. Als ik een nieuw systeem koop kijk ik niet naar prijs per onderdeel maar houd ik liever aan een budget vast
GTX 1070 Ti;2;0.39584484696388245;Het is inderdaad op het moment van schrijven geen interessante videokaart. Zeker na het nieuws dat amazon.de ideal betalingen toe gaat staan, wat het wel heel gemakkelijk maakt een 1080 te bestellen bij onze oosterburen. Daar heb je ze al voor rond de 500 euro en zou de keuze voor een 1070 ti erg vreemd zijn.
GTX 1070 Ti;3;0.5085630416870117;"Misschien is het een idee om in de toekomst ook VR prestaties mee te nemen. De meeste kaarten komen toch wel op een niveau dat VR tot de mogelijkheden gaat behoren. Misschien is er een manier om dit ook in de revieuws mee te gaan nemen. Je krijgt overigens met deze ""tussenkaarten"" wel een verzadiging van de ranges op nvidia gebied. De MSI gaming heeft in bepaalde gevallen ook nog een onderverdeling met 2 a 3 kaarten."
GTX 1070 Ti;2;0.4193623661994934;Misschien was het om Vega 56 de loef af te steken, misschien om afgekeurde GTX 1080-chips niet meteen te hoeven downgraden tot een GTX 1070. Dit lijkt me de meest logische reden.
GTX 1070 Ti;3;0.2746650278568268;Moet je voor de gein eens proberen om Rise of the Tomb Raider op ultra te draaien (dus ook met SSAA x2 of x4)
GTX 1070 Ti;3;0.3264405131340027;Waarom ssaa ? Gewoon fxaa gebruiken en je ziet haast geen verschil maar in je fps wel.
GTX 1070 Ti;3;0.5756517648696899;"Ik zie het verschil wel degelijk (zeer duidelijk) op 27"" 1440P. Voor mij maakt het ook niet zo gek veel uit, mijn 1080Ti red het net aan om met SSAA x2 op 60fps te draaien."
GTX 1070 Ti;2;0.5180655121803284;Ze vergelijken de Vega 56 constant met de FE edities van de 1070 op stock clocks. Maar bijna alle 1070s die nu op de markt zijn zijn een fors hoger gelocked en boosten al rond de 2000mhz uit de doos. Dat zorgt er al voor dat deze 1070s erg dichtbij de 1070TI presteren en dat voor minder geld en is het alsnog een goede tegenhanger van de Vega 56.
GTX 1070 Ti;2;0.46601706743240356;Vorig jaar rond deze tijd heb ik een nieuwe desktop aangeschaft, en die bevat een GTX 1070. The Witcher 3 (het enige spel dat ik niet fatsoenlijk meer kon spelen met mijn oude GTX 560 Ti...) loopt op 1920x1200 met iets van 85-95 FPS, en het ene andere spel dat ik nog overweeg (Divinity: Original Sin II) draait hier ook prima op. Tel daarbij op dat ik spellen op 1920x1200 speel, mijn monitor waarschijnlijk nog jaren niet ga vervangen (high-end Eizo), ik altijdj alles met vSync op 60 FPS cap, mijn kaart vanuit de fabriek is overgeklokt (met een boost tot tegen de 1800 MHz ipv de standaard 1680), en ik zelden nieuwe spellen speel, en ik zie mezelf niet echt upgraden de komende paar jaar. Eigenlijk verwacht ik dat de GTX 1050, 1060, 1070, en 1080 uiteindelijk allemaal een Ti-variant zullen hebben, waarbij de 1050 Ti net onder 1060 zit, de 1060 Ti net onder de 1070, en de 1070 net onder de 1080. Als dat rond is, zullen de niet-Ti kaarten gewoon uit de handel gaan.
GTX 1070 Ti;2;0.49226096272468567;Laat de xx80 ti dan blijven want die maakt juist wel en groot verschil in FPS. En de Titan prijs gaat nergens over.
GTX 1070 Ti;1;0.43550994992256165;of je koopt gewoon de kaart direct van nvidia voor de advies prijs dat 100,- lager ligt dan de gemiddelde 1080 prijs.
GTX 1070 Ti;5;0.2954469621181488;Ik blijf het opvallend vinden dat Nvidia zo laat in de levenscyclus van deze generatie nog een nieuwe kaart uitbrengt (1,5 jaar sinds release). Ik denk dat we, ook gezien het feit dat AMD momenteel niet kan concurreren in het hoge segment, nog een flink tijdje zullen moeten wachten op de nieuwe generatie.
GTX 1070 Ti;1;0.5329258441925049;Waarom de ondertitel van de review baseren op zoiets variabels als de prijs, iedereen weet dat die prijs nu even kunstmatig hoog is en in de pas gaat lopen over enige tijd...
GTX 1070 Ti;5;0.715351939201355;Enige tijd... juist die tijd is key voor elektronica. Prachtig spul wat je een paar jaar geleden graag in je computer wilde hebben waarbij je er nu niet over peinst om het aan te schaffen voor 1/4 van de prijs van toen omdat er over 2 maanden iets beter uitkomt. Dat is immer met elektronica, dus de tijd meenemen in deze recensie is erg belangrijk.
GTX 1070 Ti;1;0.8276582956314087;Je kan morgen dood zijn of over 4 maanden, koop die handel gewoon... Een buurman hier.. altijd gezond... ineens kanker en 3 weken later dood. Een kerel die bij mij op school zat .. 30 jaar ineens hartaanval dood Life can be short en mensen moeilijk doen over prijzen ... en wachten en wachten etc etc iedereen was aan het wachten op VEGA.. en nu wat weer??? tot de geheugen prijzen 4 tientjes zakken?
GTX 1070 Ti;1;0.32390284538269043;"Een review is toch ook een momentopname van de situatie zoals deze nu is? Bovendien staat dit in de conclusie ook duidelijk aangegeven: ""Voorlopig zouden we dus even wachten met de koop van een 1070 Ti, ..."""
GTX 1070 Ti;3;0.2711066007614136;Enige tijd duurt nu onderhand al best lang. Op de consumenten na vindt iedereen die hoge prijzen prima volgens mij.
GTX 1070 Ti;1;0.7401880621910095;Ik heb een ASUS ROG GTX 1080 gekocht voor €540 3 maand terug en voel me nu best fucked... Ik snap echt niet dat we een GTX 1070Ti hebben gekregen, een GTX 1060Ti was veel beter geweest voor de markt!
GTX 1070 Ti;2;0.4105682969093323;hoezo voel je je nu fucked 1070ti is maar ietsje goedkoper maar ook langzamer, dus waarom je je nu ficked voeld, kan je betee nooit iets nieuws kopen jij.
GTX 1070 Ti;1;0.5596931576728821;Ik koop altijd zo mijn producten dat ik ze voor evenveel of meer kan verkopen. Mijn eerste gpu was een GTX 970 gekocht voor €210 na 9mnd gebruik voor €190 verkocht. Toen een gtx 1070 gekocht voor €440 en 6mnd later verkocht voor €450. Nu dan een GTX1080 gekocht voor €540 en die moet ik dan over een jaar verkopen voor €400 ofzo. Mijn eerste grote verlies op een pc onderdeel dus. Ik ben normaal heel voorzichtig met waardebehoud van de onderdelen die ik koop. Dus vandaar dat dit even extra zeer doet.
GTX 1070 Ti;2;0.3415330648422241;Het is normaal dat je minder voor je hardware terug krijgt dan wat je ervoor hebt betaald. Dat je met je laatste 2 gpu's weinig verlies hebt gemaakt, komt meer door de verneukte gpu markt nu, ik heb ook een jaar geleden een 480 gekocht voor 200 euro, en pas verkocht voor 300 euro, maar dat is gewoon geluk hebben. Koop je je kast, voeding, cpu, moederbord ook allemaal dat je ze weer voor evenveel of meer kan verkopen? Lijkt me lastig.
GTX 1070 Ti;2;0.41811904311180115;Ik switch op de 6mnd van systeem omdat ik dat leuk vind. Bepaalde onderdelen bewaar ik wel altijd zoals ssd's en ram etc. Maar sinds een GTX1080 gewoon zo snel is zie ik mijzelf deze niet weg doen voor het komende jaar wat betekend dat ik dus achterblijf met de waardevermindering.
GTX 1070 Ti;2;0.38708043098449707;Ja begrijpelijk, hoe meer high end het product hoe groter de afschrijving vaak. Ligt er ook weer aan hoeveel sneller de nieuwe gen gpu's gaat zijn in dezelfde prijsklasses als nu. Als de 1180 straks bijvoorbeeld 35% sneller is dan de 1080, dan heb je al instant een grote afschrijving natuurlijk, net als de 980ti toen de 1080 uitkwam. Ik had een strix 980ti gekocht voor 310 euro tweedehands, en degene waarvan ik hem kocht had er 900 euro voor betaald.
GTX 1070 Ti;1;0.5695850253105164;Van €900 naar €310, dat is nog erger dan de afschrijving op je nieuwe auto! Ik let hier meestal goed op, maar die 1070Ti zag ik niet aankomen tbh.
GTX 1070 Ti;2;0.48880040645599365;Dat van die 970 en 1070 heb je netjes gedaan dan. Ikzelf heb voor 570 een 1080 gekocht, als ik die voor 400 zou kunnen verkopen volgend jaar zou ik tevreden zijn. Mijn idee is dat computer onderdelen morgen alweer ouderwets zijn en de waarde hierdoor heel erg snel zakt. Kortom, afschrijving is jammer genoeg aan de orde
GTX 1070 Ti;5;0.5563919544219971;Je hoeft je niet FUCKED te voelen hoor. Een 1080 voor 540 euro is erg netjes. De 1080 heeft nog net wat meer Cuda Cores en GDDR5X. Je doet het wat mij betreft erg goed.
GTX 1070 Ti;2;0.45456963777542114;Haha thx, gaat meer om het verlies op de kaart. Ben niet gewent om een groot verlies te lijden op producten.
GTX 1070 Ti;5;0.4395204484462738;leiden=lijden
GTX 1070 Ti;2;0.35157039761543274;Hij bedoelt dat tie veel verlies had in Leiden op bepaalde producten
GTX 1070 Ti;1;0.45576056838035583;Ik zie niks hoor
GTX 1070 Ti;1;0.7760215997695923;540 is niets man ..... voorheen had je altijd rotte appels voor dat geld high-end of niet, hitte problemen of andere dingen zoals slecht overklokbaar, slechte pwms.. of te lang op 28nm. Zoveel beter is een 1080ti geeneens in bijvoorbeeld 2k reso
GTX 1070 Ti;5;0.5499597191810608;Dit is geen rotte appel, het is zelfs een Asus ROG STRIX-GTX1080-A8G-GAMING met nog 1,5jr garantie en alles er op en er aan. Vond het destijds ook een prima deal!
GTX 1070 Ti;3;0.2929365038871765;Niche product en dan nog gelimiteerd qua overklokken ook. Gewoon voor de 1080 gaan dus mensen.
GTX 1070 Ti;1;0.5017418265342712;Jezelf beter informeren is beter voordat je dom begint te lullen (niet verkeert bedoeld) De 1070ti kun je zelf zover overklokken als je maar wil......vanuit de fabriek mogen ze het alleen niet. Blijft een feit dat dit een overbodige kaart is. 1070 voor 350€ en de 1080 voor 450€ bijvoorbeeld was een betere stap geweest.
GTX 1070 Ti;1;0.2534717917442322;Zag net bij JaysTwoCents dat EVGA zelfs al een tooltje heeft uitgebracht die de overclock voor je doet.
GTX 1070 Ti;1;0.49657025933265686;Maar niet voor de GTX 1070 en GTX 1080 eigenaren haha. Maken we even €100 verlies op onze gpu'tjes...
GTX 1070 Ti;3;0.28446730971336365;Ja want gpu's moet je kopen als investering.
GTX 1070 Ti;1;0.5097767114639282;Zeg ook niet als investering maar om er groot verlies op te draaien is ook niet leuk!
GTX 1070 Ti;1;0.3707118034362793;Inderdaad. Het hele feit dat ze vanuit de fabriek niet overgeclocked geleverd mogen worden zegt al genoeg.
GTX 1070 Ti;4;0.4309159517288208;Gaap, het wachten blijft op Volta volgend jaar, dan wordt het pas interessant.
GTX 1070 Ti;3;0.3188590109348297;Kan je lang wachten, Nvidia is tot nu toe niet van plan daar Gaming kaarten van te maken, maar gewoon bij de GTX10xx serie te blijven. Waarschijnlijk wordt dat ook weer hun volgende generatie kaarten alleen dan weer op een kleiner procedé en nog meer geoptimaliseerd en hogere clocks
GTX 1070 Ti;2;0.2714669406414032;Lang wachten, mwa, zoals ik al zei, volgend jaar. Want natuurlijk gaan ze daar wel ooit gaming kaarten van maken. Lijkt me nogal wiedes dat je als Nvidia zijnde een veelbelovende nieuwe architectuur niet louter ontwikkelt voor pro/industriële doeleinden (Quadro), maar dat er, zoals elke keer nog, ook gewoon een gaming variant van komt.
GTX 1070 Ti;2;0.3830074369907379;Waarom niet? de architectuur is gemaakt voor Pro doeleinden, en dus als gaming kaart veel minder efficiënt. Plus dat in de Pro markt de marges veel hoger zijn en daar veel meer te verdienen is, daar komt ook nog bij dat ze makkelijk door kunnen gaan met hun huidige generatie game kaarten als ze die verder optimaliseren en verkleinen
GTX 1070 Ti;1;0.6258959770202637;Waarom niet, omdat ze dat nog nooit gedaan hebben. Van elke nieuwe architectuur is een gaming variant gekomen, maar als jij graag wil geloven dat ze dat bij Volta niet gaan doen, ga je gang hoor.
GTX 1070 Ti;1;0.477817565202713;"""uit het verleden behaalde resultaten zijn geen garantie voor de toekomst"" uiteindelijk gaat het allemaal om geld, als ze meer kunnen verdienen met Volta als professionele kaart en de huidige generatie als gaming kaart, zullen ze dat gewoon doen en nVida heeft wel degelijk professionele kaarten die niet zijn uitgekomen als Gaming kaart de Quadro GP100 bijvoorbeeld"
GTX 1070 Ti;1;0.7399972677230835;"Dit slaat toch echt helemaal nergens op.... GTX 1080 is prima geprijsd rond 530 euro. Waarom? toen de dollar koers zo laag was had je ;;vroegah;; voor 400-500 euro een 480 of gtx 580... maar die kaarten zogen bigtime en je had er alsnog 2 nodig in SLI voor battlefield in 1080p voor 100 fps.. ZZZZzz... poef 900 euro weg 530 euro is een prima prijs voor gtx 1080 (is toch HIGH end als een malle en goede fps!!!) en een 1070ti is bullshit. 1070ti is shit vanwege te laag bandwith voor de verhouding! je bent toch geen zwerver, koop die 1080 of koop gewoon helemaal niets en spaar door.... en dan is de 1070ti nog duurder ook dan 1080 xd"
GTX 1070 Ti;1;0.5766875743865967;Wat een onzin verkondig je. De 680 was uitstekend voor zjin tijd en de prijs was gerechtvaardigd. De prijzen zijn hoger geworden dankzij miners en het nieuwe beleid van Nvidia, Referentie kaarten waren goedkoper en daar klaagden de kaart fabriekanten over die met hun aftermarket koelers kwamen en ermee moesten concurreren. Om dat op te lossen zijn de referentie kaarten omgedoopt tot founders edition kaarten met een mooiere koeler en een hoger bedrag. Alleen gingen andere kaart fabriekanten hun kaarten ook nog eens hoger prijzen. Met een enkele 1080 doe je ook geen 100fps gemiddeld in 1440p wat nu de huidige fhd is en 4K is nog een stap te ver. En waarom is iemand meteen een zwerver als die geen meer dan 400 euro voor een kaart over heeft?
GTX 1070 Ti;1;0.47478097677230835;Is er eigenlijk iets mis met die founders edition kaarten? Ik zit er namelijk over te denken deze te kopen via de website van Nvidea (€469,00) want voor een versie van een andere kaartenfarikant betaal je gelijk 100 euro meer (allemaal rond de 579 op Azerty). Is dat een goede keuze?
GTX 1070 Ti;3;0.48843470215797424;De koelprestaties zijn iets minder en geen standaard overclock. Wel naar mijn mening de mooiste koeler.
GTX 1070 Ti;5;0.7393865585327148;Geweldige kaart, ik denk dat iedereen hier op zat te wachten, wat een vernieuwing!
GTX 1070 Ti;4;0.35757946968078613;Makkelijk cashen met minimale moeite, aandeelhouders zijn weer tevreden. En ondertussen betaalt iedereen nog steeds veel te veel voor een grafisch kaartje
GTX 1070 Ti;2;0.3884581923484802;Ik lees dit bericht over de videokaart alleen omdat ik graag een redelijk betaalbare computerset wil aanschaffen die:geschikt is voor Virtual Realityhigh-end (video=GTX1070 of hoger en GPU is i7 of hoger en vergelijkbaar)draagbaar isdraadloos iseyetracking heeftOp dit moment voldoet geen enkele set aan al deze eisen. Al verwacht ik dat in de komende tijd (maanden) er verandering in prijs zal zijn. Op dit moment zit de draadloos en eyetracking er nog niet in. Het is echter wel handig om alvast een prijsbereik voor de VR-rig van het komende jaar in te schatten. Mijn uitgangspunt is: HTC Vive met controllers = 699,00 euro 15,6 inch laptop van MSI = 1899,00 euro En dat is al zo'n 2600,00 euro. Ik hoop dat als de videokaarten qua capaciteiten beter worden en de high-end VR-sets ook iets goedkoper en beter worden dat er komend voorjaar met een totaalprijs van 2000,00 euro een mooie VR-set beschikbaar komt. En dan stap ik privé ook in de VR.
GTX 1070 Ti;1;0.4942834973335266;"""We hebben vier GTX 1070 Ti-kaarten getest en zijn tot de conclusie gekomen dat de nieuwe gpu een rare videokaart oplevert. We vragen ons dan ook af waarom Nvidia deze gpu gemaakt heeft. Misschien hebben de Chinesen zich vergist in de gtx 1070, en zei de baas gtx 1060 ti"
GTX 1070 Ti;2;0.462228000164032;Heb het idee dat NVIDIA Intel een beetje achterna gaat met al die GPU's, de ene overklokbaar, de andere niet, de ene net wat hoger geklokt, die weer wat lager, verschillende geheugensnelheden en minimaal prijs verschil. Moet zeggen dat ik niet meer zo in de videokaarten zit, maar vind het er als consument nou allemaal niet veel duidelijker op worden. Een aantal jaar geleden had je gewoon een budget-kaart, mid-end kaart en een high-end kaart. Natuurlijk had je daar ook verschil in geheugentype en kloksnelheden, maar dat was dan wel dezelfde chipset.
GTX 1070 Ti;1;0.497831255197525;Ze zijn allemaal overclockbaar hoor zo ook deze 1070ti. Alleen mogen board partners deze niet in de fabriek al overclocken omdat die dan tegen de prestaties van een 1080gtx zit. Die laatste is natuurlijk ook over te clocken.
GTX 1070 Ti;5;0.32641083002090454;Ben gewoon aan het sparen voor een MSI GeForce GTX 1080 Ti Gaming X Trio heb ik nergens meer last van
GTX 1070 Ti;1;0.29497724771499634;Als ik die benchmark resultaten zie, ben ik blij dat ik gewoon nog een 980ti tweedehands gekocht heb. Met simpele overclock kom ik op een superposition score van 10074 en dit op een ouwe i7 2600k @ 4.1ghz.
GTX 1070 Ti;5;0.7232144474983215;daarom altijd gewoon de beste TI kopen heb je altijd waar voor je geld
GTX 1070 Ti;3;0.563238263130188;Leuke review met amusements waarde. Maar eh ...........waar zijn de mining performance waardes. Want daar koop je dit voor, toch.
GTX 1070;2;0.32457172870635986;"En waarom is de 970 niet meegenomen in de vergelijkende testen? De 1070 is toch immer de directe opvolger van de 970. De 1070 zal waarschijnlijk een veel groter publiek bereiken dan de 1080. Dus is wat mijns inziens dé belangrijkste vergelijking niet gemaakt. Huidige ""enthousiast"" kaart versus de nieuwe generatie ""enthousiast"" kaart."
GTX 1070;3;0.37810835242271423;Qua naamgeving is de 1070 inderdaad de direct opvolger van de 970, maar qua prijs is 'ie op dit moment meer een opvolger van de 980. Die 980 hebben we wel opnieuw getest en zit dus ook in de review en als je weet dat die (afhankelijk van je kloksnelheden) ongeveer tien procent sneller is dan een GTX 970 kun je wel een beeld krijgen van de verhoudingen. Kleine pleister op de wonde: testresultaten hebben we sinds kort ook bij de specificaties in de pricewatch staan. Die van de Inno3D GTX 970 staat hier: pricewatch: Inno3D GeForce GTX 970 OC 4GB (maar is wel voor het laatst volledig getest met driver 358.91)
GTX 1070;2;0.3795861601829529;Op dit moment niet, maar (veel) meer gamers hebben een 970 dan een 980. (Bron Zo heb ik ook een 970 in mijn pc zitten, en voor mij is dus op een gegeven moment een relatief simpele vraag: tik een goedkope 970 op de kop voor sli, of ga ik over naar de 1070. Toen ik mijn 970 kocht was die ook 380 euro, dus dat ontloopt de 1070 amper. edit: Het mag dan inderdaad 100-150 euro zijn maar op een totaal pc is dat te verwaarlozen )
GTX 1070;1;0.4192299544811249;Amper? Dat is 35% duurder! Het is niet handig om de naamgevingen van de fabrikant blindelings te volgen. Anders zit je straks met de 1270 ineens op 900,- euro, maar ja -- het is wel de opvolger.. De 1070 valt in dezelfde 'price bracket' als de 980 en is daarmee for all intents and purposes z'n opvolger. Of schaalt jouw budget ook mee met de msrp's van nvidia? De echte opvolger is er nu nog niet, maar waarschijnlijk zal dat de 1060(ti) worden. Een andere mogelijkheid is dat nvidia nu nog zoveel mogelijk probeert te melken en tegen het einde van het jaar de 1070 naar 350 ~ 400,- verlaagt. Tot die tijd heeft de 1070 helemaal niks (m.u.v. de naam) te maken met de 970. Dat staat niet los van het feit dat in iedere benchmark die momenteel gepubliceerd wordt volledig incompleet is zonder een 970 mee te nemen. Puur vanwege het feit dat dat de meest gebruikte kaart op het moment is (en vrij evenredig presteert als de 290/390). Zonder de 970 (met huidige drivers .e.d) mee te nemen wordt iedere benchmark al snel nutteloos.
GTX 1070;3;0.31679871678352356;Inderdaad. Ik ben best benieuwd hoe deze kaart in vergelijking met mijn huidige GTX970 presteert. Die conclusie valt nu niet te trekken.
GTX 1070;3;0.3953971862792969;Omdat de 1070 in een compleet andere prijsklasse valt dan de 970 misschien? Behalve het nummertje 70 is er niet veel te vergelijken aan deze kaart, en qua prijs valt hij in de 980TI klasse dus hij zal hiermee moeten gaan concurreren.
GTX 1070;2;0.3651469945907593;Momenteel wel ja. Wacht 6 maandjes als er genoeg aanbod is. En ze zijn 400 euro. Wat ongeveer de zelfde prijs is als toen de 970 uit was. En wacht dan nog eens 6-12 maanden en je kan hem ook voor 350 kopen.
GTX 1070;2;0.47878456115722656;Ze zijn zeker geen 400 euro. 550 + is meer de regio waar je het gaat moeten zoeken. over 6 a 12 maanden voor 350 gaat hem helaas ook zeker niet worden. en zo ook andere sites (zie pricecheck)
GTX 1070;3;0.3993755877017975;Dit worden, als de prijzen een beetje stabieler worden, de 'hogere' prijzen: EVGA verkoopt haar kaarten direct via de website voor vaak ongeveer 10% meer dan bij de meeste webshops namelijk
GTX 1070;2;0.45133674144744873;De advies prijs is 399$ voor de 1070. Momenteel is de vraag er groot en het aanbod karig. Dus gaat de prijs omhoog. Vandaar dat ik zij wacht 6 maanden. zodat er meer aanbod is als vraag. En de prijs zal zakken. pricewatch: MSI GeForce GTX 970 GAMING 4G Puur kijkend naar de 970 zie je ook dat hij een hele tijd 375-400 euro heeft gekost(gemiddeld). Dus komt de prijs redelijk overeen wat Nvidia als advies prijst geeft. Zoals je hier ook kan lezen. nieuws: Nvidia publiceert specificaties van GTX 1070 met 1920 cudacores Maar je kan de huidige prijzen niet vergelijken met wat je moeten gaan kosten. pricewatch: Sapphire R9 290 4GB GDDR5 OC Vapor-X Dit is een mooi voorbeeld van hoe het kan gaan Begon bij 425ish en is op een bepaalt moment 6/7 maanden later te koop voor 275 ish. Dat is een prijs drop van 150 euro. Dus het kan prima. Maar om de huidige prijzen als draad te nemen van wat de 1070 moet gaan kosten als ze op voorraad zijn vind ik overdreven.
GTX 1070;1;0.6148936748504639;399 is sowieso al zonder 21% BTW en dan moeten ze ook nog geïmporteerd worden.
GTX 1070;1;0.6321238875389099;Wat na dollar>euro + btw eigenlijk altijd rond het zelfde uitkomt. De 1070 blijft echt niet rond de 529-599 steken hoor.
GTX 1070;1;0.6479958891868591;Op newegg ook allemaal in Back order voor 430 - 470 dollar. Hier 530+ euro. Hoe werkt dat dan?
GTX 1070;2;0.4097636938095093;Waarschijnlijk omdat de VS er genoeg van nvidia kan krijgen. pricewatch: MSI R7 370 Gaming 4GB Prijs is zelfs iets goedkoper. Dus kom je op vraag en aanbod verhaal. Als nvidia er 50.000 naar de vs stuurt en maar 500 naar de EU(aantallen zijn niet echt). Dan kom je vanzelf uit dat de prijs bij ons vele malen hoger zou zijn. Daarom zij ik ook even 6 maanden wachten(dit kan ook bv over 1 maand zijn) maar als er genoeg aanbod is en minder vraag daalt de prijs vanzelf naar normale waarde. En zit het ineens weer rond de 970.
GTX 1070;1;0.7910454869270325;Ik hoop dat je snapt dat import voor een bedrijf bijna niks kost? Invoer rechten is bijna 80% btw wat een bedrijf weer terug krijgt van de belasting. Vervoer kosten is niks meer als dat de Amerikaanse markt moet betalen. Die kaarten worden immers zoiezo in China of een ander Aziatisch land gemaakt en vanaf daar verscheept.
GTX 1070;1;0.4390033185482025;Klopt. En in china krijgen de bedrijven het wereldwijd verschepen bijna(al niet helemaal). gratis.
GTX 1070;1;0.8505843281745911;Tuurlijk...dat een aliexpress voor een paar euro een pakketje verstuurd wil niet zeggen dat de containers van Xiamen/Ningbo etc. naar het Westen niks kosten. Kost je gewoon duizenden euro's afhankelijk van een hoop factoren.
GTX 1070;2;0.42506444454193115;Paar duizend euro op een container waar duizenden videokaarten in zitten dan valt het wel mee. Wat ik er mee bedoelde dat vervoer kosten voor Europa niks hoger zijn als voor Amerika. Dus het is raar om dat als argument te noemen dat de kaarten op de Europese markt duurder zouden zijn door vervoer.
GTX 1070;1;0.5170173048973083;Mijn reactie was op Loki maar wat je zegt is inderdaad waar. De videokaart is een product met veel marge dus die transportkosten van 2 euro per kaart zal ze een worst zijn. Die vervoerskosten worden pas interessant wanneer het product dat je verkoopt 2 euro kost en de vervoerskosten 0,25 of 0,5 euro per stuk zijn.
GTX 1070;1;0.7197301983833313;399 dollar is bij mij geen 600 euro.....dus weer puur oplichterrij
GTX 1070;1;0.6116737723350525;Waarom zou je het vergelijken met hoeveel het over een jaar gaat kosten? Dat is het meest belachelijke wat je maar kan doen. De huidige prijs-kwaliteitsverhouding is wat het nu kost. Tevens is er geneens stock van de videokaart, het gaat nog een lange tijd duren voordat de prijs zo laag wordt.
GTX 1070;3;0.42778757214546204;Dat kan. Maar puur advies prijs van nvidia uitgaant(het enige wat wij hebben) zal hij ongeveer even duur zijn als de 970 in het begin was. Maar we zullen zien.
GTX 1070;1;0.5883432626724243;Hahaha de adviesprijs van Nvidia. Als je echt de marketing van die hufters gaat geloven is de 1080 een geweldige overclocker en is de 1070 beter dan een 980TI. Nvidia heeft hun adviesprijs 100$ lager gezet en vervolgens een Founders Edition uitgebracht, vervolgens welke zelfs ook niet in stock is waardoor de prijs NOG verder opgedreven wordt . Het allerlaatste wat je wil gaan doen is Nvidia geloven op hun bullshit waar alles er honderd keer leuker uitziet dan in de realiteit. We moeten kijken naar de echte prijzen, ook niet voor Amerika, maar hier in Nederland, omdat dit de echte relevante resultaten zijn voor ons.
GTX 1070;1;0.42709195613861084;Je vergelijkt de adviesprijs en die is 399 dollar en volgens mij was de 970 destijds 359 MSRP Simpel...
GTX 1070;1;0.4273013770580292;Ja man een advies prijs is echt een werkelijke uitbeelding van de realiteit. Vooral als hij in alle winkels anderhalf keer zo duur is
GTX 1070;3;0.5439761281013489;Vraag en aanbod, zoals door anderen is aangegeven. Ja de prijs zal wat hoger liggen, maar niet zo veel hoger als nu, ik verwacht dat we tegen augustus een prijs hebben rond de 425 euro.
GTX 1070;2;0.36949941515922546;De releaseprijs van de 970 was een stuk lager dan die van de 1070. De 970 was de goedkoopste 70 serie bij release.... Dus mensen die hopen dat hij onder de 400 euro gaat uitkomen verwacht ik dat ze bedrogen uitkomen, tenzij AMD er echt iets goeds tegenover weet te zetten.
GTX 1070;2;0.42081376910209656;niet onder maar wel rond de 400. 420 dollar. En als je kijkt naar pricewatch: MSI R7 370 Gaming 4GB scheelt het je 20 euro. Dus kan het wel. Alleen is de voorraad voor europa waarschijndelijk erg laag. En schiet de prijs omhoog.
GTX 1070;2;0.41566720604896545;Dan mag de euro wel een stuk sterker worden tov de dollar Dollar=Euro gaat al even niet meer op namelijk. Ik verwacht ze voor December niet onder de 450 te zien, zeker de custom designs niet.
GTX 1070;1;0.4886019825935364;vertel mij dan maar is warrom de 370 duurder is bij n ewegg als euro>dollar niet opgaat.
GTX 1070;1;0.3709820806980133;Vraag en aanbodverhaal gewoon. Die kaart is ook al een hele tijd op de markt, plus dat je nu de goedkoopste uit de pricewatch vergelijkt met een random winkel (newegg). De 370 wordt ook nog maar door 2 winkels verkocht volgens de Hardware USA prijsvergelijker. Hier door veel meer winkel dus meer concurrentie. De vraag/aanbod is gewoon heel anders dan voor de GTX1070. De prijs, met de huidige waarde van de euro tov de dollar, is altijd hoger in euro's dan in dollars. Pas als de euro sterker wordt (1,21 dollar per euro) gaat dat misschien omdraaien.
GTX 1070;1;0.8165620565414429;Dat roept iedereen maar echt veel merken doen we er niet van. En misschien gaat hij 10 euro duurder worden.(wat ik betwijfel maar daar zullen we nooit achter komen). Dan nog zit hij ver onder wat de NL en hoogstwaarschijndelijk heel europa er voor vragen. Tevens zou de foundereddition over 100 dollar duurder zijn als die met aangepaste PCB. En de foundereddition is voor 490 te koop. Haal daar 88 van af. En hij komt verdomde dicht bij de 400..
GTX 1070;2;0.5081929564476013;"Helaas dat er ook al tussen neus en lippen door is verteld door insiders dat er geen bedrijf is dat de 1070 winstgevend kan leveren voor de prijs die Nvidia opgaf voor de goedkoopste custom kaarten. Nvidia heeft gewoon een leuke streek geleverd met hun ""De founders editie is duur, maar de customs worden veel goedkoper"". Waarom zouden fabrikanten de customs goedkoper in de markt gaan zetten? Ze moeten voor de custom kaarten R&D draaien en ontwikkelen. Voor de Founders Edition zit veel minder R&D voor de partners. Ze maken dus minder winst op een product wat meer geld kost om te maken. We gaan zien wat de prijzen doen, maar met de huidige dollarkoers gaan ze het komende jaar gaat 400 dollar (zonder btw) niet gelijk zijn aan 400 euro (inclusief btw)"
GTX 1070;3;0.43782752752304077;Nu vallen ze nog in een andere prijsklasse, voornamelijk door de schaarste, en het is even afwachten hoe de Polaris RX 480 het gaat doen. Als die qua performance in de buurt van de 1070 komt, zullen de prijzen sneller gaan zakken. De prijs van de 970 is over een langere periode niet echt gezakt en redelijk constant gebleven..
GTX 1070;2;0.3969030976295471;waarom de 290X wel dan, die is een krappe 50e duurder, de logica van de 970 als een de meest favoriete gpu's op dit moment niet mee te nemen is telefoons vergelijken zonder een samsung galaxy s6 in die vergelijking mee te nemen, omdat die nu wat goedkoper is dan de nieuwste telefoons. Als je echter wat vaker de reviews leest kan je op zich ook wel zelf bedenken dat een 970 tussen de 980 en 290x in zit qua prestaties, soms wat er onder soms gelijk etc. gem. gezien ong 10-15% minder snel dan een 980.
GTX 1070;3;0.4754980802536011;Interessante review maar mijn conclusie is dat je je 970/980/980ti/290/290x/390 etc niet in hoeft te ruilen. De nieuwe kaarten maken nergens het verschil tussen speelbaar en onspeelbaar (behalve mss in VR). Als je eenmaal boven de 60fps zit maakt het voor de meeste mensen niets meer uit (tenzij je een scherm net een hele hoge refreshrate hebt). Wel mooie kaarten op zich maar ook behoorlijk prijzig. Hoop dat AMD ook speelbare framerates neer kan zetten voor een lagere prijs.
GTX 1070;3;0.5897031426429749;Totdat het volgende spel met dikke graphics uitkomt. Ik vind het de laatste jaren wel maar er langzaam opschieten hoe veel mooier spellen zijn geworden. Er zijn na gta v zat spellen uitgekomen die een stuk minder mooi waren waarbij dat echt wel beter had gekund.
GTX 1070;4;0.5141400098800659;Fijn weer een review die de prullenbak in kan. Jullie gebruiken een aftermarket 1070 die boost snelheid kan houden tot 2 GHz maar een reference 980ti? Gebruik dan ook een MSI 980ti of een nog betere versie. Uit andere revieuws is gebleken dat een 980ti en 1070 even snel zijn, maar de 980ti is eerder sneller, mits je een editie hebt die ver turbo'd. Mijne gaat al naar 1430MHz core zonder handmatige overclock.
GTX 1070;2;0.46838533878326416;Wat bvk zegt: we hebben hier van de meeste videokaarten alleen het reference-model liggen en Nvidia heeft ons geen reference-versie van de GTX 1070 doen toekomen om te vergelijken met de reference 980Ti die we al hadden. Sowieso kunnen we niet alle kaarten die op de markt zijn testen, en liggen de prestaties van de 1070 en 980Ti inderdaad dicht bij elkaar, waardoor je al kunt concluderen dat een overgeklokte 980Ti het beter gaat doen tegenover een 1070 op standaard kloks. Als ik persoonlijk vijfhonderd euro voor een videokaart over zou hebben zou ik overigens wel voor de 1070 gaan gezien de verbeteringen en nieuwe features van Pascal ten opzichte van Maxwell.
GTX 1070;2;0.43338683247566223;dat is helemaal niet zo duidelijk in de review eerlijk gezegd. pas op de pagina ' overklokken ' komt naar voren dat de overklokruimte door nvidia ' s gpu boost 3. 0 zelf wordt weggesnoept en dus gewoon onderdeel van de marketing is, terwijl dat bij de 980ti niet het geval is. het is nogal een verschil, dat de 980ti met simpele oc zo ' n 15 - 18 % sneller wordt terwijl er bij de 1070 nog maar 5 - 7 % in zit. moet de lezer ' maar concluderen ' dat een 980ti dus in sommige gevallen zelfs sneller is, terwijl de 1070 in elke bench het lijstje aanvoert? dat is fijn reviewen zeg! zeker na de introductie over hoe pascal verschilt van maxwell, en dat je eigenlijk moet concluderen daaruit dat er weinig is veranderd behalve wat ' features ' en extra compressie. als je weet dat pascal zelf al zwaar op de oc leunt, dan moet je daar oc versies van de 980ti naast zetten om dit weer te geven, en niet met twee halve zinnen iets kenbaar maken. verder, de fe modellen zijn constant aan het throttlen en de msi gaming x, de asus strix en gigabyte ' s g1 koelers doen dat niet. nogal een verschil, want het levert een gemiddeld klok verschil op van zeker 50 - 80 mhz ( constant, dus zelfs met de licht fluctuerende clocks van pascal ) en de variatie is ook stukken kleiner. dat zegt nogal wat over hoever nvidia pascal zelf al heeft gepusht - zelfs op 16nm is de oude nvttm shroud niet genoeg om pascal in de hand te houden. voor een dergelijk rijkelijk late review van pascal had ik meer verwacht. er zijn al verschillende aib ' s op de markt en gereviewed, maar ik zie hier alleen msi. daardoor ontbreekt het ook aan de mogelijkheid om een scherpe conclusie te trekken, want elke aib koeler doet het vooralsnog stukken beter dan de fe. het enige dat eigenlijk over de fe wordt gezegd in de review, is dat hij luidruchtiger is. maar eigenlijk is het gewoon een slappe rotkoeler waar je 100 piek extra voor mag neerleggen.
GTX 1070;4;0.3965507447719574;Om aan je verhaal toe te voegen:
GTX 1070;1;0.7126539349555969;Oeps Nvidia heeft ons een zwaar gebooste kaart t.o.v. de reference gegeven om de resultaten er beter uit te laten komen en dit hebben we zonder hem gewoon terug te clocken naar stock speeds maar geaccepteerd en niet duidelijk in de review vermeld want ookal is dit een complete vervalsing van test resultaten we hebben niks anders aanwezig haha Dit is hoe je nu klinkt.
GTX 1070;1;0.2687590718269348;Nvidia heeft ons een GTX1080 doen toekomen. De GTX 1070 en 1080 waren afkomstig van MSI en zijn ondertussen ook weer terug naar MSI. Zomaar terugklokken naar stock speeds ligt nog niet zo simpel, melden verschillende mensen in de reacties hierboven al. En de kloksnelheden staan wel duidelijk in de review vermeld, te weten op pagina vijf.
GTX 1070;1;0.6566672921180725;De gemiddelde lezer heeft weinig interesse in de losse kloksnelheden, en dit zegt vele mensen ook helemaal niks.We willen kijken naar de grafieken en op basis daarvan een visuele vergelijking kunnen maken die representatief is aan de werkelijkheid. De reden dat mensen reviews lezen is omdat vele mensen niet goed genoeg geinformeerd zijn over wat alle nummers precies betekenen, en gewoon willen kijken naar wat sneller is en meer waar voor hun geld. De 980TI werd altijd geprezen als een super overclocker, en om dan ineens in de test zo uit te voeren dat de 1080 erbovenuit komt is gewoon visueel bedrog. Tevens lijkt het me echt niet moeilijk voor jullie om aan een goede 980TI te komen, er is vast wel iemand in de staff die er een thuis heeft liggen die je evt zou kunnen gebruiken.
GTX 1070;2;0.5373885631561279;Ehm... de GTX 1080 IS ook gewoon sneller dan een 980ti hoor. Het is de 1070 waarmee het stuivertje wisselen is met de 980ti. Je maakt een boel geluid maar er klopt niet heel veel van... Het gaat ook niet om 'zwaar gebooste' kaarten. FinFet klokt gewoon hoger en Nvidia heeft met GPU Boost 3.0 die ruimte heel goed weten te benutten. Dat is met Maxwell nog niet zo, het overklokken werkt nu anders, maar Pascal klokt vrijwel altijd tussen de 2050-2114 mhz, dus 'gebooste kaarten gegeven' is 100% complete onzin want daar gaat dit hele verhaal niet over - ze klokken allemaal ongeveer hetzelfde.
GTX 1070;1;0.5165359377861023;Mijn fout moest 1070 typen maar was een typfoutje.
GTX 1070;4;0.32010525465011597;"Er onstaat een coherent verband met MSI en Asus de GTX 1080 ""review"" kaarten die ze aan de persleden hebben gegeven en zelf hebben aangepast zodat ze (iets) hogere kloksnelheden hebben, waardoor ze beter uit de test komen. Bron: link, en link."
GTX 1070;3;0.5950523018836975;2e hands 980ti zijn wel voor een stuk minder te krijgen Daarbij snap ik het argument wel, maar er ontstaat wel een kromme vergelijking. Nu lijkt een 980ti -> GTX1070/1080 een grote stap, terwijl het in de praktijk -5 tot +0% en 15-20% verschillen zijn.
GTX 1070;3;0.2835366725921631;Ik snap je punt en ik zal eens kijken hoe we dat de volgende keer beter kunnen doen, bijvoorbeeld door een kaart te onderklokken zodat 'ie op dezelfde speeds loopt als de reference kaart zodat je iig reference vs reference-snelheden hebt.
GTX 1070;2;0.36619359254837036;"Kan, maar 75% van de mensen koopt in mijn ervaring edities met non-reference coolers. nVidia heeft de reference edities opvallend hoog geclocked t.o.v. de non-reference kaarten, waardoor het gat reference 980ti-1080 veel groter is (~30%) dan het gat non-reference 980ti-1080 (15-20%) zou zijn. Ook is ""stock clocks"" een raar begrip, want gpu 3.0 clockt eenmaal zo ver als de koeling het toe laat. HWI haar 1080 reference ging tot 1880MHz geloof ik, maar als je dat ding onder een waterblock gooit zal die automatisch verder clocken. Persoonlijk zou ik graag zien dat een vendor gpu tegenover andere vendor gpus wordt gezet, hopelijk kan dat vanaf nu ook. Neem bijvoorbeeld de MSI gamer editie, zeer populair zoals jullie ongetwijfeld ook zullen weten. MSI 980ti vs MSI 1070 vs MSI 1080 lijkt mij een leuke vergelijking waar men ook echt iets aan heeft."
GTX 1070;3;0.5589324831962585;Ze zijn afhankelijk van wat ze opgestuurd krijgen. Dus ook merk vergelijken wordt moeilijk. Daarnaast zal een MSI meerdere versies uitbrengen...
GTX 1070;2;0.2940291464328766;Dit onderklokken zou bij GTX500 serie kaarten nog werken, maar sinds GPU-Boost er is. Is geen enkel scenario met eenzelfde geklokte kaart hetzelfde. Zo zal een MSI GTX1080 op stock snelheden onderklokt, nog steeds beter scoren dan de FE op stock clocks. Dit te maken met de power target die standaard hoger is, en de temperatuur die lager blijft door de betere koeling, hierdoor zal de GPU Boost hoger clocken dan met een FE editie kaart. Het enige wat je wel gewoon kunt doen is FE kaarten met FE kaarten vergelijken en aftermarket kaarten met andere aftermarket kaarten.
GTX 1070;1;0.2606719732284546;Zoiets als deze reviewer bij HWI heeft gedaan:
GTX 1070;3;0.3159683048725128;of misschien kunnen jullie een kaart lenen van een tweaker ik zou bijv er geen problemen mee hebben als jullie iets willen lenen voor bijv een benchmark om de community te laten zien wat het kan bijv: jullie starten een topic op het forum als jullie iets willen lenen lijkt mij wel een mooi plan
GTX 1070;2;0.5415335297584534;Underclocken van de nieuwe kaarten is niet representatief, de reference kaarten gaan namelijk eerder throttelen dankzij de brakke koeler en brak pcb, eigenlijk iedere custom kaart zoals een asus of msi blaast de reference compleet uit het water en is geen vergelijking. Als je een koude kaart op een testbench 3D mark laat draaien dan is het verschil merkbaar maar niet enorm, als je de kaart in een systeem inbouwd en de hele dag laat draaien dan is het verschil een stuk groter in het voordeel van de custom kaarten.
GTX 1070;3;0.3908804655075073;Jammer dat jullie geen reference GTX 1070 hebben gekregen van Nvidia Zeker aangezien de conclusie is dat de 1070 eigenlijk de beste bang-for-the-buck geeft: ik vermoed dat er dus veel mensen in dat model geinteresseerd zullen zijn, wat het deste interessanter maakt om te zien hoe de reference 1070 t.o.v. van de aftermarket presteert. Dat zou ik althans erg graag willen weten vóór ik één van de twee zou aanschaffen... Maar goed, niet jullie 'fout' natuurlijk, je moet roeien met de riemen die je hebt. Ik zat me wel af te vragen: zijn jullie helemaal van Nvidia afhankelijk voor deze keuze? Ik meen dat er ook wel eens webshops zijn die review samples uitlenen toch? Overigens, ik zie een hoop negatieve commentaren weer op de review, dus wilde ook even een positief geluid laten horen. Als multi-monitor gamer klinkt de smp feature mij veelbelovend in de oren en dat wordt toch mooi even besproken. Natuurlijk is het zo dat er websites zijn die nóg meer 'in-depth' de architectuur bespreken, maar mijns inziens heeft Tweakers altijd een prettige balans: de technische kant wordt niet geschuwd, maar het blijft beknopt genoeg zodat het artikel niet al teveel leestijd kost. Zo heb ik in een kwartiertje een aardig beeld van wat de nieuwe kaarten te bieden hebben. Dus wat mij betreft: dank voor weer een mooi overzicht
GTX 1070;3;0.3746863603591919;Hardware.info heeft een review waarin ze wel de referentiekaarten meenemen. Zie onderaan dit bericht. Verder wilde ik opmerken dat hardware.info vrijwel zonder uitzondering de betere reviewsite is voor computer onderdelen. Ze testen bijvoorbeeld ook uitgebreid voedingen, koelers, moederborden en behuizingen. Sowieso testen ze veel meer onderdelen en varianten van hetzelfde onderdeel. Daarnaast zijn de tests vaak net uitgebreider. Tweakers komt wat dit betreft niet eens in de buurt. Echter, ik heb ook niet het idee dat dit doel van Tweakers is. Ik vraag me eigenlijk af of Tweakers zelfs wel weet waarom ze dit soort reviews publiceren, terwijl ze duidelijk niet focussen op het uitgebreid reviewen van componenten. Blijft het dan niet altijd net niet? Daarentegen heeft Tweakers ook erg sterke kanten. Tweakers wint het ruim met de pricewatch, de reacties (als je door de onzin heenkijkt zit er veel waardevols in) en het nieuws. Wat mij betreft heb je met de sites samen een goed overzicht. Review van de gtx 1070 Onderlinge vergelijking verschillende 1080 modellen:
GTX 1070;1;0.5040481090545654;geen van beide is bang for the buck, ze zijn alle 2 rete duur
GTX 1070;2;0.3992059528827667;nee dat zeg ik niet, het is eerder bij launch en zonder concurrentie momenteel dat kaarten te duur zijn.... meestal bij invoer van lagere reeksen en competitie gaat de prijs zich ook beter plaatsen. AMD fanboy, lol ik denk dat je bril te groen ziet typische auto spam reactie
GTX 1070;1;0.6761013865470886;Volledig met je eens. De prijzen bij Nvidia vind ik gewoon te belachelijk gestegen. Rond de 500 euro voor de top versie zou het uitgangspunt moeten blijven en niet richting de 1000 euro. Ik zou anders de 980 weer verkocht hebben en opgewaardeerd. Nu doe ik dat gewoon niet.
GTX 1070;3;0.42629173398017883;Je kan natuurlijk wel downclocken naar stockniveau.
GTX 1070;3;0.5019149780273438;Bedenk wel dat Tweakers alleen de kaarten kan testen die ze aangeleverd krijgen of wat ze op de plank hebben liggen. Nou liggen er op het HQ best aardig wat kaarten, maar de genoemde zullen daar niet tussen gezeten hebben. Want anders hadden ze die wel getest natuurlijk...
GTX 1070;1;0.5618345737457275;Ze zijn normaal gesproken ook niet te berierd on cijfers uit vorige testen te hergebruiken. Ik heb het gevoel dat NVidia hierachter zit en dat ze dat soort kaarten niet mogen gebruiken. ( zal wel onderdeel zijn van de overeenkomst om de kaarten te mogen testen ) Zelfde met de SLI testen, ineens zijn alle 980(ti) / 970 kaarten afwezig.
GTX 1070;1;0.7183535099029541;Dat is niet waar. Het is zelfs zo dat Tweakers in eerste instantie niets van nvidia gekregen heeft... Wat dan wel weer waar is is dat deze review gewoon zonde is, omdat uit de vergelijkingen geen conclusies getrokken kunnen worden door de gebruikte kaarten.
GTX 1070;3;0.2378612756729126;Ja,maar ik vind dat een site ,zo groot als deze ,desnoods ,gewoon ,moet kopen. .Dat is onafhankelijk ,objectief ,en professioneel . Ze hebben een verdien model en groeien nog steeds.Het is allang geen hobbysite meer ,maar onderdeel van een bedrijf.
GTX 1070;2;0.49926769733428955;Dat is leuk en aardig dat kopen, maar dan moet alles wel beschikbaar zijn en dat waren ze eerst niet. Begrijp me goed, ik verdedig tweakers niet, want het tweak gehalte is behoorlijk afgenomen (zelfs voor een niet tweak-er als ik duidelijk).
GTX 1070;1;0.4348539710044861;Tip: spaties zet je achter je komma's, niet ervoor.
GTX 1070;1;0.4466451108455658;Zeker mee eens hoor. Maar los van het kopen of niet, dit is gewoon onacceptabel ook al is Tweakers niet echt op de hardcore pc-bouwer gericht.
GTX 1070;1;0.6874906420707703;"Wij gebruiken juist vrijwel nooit cijfers uit vorige tests omdat drivers zo veel invloed kunnen hebben op de prestaties van videokaarten in bepaalde games. Nvidia heeft hier niks mee te maken; in de gevallen dat ze ons een sample leveren hebben ze verder niets te zeggen over het vergelijkingsmateriaal."
GTX 1070;1;0.4599454402923584;Denk je dat men probeert reviews te sturen door bepaalde sites bepaalde kaarten niet te sturen?
GTX 1070;3;0.5340694189071655;Dat is wel een beetje overdreven, niet? De aftermarket kaarten zijn dan wel iets sneller geklokt, maar groter dan enkele procenten is het verschil meestal niet. Het is en blijft dezelfde GPU. Trek van de cijfers van de aftermarket kaart 2 a 5 fps af, en je hebt de reference cijfers. Het grootste verschil tussen de aftermarket en reference kaarten is de koeler. De goed ontworpen aftermarket kaarten zijn normaliter stiller en koeler. EDIT: Of toch niet. Volgens deze benchmarks is de aftermarket 980ti wel degelijk een pak sneller dan de reference:
GTX 1070;3;0.4548732042312622;Refferenced 980 Ti kaarten hadden heel veel op headroom. Aftermarket kaarten speelde daar aardig op in het verschil was erg groot. En die kon je vaak ook nog eens een stuk verder overclocken. Een refferenced kaart kan je ook ver overclocken maar stock draait die op bescheiden snelheden.
GTX 1070;3;0.3537595868110657;Ik zou ook graag het vergelijk zien tussen een 980Ti op circa 1450Mhz. Dat doen vrijwel alle 980Ti's namelijk. Dus het is wel appels met peren vergelijken nu: non-reference 1070/1080 met reference 980Ti. Eerdere reviews laten zien dat een non-reference 980Ti zo maar 25-30% sneller is Oc'd.
GTX 1070;3;0.24768629670143127;Hier heb je een vergelijking op 1450 Mhz tov een GTX 1070 op 2050 Mhz. Beide referentie modellen.
GTX 1070;2;0.4897327721118927;"Precies, ik wachtte op deze review om de vergelijking te maken tussen een MSI GTX 1070 met een aftermarket GTX 980 Ti. Nou ben ik weer niks wijzer geworden. Het lijkt bijna marketing van Nvidia om ""early reviewers"" om te kopen om de kaarten met een reference kaart te vergelijken. Natuurlijk presteert de nieuwe kaart dan beter... ik verwacht dat hogere geklokte Gigabyte 980 Ti OC of KFA2 Hof beter presteren dan een MSI GTX 1070 of andere aftermarket cooler. De 1070 zit met zijn locked BIOS al bijna tegen de max. clock aan. Misschien wint een aftermarket 980 Ti niet op TDP efficiëntie maar wel op een verantwoordelijke boost clock."
GTX 1070;2;0.41275474429130554;Dat klopt, voor de echte vergelijking moet je vrijwel nooit bij tweakers.net zijn. Je kunt hoogstens zien wat de fps van de individuele kaart is en andere info zoals geluid/temperatuur is interessant. Persoonlijk moet ik altijd heel wat verschillende websites af om een goede relevante vergelijking te vinden. En vaak kom je dan tot andere conclusies.
GTX 1070;1;0.41066262125968933;Een overclockte 980ti is veel sneller nog zelfs, volgens dit filmpje tenminste
GTX 1070;1;0.41202089190483093;Dit klopt dus echt niet. Een GTX 1070 OC op 2050 Mhz is sneller dan een GTX 980 Ti OC op 1450 Mhz. Ja er zijn snellere 980 Ti's te vinden maar dat geldt ook voor de GTX 1070. De GTX 1070 is gemiddeld genomen een snellere kaart dan de 980 Ti en het verschil zal met verdere driver optimalisaties en DX12 ondersteuning alleen maar gaan toenemen.
GTX 1070;1;0.4795638620853424;Dit is niet waar. Een 980TI heeft enorme overclocking headroom, terwijl een 1070 al bijna op zijn limiet zit. Een 980TI met een goede overclock is beter dan een 1070 met max overclock.
GTX 1070;2;0.4216087758541107;Sommige 980 Ti modellen kunnen overklokken tot 1500 Mhz of iets hoger nog maar dat is bij lange na niet het geval voor elke 980 Ti. Een 1070 kan overklokken tot 2100+ maar dat is ook niet voor elk model het geval vandaar dat ik de vergelijking post van een 1070 op 2050 en een 980 Ti op 1450. Dat zijn voor de meeste kaarten haalbare kloksnelheden en dan is de 1070 gewoon beter. Maar feiten zijn al lang niet meer relevant op tweakers.net. Vandaar dat deze reactie ook gedownmod zal worden.
GTX 1070;1;0.5847102999687195;Wat een onzin over CPU het verschil tussen deze en volgende generatie CPU is 1 a 2 fps met de meeste games.
GTX 1070;1;0.4564380347728729;Vind de prijzen belachelijk hoog wacht eerst review af van de rx480 koop daar liever 2 van mochten ze sneller zijn dan de 1080
GTX 1070;3;0.4109923243522644;"Ondanks dat ik 200 eur voor zo'n unieke kaart sowieso wel over heb; stel dat crossfire ruk blijkt te zijn (met name voor VR.. iemand hier ervaring mee?) en die lekkere performancecijfers die we tot nu toe zien de uitzonderingen blijven.. dan toch maar de reviews afwachten? misschien is zo'n 1070 achteraf toch geen slecht idee."
GTX 1070;5;0.7199713587760925;Dual GPU wordt juist behoorlijk gepromoot voor VR, ene GPU is voor je ene oog, andere voor je ander, waardoor microstutter bijv. geen probleem meer is. En in DX12 hoort erg goede dual gpu ondersteuning mogelijk te zijn, veel beter dan DX11 het heeft.
GTX 1070;1;0.5073012113571167;"Mwa, door SMP kunnen de 1070/80-kaarten straks zonder performanceverlies beide ogen tegelijk renderen, dus dat haalt 't nut van 2 kaarten daarvoor al weg.. en ben zelf zo'n sukkel geweest die meteen 2 6800gt's in SLI zette in 2003, en er pas echt wat aan had tegen de tijd dat de 8800gts er was nu heb ik een rift liggen, en ik ga het me niet laten gebeuren dat ik straks met 2 kaarten weer loop te vloeken dan maar 1 beest van een kaart. [EDIT] OT Vraag: Konden we met DX12 niet zo'n 1070 en RX480 samen laten spelen? Antwoord: ""DirectX 12 explicit asynchronous multi-GPU"" ..shit, dan maar opnieuw de mist in, moet ik proberen"
GTX 1070;3;0.27786973118782043;Dat kon nog wel eens een heus gevecht worden..
GTX 1070;2;0.5048021674156189;De Radeon RX480 moet het doen van zijn prijs, niet zijn snelheid, hij zal ongeveer rond de Geforce GTX 980 zitten en tussen de Radeon R9 390 en R9 390x. En ik heb het eigenlijk ook niet zo op Crossfire en SLI, omdat er genoeg spellen zijn die er niet goed mee werken, en je problemen kan ondervinden, en dat het een stuk meer energie kost. En erg jammer dat Tweakers 4,5 weken later pas komt met een review van de Geforce GTX 1080, dat is gewoon triest, en daardoor was het voor mij niet nodig om nog de hele review te lezen, alleen de VR stukje was nog interessant, maar dat waren maar 2 spellen. En vind het erg jammer dat bij 4K jullie de spellen op High of Medium zette, had leuker geweest als jullie die ook gewoon dan op Ultra draaide, om te laten zien dat veel spellen niet goed draait op 4K op Ultra en 60fps op zelfs een Geforce GTX 1080, dat is ook een van de rede dat ik een 21:9 monitor koop met een resolutie van 3440x1440, dan kan je ruim 90% van alle spellen op 3440x1440 met Ultra draaien op 60+ fps. TECHGAGE heeft een hele mooie review van 4K & Ultra-wide Gaming. En dat het er beter uit ziet dan een 4K monitor omdat je link en rechts VEEL meer ziet, sinds ik een 21:9 2560x1080 monitor heb wil ik echt niet terug naar een 16:9 2560x1440 monitor, 21:9 geeft gewoon veel meer diepte in je spellen, het is gewoon mooier, en de meeste spellen werken gewoon op 21:9, al moet je met sommige wat tweaken om op 21:9 te draaien.
GTX 1070;1;0.4304434657096863;Er is meerdere malen vermeld op tweakers, in ieder geval in de comments, dat tweakers geen 1080 heeft ontvangen voor review .
GTX 1070;2;0.3761254549026489;Dat kan, maar dan ga ja er toch achter aan, dan hadden ze het al 3 weken terug kunnen plaatsen hier op Tweakers, nu is het 4,5 weken later, nu zijn er niet veel mensen meer die nog niet een review van de Geforce GTX 1080 gezien hebben, voor de meeste mensen was deze review niet zo interessant meer omdat all de andere allang meerdere reviews gemaakt hadden van verschillende Geforce GTX 1080.
GTX 1070;1;0.4605395793914795;Het ging mij hier om de AMD en Nvidia in 1 systeem samen laten werken, weet niet waarop jij reageert?
GTX 1070;1;0.488450288772583;Begin er maar niet aan, want dat is een SLI verhaal in het kwadraat met alle lasten en bijzonder weinig lusten. Er is tot nu toe maar één game met support voor MGPU en dat is Ashes, en daar is stutter eerder regel dan uitzondering. Bij DX12 is multi GPU gewoon per definitie overgeleverd aan de grillen van elke game developer, dus reken er maar op dat het niet beter wordt dan nu - eerder slechter door de veelheid aan mogelijkheden ipv 'alleen twee identieke GPU's'.
GTX 1070;3;0.2759261429309845;"Ondanks dat de DX12-manier van SLI nog niet in gebruik is of breed voor ontwikkeld wordt, zou het nog wel nut kunnen hebben ""Linked mode is available when there are multiple near-identical GPUs in the system ... to be combined into one larger addressable unit. So if let’s say we have two R9 390 cards ... twice the memory and twice as many GCN shader units. Essentially creating a 16GB, 5160 GCN core behemoth."" Gaat dan niet over mixed vendor maar klinkt toch echt als iets gaafs dat tot nu toe in SLI/CF nog niet is gedaan"
GTX 1070;2;0.3987670838832855;Theoretisch heel mooi, en de dagelijkse praktijk van nu en de afgelopen 10-15 jaar wijst uit dat de praktijk altijd bijzonder tegenvalt en het geheel staat of valt met een dikke investering van zowel GPU fabrikant als developer/publisher. Plus de nazorg om alles ook na release mooi draaiende te houden. Dat is bij heel veel games niet het geval. Allemaal dus heel erg gaaf, maar zéker met DX12 en VR ben je het beste af met één sterke GPU. Dat gaat echt sterker gelden dan ooit tevoren, want ook onze eisen met betrekking tot minimale FPS gaan omhoog. Een console draait op 30 fps, of soms zelfs minder, maar VR vraagt om 90 of meer. Kleine variaties in frame pacing worden daardoor veel sterker voelbaar - zowel op een monitor als in VR - en linked of multi GPU is daar bijzonder gevoelig voor. Nog een kleine tip: neem WCCFtech met een paar kilo zout. 95% is clickbait.
GTX 1070;5;0.2467714101076126;Duimen en hopen dan maar dat met tijd, doorontwikkeling en dev-adoptatie het waarmaakt wat ermee mogelijk schijnt te zijn.. ik zou alvast 2 rx480s linked willen zien draaien
GTX 1070;3;0.41406628489494324;Prijzen zullen eventueel nog wel zakken doordat de beschikbaarheid op het moment schaars is
GTX 1070;3;0.4019579589366913;Heb zelf nu 2e hands heel goedkoop een GTX 970 op de kop kunnen tikken. Daarmee zing ik het wel een jaartje uit en zie dan wel wat de prijzen doen van deze kaarten. Vooralsnog is meer dan 500,- voor een 1070 veel te veel. Daar kan nog wat vanaf
GTX 1070;3;0.3903186023235321;330 zou een nette prijs zijn
GTX 1070;1;0.3687099814414978;Je zou daarom ook je kans kunnen grijpen en nog een oude Maxwell-videokaart op de kop kunnen tikken, want daarvan heeft Nvidia de prijzen behoorlijk verlaagd. Een 980 Ti heb je al voor 500 euro, terwijl de goedkoopste GTX 980 380 euro doet. Deel je die prijs door de prestatie-index, dan kunnen we zien welke videokaart de meeste waar voor je geld biedt. MSI's GTX 1070 kost een euro of 500, maar de GTX 1080 kost meer dan 800 euro, terwijl er al GTX 1080-videokaarten voor ruim 100 euro minder te bestellen zijn. Geen hint naar Polaris hier?...
GTX 1070;1;0.46506232023239136;Waarom zouden ze? Polaris is nog niet op de markt, en afgezien van AMD's eigen benchmarks hebben we nog geen (geloofwaardige) prestatie indicatie. Zou in mijn ogen niet goed zijn als Tweakers hints of adviezen zou geven op basis van geruchten.
GTX 1070;1;0.2894507348537445;Tsja, copy pasta. Zie de reactie naar PiweD
GTX 1070;2;0.48665693402290344;De hint naar Polaris staat in de conclusie , maar daarvan vind ik het altijd riskant om alvast conclusies te trekken op basis van benchmarks van AMD zelf, want die hebben er belang bij om de zaken zo rooskleurig mogelijk voor te stellen, en aan de andere kant van leaks waarmee getest wordt met een oude driver, die zeker niet de launch-driver van die videokaart wordt.
GTX 1070;2;0.40175917744636536;Dat is niet echt waar ik op doel. Het gaat mij om het stukje waar ze een advies geven aan de koper, hier staat niet vermeldt dat AMD right around the corner is. Benchmarks terzijde, dat zal allemaal wel
GTX 1070;1;0.719986617565155;"Zo lang er geen 16NM 250-300w kaarten zijn slaat het allemaal nergens op. Er wordt weer gemikt op 1-2k en NIET op 4k en minimaal 60fps. Wachten dus tot einde van het jaar. Wat moet je in godesnaam met een kaart die gemiddeld amper 2x sneller is dan de 3 jaar ;;stokoude;; 290x AKA de 1080 Tja, denk daar maar eens goed over na! 16nm leuk!! maar wat moet je er mee als de kaarten niet minimaal 60fps draaien bij 4k En geld besparen op stroom??? Als jeje auto pakt pomp je ook zo 10 euro weg als je een klein stukje rijdt. Verbruik is geen excuus, je gaat geen 800 euro lappen omdat je videokaart zo lekker mileuvriendelijk is Vooralsnog zie ik geen 4k games die 60-144fps fps draaien op een enkelle 1070 of 1080. Waar betaal je dan die 800 euro voor?"
GTX 1070;2;0.5069958567619324;Twijfel zelf ook heel erg, heb nu 2 x 290x en ik kan nergens een echt goede vergelijking vinden tussen 2 x 290x vs GTX1080.. Ik wacht nog rustig af en hoop dat AMD met de Vega iets moois neerzet. Tegen die tijd zal de 1080TI er ook wel zijn. En 4K is voor een hoop mensen helemaal niet van toepassing, heel af en toe start ik wel eens een spelletje dat Eyefinity gebruikt en dat 3 schermen benut en dan komt een 290x CFX setup nogsteeds redelijk mee. Zolang ik nog op 1080p zit zie ik niet veel nut. 1 x GTX1080 lijkt me een redelijke vervanger voor 2 x 290x ben benieuwd..
GTX 1070;4;0.2662082016468048;Goed punt. Goeie kans dat die kaart er al ligt, maar dat ze deze even op de planken laten liggen tot de markt ernaar vraagt, zodat jij en ik over 2 jaar opnieuw een paar honderd euro aan een upgrade uitgeven
GTX 1070;5;0.3765402138233185;Die kaart ligt er al, en heet GP100. Een paar aanpassingen, memory controller van de GP104 erop, NVlink en HBM er vanaf, GDDR5X ertegen aan, twee SMX'en eruit en presto, een gehandicapte GP100 voor zo'n 900-1100 euro. Mark my words...
GTX 1070;1;0.43147507309913635;Goeie kans dat die kaart er al ligt, maar dat ze deze even op de planken laten liggen tot de markt ernaar vraagt. AMD in de buurt dreigt te komen... FTFY
GTX 1070;3;0.28139516711235046;Klopt, het is toch ook gewoon het standaard spelletje van Nvidia/AMD. Elk jaar is hetzelfde, er komt een 'fantastische' nieuwe gpu reeks uit die je moet hebben! In de praktijk is de snelheidswinst maximaal 30% maar er word wel de hoofdprijs voor betaald. Een echte vorm van concurrentie is er natuurlijk al lang niet meer bij deze bedrijven. Dit is gewoon het verdienmodel, elk jaar een paar stapjes sneller. Ik durf te wedden dat ze prima een kaart kunnen uitbrengen die met gemak minimaal 2x sneller is. Maar waarom zouden ze dat doen? Dat maakt het werk alleen maar lastiger en ze verdienen dan minder.
GTX 1070;3;0.5959058403968811;Met sommige dingen ben ik het zeker eens. Maar het laatste ben ik nog niet zo zeker van. We kunnen pas weten wat amd doet als de kaarten gereleased zijn. Ja ze mikken op het midden segment 150-300 euro, maar ik denk dat hier de beste prijs/prestatie verhouding uit voorkomt. Ook vind ik de winst die het bied kwa frames voor die gigantische prijs niet echt heel goed. Als je een 980TI OCed kom je heel dicht bij die 1070. Ook de temperaturen vallen mij zeker tegen voor deze nieuwe generatie. Ook vind ik het raar dat er voor DX12 deze titels word gekozen terwijl bekent is dat DX12 kapot is op Rise of the tomb raider. Wil je echt het verschil zien kies dan ashes of the singulirty op DX12, want deze titel laat echt zien wat DX12 gaat betekenen later.
GTX 1070;3;0.281554251909256;Er wordt ook enkel geconcludeerd dat je voor de snelste kaart bij de 1080 van NVidia moet zijn:
GTX 1070;1;0.41240498423576355;Vind ook dat tweakers moet laten weten dat een GTX 970 of een r9 390 kopen nu dom is, omdat de rx480 voor hetzeflde geld 1.5 keer de performance gaat leveren.
GTX 1070;3;0.3614090383052826;Enige benchmark die ik lag legde de RX 480 tussen de 970 en 980 in en ik verwacht dat hij ook niet sneller zal zijn dan een 390x tbh, alleen lager verbruik en ~240 euro.
GTX 1070;4;0.37930014729499817;Ik zag enkele benchmarks waar hij rond de 300puntjes hoger zit in FireStrike dan een 390x. Dus dat is zeer interessant.
GTX 1070;3;0.3184392750263214;Op het niveau van een 980 dus, dat is wel interesting voor 240 euro
GTX 1070;1;0.40996721386909485;laten we dan eerst maar eens echte onafhankelijke benchmarks kijken voordat je uberhaupt gaat beweren dat de rx480 1.5 keer de performance gaat leveren...
GTX 1070;2;0.322421669960022;"totdat er een review is mag Tweakers dat naturlijk niet zo direct zeggen ze kunnen natuurlijk wel zeggen dat het erop lijkt dat binnenkort AMD met een beter ""budget"" alternatief gaat komen. (welke ik inderdaad wss persoonlijk ga halen)"
GTX 1070;2;0.2976754605770111;Ik denk dat AMD wel een paar goede kaarten kan neerzetten. Ze zijn daar hard bezig, nu nog iedereen proberen te overtuigen. Ik zou bijvoorbeeld liever geen AMD kopen. Nvidia lijkt nog altijd meer en beter te zijn, hoor bij AMD systemen ook nog wel eens gezeur.
GTX 1070;2;0.4072946012020111;Dit is het probleem. Marketing, vraag de menige casual welke grafische kaarten zijn beter zeggen ze nvidida.... De r9 390 is nu beter dan de 970 ivm vram en word beter voor geopitmaliseerd en inkomende dx12. Amd moet gewoon heel hard aan zijn imago werken om ons als gamer te overtuigen. Eind dit jaar komen hun concurrente voor de 1080 aan en nu focussen op de mainstream
GTX 1070;2;0.37256669998168945;"""Ook onder belasting zijn de MSI-koelers buitengewoon stil, in tegenstelling tot de standaardkoeler van Nvidia, die aardig wat geluid produceert."" Hier had wel wat meer over gezegd mogen worden en 30 en 40 db zegt niet zoveel alleen dat 40 drie keer zo hard als 30 is. Iets van het klonk wel harder dan de MSI kaart maar was geen moment storend onder het browsen. Of onder gamen was het wel te doen en viel het geluid niet op maar je wilt deze kaart in een pc op je bureau op 50cm afstand hebben terwijl je browsed."
GTX 1070;1;0.5229246616363525;"40dB is 10 keer zo ""hard"" als 30 dB. dB is een logaritmische schaal. (Op basis van 10)"
GTX 1070;1;0.47817960381507874;Zeker weten? Stond me bij dat elke 3 db erbij een verdubbeling is van het geluid. Dus van 30 naar 40 is dan drie keer een verdubbeling. 10 x zo hard is dan idd een wereld van verschil wat een lezer er niet zo snel uit zou halen zonder kennis vandaar ook mijn opmerking om niet alleen grafieken te tonen.
GTX 1070;3;0.41918298602104187;Het is 10x de geluidsenergie en 3x de amplitude van de geluidsgolf bron dus het is eigenlijk allebei waar Maar vergeet niet dat onze oren ook logaritmisch werken dus van 30 naar 40 klinkt niet 10x zo hard. Verder is dB een relatieve schaal alleen bedoeld voor vergelijkingen tussen twee waarden, dus hoewel het inderdaad betekent dat 40dB 10x zo veel energie is dan 30dB, zegt het niks over de absolute waarde. Daarom wordt voor geluid (net zoals hier) meestal dB(A) gebruikt, dat ook nog eens speciaal is aangepast aan het logaritmische gedrag van onze oren. En voor radio heb je bijvoorbeeld dBm, waarbij 0 dB gelijk staat aan 1 milliwatt. Door de 0-waarde vast te leggen, kan je toch absolute waarden meten. Maar inderdaad zijn de stock koelers vrij luidruchtig. Ik heb ook een stock 970GTX in mijn Game PC (Alienware X51) omdat er geen grotere in passen met uitstekende heatpipes, dus de stock was de veiligste keuze omdat Dell die zelf ook levert. Maar inderdaad, je hoort hem wel. En ik game toch altijd met een hoofdtelefoon op dus #care
GTX 1070;3;0.2514682114124298;Ik weet niet hoe betrouwbaar deze bron is, maar van wat ik zie is 40dB nog steeds super stil
GTX 1070;3;0.39632123708724976;Hangt ervan af wat de reference was. Maar daar er dB(a) genoemd wordt zal er gewoon goed gemeten zijn. (hopen we dan)
GTX 1070;3;0.3019765019416809;Dat een 1070 tot bijna een factor 3 sneller is dan een 970 in de VR test van Project Cars verbaast me wel enorm. Wat een topkaart voor VR blijkt het te zijn! Nu ga ik toch twijfelen om te upgraden, racing games zijn totnogtoe gewoon geen optie in VR met een 970.
GTX 1070;2;0.31632307171821594;Ze hebben de 1070 al 2x zo efficient gemaakt met VR door te zorgen dat hetzelfde beeld niet 2x hoeft worden uitgerekend te worden (is mijn vage begrip ervan)
GTX 1070;1;0.4390335977077484;Die techniek heet SMP maar wordt, zoals in het artikel staat, nog niet gebruikt in games. Dus dat houdt in dat die fps-verdubbeling voor een hoop VR-games nog aan zit te komen ook! ik ga maar weer sparen
GTX 1070;2;0.26905500888824463;Mwah, VR staat nog in kinderschoenen, voordat er echte games voor zijn ben je zo 1/2 jaar verder en zijn de 1070/1080 kaarten alweer vervangen of veel goedkoper. Nu zie ik het nut voor een upgrade iig nog niet.
GTX 1070;3;0.3844093978404999;Maar zelfs al zou de RX480 niet in de buurt komen van 1080, voor 200 dollar verwacht ik dat ook niet. Dat is minder dan een kwart van de verkoopprijs van de 1080 dus als deze de helft zo goed is vind ik het al een goed product.
GTX 1070;3;0.5586281418800354;Hopelijk zorgt het voor wat prijsverlaging bij het groene team, tegen die tijd zullen die kaarten ook beter beschikbaar zijn.
GTX 1070;4;0.2937898337841034;Hij zal waarschijnlijk (ff afkloppen op hout) tussen de 970 en 980 inkomen, als je hem met vorige generatie Nvidias wil vergelijken. Mss kom je met twee 480s in Xfire ongeveer op het niveau van de 1070, en dat zou qua prijs dan nog aardig kunnen kloppen ook.
GTX 1070;2;0.40182244777679443;Als je de leaks in de gaten hebt gehouden zou een rx 480 over 1,5ghz kunnen oc'en. Dit tegenover een boost/bassisclock (ben ff vergeten welke het was) van 1,266ghz. Met deze oc zou de kaart dicht in de buurt komen van een 1070. Maar het blijven natuurlijk leaks dus die kan je met een heel groot korreltje zout nemen. Deze performance voor een price van 300$ (dat is wat ze verwachten voor deze superclocked kaarten) zal de price van de 1070 waarschijnlijk laten zakken naar MRSP voor sommige kaarten. De 1080 zal denk ik niet zo heel veel in prijs zakken omdat die kwa single card performance nogsteeds een stuk hoger ligt. Maar het blijven leaks dus wie weet. (gebed aan GaKa goden )
GTX 1070;2;0.3625115156173706;"""Uit de prestatie-index bleek al dat de gemiddelde framerate van de GTX 1080 in de door ons geteste games ongeveer een derde hoger ligt dan die van de GTX 980 Ti. "" Waar dan? ik zie dat niet hoor"
GTX 1070;3;0.35413631796836853;"Dat zal afhangen vanaf welke kant je rekent, maar in mijn interpretatie is de hier genoemde formulering wel juist 1000/746 = 1,34 -> 34% betekent min of meer ""gtx1080 is 34% sneller dan gtx980ti"" 746/1000 = 0,746 -> -25% betekent min of meer ""gtx980ti is 25% trager dan gtx1080"""
GTX 1070;1;0.2774759829044342;Ja, dan gebruik je een OC 1080 vs een stock 980ti.....
GTX 1070;3;0.6159079670906067;Het verschil is goed en wel 20% met een OC op beide kaarten, varieert wat per game. Met name games die op een redelijk zware engine draaien zoals CryEngine laten een kleiner gat zien tussen de 980ti en de 1080, soms zitten ze zelfs tot 12-15% van elkaar af. Dat komt met name doordat de 980ti meer resources ter beschikking heeft, op lagere clocks - bredere bus, meer shaders. Over het algemeen dus weinig reden om naar een 1080 te gaan als je nu een 980ti hebt die aardig klokt.
GTX 1070;4;0.4592796564102173;Zeer nette review, waarvoor dank. Ik had gehoopt dat er inmiddels enkele reviews zouden komen welke de verschillende custom 1080s en 1070s tegenover elkaar zetten in een vergelijking, doch begrijp ik dat het - net zoals de gemiddelde tweaker - het ook voor Tweakers.net lastig zal zijn om voldoende customs 1080s en 1070s in handen te krijgen voor een fatsoenlijke review. Gezien de huidige afwezigheid van voorraden bij de webshops (en het feit dat ik niet onnodig 100-150 euro extra wil neerleggen bij de webshops welke ze wel voorradig hebben) zal het waarschijnlijk nog een paar weken duren voordat ik mijn 1080 heb gevonden.
GTX 1070;2;0.40389421582221985;We hebben nog getwijfeld om de review later te plaatsen en te wachten op meer custom kaarten, maar omdat veel fabrikanten geen levertijden konden beloven hebben we dat toch niet gedaan.
GTX 1070;1;0.6250255107879639;Schiet inderdaad niet op met die custom kaarten. Founders editions zijn pas net beschikbaar en die partner cards staan zo'n beetje allemaal op levertijd >12 dagen. Binnenkort heb ik vakantie en zou dan graag een 1080 in m'n PC droppen maarja...
GTX 1070;1;0.35673102736473083;waar zijn de 4k benchmarks ? ik kan wijnig wijzer worden van deze review of de kaart nou echt veel beter is. en waarom staat de 290x in de lijst maar niet de 390x
GTX 1070;1;0.23943354189395905;De grafiekjes bestaan uit tabs, het is steeds voor elke game 1920, 2560 en 4k.
GTX 1070;3;0.5173630118370056;ok thankz
GTX 1070;3;0.3039552569389343;Ik vind dit aan de koper zelf om te beslissen of een stille kaart aantrekkelijker is.blindstaren op prijs/fps <> rust in huis
GTX 1070;3;0.4566956162452698;"Klopt. Je ziet wel vaker in reviews dat ze (voornamelijk) redeneren vanuit één perspectief; bij videokaarten wordt dus meer gekeken naar prestatie, en soms naar prijs/prestatie-verhouding, dan vanuit stilte of stroomverbruik. Op zich ook wel logisch, aangezien die prestatie toch het primaire doel is van een videokaart. Zolang ze maar wel duidelijk maken inzichtelijk maken voor de lezer wát die afweging is, en de moeite nemen om andere kengetallen (zoals stroomverbruik of Decibel) ergens te noemen, al is het maar in een tabelletje of feitenlijstje onderaan het artikel. Dan kan een consument zoals jij die daar wél naar kijkt altijd zelf zijn afweging nog maken."
GTX 1070;2;0.31563040614128113;Ik mis een partner board van een 980ti bv Gigabyte 980Ti Gaming 6G Windforce, nu zie ik eigenlijk nog steeds niet of het een waardige upgrade is voor 1440p @60fps.
GTX 1070;3;0.47200170159339905;Nette review alleen mis toch wel de vergelijkingen met de GTX 970 in de benchmarks. Heel jammer.
GTX 1070;3;0.3976786732673645;Ik vind het voorbarig om de RX480 nu al the neglecten zonder dat we hiervan benches gezien hebben. Voor mij betekent de RX480 een mogelijkheid om de tijd te overbruggen tot aan de volgende upgrades. Ook al zijn deze kaarten met name geschikt voor VR, ik vind het toch fijner om niet gelijk toe te happen in generatie-1 met deze VR-voordelen. Ik wil een buffertje hebben voor VR als ik hier een kaart voor koop, de ti-versies lijken mij pas echt de voordelen te gaan brengen van deze eerste 16nm's.
GTX 1070;1;0.5599218606948853;Waarom denk je dat. Omdat AMD het belooft of omdat iedereen het zegt. De drivers van AMD zin klassiek slecht bij introduktie van nieuwe modellen. Het is dus echt wel afwachten en wat er niet is kun je ook geen rekening mee houden.
GTX 1070;2;0.4143582284450531;Omdat €200,- voor een 6,5 beter is om risico's te vermijden dan een 8 voor €500, en waar bij de laatste je zeker weet dat er nog niet alles uit gehaald is. Ik koop liever de AMD nu en dan verdien ik later die €200 wel weer terug nadat de 1080 ti uit is. En tegen die tijd zijn de brillen ook waarschijnlijk weer een paar honderd goedkoper.
GTX 1070;3;0.5903084874153137;Jammer dat jullie geen 980SLI setup in de benchmark hebben meegenomen. Wel interessant om te zien hoe die prestaties zich verhouden ten opzichte van de 1080.
GTX 1070;2;0.3904440999031067;Mooi dat jullie eindelijk ook de kaarten gereviewt hebben. Enige wat ik jammer vind aan deze test is dat de 980ti die is meegenomen een stock edition is die hier tegenover een custom kaart wordt gezet. (daar ga ik van uit op basis van de geluidsproductie). En we weten van custom 980ti's dat ze enorm goede overclockers waren, 1400-1500 mhz was meer de regel dan de uitzondering. Ik had wel eens willen zien hoe de 1070 het daar tegen zou doen. Daarnaast vind ik het gewoon eeuwig zonde dat AMD heeft besloten om voor de rest van het jaar geen high end kaart uit te brengen (het wachten is op Vega). Dit is niet goed voor de concurrentie en ik denk daarom dat de prijzen van de Pascal kaarten een stuk langer dan anders hoog zullen blijven. Wellicht dat ze ook nog langer zullen wachten met het uitbrengen van de 1080ti, als je immers al de snelste kaart op de markt hebt is het stom om de 1080ti daaroverheen te gooien. Dan kannibaliseer je, je eigen verkopen. Een suggestie voor toekomstige reviews: Een lijst met alle kaarten die je in je grafiek toont, en dan precies het type zodat we weten of het custom of reference kaarten zijn waarmee de kaart vergeleken wordt.
GTX 1070;4;0.4913327693939209;Goed punt appels en peren.
GTX 1070;2;0.43704143166542053;Ze zouden deze keer écht aandacht spenderen aan hun review van hun GPUs. Want tja, die waren in het verleden nou eenmaal niet zo sterk. Oh, en er is uiteraard sprake van een gigantische vertraging. En om dan te lezen dat ze reference kaarten met aftermarket ontwerpen vergelijken... Tja daar word ik dan toch een beetje verdrietig van. En kun je nagaan, ik had mijn verwachten al niet hoog staan. Triest.
GTX 1070;2;0.4608789384365082;Wat is er aan de hand met de Project Cars benchmarks? Vreemde resultaten.. Trouwens geinige kaarten maar voor de huidige prijs minder rendabel.. Zeker gezien de prijs die beloofd werd.
GTX 1070;4;0.48425546288490295;Fijn om de VR testresultaten te zien! De GTX1070 is een mooie teamspeler voor de Oculus Rift :-) Nu nog even duimen dat de GTX1070 kaarten leverbaar en daardoor ook betaalbaarder worden en dat de Rift preorder eindelijk geleverd gaat worden.
GTX 1070;3;0.6576923131942749;Beetje jammer dat de sli opstelling niet op high of ultra is getest op 4k. Had graag willen zien of de gtx 1080 in sli ook op 4k in high of zelf ultra settings ook gewoon 60fps uit de kaarten weet de persen.
GTX 1070;2;0.4332916736602783;In de review staat: AMD heeft de nieuwe RX480 al wel aangekondigd, maar het is Nvidia dat aan het langste eind trekt en zijn nieuwe videokaarten, de GTX 1070 en 1080, daadwerkelijk in de schappen heeft liggen. Dat vind ik echt niet zo, de gtx 1000 lijn is niet daadwerkelijk in de schappen, je kan ze amper krijgen nog. En gene die het wel leveren, is het allemaal veel te duur.. Bij de meeste shops staat nog steeds pre-order of levertijd 3 weken
GTX 1070;2;0.48257899284362793;Erg jammer dat onder andere The Witcher en GTA 5 niet in ultra settings wordt getest. Ook mis ik de GTX 970 wel een beetje in de benchmarks, denk dat er veel mensen met een 970 zijn die duidelijk willen zien of een 1070 kopen voor hun zin heeft.
GTX 1070;5;0.29419371485710144;Gezien 3D printers en 3 tot 5 assig CNC freesbankjes steeds meer een normaal huis vinden vraag ik mij ook af hoe programma's zoals Autocad en Solid works profijt hebben van deze kaart tov een nVidia Quadro. Natuurlijk is het een gamerskaart maar 3D CAD is ook steeds meer gebruikt.
GTX 1070;1;0.586798906326294;hoe brak kan je een vergelijking tussen resoluties maken ?? Zelfs al is het onspeelbaar traag op 4K, laat dat dan tenminste uit het cijfermateriaal blijken ipv naar medium terug te schroeven om dan plots een HOGERE framerate te noteren dan voor 2.5K waardeloos vergelijingsmateriaal, dus maar weer uitwijken naar een andere site waar ze wel een rechte lijn trekken in hun benchmarks onderling
GTX 1070;5;0.33302727341651917;Mee eens. Onbegrijpelijk
GTX 1070;1;0.42375633120536804;De toon is gezet Afhankelijk is een groot woord. AMD heeft zelf meeontwikkeld aan zijn eigen 14nm tech met afgescheiden dochter GloFo (en diens BFF Samsung). Heeft ook een flinke duit in het zakje gedaan bij TSMC's 16nm tech. Mag er wel bij vertellen dat Nvidia met een fabrieksgemiddelde communiceert, AMD communiceert met een technische max. Voordat je straks in de Polaris review gaat roeptoeteren dat de Polaris meer vermogen voorschrijft. Benoemd ff vlug tussen neus en lippen dat SLI softwarelocked op 2 is. Komt verder niet terug in de review of conclusie. Ook niet wat dit betekend voor de multi-gpu features van DX12... Ow wacht, DX12 support is nog steeds waardeloos. Dan maar weinig woorden er aan vuil maken en op naar de DX9(DX11) games. Opvallend ook dat er geen Fury kaarten in multi-setup voortkomen, maar wel de 1080 in SLI. Schijnbaar mag een Nvidia kaart niet slecht presteren, dus dan maar het spel schrappen (?). Geen idee ook waar AoS niet als gewoon spel is meegenomen. Vergeet er voor het gemak ff bij te vermelden dat AMD nog niet genoodzaakt is geweest om oude voorraden weg te doen door prijzen drastisch te verlagen. Prijzen van 1070/1080 worden ook uiterst gunstig aangehouden, vrij letterlijk in de sidebar voor 518/779, 525/750 lijkt me een realistischer gemiddelde. Geen idee waar je een FuryX voor 580 haalt (als je m uberhaubt nog ergens kan halen). Van te voren werd er trouwens nog hard geroepen dat de FE kaarten duurder zouden zijn dan die met vendor-specific koelers. Dat is duidelijk niet het geval. Was ook wel het noemen waard geweest. Hilarisch.. Is daar ook niet voor bedoeld. Dump 'm straks maar eens in een 3weg opstelling, ben benieuwd wat dat gaat doen tegen de 1080.
GTX 1070;5;0.3189561665058136;Topreactie! Ik ergerde mij precies aan dit soort punten. Twee dure kaarten tussen neus en lippen met een kaart van 200 dollar vergelijken om het goedkope kaartje als een misgeboorte af te doen. Ik verwacht geen totale objectiviteit maar dit artikel is mij veel te subjectief geschreven.
GTX 1070;2;0.35193192958831787;"Citaat: ""Ga je voor de snelste kaart van dit moment, dan zul je daar dus aardig wat voor moeten neerleggen en we vermoeden dat Nvidia zich voorlopig niet gedwongen voelt om de prijs te laten zakken."" Dat laatste lijkt me dan wat optimisch van Nvidia. Want verreweg de meeste klanten zullen wel degelijk naar price/performance verhouding kijken, en dan wordt de RX 480 toch een behoorlijke concurrent... die gaat véél beter presteren wat dat betreft, gebaseerd op de nu gepubliceerde benchmarks en prijzen. En er is ook nog zoiets als crossfire. Duw er 2x RX 480 in en je bent nog steeds goedkoper uit. Dus hoeveel mensen gaan daadwerkelijk € 700 voor een videokaart betalen? Ja, die zijn er best wel wat tegenwoordig, maar ik neem aan dat Nividia er veeeel wil verkopen."
GTX 1070;2;0.3687935471534729;ik zei de gek XD. OT: je weet dat 1x 1070 ook bijna aan dat bedrag zit en dan nog minder stroom verbruikt ook met zga de zelfde prestaties als 2x RX480? uiteindelijk is de 1070 dan wel goedkoper maar dan puur door de stroom kosten uit eindelijk. tenzei de RX480 ook de helft aan de stroom gebruikt van een 1070
GTX 1070;3;0.3981418311595917;"De 1070 is inderdaad wel wat goedkoper ja. Ik zie in de Pricewatch diverse modellen staan variërend van (minimaal) € 470 tot (minimaal) € 520 bij de goedkoopste aanbieders. Er van uitgaande dat de Rx 480 daadwerkelijk rond de € 200 gaat kosten (x 2 = rond de € 400), is dat nog wel ietsje goedkoper dan een Gtx 1070, maar goed ik besef me dat ""rond X euro"" een breed begrip is. Het is dus ook even afwachten wat de 480 echt gaat kosten. Hoe dan ook, mijn punt is inderdaad, dat voor het overgrote gedeelte van de consumenten de afweging van prijs/kwaliteit, inclusief de vragen ""hoeveel ga IK er daadwerkelijk van merken en hoe essentieel vind ik dat?"" nog wel zeker een rol zullen gaan spelen. Zeker voor de Gtx 1080 zal dat betekenen dat verreweg de meeste consumenten voor een prijskaartje van € 700 nog wel 2x zullen nadenken, en andere alternatieven gaan afwegen... inclusief de 1070 dus die toch alweer een flink stuk goedkoper is inderdaad. Voor mezelf sprekend, ik heb nu een HD 4890 waar ik nog redelijk wat games behoorlijk op kan spelen. Het begint nu een beetje te nijpen, maar goed, ik heb de kaart ook al sinds mei 2009 (= 7 jaar dus!!). Ik zit er dus nu over te denken om te gaan upgraden, maar dat zal eerder een upgrade worden van € 200 á € 250. Een prijs van € 700 heb ik gewoon echt niet over voor een videokaart, zeker niet als je ziet hoeveel stappen ik voor pakweg € 200 vooruit kan zetten! De Rx 480 is dus zeker ook een kandidaat die ik even afwacht. Moet je eens kijken hoeveel meer performance ik dan heb ten opzichte van een HD 4890 die me ook tot op de dag van vandaag nog goed genoeg voor mijn gaming-maatstaven heeft kunnen bedienen. Ik ben (dus) niet het type gamer dat nog eens € 400 á € 500 EXTRA gaat neertellen voor net die paar fps of resolutie méér."
GTX 1070;3;0.4165019392967224;voor mij was dat dus wel het geval. met basis hardware van 8 jaar oud was ik toe aan een nieuwe top of the line computer. daar heb ik dus de nieuwste I7 in gezet met (foutje van de winkel) 64gb aan ram geheugen, en de Videokaart was een GTX 780. echter had ik al een nieuw scherm gekocht en ik heb benchmarks bekeken en 3440x1440 kan de GTX 1070 wel aan maar de 1080 was voor mij gewoon de keuze om er zeker van te zijn dat ik niet zo snel een 2e kaart erbij hoef te zetten. en ja 2x een GTX1070 zal meer performance hebben dan de 1080 en ook voor ongeveer de zelfde prijs. maar ik zit net wat confortabeler met maar 1 vid in dat geval. vandaar dat dat mijn keuze was maar als ik op 1080P had blijven gamen was de 1070 meer dan voldoende geweest.
GTX 1070;3;0.4075542688369751;Maar met 2x480, ga je over de 1070 heen. Waarschijnlijk tipt het aan de 1080. En dat is erg gunstig. Ik denk dat we ff de release van amd moeten afwachten, maar ik denk dat de 480 de winnaar gaat worden qua prijs/speed.
GTX 1070;2;0.4345880150794983;"Voor mij persoonlijk gaat de Rx 480 zoals het er nu naar uitziet sowieso winnen, omdat die een stuk betaalbaarder is, voor mij qua prestaties méér dan genoeg stappen vooruit maakt, ten opzichte van mijn huidige HD 4890 En mede daardoor dus qua prijs/kwaliteit verhouding voor mij niet te overtreffen is, door een kaart van 500 á 700 Euro (wat ik er gewoon niet voor over heb). Iets waar iemand anders mij echter wel terecht op wees ten aanzien van 2 kaarten in Crossfire/SLI plaatsen: het is altijd maar afwachten hoe goed dat ondersteund wordt per game. Bij de ene game haal je bijna 2x zoveel prestatie als met 1 kaart, bij de andere game krijg je helemaal geen beeld totdat je de Crossfire/SLI uitschakelt en dus noodgedwongen terug gaat naar 1 kaart. Dus de redenering ""met 2x Rx 480 ben ik nog steeds goedkoper uit, en presteer ik misschien wel bijna net zo goed of beter als een 1070"", gaat maar gedeeltelijk op, voornamelijk dat tweede gedeelte van die zin."
GTX 1070;2;0.3301245868206024;"""Dus de redenering ""met 2x Rx 480 ben ik nog steeds goedkoper uit, en presteer ik misschien wel bijna net zo goed of beter als een 1070"", gaat maar gedeeltelijk op, voornamelijk dat tweede gedeelte van die zin. :P"" Het is mogelijk. Vandaar ook mijn zin: ""Ik denk dat we ff de release van amd moeten afwachten, maar ik denk dat de 480 de winnaar gaat worden qua prijs/speed."""
GTX 1070;2;0.4613456726074219;Ja afwachten tot de release van de Rx 480 is sowieso een goede idee, maar mijn punt dat Crossfire/SLI maar gedeeltelijk (niet door alle games, of niet even goed) ondersteund wordt, is een algemene. Die zal niet ineens veranderen met de release van de Rx 480 denk ik?
GTX 1070;2;0.4952441453933716;Nja, volgens mij is het een algemene dat de meeste games vrij goed schalen. Winsten tussen de 130 en 190% zijn al gauw mogelijk. Maar er zijn zeker ook games waarbij schaling weinig tot niets doet. Echter moet ik die spellen nog spelen. De games die ik zelf draai, draaien stuk voor stuk beter in CFX. Het is dus voor de gamer altijd een afweging of zijn/haar gebruikte opstelling past bij de interesse in de games denk ik.
GTX 1070;5;0.26110929250717163;Heb nog nooit zo'n stelletje zuurpruimen massaal zien verzamelen onder een review die ze zelf niet beter doen. Ga ff iets leuks doen ofzo, vrolijk jezelf op aub. Zou hier ook geen moeite meer in steken met zulk publiek, dan is de lol om moeite in je artikel te steken er wel snel vanaf.
GTX 1070;2;0.3961543142795563;"Ach ja, het geeft natuurlijk ook een vertekend beeld, want wie neemt er voornamelijk de moeite om onder een artikel te reageren? Voornamelijk NIET de mensen die denken ""goede review, bedankt voor de nuttige info"" of die denken ""ik heb eigenlijk niets toe te voegen""."
GTX 1070;3;0.5028167366981506;Armando Ferreira heeft de kaart ook gereviewd, zijn conclusie was dat het niet een grote upgrade is van de vorige versie. Uiteraard wel een goede aanschaf als je een veel oudere kaart hebt of een nieuw systeem aanschaft. Echter had ik het idee dat ze er meer van hadden verwacht.
GTX 1070;3;0.6032195687294006;Dit leest inderdaad als een genuanceerd verhaal, zoals Tweakers in de volgende paragrafen ook aangeeft. De pre-emption wordt fijnmaziger uitgevoerd, waardoor als gevolg van dit pre-emption proces kleinere correcties hoeven te worden uitgevoerd op al berekende output. Keerzijde is dat het proces zelf veel actiever moet zijn om zo fijnmazig te kunnen ingrijpen. Zoals ik het lees zal het huidige proces meer een continue load voor de kaart betekenen, terwijl het voorgaande proces door de grove correctie meer met bursts zal hebben gewerkt. Voor de overall performance een beetje koffiedik kijken. Wat mij aan de verandering wel aanspreekt is dat ik verwacht dat de fijnmazigheid kan helpen beeldafhandeling vloeiender te laten verlopen, met kwalitatief betere rendering tot gevolg. Zelf vind ik dat wel belangrijk, omdat mijn bias meer bij kwalitatieve rendering ligt. Fps-sen zijn ook belangrijk, maar in het verleden is de renderkwaliteit daaraan nog wel eens ondergeschikt gemaakt.
GTX 1070;5;0.3457888662815094;Super dat jullie ook vr benchmarken tweakers! Vraag nee af hoe dat dan werkt want hoofdbewegingen hebben toch effect op wat er gerendert moet worden hoe standaardiseren jullie die dan? In de toekomst stel ik voor dat jullie ook elite dangerous in VR meenemen. Met hoge settings zou die denk zelfs een 1080 op de knietjes moeten kunnen krijgen.
GTX 1070;1;0.7533189058303833;Even een domme vraag misschien, maar waarom zijn de kaarten zo slecht leverbaar? Was dat ook zo bij de launch van de 980 en 970? Of zijn het eigenlijk nog steeds de eerste paar exemplaren en zijn ze nog niet echt verkrijgbaar voor de customers?
GTX 1070;3;0.5150008201599121;Redelijke review, meer kan ik er niet van maken. Test zou wat uitgebreider kunnen zijn, zeker na zo lang wachten, aan de andere kant, wat kan je dan nog schrijven wat al niet door andere sites gemeld is? Voor de toekomst hoop ik dat Tweakers iets sneller is met reviews zonder dat het ten koste van de kwaliteit gaat. Kom op, even de banden met Nvidia en AMD versterken Tweakers! Of doe zoals voorgesteld, een beroep op de community. Alles beter dan hoe het nu gegaan is.
GTX 1070;4;0.5171955227851868;Mooie review en testen. Leuke uitslagen. Is het de moeite waard om over te stappen op een GTX 1070? vanaf een Radeon R9 290. Ik heb een bod via marktplaats voor €175 momenteel. Ik speel op Full-HD
GTX 1070;2;0.31878072023391724;Hoe kan het nou dat er zo een klein verschil is in performance met de VR benchmark tussen de 1070 en 1080? Project Cars VR, heeft 4 fps verschil tussen msi 1070 en msi 1080? Adr1ft, heeft 1 fps verschil tussen msi 1070 en msi 1080? Ik voel me best genaaid met mijn msi gtx 1080, sinds ik het voor VR gekocht heb
GTX 1070;2;0.44982093572616577;"Ik vind het erg jammer dat de reviewer AMDs nieuwe kaarten naast die van NVIDIA legt, terwijl het om een compleet ander marktsegment gaat. Daarmee lijkt de reviewer te zeggen dat AMD ondermaats heeft gescoord terwijl het appels met peren vergelijken is. Wel wordt er geschreven dat de 1070 een betere keuze is dan de 1080 gezien de meerprijs van de 1080 niet evenredig performance oplevert. Tja, dat zou je dan toch ook door kunnen trekken naar de AMD 480 met een veel kleiner prijskaartje? Hoewel AMD en NVIDIA aartsrivalen zijn, en de stoten die links en rechts worden gegeven iedereen doet lekkerbekken, hoort AMD niet thuis in deze review; in ieder geval niet zoals deze nu wordt weergegeven."
GTX 1070;2;0.5623285174369812;Ik ben persoonlijk wat teleurgesteld met de GTX1080. Het is weinig meer dan een overklokte Maxwell (GTX980). Kijkende naar puur de Cuda cores en de kloksnelheid. de kloksnelheid van de 980 was 1216MHz, die van de 1080 is 1733MHz. 1733/1216= (ongeveer) 1.4, dus 40 hoger. De 980 had 2048 Cuda cores, de 1080 heeft er 2560. 2560/2048= 1.25 (25% meer). Vervolgens kun je het simple sommetje maken, 1.4*1.25=1.75, ofwel, puur op cuda cores en kloksnelheid moet de kaart 75% beter presteren dan de 980, en laat dat nou ongeveer zijn wat ie doet. Dan kijken we naar de prestatie index van Tweakers, de 980 staat op 0.564, 0.564*1.75= 0.987. De 1080FE haalt 0.953, net iets minder dan je zou verwachten. Ik zie niets anders dan een overklokte Maxwell GPU hier en dat stelt wat teleur.
GTX 1070;3;0.42729315161705017;Leuke review om te lezen en handig dat de testresultaten per resolutie een eigen tabblad hebben, top! Op dit moment staat de line up van AMD er triest bij, niet de snelste kaart, niet de beste prijs prestatie verhouding. Hoop voor AMD en voor ons dat de RX480 daar snel verandering in gaat brengen want die prijzen voor de GTX 1070 en 1080 vind ik te hoog.
GTX 1070;2;0.34985145926475525;Zouden de Non-reference edities niet goedkoper worden dan de FE edities??
GTX 1070;2;0.41390886902809143;"Je bedoelt dat de non-reference kaarten goedkoper zouden zijn dan de reference kaarten (tegenwoordig dus founders edition), en dat klopt helemaal. Maar ik denk dat Nvidia niet had begrepen hoe veel geld mensen over hebben voor een kaart die er helemaal ""gaming"" uitziet. Andere voordelen zijn dus geluidsproductie, verlichting en een hogere standaardklok."
GTX 1070;1;0.6558144092559814;"Het stomme is dus nu dat al dat ""extra"" geld allemaal naar MSI, ASUS etc. etc. gaat ipv dat het goedkoper wordt"
GTX 1070;3;0.31141459941864014;FE is toch Reference?
GTX 1070;1;0.5417328476905823;Sorry, ik bedoelde de after market edities
GTX 1070;5;0.4479990303516388;Founders Edition == Reference Edition
GTX 1070;4;0.31524255871772766;Zie mijn reactie hierboven
GTX 1070;3;0.2778182923793793;En watvoor voeding heb je daarbij nodig, is 700 genoeg of meer? wordt Eneco blij van
GTX 1070;3;0.35750943422317505;De nieuwe kaarten zijn juist een stuk zuiniger dan de vorige generaties, vanwege het 16nm proces.
GTX 1070;5;0.42896974086761475;Staat duidelijk hier. 400W voeding is genoeg.
GTX 1070;1;0.384357750415802;SLI heeft minder performance dan 1x MSI gaming versie? Edit: Hoezo is dit nou weer -1, ik ben gewoon verbaasd.
GTX 1070;2;0.41863036155700684;SLI is eigenlijk overbodig op 1080p, zoals je kunt zien bij 2K/4K waar de GTX1080 SLI ineens vleugels krijgt als je b.v. bij Project Cars kijkt zie je ineens dat de FPS op 4K hoger ligt dan bij 1080p, daarbij is het ook weer erg afhankelijk van de engines die gebruikt zijn voor spellen. De Witcher 3 (REDengine) engine schaalt zeer goed met SLI (Crossfire ook) met de laatste drivers/game patches. Ook had ik wel willen weten bij de SLI tests of de 'game experience' niet lijd onder de enorme verschillen tussen min/max fps bij sommige benchmarks is het verschil tussen min/max fps bijna 50%. Bij bepaalde titels is het toch heel fijn om niet alleen een 100+ fps te hebben maar ook te houden (als voorbeeld) bij BF4 merk ik wel als mijn FPS niet op ~100 zit maar ineens op 60 zit dan voelt het 'stroef' aan. Nou vind ik het ook jammer dat er geen G-Sync of Freesync monitor gebruikt is. En DX12 heeft (nog) geen SLI/CFX support dus heeft het weinig zin om te testen.
GTX 1070;1;0.45448678731918335;Hmmm,,, Geen 980TI in SLI Geen 980 in SLI Geen TitanX (ook niet in SLI) Geen FuryX in CF Geen 290X in CF Geen 380 in CF Het rijtje met de geteste kaarten is erg schaars wel. Bovenstaande lijken me erg leuke tegenhangers, of referentie testmodellen/configuraties. Zou zomaar kunnen dat een 290X in CF tot aan de 1070 komt, of er zelfs overheen gaat. En dan kun je je dus afvragen of Nvidia echt de snelste is, voor het geld........ik vraag het me af.
GTX 1070;3;0.3997938930988312;Klopt ik heb zelf ook een 290x CFX setup en kan bar weinig _recente_ benchmarks vinden die dat dus vergelijken. Bij een upgrade verwacht ik minimaal dat 1 GTX1080 mijn 290x CFX overtreft. Daarbij komt ook dat de meeste benchmarks al ouder zijn, oudere drivers etc. Zelf zie ik ook wel dat bepaalde titels een boost hebben gekregen (of CFX support) met latere drivers. Zo ook is het 'min/max fps' verhaal best belangrijk.
GTX 1070;1;0.5412827730178833;Wat ik me eigenlijk afvraag is of nvidia nog kaarten uit gaat brengen die rond de 50 Watt verbruiken? Voor de mensen die niet de intel GPU willen gebruiken maar ook geen rekenmonster als GPU nodig hebben. Wellicht zelfs nog passief gekoeld. In het verleden had je nog de 710, 720, 730 en 740 series, maar hierna heeft nvidia nooit meer deze lage series uitgebracht.
GTX 1070;5;0.3669167160987854;Wow wat een zure reacties zeg.
GTX 1070;2;0.35714176297187805;Inderdaad zeg, mijn god. Klagen dat Tweakers afzakt naar een niveautje Nu.nl, maar de gemiddelde reageerder hier zit ook zo'n beetje op het niveau NuJij
GTX 1070;2;0.581623911857605;Begin me ook per dag meer te ergeren aan de reacties hier naar de redactie toe. Er is bar weinig respect meer en dat is jammer. Dat ze soms steken laten vallen is duidelijk maar is dat nu een reden om maar onbeleefd te worden?
GTX 1070;3;0.4658755958080292;"Soms? O, wel. Dat laat ik even in het midden. Wat ik o.a. wel mis is een uitgebreide conclusie; dit is nu nogal summier. Zoals ik niets lees in de conclusie over de SLI performance en/of het gebrek eraan. Zomaar een voorbeeld."
GTX 1070;1;0.5921130776405334;Voor een SLI performance hebben ze 2 kaarten nodig brein... En die hebben ze niet. Dus kunnen ze geen SLI performance geven. My god zeg... Ook het licht niet uitgevonden he??
GTX 1070;1;0.4303373098373413;"Sorry, maar ik zie toch echt SLI-scores van de GTX 1080. Wie is er nou blind hier Zal je ff op weg helpen: ""GeForce GTX 1080 SLI"" staat er bij."
GTX 1070;2;0.5188214182853699;Als de concurrentie consistent sneller en meer inhoudelijk is dan is het duidelijk dat je daar meer aan hebt. Dat de 10x0 nu nog zo duur zijn heeft grotendeels met het beperkte aanbod te maken. Gek genoeg zijn de 1070 OEM nog duurder dan de FE terwijl je zou verwachten dat die ong €50 goedkoper zouden zijn. Er zijn een paar prijsvechters zoals een KFA2 die de laagste prijzen hebben en waarschijnlijk andere merken ook met hun prijzen moeten gaan zakken om marktaandeel te houden. Wil je minder betalen zul je nog wel even moeten wachten totdat de prijzen naar ong. €400/€600 zakken. De enthousiasts die niet willen wachten hebben de hoofdprijs betaald. Daar hoor ik dan ook bij.
GTX 1070;1;0.40111401677131653;Ik zit niet echt meer in de gaming, maar deze kaart is wel een beest! Dikke Macboe wacht op antwoord (van AMD)!
GTX 1070;3;0.40000125765800476;Ik ben benieuwd hoe de 1070 in Star Citizen performt. Dat zou voor mij toch wel de grootste reden zijn over te gaan op een nieuwe kaart.
GTX 1070;2;0.368935227394104;?huh? Das gek? Hier staat dat de 1080 project cars draait op 80-90 fps high sett.. Op 1920x1080 Ik draai project cars op ultra met 2xmxaa op 1920x1080 op 70 fos met mijn gtx 960 (ge oced naar 1420mhz core 1485 boost) Ik had gedacht dat de 1080 veel sneller zou zijn?
GTX 1070;3;0.6621522307395935;Er is iets mis met de balkjes van jullie charts. Bv. Alien: Isolation - 3840x2160 - GTX 980 Ti en Fury X zijn even snel en toch is het balkje van de GTX 980 Ti langer. De GTX 1070 is dan weer sneller dan de GTX 980 Ti maar het balkje vrijwel even lang. Bv. Project Cars - 1920x1080 - Twee R9 kaarten en de GTX 1070 hebben ongeveer 74fps maar toch is de GTX 1070 balk net iets langer. Duidelijk is dat er iets verkeerd gaat bij het maken van de balk in relatie met de minimum FPS.
GTX 1070;5;0.5083492994308472;dan is mijn geforce 4 ti4600 toch eens aan vervanging toe denk ik, maarja doe t dan ook nog met een pentium 4 computer met agp aansluiting haha. win 8.1 draait toppie.
GTX 1070;1;0.4005744755268097;Geen test met de vive? wtf?
GTX 1070;1;0.5112941265106201;Geen Asus strix 1070 getest?
GTX 1070;1;0.3271648585796356;Ik denk AMD wacht tot volgend jaar om nvdia en intel pijn gaat doen met hun sterke producten en ook nog goedkoper dan hun en meeste mensen die nu zonder geduld dit spullen gaat kopen gaan ook de pijn voelen,AMD weet dat hun processor is snel en hun gpu, 2017 wordt spannende jaar voor AMD...
GTX 1070;1;0.3181041479110718;GTX 1080 van MSI? Hmmm....
GTX 1070;1;0.29719629883766174;"Ik lees nu net op verschillende sites dat MSI en Asus de GTX 1080 ""review"" kaarten die ze aan de persleden hebben gegeven, zelf hebben aangepast zodat ze (iets) hogere kloksnelheden hebben, waardoor ze beter uit de test komen. Bron: link, en link. Is dit te achterhalen bij jullie?"
GTX 1070;3;0.33455589413642883;"GTX 980 TI is via Amazon Deutschland al voor 450 euro te krijgen, goedkoper dan de prijs die hier in de prestatie index gehanteerd wordt. Daarbij is de GTX 1070 niet voor 500 euro te krijgen; tel er minstens 50 bij op. Dit maakt de prestatie/prijs verhouding met de GTX1070 nagenoeg gelijk. Voor mij een makkelijke keus: met de GTX 980 TI krijg je bovendien veel meer cuda's (in een lagere frequentie weliswaar). En niet onbelangrijk de Pascal architectuur moet zich nog bewijzen, daar waar de vorige generatie goed is uitontwikkeld."
GTX 1070;2;0.5437920093536377;Terwijl iedereen loopt te twisten over wat nou sneller is of wat de beste prijskwaliteitverhouding is, blijft het aanbod laag van alle nieuwe gtx kaarten en is de prijs belachelijk hoog.
GTX 1070;3;0.31197795271873474;Ik denk dat in heel wat benchmarks bij online reviews dx11 wordt gebruikt. Weet iemand of er ergens Battlefield benchmarks zijn gemaakt met Mantle (op 4K)?
GTX 1070;1;0.5646752119064331;Het gemiddelde intellect van een hoop tweakers is aan deze reacties te zien bedroevend laag. Kunnen mensen nou niet op een normale manier reageren? Een vraag stellen van ''goh, had dit misschien nog handig geweest''? Het is echt om te janken wat een ontzettend kinderachtige reacties er worden gegeven. Er is gewoon een degelijke review gegeven, waarin ik als persoon die op zoek is naar een nieuwe videokaart, wederom bevestiging krijg dat de 1070 een goede keuze is als je budget dat toe laat. Al die 980ti fans die maar lopen te blazen dat die sneller is, en beter... Nou en dat die misschien even snel is in sommige benchmarks wanneer die overgeclocked wordt. De 1070 is goedkoper. Wat wil je nou? Ik zeg: grow up!
GTX 1070;1;0.687520444393158;Eensch. Het is niet de inhoud maar vooral de vorm wat in dit draadje nogal lomp en arrogant is. Het feit dat je -denkt dat je- een punt hebt geeft je niet het recht om je manieren links te laten liggen en eigenlijk te gaan zeiken en afkraken. Daarbij snap ik ook inhoudelijk niet wat er nou zo vreselijk mis is aan deze review. Dus andere reviews hebben andere of betere info... Soi. Dan deel je die info. Punt.
GTX 1070;1;0.5003765821456909;De 1070 is helemaal niet goedkoper.
GTX 1070;1;0.4310351610183716;Dat is ie wel De prijs van een 980ti zit op ongeveer €525 / €550 gemiddeld. €550 voor de betere. De prijs van een GTX1070 zit nu op €520 gemiddeld. Dus goedkoper. Daarnaast is de consumentenadvies prijs van die kaart €430 (afgerond). Dat ze nu duurder zijn is omdat ze net uit zijn. Maar zullen snel genoeg een stukje zakken. Kortom, nu al goedkoper en over een enkele maand een paar tientjes goedkoper.
GTX 1070;1;0.4000546336174011;"Ja, meer mensen zouden moeten denken: deze review bevat informatie, ik doe daar (al dan niet) mijn voordeel mee, that's it. Maar sommige mensen reageren alsof de review een politiek stuk is dat voornamelijk om een mening draait, en waar dus persé op gereageerd moet worden om dingen recht te zetten! Het is gewoon consumenteninformatie, niet meer, niet minder, waar je (ook als je het niet met iedere zin in het artikel eens bent) je voordeel mee kunt doen om je aankoopkeuze al dan niet te beïnvloeden. ""Ja maar een andere kaart presteert op dit of dat onderdeel, of door die en die bril bekeken veel beter, en dat had hier vermeld moeten worden!"" Joh... als die informatie die jij mist nou bij jou wél in je hoofd zit, dan weeg je die toch mee bij jouw keuze? Good for you, succes zou ik zeggen! Waarom hebben mensen toch zo'n behoefte om hun eigen voorkeur zwart op wit bevestigd te zien in elke review die er over schrijft? Mensen reageren en doen daarbij alsof ze in feite in het belang van anderen reageren, zo van: die info die ik noem is nuttig en had in het artikel gemoeten zodat anderen ook kennis van die info kunnen nemen. ""Niet voor mijzelf, want ik ben mega slim en wist dit natuurlijk zelf al, daarom reageer ik ook! Wat ben ik, en mijn inbreng, toch nuttig en belangrijk..."" Terwijl het af en toe gewoon een verkapte fanboy-discussie wordt, vooral de manier waarop de discussie gevoerd wordt. Dat gaat er helemaal niet meer om medelezers te helpen, het gaat dan gewoon om je gelijk te halen."
GTX 1070;1;0.4078507721424103;"@JustTweak & RagingR2. Herken ook wel wat jullie zeggen. Op zich kunnen in de review conclusies getrokken zijn waarop wat is af te dingen. Of zijn er in een review aspecten niet meegenomen. Ik had graag de 970GTX willen zien in de grafieken, toch een algemene kaart op dit moment. Toch snap ik ook wel dat keuzes gemaakt moeten worden. En uit andere reviews is die info nog wel te destilleren. Nog altijd zijn conclusies in een review een resultaat van een interpretatie die (naar ik hoop) direct resultaat zijn van wat is gemeten. Dat een review hiermee een afwijkend geluid laat horen van andere reviews....het zij zo. Hellend vlak wordt het wel wanneer uitspraken gedaan worden over onderwerpen waarover de reviewer zelf geen eerste hand informatie heeft. Ook in een review kan dat dan nog wel gedeeld worden (als het relevant is als kader) maar het moet dan wel duidelijk zijn waar de informatie vandaan komt. Een valkuil van meetgegevens is dat ze suggereren absoluut zijn, terwijl ze slechts een facet geven van een hele complexe meetwerkelijkheid. Roepen velen ""onwaar"" neig ik eerder om te denken ""interessant"". Dat een conclusie in een review soms een bocht afsnijdt kan in de discussie door de tweakers wel worden gepareerd. Droog argumenteren is hiervoor wat mij betreft ruim voldoende om een reviewer te nuanceren. De hoge toon voegt hierin wat mij betreft niets toe. edit: typo's"
GTX 1070;1;0.5003035068511963;"Inderdaad, zolang de reviewer maar duidelijk (expliciet) maakt waaróp hij zijn conclusie baseert, en bij meetgegevens: wat hij precies gemeten heeft, dan weet je als lezer in elk geval de eerlijke informatie. Als je het niet eens bent met de conclusie op basis van de gegevens... is dat geen probleem, en eerder het bewijs dat de reviewer bewust inzage heeft gegeven in die gegevens! Je snapt ook meteen dat dat voor reviewers dus een spagaat is; geef je de achterliggende gegevens niet, of zeg je niet duidelijk genoeg wat je precies gemeten hebt, dan is het niet goed want dan is de review ondoorzichtig en waardeloos. Geef je de achterliggende gegevens wel, dan heb je altijd mensen die zeggen, je had iets anders moeten meten, of die het dan niet eens zijn met de conclusie die daar volgens jou uit voortvloeit."
GTX 1070;3;0.26912549138069153;Amen. Voor mijzelf sprekend dan toch maar de voorkeur gevend aan het geven van (grinding) veel informatie over meetopstelling, randverschijnselen, en disclaimers om als reviewer in ingedekt te zijn tegen de commentaren die je noemt. Is trouwens een commentaar op hoe is gereviewed terecht dan biedt het ook een leermoment. Al verwacht ik van diegene die reageert wel dat die de moeite heeft genomen om te kijken of het commentaar al ergens in de review wordt genoemd. Dat kan met een lange review nog best een weekje zijn. Ik realiseer mij dat het bovenstaande politiek correct klinkt. Misschien moet gewoon niet teveel gefocussed worden op de toon van de reacties. mensen de moeite nemen om commentaar te geven dan is de review in ieder geval in bepaalde zin van belang. Dat kan door de reviewer als een compliment worden uitgelegd . En er is een levendige discussie, en dat is altijd winst.
GTX 1070;1;0.3058907985687256;Ik zat hier al zo lang op te wachten. Het duurde wel heel lang voordat jullie deze review hebben geschreven!
GTX 1070;4;0.30636167526245117;Gaat tweakers ook nog een review doen van alle aftermarket GTX 1080's? Ben benieuwd of the ASUS 1080 ook écht de snelste is vergeleken met de EVGA FTW edition, of misschien is de een wel koeler of stiller dan de ander.
GTX 1070;1;0.47379475831985474;Er werd bij een vorig artikel door de redactie gesteld dat jullie nog geen review hadden omdat jullie geen kaart hadden gehad, maar dat jullie deze inmiddels wel ontvangen hadden. We moesten wel een beetje geduld hebben voor de review want aangezien jullie toch niet bij introductie konden reviewen wilden jullie het goed doen en wat extra doen dan de andere sites. Nu, ik heb deze review gelezen en ik heb niets, maar dan ook niets gelezen wat ook niet al op andere sites te lezen was, weken geleden. Dus wat hebben jullie precies extra gedaan om een betere en uitgebreidere review te bieden in en de extra tijd die jullie er voor genomen hebben? Deze reactie zal wel weer weggemod worden, maar jullie bieden weinig tot niets meer voor de tweaker. Jammer. Ik loop hier al rond sinds 2001 maar er is vrijwel niets meer wat het waard is om te komen. Admin-edit:Spelfouten en/of ander commentaar m.b.t. nieuwsposts horen thuis in Geachte Redactie. Gebruik de reacties hier om de review te bespreken, te verduidelijken, producten en resultaten te vergelijken, te speculeren, etcetera, maar de discussie over de toegevoegde waarde van deze review hoort op het forum, hier staat hij de ontopic discussie in de weg.
GTX 1070;2;0.5338374376296997;Couldn't agree more. Maar hoeveel uitgebreider kun je gaan als die van Guru3D? Die warmtecamera's gebruikt voor gedetailleerde warmtebeelden en FCAT apparatuur voor frametimes. Het is dan ook lastig om daarmee te concurreren. Tweakers heeft zich nu eenmaal meer gefocust op de normale mens. Reviews zoals deze zullen in-depth genoeg zijn voor veel mensen. Echter niet voor veel tweakers. Daarom dat ik eigenlijk ook nooit meer echt een review van tweakers nog lees. Ze gaan niet meer de diepte en dat is toch wel jammer. Tweakers was toch echt mijn go-to site voor al het tech-news en reviews. Maar is langzamerhand erg overbodig aan t worden
GTX 1070;3;0.609060525894165;Niet zo vreemd.. tweakers is het tech broertje van nu.nl geworden. Tech voor Henk en Ingrid. Sanoma gaat voor het grote advertentiegeld, kwaliteit komt op de 2e plek.
GTX 1070;3;0.43554335832595825;Wel erg grappig dat je Nu.nl in je profiel als homepage hebt ingevuld
GTX 1070;3;0.3208273649215698;Inderdaad oud grapje
GTX 1070;2;0.381702184677124;Tweakers hoort bij De Persgroep en niet bij Sanoma Ik denk dat het voor Tweakers gewoon niet mogelijk is om bestaansrecht te hebben voor een kleine groep die hard Tweakers. Willen ze blijven bestaan dan moeten ze wel voor de massa (=kassa) gaan schrijven. Gelukkig vind je op het forum vaak genoeg info voor iemand die echt iets wilt weten. Dus nieuws voor het volk, en het forum voor de Die Hards.
GTX 1070;1;0.3039318025112152;"Sorry, maar ik sluit me volledig aan bij deze comment. Maar goed, ik heb een paar jaar geleden al de hoop opgeven voor deze site om ook daadwerkelijk wat nieuws te verslaggeven. Ik kom hier nu meer voor de amusement om eerlijk te zijn en niet zo zeer om de hotste nieuwtjes over bepaalde categoriën te weten te komen. Ze hebben namelijk al vaak genoeg laten zien dat dit niet bijgebeend kan worden t.o.v. de internationale concurrentie. Hoe je het ook wend of keert, de competitie is op dit gebied internationaal en als je wilt bijbenen moet je je aanpassen. Nogmaals, ik adviseer je om dit ook te doen en bij iedere nieuwsitem het best even naar de comments te gaan. Daar zit vaak het echte nieuws, waarop de ""redactie"" weer een update gaat plaatsen bij het nieuwsitem, omdat er ""iemand"" weer te lui was om goed journalistiek werk te verrichten. Dit kritiek mag dan wel hard gevonden worden, maar het mag ook wel een paar keer gezegd worden. De richting waar deze site naartoe gaat is lachwekkend. Maar goed, als dat het doel is, dan doen jullie het goed? Edit: Lol admin-edit voor damagecontrol? Give me a break. Zijn reactie is een toegevoegde waarde aan deze review aangezien zijn reactie aantoont wat nu de echte nieuwswaarde van dit artikel is. Dit kan mensen een hoop tijd besparen alvorens ze al die pagina's gaan lezen. Zijn comment was meer informatief, dan de conclusie op dit artikel. Leuk dat alle vormen van kritiek achter een forummuur verscholen moet worden, waar er vervolgens niets mee gedaan wordt (lees: afgelopen 5 jaar). @oef, lees de eerste zin van mijn laatste alinea."
GTX 1070;1;0.4781274199485779;"Ik kom hier nu meer voor de amusement om eerlijk te zijn Daar zit vaak het echte nieuws, waarop de ""redactie"" weer een update gaat plaatsen bij het nieuwsitem, omdat er ""iemand"" weer te lui was De richting waar deze site naartoe gaat is lachwekkend. Maar goed, als dat het doel is, dan doen jullie het goed? Je post is op zo veel manieren denigrerend, ondankbaar en egocentrisch dat ik mijn afkeer van je schrijfsel niet eens onder woorden kan brengen."
GTX 1070;2;0.4298621714115143;Niet iedereen neust op andere tech-websites. Ik kijk bijvoorbeeld altijd op Tweakers voor de snelle hap op dagelijkse basis. Zo af en toe in depth vind ik prima, en als het hier niet goed genoeg is kan ik altijd nog uitwijken. Is Tweakers wat meer richting de massa gegaan? Ja, uiteraard. Maar als ze dat niet hadden gedaan was het doek waarschijnlijk al lang gevallen. We leven hier in Nederland en de markt die Tweakers aanboord met technische artikelen is niet zo gek groot. In ieder geval lang niet groot genoeg om alles te doen wat er van ze verwacht wordt. Een internationale website heeft een vele malen groter bereik, meer middelen, meer budget en betere connecties binnen de wereld. Als je Tweakers niet meer waard vindt om te komen, dan kun je ook gewoon vertrekken. Daarbij kom ik hier persoonlijk meer voor de community dan voor het nieuws want dát is wat Tweakers leuk maakt. Het nieuws is gewoon een aardige bijkomstigheid.
GTX 1060;3;0.35831815004348755;Andere reviews... Uitgebreide Pascal (inclusief GTX 1060) test met mogelijkheden om te vergelijken met recente kaarten Foritains - MSI Gaming X Tomf1- MSI Gaming X TechPowerUp - Founders Edition TechPowerUp - MSI Gaming X Guru3D - Founders Edition Harware.Info - Founders Edition PC Perspective - Founders Edition Computer Base - Founders Edition, Palit Jetstream, MSI Gaming X PCWorld - Founders Edition Ars Technica - Founders Edition Hardware Canucks - Founders Edition Bit-Tech - Founders Edition TechSpot - Founders Edition Gamers Nexus - Founders Edition, MSI Gaming X Hot Hardware - Founders Edition Hexus - Founders Edition KitGuru - Founders Edition HardOCP - Founders Edition OverclockersClub - Founders Edition PC Games Hardware - Founders Edition, Palit Jetstream, MSI Gaming X Toms Hardware - Founders Edition IXBT - Founders Edition Hardware Heaven - Founders Edition Clubic Tech - Founders Edition Les Numeriques - Founders Edition En voor zij die niet bang zijn om een dag of twee extra te wachten op de bezorger...
GTX 1060;3;0.35547319054603577;"Thanks, dit geeft een ander beeld (meer in balans ipv het voordeel voor Nvidia) ....in mijn geval;"
GTX 1060;1;0.8375303149223328;Zitten een heleboel bullshit benchmark uitslagen in. Bijvoorbeeld Fallout 4 met 3840 x 2160 [Kein AA/16xAF] 11,4 FPS GTX1060 versus 12,3 FPS voor de RX480. Wat totaal onspeelbaar is. Als om het speelbaar te maken Physx moet uitgezet worden dan schiet AMD daar erg veel mee op maar nVidia niet en als dat bv TressFX is dan is het precies andersom. Dit soort uitslagen heb je als gebruiker/koper niks aan.
GTX 1060;3;0.34183651208877563;Reviews cherry picken kan iedereen maar de consensus is toch licht in het voordeel van NVIDIA.
GTX 1060;3;0.25621452927589417;als je de factor prijs niet meerekent
GTX 1060;3;0.4395635426044464;Toch wel, bekijk de reviews maar. En al zeker als je de factor warmte en geluid meerekent
GTX 1060;3;0.379138708114624;Als de prijs van gas stijgt piep jij wel anders! Toch liever zo HR ketel in je PC gehad .
GTX 1060;3;0.4394964575767517;ik vind het erg jammer dat tweakers de spellen vaak op medium zet met 4k en soms op high, zet ze ook gewoon met 4k op ultra ( net als guru3d en vele andere review sites ), iedereen weet toch dat je de settings lager moet zetten in spellen als je ze op 4k wil spelen, jullie zetten zelfs in the witcher 3 alle resoluties op high, is helemaal niet nodig, op 1080p ultra speelt de the witcher 3 60fps op een geforce gtx 1060, dus waarom hem dan op high zetten??? en doom is alleen op 1080p gereviewd, erg jammer, doom is met gemak op 2560x1440 te spelen met een geforce gtx 1060, zit je ruim boven de 60fps. en dat de radeon rx 480 in total war : warhammer zelfs sneller is dan de geforce gtx 1080 op 1080p, is omdat jullie cpu de bottleneck is, guru3d gebruikt de intel core i7 - 5960x extreme edition ( 8 - cores ) oc naar 4. 4ghz, en die heeft een stuk minder last er van op lage resoluties, omdat die cpu veel sneller is en dat grafische kaarten minder last heeft van een cpu bottleneck, heeft dus niks met dx12 te maken, want op 1080p is de geforce gtx 1080 de snelste bij guru3d vanwege de snellere cpu, en is de geforce gtx 1060 maar 2 fps langzamer dan de radeon rx 480 8gb. verders wel een leuke review, maar vind het wel jammer dat er niet meer spellen getest worden. en ik had gelijk de prijs van de geforce gtx 1060 is bijna net zo veel als de radeon rx 480 8gb, meerdere geforce gtx 1060 kan je al kopen onder de €300 ( ja ik weet ze zijn nog bijna nergens op voorraad, maar je kan ze wel al vast kopen voor onder de €300 ), net als de radeon rx 480 8gb, ja amd krijgt het hier door ( jammer genoeg ) moeilijk door. geforce gtx 1060 radeon rx 480 8gb
GTX 1060;2;0.42943498492240906;Die CPU is toch totaal onrealistisch in combinatie met een gtx1060. Wie besteedt er 1k+ aan een cpu om vervolgens te gamen op een 300€ gputje. De resultaten hier zijn een stuk nuttiger. Dat de cpu nog wel eens tot bottleneck kan verkomen is helemaal niet onrealistisch door de slordige programmering van veel games op dat gebiedt. Dat dx12 daarbij belasting van de cpu naar de gpu verplaatsen als de eerste een bottleneck wordt en dat de AMDs dat aanzienlijk beter kunnen dan de nvidias is best een belangrijk resultaat. Iig nuttiger dan een benchmark van een gpu naast een cpu die 5x meer kost dan de gpu met een overclock die ook nog eens een een koeling vereist die meer kost dan deze gpu.
GTX 1060;2;0.3924686014652252;Dat zou een prima argument zijn bij de review van een systeem, maar niet bij de review van een videokaart. Je wilt weten hoe snel een GPU is en niet hoe snel een bepaald systeem is onder bepaalde condities. Genoeg sites doen vaak zat een CPU test met games en van daaruit kun je prima bepalen welke combinatie CPU/GPU bij elkaar past. Daar mag absoluut geen plaats voor zijn bij een review over een nieuwe videokaart. Want dat argument gaat dus ook op voor elke kaart die sneller is. Een 1080 zal in dat geval dus even snel lijken als een 1060, en hoe kun je dan een fatsoenlijke conclusie over een videokaart trekken.
GTX 1060;2;0.4047296941280365;Hoe voor deze post een +2 krijgt is mij een raadsel. Ja uiteraard wil je weten hoe snel de gpu is. Dat kanjer ook duidelijk zien in het gros vande Benchmarks. Het compenseren voor een zwakker cpu is echt wel degelijk een interessante eigenschap van de gpu. Daarin is de 480X klaarblijkelijk beslist beter dan de NV tegenhangers. En dat is een meerwaarde, die anders volstrekt wordt genegeerd als alle reviewers het zouden aanpakken als jij stelt. Bovendien is de cpu in het test setup van tweakers.net niet bepaald zwak. Sterker nog, op die ene cpu na, die dat andere magazine heeft is hij zowat de sterkste cpu die je kan krijgen. Als het daar dan al speelt, dan is dit een nog feature van die kaart. Als kaarten hierdoor even snel lijken, mapje als reviewer gerust op de cpu als bottleneck wijzen en hoeft hem niet mee te nemen in calculaties voor verschillen. Als je echter ziet, dat een kaart juist de cpu bottleneck weet te verhelpen dan is dat cruciale informatie die inde review van de gpu hoort. Het is namelijk niet een eigenschap van een gpu/cpu combinatie maar specifiek van deze gpu
GTX 1060;2;0.4530509412288666;"En die test wordt ook vaak genoeg uitgevoerd, genoeg sites testen CPU/GPU combinaties met games. Maar dat is geen GPU review, dat is een systeem review. Hoe moet je nou weten hoe snel een kaart in een bepaalde game is als er een CPU bottleneck is? Prima als dat óók getest wordt, maar een 49xx op 4,4ghz is in games trager dan een 6600K op stock, dus hoe reëel is het om dat te roepen dat het bij een GPU review hoort? Helemaal niet, daar kun je zoals ze bij zoveel sites doen prima een review over maken, maar vervuil daarmee de performance van sec een GPU niet. Maar ik weet wel waar dit vandaan komt. Het is dat gehuil over die zogenaamde ""CPU bottleneck van kaarten"" specifiek als AMD er beter van lijkt te worden. Waar wat deze ophef toen de overhead van de AMD drivers onder DX11 zodanig was dat ze niet eens een derde van de drawcalls haalde. Met als resultaat da je een ontzettend veel sneller cpu nodig had? Bekijk de reviews van de FuryX er maar op na, het is geen toeval dat hij op lage resoluties niet uit de verf kwam. Overal werd maar gehamerd op de CPU bottleneck. Maar nu is dat ineens niet meer boeiend in een GPU review? Dat zijn dubbele standaarden en daar ga ik sowieso niet aan mee doen."
GTX 1060;3;0.5230923891067505;Ik denk dat het toch nuttig is om met 3 CPU's te testen, juist omdat je bij de koper van de GTX960/RX480 in de praktijk vaker een i5 of i3 zult zien, dan een i7 6 core(+). Je wilt als koper toch weten welke kaart het beste uit je systeem haalt. Mij maakt het niet uit of de GPU uit het groene of rode kamp komt, als ie maar vlot is in mijn configuratie. Het zou wel inzicht verschaffen als ipv uitsluitend de gemiddelde en minimum FPS van de games ook de gemiddelde CPU belasting wordt meegenomen. Dan zie je ook dat stevige kaarten vaak een stevige CPU nodig hebben.
GTX 1060;2;0.47179746627807617;Natuurlijk is het nuttig om die test te doen, dat geef zelfs meermaals aan... Alleen om dat in een GPU review te proppen is ook zo zonde. Performance van combinaties van hardware over een brede linie bepalen is ontzettend veel werk als je het goed wilt doen. Dat heb ik liever een deep-dive over bijvoorbeeld CPU/GPU combinaties in combinatie met DX12. Om maar wat te noemen. Het gaat alleen om het feit dat je een GPU niet moet testen met een bottleneck op een ander onderdeel als je sec de performance van de GPU wilt weten. Ik neem aan dat ik nu wel eens duidelijk ben geweest. Overigens is het feit dat een CPU 6 cores heeft helemaal niet bepalend voor zijn in game performance. Mijn i7 4930K houdt zich prima staande, maar is absoluut geen game monster. Ik zou tweakers eerder aanraden om met een i7 6700K te testen.
GTX 1060;5;0.72513347864151;De reden waarom er de beste cpu word gebruikt is om zeker te weten dat er geen mogelijke Bottleneck is en je dus de maximale performance krijgt te zien.
GTX 1060;3;0.37956827878952026;Ik heb het ook over de GeForce GTX 1080 op dat stukje, als je iets beter gelezen had.
GTX 1060;5;0.38710904121398926;wat duidelijk aan geeft dat nvidia veel in de driver moet proberen op te lossen wat AMD in hardware kan wat veel extra cpu overhead met zich mee brengt voor nvidia. en aangezien bijna niemand een 1060 met een extreme edition CPU zal combineren lijkt me dat zeker iets om mee te nemen.
GTX 1060;1;0.4440930187702179;Sorry maar naar mijn idee is dat echt dikke onzin. Je wil testen op instellingen die zo dicht mogelijk bij de instellingen komen waarop mensen werkelijk gaan spelen. Als ik de Guru3D test zie en lees dat de 1060 maar gemiddeld 27 fps haalt met hun instellingen, dan weet ik dus alleen maar dat het op die instellingen eigenlijk niet speelbaar is. De test van Tweakers laat me dan juist zien hoeveel het helpt om de instellingen omlaag te draaien. Bij de benchmark van Guru3D denk je misschien, mwa als ik de settings omlaag draai dan wordt het vast goed genoeg. Bij de test hier zie je dat het nog steeds (net) onder de 30 fps dipt en voor mij zou dat dus niet goed genoeg zijn. Uiteraard zou het het beste zijn als het beide in dezelfde review getest wordt, maar als je maar tijd hebt om één instelling op te nemen, dan zou ik gaan voor degene die het meest in de buurt komt van hoe het in de praktijk gebruikt gaat worden.
GTX 1060;3;0.4208437502384186;zeg toch duidelijk dat het voor mij van toepassing is..... dus ja, cherry picking, omdat ik niet op alle resoluties speel en niet allerlei waardes AA heb aanstaan.
GTX 1060;3;0.35558244585990906;Mee eens, kijk maar naar het verschil in fps in totaal, en dan het verschil in prijs. Link hier.
GTX 1060;3;0.4669622480869293;Tja, 7% sneller maar de prijs in vrijwel altijd meer dan 15% hoger... de RX480 blijft dus de betere prijs/prestatie verhouding hebben.
GTX 1060;2;0.3677564561367035;Waar heb je het over, je kan ze al bestellen voor onder de €300,- De goedkoopste Radeon RX 480 8GB kost € 275,50. De goedkoopste Geforce GTX 1060 kost € 286,- Scheelt maar € 10,50, dus de Radeon RX 480 8GB heeft NIET een betere prijs/prestatie verhouding, dat heeft nu de Geforce GTX 1060, omdat hij sneller is (in de meeste spellen) en bijna net zo duur zijn.
GTX 1060;4;0.41336002945899963;sneller in oude spellen op basis van dx11. De kaart met verweg de beste prijs/prestatie verhouding is overigens zonder meer de 480 4GB.
GTX 1060;3;0.3452472686767578;Waar heb je het over, in Rise Of The Tomb Raider DX12 is de Geforce GTX 1060 veel sneller, en in Total War: WARHAMMER DX12 scheelt het maar 1 a 2 fps, en in Ashes Of The Singularity DX12 het zelfde, maar 1 fps, alleen in Hitman (2016) DX12 is de Radeon RX 480 veel sneller.
GTX 1060;2;0.3939897418022156;tombraider is een nvidia game, en zelfs nvidia krijgt er letterlijk NIKS bij met dx12 vs dx11 met de 10 serie. De 480 verliest zelfs een 1 FPS met dx12, en de oude nvidia 9 serie kaarten nog wat meer dat lijkt me genoeg zeggen over de kwaliteit en representativiteit van de DX12 performance in Tombraider. verder zit je naar een overclockte, en dus duurdere, 1060 te kijken en die te vergelijking met stock 480. op stock snelheid zie ik in ashes op 1080p bijna 3 FPS, en het heeft HEEL lang geduurd voor nvidia hier een beetje in kon presteren. warhammer zie ik 6FPS, en in doom (vulkan) 5 FPS. en dan idd hitman met 11. en zelfs in de door nvidia gesponsorde gears of war wint de 480 met 4FPS. nvidia weet dus zelfs maar 1 van de 2 door hen zelf gesponsorde dx12 game te winnen, en voor de rest verliezen ze elke test met een low level API.
GTX 1060;1;0.3328615128993988;Ja en Hitman is een AMD spel, so whats your point? En bij bijna alle review sites is de Geforce GTX 1060 sneller dan de Radeon RX 480 in Rise Of The Tomb Raider DX12. Dus die ene die afwijkt kijk ik dus niet naar. En nee ik kijk helemaal niet naar een OC en duurdere 1060 te kijken, ik keek naar de standaard versie Geforce GTX 1060, en die staat gewoon in die review, en die heeft een gemiddeld fps van 52 fps bij 2560x1440 (maar 1 fps lager dan de OC Geforce GTX 1060), en de Radeon RX 480 heeft een gemiddeld 45 fps bij 2560x1440. En dat zie je HIER ook, en ook Hier, en ook Hier, en ook Hier, en ook Hier, you get the point, de Geforce GTX 1060 is gewoon sneller dan de Radeon RX 480 in Rise Of The Tomb Raider DX12. Dus die van Tweakers en HardOCP neem ik met een korreltje zout.
GTX 1060;1;0.6188127398490906;uhmm, ik heb nergens gezegd dat de 480 sneller was in tombraider, en ik geloof dat je totaal niet begrepen hebt waar ik op doelde. nogmaals dus: De dx12 implementatie van tombraider is zo slecht dat zelfs nvidia nieuwste generatie er niks bij krijgt in vergelijking met DX11. AMD en hun eigen 9 serie verliezen zelfs performance in DX12 in vergelijking met DX11 in tombraider. de ene afwijking is dus tombraider. de resultaten van tombraider zijn dus gewoon geheel te negeren omdat ze duidelijk op geen enkele manier representatief zijn voor de performance bij dx12 of vulkan. en laat tomb raider nou net de enige game zijn die nvidia wel kon winnen in DX12. duidelijk omdat het op geen enkele manier gebruik maakt van de mogelijkheden van DX12. in elke andere low level API game wint AMD, (games die ook allemaal winst laten zien ten opzichte van DX11.) deed je wel. en wel toen je het volgende zei en dit is alleen waar met de OC 1060, en niet met de stock. Met stock zijn het de cijfes die ik op noemde.
GTX 1060;1;0.6562519669532776;Wow, je ziet HIER duidelijk gewoon de stock er tussen staan, hij haalt 49 fps bij 2560x1440 met Total War: WARHAMMER DX12 (de niet OC versie), en de OC versie die heet Palit GeForce GTX 1060 Super Jetstream, zit op 51 fps, zelfde als de Radeon RX 480 8GB, dus JA het scheelt maar 2 fps met de NIET OC versie. En HIER zie je dat hij (de niet OC versie) 49 fps haalt in Ashes Of The Singularity DX12 bij 2560x1440, en de Palit GeForce GTX 1060 Super Jetstream haalt 51 fps, en de Radeon RX 480 8GB 50 fps, dus scheelt het maar 1 fps met de NIET OC versie. En sorry een paar fps is niks, en de GeForce GTX 1060 concurreert met de Radeon RX 480 8GB, en ze zijn met de paar DX12 spellen die uit zijn dus net zo snel, en dat is geen probleem, want ze kosten ook ongeveer het zelfde, alle twee al voor net onder de €300,- voor de Radeon RX 480 8GB, aangezien er ook een 3GB versie komt, en die gaat dan concurreren met de Radeon RX 480 4GB.
GTX 1060;3;0.4064798951148987;ah, is dat de verwarring, ik zat naar 1080p te kijken (wat nog altijd veruit de meest gebruikte resolutie is). we kunnen overigens forza 6 nog aan het rijdje toevoegen, waar AMD wint met 13 FPS en quantum break waar AMD wint met 6FPS. (of 10 en 4 respectievelijk op 1440p) We hebben het toch vaak over 5 a 10%, soms zelfs meer verschil. en ze kosten misschien even veel maar je krijgt wel 2GB meer geheugen met de 480. De 3GB versie van de 1060 gaat langzamer zijn als de 6GB versie met een lagere clocksnelheid en minder shaders. die zal dus mogelijk eerder met de 470 gaan concurreren als met de 480 4GB. (en 3GB is ook wel erg krap voor dit performance segment... mensen maken zich al zorgen met 4GB) en zelfs de dx11 tests, als we even de games die extreme in het voordeel van nvidia zijn even negeren (game works) dan is de 480 behoorlijk consequent maar net iets langzamer als de 1060 in dx11, en consequent net iets sneller met dx12 en vulkan. en met het oog op de toekomst ga ik dan toch echt voor dx12 en vulkan performance.
GTX 1060;3;0.3888205885887146;Das waar, maar steeds meer mensen gaan naar het 2560x1440 of Ultra Wide 2560x1080 resolutie toe, en daar is de GeForce GTX 1060 en de Radeon RX 480 8GB perfect voor. En ook steeds meer mensen gaan voor de Ultra Wide 3440x1440 resolutie, die perfect is voor de Geforce GTX 1080, en je bij 99% van de spellen op gemiddeld 60+ fps zit alles op Ultra.
GTX 1060;3;0.32528531551361084;Prijzen zijn nu nog niet met elkaar te vergelijken. Even 3 maandjes wachten, dan weten we veel meer.. Introductie prijs RX 480 8 GB was €249,-- die echt op voorraad waren. Ik zie nu in de prijzen in de lijsten die echt beste laag zijn, maar niet op voorraad en nooid binnen gehad ook. Ik denk dat de RX 480 8 GB op den duur veel goedkoper uit zal vallen dan de GTX 1060. Ik moet wel zeggen dat ik de voorkeur heb aan zo'n AVGA SC GTX 1060. Leuk dingetje.
GTX 1060;2;0.31739476323127747;LANG verhaal kort.. de 1060 is EN zuiniger.. EN stiller EN klopt de 480 (non-cf) op 75% van de test op 1080p... best logisch eigenlijk. AMD gaat pas een rol spelen in CF op 4k ofzo...
GTX 1060;3;0.3952636122703552;Dit is wel erg kort door de bocht. De amd is gewoon €80 goedkoper. Dit is een ander prijssegment en daardoor is de vergelijking al niet helemaal kloppend.goede Dx12 ondersteuning en SLI mogelijkheden die de amd erg interessant maken, helemaal met het oog op de toekomst
GTX 1060;2;0.3912278115749359;"Ik word over 2 dagen 35, ik ben al bezig met PC's bouwen en knutselen sinds een 4*86 DX2; het hiel wel enorm dat m`n familie in de electronica branch zat. Goed. Dat gezegd te hebben, heb ik al jaren gewoon geen tijd c.q zin om uren lang naar oplossingen te zoeken en wil ik dat mijn producten out of the box gewoon werken voor wat ik in ogen heb. En daar is Nvidia nou 1 maal beter in vanwege hun betere drivers en software. Ik koop dan wel hun hogere segment (580->780TI->1080) maar het argument geld door hun hele linie. Namelijk ik kan out of the box meteen streamen en/of opnemen via shadowplay met 0 frameloss/extra resources; terwijl bij het AMD kamp ik daar Xsplit etc. voor moet aanschaffen/instaleren/configureren/testen en/of OBS maar again; configuratie/testen; het zijn niet de meest intuatieve stukjes software helaas. Driver support: Altijd beter geweest, of het nu de RAGE series van ATI waren geweest tot een 9800 die ik had gehad ; er is altijd wel hier en daar geknutsel met nieuwe games en kan je altijd wel verwachten dat je na 2 weken een patch driver van AMD/ATI krijgt om iets op te lossen. Gewoon geen zin in. TLDR: Die 80 euro heb ik liever meer, voor de betere software support. Daarnaast dat hele SLI / CF argument is ook grappig. Mensen die juist mid-range of low-end kopen hebben meestal niet een pimp moederbord met SLI/CF support; die zijn blij voor 69 euro een kast met een voeding te hebben, een basic mobo van 60 euro en gaan met die banaan."
GTX 1060;3;0.2508469820022583;"Sinds de dagen van de 9800 is er op driverfront echt wel wat veranderd in het AMD-kamp, hoor. En dat zeg ik als ex-eigenaar van een 9800, toen je nog cleaners moest draaien om alle oude drivers ""echt"" te verwijderen voor je een upgrade kon doen, anders speelde je met je leven. Tegenwoordig is het qua drivers lood om oud ijzer: soms rijdt Nvidia een scheve schaats, soms AMD, maar het is niet meer zo dat je bij AMD hoeft te vrezen dat je de system restore in moet om het weer aan de praat te krijgen, noch dat je bij Nvidia altijd snel een oplossing krijgt als je een issue hebt. Van dat stigma dat AMD/ATI destijds heeft opgelopen komen ze waarschijnlijk niet meer af, maar in mijn ervaring (wederom persoonlijk) is het tegenwoordig niet meer gerechtvaardigd om te roepen dat Nvidia het beter doet qua drivers. Gaat ongeveer gelijk op nu. Waar Nvidia al een hele tijd de betere kaarten heeft (en zal houden) is efficiëntie van de architectuur, met navenant mindere temperaturen, verbruik en geluid. Of dat je 80 euro waard is is een tweede, maar dat vind ik een sterker argument dan drivers."
GTX 1060;5;0.3248283863067627;Sterker nog, als ik puur naar de drivers kijk dan heb ik tegenwoordig een (kleine, maar toch) voorkeur voor AMD. Sinds de crimson-versies is het echt behoorlijk netjes allemaal. * BramT doet duit in zakje.
GTX 1060;3;0.39567169547080994;Succes met AMD drivers onder Linux Driver support voor AMD kaarten is (helaas) nog steeds niet zo best. Als je fatsoenlijke openGL wilt, en multihead support, ben je nog steeds een half weekend bezig, worden de nieuwste kernels niet ondersteund, etc... Met Nvidia werkt het gewoon wel, en dat is al jaren zo. Alleen al daarom, voor mij Nvidia hardware!
GTX 1060;1;0.3382161855697632;"Bedankt! Heb er alleen weinig aan omdat ik geen linux draai. Dit argument is een hele goede. Als je een Linux gebruiker bent, dan is Nvidia inderdaad de betere keuze. Helaas is de markt zo klein. Dat is ook nog steeds 1 van de redenen dat ik het niet prive draai. De Native ondersteuning voor veel dingen. En de dingen die Linux beter doet als Windows, daar heb ik in m'n privésfeer niet zo veel aan. Het jammere is dan alleen wel dat ik door te blijven wachten tot het allemaal netjes werkt, de markt kleiner houd, waardoor het dus langer duurt voordat Linux ""mainstream"" wordt."
GTX 1060;5;0.266504168510437;Mjah, dat kan ik me voorstellen. Ik ben niet gebonden aan Microsoft software, en ik game niet heel erg veel. Dus voor mij geldt eigenlijk precies het omgekeerde. Vandaar dat ik al jaren Linux gebruik, met af en toe een uitstapje naar OSX Anyhoe, on-topic, ik denk dat het altijd goed is als er concurrentie is, en hiermee zet Nvidia AMD hopelijk weer op scherp!
GTX 1060;1;0.4695530831813812;Succes met Gamen onder Linux. Je geeft een non valide argument. Er zijn gewoon héél weinig games die draaien op Linux. En dan bedoel ik uiteraard dat je géén gebruik maakt van Wine. Ik bedoel, stop dan met eigenwijs Linux gebruiken en installeer dan in een keer Windows. Ja, ik weet dat er op steam best wel wat indie games zijn die ook onder linux te spelen zijn. En dat met de steambox mogelijk meer gaat worden. Fact of the matter, iets van een halve procent gamed onder Linux. Bron: Steam Survey. En AMD kiest er gewoonweg niet voor om die niche markt te bedienen. En verder snap ik niet wat je zit te klagen, dat je een half weekend verder bent is toch juist de charme ervan?
GTX 1060;2;0.44989195466041565;Ik snap je punt niet zo goed Ik gebruik geen Windows, omdat ik het een waardeloos systeem vind. Dat is mijn persoonlijke mening, iedereen mag daar uiteraard zijn eigen mening over hebben. Onder Linux, wat ik een stuk beter vind, werken de drivers van AMD een stuk minder goed dan die van Nvidia. Dus kies ik voor Nvidia. Dat gamen onder Linux minder goed werkt dan onder Windows ben ik zeer met je eens. Maar dat boeit mij niet zoveel. Als ik wil gamen haal ik wel een console! Voor mijn computer gebruik is Linux zeer geschikt en ik zoek mijn hardware er dan ook op uit dat het daarmee goed samenwerkt. Hetzelfde zou ik met Windows overigens ook doen.
GTX 1060;3;0.35839930176734924;"""voor gamen koop ik wel een console"" waarom koop je dan een grafische kaart? Een GTX 1060 is een beetje overkill voor non gaming aplicaties vindt je niet? Kun je beter met de geïntegreerde graphics van je CPU werken. Die ondersteunen tegenwoordig al resoluties tot 4k."
GTX 1060;5;0.3637579083442688;Waar heeft borft gezegd dat hij een GTX 1060 neemt? Nvidia heeft wel meer modellen. Het is niet alleen gaming waarvoor je een grafische kaart kunt gebruiken, renderen of mining gaat bijvoorbeeld met een discrete GPU beter dan bij een APU.
GTX 1060;1;0.4681400954723358;Amen! geintegreerde graphics heeft weer allerlei andere issues, zoals problemen met video playback, de eerder genoemde multihead problemen. En zo af en toe mag ik graag een spelletje spelen, en dat gaat zelfs op de meest crappy Nvidia kaart beter dan op Intel
GTX 1060;2;0.5508041381835938;"Ja, slechts zo'n 2000 titels op Steam alleen al... Inderdaad heel veel indie titels zoals je hieronder zegt, maar er zitten ook meer dan genoeg triple A titels tussen en er komen er steeds meer bij. Overigens vind ik dat niet erg, veel indie games bieden betere gameplay dat de gemiddelde triple A game IMHO. Er zijn andere valide redenen om geen Windows te willen gebruiken, zeker Windows 10 niet met alle phone-home features er in. Maar goed, dit soort uitspraken nodigt helaas meestal uit tot een flame war. Overigens gebruik ik zelf geen Wine. Je hebt de bron niet goed gelezen. Het Linux gebruik in de Steam survey zweeft al jaren rond de 1 procent, als je kijkt naar browser user agent data dan zou het Linux gebruik wereldwijd zo rond de 2 procent liggen. Geen enorme markt dat geef ik toe, maar het is meer dan men vaak beweert. En dat is nu juist niet waar. AMD is altijd een erg Linux vriendelijk bedrijf geweest. Het 'probleem' ligt bij het voormalige ATI. Die hebben al sinds jaar en dag slechte Linux drivers voor hun GPU's en AMD zit daar nog steeds mee in hun maag. De ondersteuning is wel verbeterd en met hun nieuwe (deels open source) AMD GPU driver en het GPU Open initiatief zal het nog wel verder verbeteren, maar ze zijn er nog lang niet. Meh. Op een gegeven moment heb je de leeftijd dat je niet meer de zin en de tijd hebt om er een heel weekend aan te besteden en je gewoon wilt dat het werkt. En gelukkig is desktop Linux inmiddels op het niveau dat dat ook gewoon kan, mits je een GPU van NVidia hebt. En voordat je gaat zeggen: ""als je wilt dat het gewoon werkt, installeer dan Windows"", dat is ook kul. Ik ben in het verleden ook hele weekenden bezig geweest met het perfectioneren van mijn Windows (2000, XP en zelfs nog even 7) installaties. Het is maar net wat je wilt en hoeveel tijd je er aan kwijt wilt zijn. Als je geen eisen stelt geeft Ubuntu of Linux Mint een prima out-of-the-box ervaring in minder dan een half uur."
GTX 1060;2;0.4775177538394928;Dit is onzin. Deze generatie zet AMD stevig in op de Linuxdrivers en met name als je opensourcedrivers wilt hebben die redelijk tot goed presteren dan moet je bij AMD zijn. Verder moet je juist recente kernels + userspace hiervoor hebben. Inderdaad, AMD's OpenGL implementatie loopt achter bij die van Nvidia (en die van Mesa al helemaal) maar ik vermoed dat dit het komende jaar aardig rechtgetrokken gaat worden en de benchmarks geven aan dat Vulkan juist beter draait op AMD. Ik ben één van de vorige generaties overgestapt op Nvidia omdat de AMD drivers steeds problemen gaven (schermcorruptie dat soort dingen), maar deze generatie stap ik juist weer over op AMD omdat ik graag goede opensourcedrivers wil hebben (waar ook een beetje normaal mee te gamen valt). Nvidia is zwaar anticompetitief (Physx, Cuda, G-sync, Gameworks, etc.) en ondersteunt opensource vrijwel niet (alleen voor hun mobiele tak). Verder, een half weekend voor multihead support? Really? Dan doe je denk ik iets fout of je hebt een rare setup.
GTX 1060;1;0.47377943992614746;Op het moment is dat met die drivers al lang niet meer het geval. Ze maken allebij wel eens een foutje. Echter heb ik nooit verschrikkelijke issues gehad met AMD maar de enige nVidia kaart die ik ooit heb gehad pleegde spontaan een vurige zelfmoord omdat er een driver defect was en daardoor de fans niet draaiden. Wat wel zo is is dat nVidia meer geld heeft, en daarom meer developers kan betalen hun games voor nVidia kaartjes te optimisen. Het is trouwens echt niet zo moeilijk om OBS op te zetten.
GTX 1060;5;0.5002955198287964;Geen enkel probleem gehad met amd de laatste jaren. Wat jij nu noemt is toch echt uit een heel ver verleden.
GTX 1060;1;0.26261594891548157;Wat een kul post. Je overtuigd hier niemand met je persoonlijke geschiedenis omtrent tech. Feit is en blijft dat de AMD kaart een stuk goedkoper is en op DX12 gebied vergelijkbare(betere momenteel zelfs) prestaties levert. De 480 werkt ook out of the box zonder problemen, en de SLI mogelijkheid kan voor budget gamers zeker wel interessant zijn omdat dit betekent dat ze over 2 jaar niet een compleet nieuwe kaart nodig hebben maar een goedkope 480 erbij kunnen prikken om prestaties gelijk aan een 1070 uit hun PC te persen.
GTX 1060;1;0.6113157272338867;En dat laatste argument van een tweede kaart erbij prikken wordt vaak gegeven en in praktijk nauwelijks toegepast. Ook is het budget technisch totaal niet handig. Er moet dan nu al rekening gehouden worden met een mobo dat de boel ondersteunt (=duurder) en moet er met de voeding rekening gehouden worden dat ie het vermogen kan leveren (=duurder). Of je dan per saldo nou zoveel goedkoper uit bent...? Ik betwijfel het.
GTX 1060;3;0.3887261748313904;zo'n beetje elk moederbord heeft wel 2 PCI-E 16x sloten, is niet iets wat je in je portemonnee gaat voelen Voeding kan je gelijk in hebben in sommige gevallen, maar als je slim bent koop je een voeding die ruim bemeten is, en niet 1 die net aan je configuratie kan trekken En de 2e kaart kan je natuurlijk over een jaar erg goedkoop krijgen, dus goedkoper ben je waarschijnlijk wel uit Het is meer de vraag of je ook beter uit bent met 2 kaarten (hangt erg van het spel af)
GTX 1060;1;0.556236207485199;"Ik ben het met je eens. Er is GEEN reden om twee kaarten te gebruiken als er een beter single card optie beschikbaar is. Geen gedoe met drivers, geen games zonder ondersteuning en geen stutter etc. Als je 2 480 koopt ben je ongeveer 500 euro kwijt. Koop 1 1070 met dan geld en je hebt 1. Betere performance 2. Geen gedoe met ondersteuning en drivers 3. Minder stroomverbruik. NVIDIA Is het hier duidelijk ook mee eens. En voor het ""Later upgraden wanneer je weer geld hebt"" ook onzin. Hoezo? Verkoop dan gewoon je 480 en haal daarna de betere kaart?"
GTX 1060;2;0.2879229485988617;Er zijn zat mensen die op een lager budget zitten, wel willen gamen en geen zin om schulden te maken. Dus upgraden wanneer je weer geld hebt (gespaard) is niet helemaal onzin, het hangt van de levensstijl van iemand af. Over een jaartje zou je een tweede RX480 tweedehands voor ca € 175 (of nieuw € 235) kunnen kopen. Stel dat de GTX1070 in prijs zal zakken naar € 450, dan moet er € 275 bijgelegd worden (RX480 voor € 175 verkopen). Of je zou een GTX 1070 tweedehands kunnen kopen, maar die zal ook prijzig zijn totdat er een nieuwe generatie is. Ben het wel eens dat een enkele krachtige kaart vaak een betere oplossing is dan SLI/CrossFire. Zelf koop ik altijd mobo's die geschikt zijn voor SLI en CF. Dat komt nu weer van pas, aangezien ik even de tijd wil overbruggen met 2* GTX 970 in SLI tot de 1080Ti (of een high end kaart van AMD) verkrijgbaar is. Die 970's schuiven daarna door naar andere PC's (1080p gaming) en vervangen een R9 380X en een GTX 770.
GTX 1060;1;0.4259882867336273;"Ik denk dat er heel weinig mensen na een jaar nog een tweede kaart kopen. Ik denk dat vele niet eens een extra kaart kopen omdat ze nog genoeg kunnen met hun huidige kaart en hun zakcentjes liever bewaren of aan iets anders spenderen. Tegen de tijd dat de RX480 or GTX1060 games niet meer op de 1080/2k Resolutie speelt met speelbare framerates zijn we alweer een generatie verder en dan kun je beter de nieuwe generatie ""budget"" kaart kopen dan een tweede 480/1060 er bij halen. Ik denk ook dat vele mensen dat doen."
GTX 1060;3;0.39082396030426025;Ik denk dat die kaarten op 1080p altijd speelbare framerates zullen hebben, maar dan met (iets) lagere settings. Ik draai al een tijdje in het wereldje mee, en het verschil in videokaarten per generatie is in de praktijk niet zo groot als de benchmarks doen geloven. Vroeger werden er grote klappers gemaakt door nieuwe technologieën, tegenwoordig duurt het 3-5 jaar wil er een groot verschil gemaakt worden. De games worden wel steeds veeleisender (hoewel het bij Doom wel meevalt). Hoe dan ook ik heb tot nu toe 2x upgrade-door-SLI-strategie toegepast, en dat is altijd goedkoop uitgevallen doordat ik de 2e kaarten voor een gunstige prijs tweedehands (na 1½-2 jaar) heb gekocht. Eventjes plezier van gehad en daarna weer doorgeschoven. Het verschil in prijs tussen mobo met of mobo zonder SLI is meestal niet zo groot, en ik hou graag zo veel mogelijk upgrade-opties over. Maar er zijn weinig mensen die SLI/Crossfire gebruiken, ik dacht zo'n beetje 10%, dus het is wel een overlappende niche markt. Misschien dat de RX480 hier verandering gaat brengen. SLI is leuk wanneer je met 2 goedkope kaartjes meer perst dan uit een enkele grote kaart die duurder is. Er zijn tijden geweest dat dat zo was.
GTX 1060;1;0.6838364005088806;yup, ze hebben de afgelopen 2 jaar namelijk al 3 keer een driver uitgebracht die kaarten omzeep helpt. echt ge-wel-dig! plays.tv zit al weer een tijdje in de drivers integrated. en nu gewoon als los stuk software en niet geïntegreerd in raptr of nog erger in die ENORM trage geforce experience bloat zooi. ik heb als sinds de 9700pro enkel ATI/AMD gehad en zit inmiddels an mijn 5de en ik heb geen flauw idee waar je het over hebt. het ENIGE dat ik SOMS in games moet aanpassen is de nvidia gimpworks zooi uitzetten of terug schroeven. bedankt nvidia. nvidia verziekt hun eigen PC games market en dat is waar IK geen zin meer in heb. buiten OEM borden en mini/micro ATX moet je toch goed zoeken wil je een moederbord vinden dat niet 2 16x pci-e slots heeft. daarbij kosten ze ook niks meer. sterker nog de goedkoopte atx met maar 1 16x slot is zelfs 10 euro meer als de goedkoopste met 2 voor socket 1151.
GTX 1060;1;0.42662057280540466;ten eerste bedoel je CF.. geen SLI (de eerste die een AMD kaart in SLI zet krijgt van mij een schouderklopje) maar de enige reden waarom ze deze kaart niet in SLI zetten is denk ik (zover ik gelezen heb) al een x of 5 genoemd.. deze kaart zou (ik herhaal ZOU) net zo snel zijn in SLI als 1x een 1080 in non-sli dus dan heb je een SLI rig voor ongeveer 550-600 euro)
GTX 1060;1;0.6309695243835449;Dat is niet waar. De rede dat ze geen SLI in de 1060 gedaan hebben is omdat het geen nut heeft. Mensen die 2x een 1060 kopen kunnen beter het geld steken in 1 1070. Daarnaast hoeft NVidia zo maar 2 kaarten in plaats van 3 te ondersteunen.
GTX 1060;1;0.5536750555038452;wie is nvidia om dat te bepalen? en wat nou als ze nu geen geld hebben voor een 1070, maar wel een 1060, en over een jaar of wat er een 1060 naast willen zetten?
GTX 1060;3;0.4333076477050781;Dan kan je beter tegen die tijd een nieuwe variant kopen.
GTX 1060;4;0.3084438443183899;over 1 jaar verwacht ik niet zo veel verschil (geen nieuwe node), en is de 1060 weer goedkoper.
GTX 1060;1;0.3866891860961914;De prijzen in Nederland zijn niet bepaald representatief, waarschijnlijk omdat het een verkopersmarkt is, omdat de nieuwe kaarten van zowel AMD als Nvidea nauwelijks op voorraad zijn. Als je in Duitsland gaat kijken, dan zijn de prijzen wat ze zouden moeten zijn. Daar is de GTX1060 slechts een tientje duurder dan de RX480. edit: er wordt hieronder gevraagd hoe ik aan mijn informatie kom. Welnu, ik heb gekeken op Duitse sites waaronder mindfactory.de. Daar zijn zo'n beetje alle kaarten van elk merk voorradig en is het verschil tussen de goedkoopste RX480 en GTX1060 precies 10 euro, oftwel 3,7%. Bij caseking.de via de link uit de firstpost van vipeax precies hetzelfde: 269 vs 279 euro. Persoonlijk vind ik een site die zo'n beetje alles kan leveren een betere graadmeter dan onze pricewatch, waar Sicomputers met zowat elk product onder de prijs van een concurrent gaat zitten, maar niet kan leveren.
GTX 1060;2;0.346940815448761;Hoezo zijn de prijzen in Duitsland representatief? Het verschil in adviesprijzen is wel meer dan 10 euro.
GTX 1060;1;0.7766291499137878;Hoe kom je daar bij. De 480X 8GB is daar a diverse keren voor minder dan 250€ te koop geweest afgelopen weken. En ook daar zijn de goedkope 1060 kaarten gewoon niet leverbaar.
GTX 1060;5;0.26976028084754944;TNX voor de tip, ABD, ik zat altijd op Idealo en Preisroboter te kijken.
GTX 1060;1;0.46587055921554565;Is hij helemaal niet, ze zijn zo goed als even duur. De goedkoopste Radeon RX 480 8GB kost € 275,50. De goedkoopste Geforce GTX 1060 kost € 286,- Scheelt maar € 10,50, dus de Radeon RX 480 8GB heeft NIET een betere prijs/prestatie verhouding, dat heeft nu de Geforce GTX 1060, omdat hij sneller is (in de meeste spellen) en bijna net zo duur zijn.
GTX 1060;1;0.5942972302436829;met een kaart van 320 euro, of zelfs 370 euro, wat VEEL meer als dat de 480 kost. en laat die 75% nou net allemaal oude games zijn, welke steeds minder relevant gaan zijn, en ze beide toch wel snel genoeg voor zijn. en die 30 watt bekommert niemand zich om.
GTX 1060;2;0.41676846146583557;De eerste weken waren de AMD kaarten ook een stuk duurder. Trekt allemaal wel bij...
GTX 1060;1;0.5025435090065002;weet niet waar jij dat '30 watt bekommert niemand zich om' idee vandaan heb.. dat heb ik namelijk niet eens gezegt... ik DENK zomaar eens dat je finaal de plank mis slaat.. en 'oude games die niet meer relevant zijn' ??? please explain because last time I checked... waren deze games - tomb raider - grid - gta V - far cry primal - the witcher 3 - doom - hitman ^ toch allemaal recente games die in ELKE benchmark gebruikt worden.. dusja please enlighten me...zolang deze games nogsteeds geen 4k 120 fps draaien met 1 van deze kaarten constant.. of hoger.. zijn ze nog altijd allemaal 'relevant' dus hierbij is je comment wasted breath material have a nice day chap
GTX 1060;4;0.2962469458580017;"Klopt. Nvidia 1060 is idd de gemiddelde winnaar in deze test. Maar in DX12 games is AMD wel degelijk erg sterk. Dat komt door de native Asynchronous Shaders support.: Asynchronous Shaders/Compute or what’s otherwise known as Asynchronous Shading is one of the more exciting hardware features that DirectX12, Vulkan and Mantle before them exposed. This feature allows tasks to be submitted and processed by shader units inside GPUs ( what Nvidia calls CUDA cores and AMD dubs Stream Processors ) simultaneous and asynchronously in a multi-threaded fashion.(bron) De vraag is echter dat Nvidia wel in staat in om deze DX12 feature ooit te benutten. Het is dus nog een open vraagstuk dat voorlopig in het voordeel van AMD wordt beslecht aangezien ze nu al kunnen aantonen dat ze de Asynchronous Shaders kunnen benutten. Wetende dat meer en meer games DX12 zullen ondersteunen is maar de vraag of Nvidia nu de beste koop is, ondanks het feit dat ze op DX11 heer en meester zijn. Koop je nu een 1060 of een 480, het zal nooit een slechte koop zijn. Nvidia is wat krachtiger in hedendaagse games + efficiënter/stiller maar AMD kan beter overweg met DX12 en kan op de koop toe in CF geplaatst worden terwijl CLI niet mogelijk is met de 1060. Vooral CF kan voor veel gamers een belangrijk argument zijn omdat je na 2 jaar uw prestaties kan eenvoudig kan upgraden voor pakweg €100 als je 2dehands koopt. Tijd zal ons wijzer maken maar het feit dat AMD goedkoper is + DX12 proof + CF heb ik het gevoel dat AMD de bovenhand heeft. Al zou ik van nature voor NVidia kiezen omdat ik in ver verleden (toen de driver ATI nog parten speelden) ondervonden had dat Nvidia toch een stukje stabieler was. Maar dat is zoals al uitvoerige besproken nu geen argument meer. Emo vs ratio ;-) Edit typo."
GTX 1060;2;0.41735008358955383;"Asynchronous Compute is geen DirectX12 feature, het bestaat al langer alleen nu is er meer oog voor en is het bereikbaar voor developers door de low-level api's. Far Cry 3/4 en Watch Dogs hebben zelfs de ""AllowAsynchShaderLoading"" in hun configuratiebestanden en staat standaard aan, alleen volgens mij heeft het geen effect."
GTX 1060;3;0.3879549205303192;De RX480 veegt anders de vloer aan met de 1060 in DirectX12 games wat erg belangrijk is voor toekomstige games. Vandaar is de RX480 de betere koop want de aankoopprijs zal nog eens lager liggen ook dan de 1060.
GTX 1060;2;0.4566771686077118;Check dat even goed. Ze zijn nergens voor een prijs in die dimensie leverbaar. Ja, je kan ze voor die prijs bestellen, maar dan heb je over een maand misschien een kaart. Tegen die tijd is de AMD ook gewoon weer goedkoper. De eerste die enigzins leverbaar is (binnen afzienbare tijd) is de Gainward van 310€ en dan is er toch al aardig verschil. En dan moet je er nog bij noemen, dat voor 1080p resoluties, de 480X met 4GB gewoon de veel slimmere keuze is (en op die resolutie is die ook niet langzamer dan de 8GB variant) en dan is het verschil gewoon 80€. En dat is wel gewoon ruim 30%.
GTX 1060;3;0.6109786629676819;Ik zie in reviews anders dat hij in Rise of the Tomb raider sneller is in DX12, maar het scheelt niet veel, in Hitman en DOOM is hij zelfs een stuk sneller
GTX 1060;3;0.3239858150482178;Hooooo, bij introductie was de AMD RX 480 8GB €249,-- en toen nog echt leverbaar ook. Wel even goed vergelijke hé.
GTX 1060;3;0.4298359155654907;"Wat een leuke lijst. Maar wat wil je ermee zeggen waarom de ..... ? Zou jij mij als je ze toch allemaal gelezen hebt de ""Critics Consensus"" kunnen geven zoals je bijvoorbeeld ziet bij rotten tomatoes ?"
GTX 1060;2;0.44737380743026733;"Omdat niet iedereen het even eens is met de kwaliteit/content van alle review sites (en terecht ). Wat ik zelf bijvoorbeeld wel mis is een prestatie-index gezien vanuit de GTX 1060. Nu moet je met allemaal rare getallen zelf gaan rekenen en als je dan op de bron link klikt (een interne Tweakers link) krijg je een 404. Mijn eigen mening is niet veel waard, gezien het antwoord geheel anders gaat zijn voor iemand die alleen MOBA games speelt, iemand die alleen de bekende AAA shooters speelt of iemand die echt alles wat uitkomt probeert. Wat ik wel weet is dat de GTX 1060 een stuk zuiniger is, terwijl de RX480 het over een jaar of twee een stuk beter zal doen dan nu (wat betreft prestaties) in de benchmarks. Als je minimaal 2-4 jaar wilt doen met een videokaart zou ik zelf een RX480 aanraden, echter als je iemand bent die om het jaar/twee jaar zijn videokaart te koop zet, dan zou ik de GTX 1060 aanraden. Ben je iemand die AMD niet vertrouwt dan zul je toch wel nVidia kopen. Ben je iemand die juist nVidia als bedrijf niet leuk vind om redenen {X, Y, Z}, dan zal die persoon toch wel een RX480 kopen. Zelf heb ik overigens een GTX 1080 gekocht, niet omdat ik graag nVidia sponsor of het ""laten we een kleine chip die eigenlijk een GTX 1060 had kunnen zijn verkopen als het top model met een hoge prijs"" gebeuren graag accepteer, maar omdat ik het verschil niet ga maken, door de kaart niet te kopen, om nVidia te forceren haar strategie te veranderen. Ik heb een ruim budget en wil dan ook liever gewoon nu op 165Hz gamen. Op dit moment biedt AMD dan ook geen opties voor het (kleine) segment van de markt waar ik mij in kan terugvinden. Hoe we er volgend jaar voorstaan zie ik dan wel weer..."
GTX 1060;1;0.7209523916244507;"Ik ben het helemaal met je eens, behalve met ""maar omdat ik het verschil niet ga maken"". Dat is natuurlijk wel raar. Dat denken tientallen miljoenen Amerikanen met een krankzinnig onzuinige auto ook. ""ik wil de Saudi Arabiërs niet steunen, maar het helpt niets als ik minder ga verbruiken"". Als iedereen dat denkt (en dat gebeurt min of meer ook) dan verandert er inderdaad niets."
GTX 1060;3;0.5544655323028564;Echter kunnen zij wel gewoon een leuke sportauto komen die wel zuinig (of zelfs volledig elektrisch) is. Ik heb geen alternatief als ik een snelle kaart wil voor die 165Hz die ik wil behalen...
GTX 1060;5;0.48417919874191284;Mooi om te zien dat het instapmodel de 60's het vrijwel op elk vlak beter doet dan de highend (non Ti) van de vorige gen. Ik doel dan vooral op de GTX980, deze heb ik zelf, die een resolutie van 3440x1440 moet pushen elke dag.. de GTX1060 doet dat niet alleen beter en sneller met zijn 6GB, maar ook nog eens een stuk efficiënter met zijn stroomverbruik! Ik denk dat ik pas naar een nieuwe videokaart ga kijken als de 1000 series met de Ti modellen komen. Dan wordt het pas echt interessant om op bijna 4K of 4K resoluties met een single GPU te gaan gamen.
GTX 1060;2;0.410000205039978;De 980 was dan ook een bijzonder matige kaart op zijn eigen prijspunt destijds, en de reden voor hele volksstammen om 'm links te laten liggen en de 970 te nemen. Eigenlijk een directe herhaling van de 670 versus de 680. Ik denk sowieso met een res van 1440p of hoger, dat deze kaarten het gewoon net niet zijn. Je gaat vanaf het moment dat je 'm gebruikt al concessies moeten doen voor die res. Dan is een 1070 of zelfs een tweedehands 980ti een veel betere keuze, in het geval van de 980ti zelfs qua perf/dollar op alle fronten beter. De x60 heeft altijd last van een bottleneck, is het nu niet, dan over een jaar al wel. Kijk hoe ver de 960 nu is weggezakt, dat staat niet meer in verhouding tot de aanschafprijs of de relatieve performance op basis van shaders. Nvidia's x60 kaarten hebben ALTIJD een memory bottleneck. Kortom, als je niet echt móet, kies het AMD alternatief op dit prijspunt en je hebt er veel langer plezier van. Eigenlijk maakt Nvidia het ons simpel: in dat kamp ben je altijd het beste af met de x80ti (top end), x70 (prijs/perf high end segment) of met de x50ti (prijs/perf midrange). Kom je qua wensen op een x60 of een x80, Nvidia lekker links laten liggen en voor AMD gaan. En SLI, alleen maar dual op een x80 of beter anders is het huilen met de pet op.
GTX 1060;3;0.5385615825653076;Wtb: Glazen bol, dan worden we allemaal beter Ik wil wel even opmerken dat de prestatieverschillen die ik de laatste dagen heb gezien feitelijk gelijk zijn op 1080p, 1440p, en 4K. Even uitgaande van DX 11 want DX 12 benchmarks is een beetje rommelig, maar die bijna ~20% neemt niet af op 4K. Om je nu direct zorgen te maken over een evt memory bottleneck vind ik wel heel erg vlot. Wel met je eens wat de positionering mbt 1440p betreft, ook uitvoerig aangehaald in mijn review: eigenlijk wil je daar een GTX 1070 voor. Aan de andere kant, ik heb ook lang zitten gamen met 1440p en een GTX 770, en ik vind persoonlijk de hele 'alles moet max en 144 FPS' beweging een beetje overdreven. Een GTX 970 met 1440p speelt ook alles lekker weg, zo groot is dat verschil tussen high en ultra vaak niet, en ik zou met een dergelijk prestatiepunt nu (oftewel je hebt een 290x / 970 / 480) niet meteen verdrietig zijn als je een 1440p scherm hebt. Hetzelfde gaat dus op als 500 euro te veel geld voor je is, dan is een GTX 1060 (of dus een RX) prima plezier aan te beleven. Maar goed, een hoop aannames in al een erg subjectief onderwerp (wat is een acceptabele prestatie etc).
GTX 1060;3;0.5187103748321533;Ja dat is zeker waar, met wat tweaken in de settings kun je op mid range heel lang doorkachelen. Ik doe het nu eigenlijk ook met m'n 780ti (=970/980) omdat ik Pascal nog te duur vind, en The Division draait nog altijd op een comfortabele 75-120 fps Maar ik vind het dan weer een stuk minder leuk als ik op een nieuwe aankoop op de native res al meteen flink zou moeten tweaken. Voor mij voelt dat dan vooral als een slechte keuze - in monitor of qua GPU.
GTX 1060;1;0.25441980361938477;"Behalve als je een ultrawide 34"" 100hz beeldscherm hebt gekocht met G-sync zoals ik Geen AMD meer voor mij dus"
GTX 1060;2;0.505845308303833;Tja, mooi dom, dat je jezelf een vendor lock-in hebt bezorgd met Gsync. Temeer omdat het niet samen gaat met ULMB of strobing en op hogere refresh rates/fps heb je er sowieso al niets aan. Dus voor die enkele game op 30 fps heb je dan Gsync... jippie Nvidia's mid range is nooit echt goed geweest en deze 1060 is ook niet wereldschokkend, de prestaties zijn gemiddeld rond de 980 maar de uitschieters zijn altijd naar beneden en niet naar boven toe, en het ziet er naar uit dat de RX 480 het gat op DX12 groter gaat maken en Nvidia achter zich laat.
GTX 1060;3;0.34654858708381653;Dat is inderdaad discutabel maar een wel overwogen beslissing.. Ik snap alleen niet dat jij als Nvidia GeForce king het verkeerd hebt op het gebied van gsync, wellicht is dit perceptie van persoon tot persoon.. maar door de komst van gsync merk ik nu dat 60hz er totaaaaaal anders uit ziet dan 60hz op mijn oude beeldscherm, zowel met als zonder vsync. Alles ziet er veel en véél smoother uit, de gsync helpt en is duidelijk zichtbaar tussen de 40 en 100fps (100 is het hoogste wat mijn beeldscherm kan) Hoe kom je hier precies bij? kan je deze twee punten toelichten?
GTX 1060;4;0.4002838730812073;Op hoge fps is het ook mogelijk om een frame rate lock te gebruiken, of adaptive Vsync, (en met Pascal: fast sync) en dat te combineren met strobing (als je paneel dat kan) - daarmee haal je echt voordeel uit de hoge refresh rate, en je balanceert af op stabiele FPS qua settings. Op dat moment heb je Gsync niet nodig. Op lage fps is Gsync nuttig, maar dan is een hoge refresh weer zinloos. Gsync is een 'luie' manier om fluctuerende fps te compenseren, maar het is veel beter om stabiele fps te zoeken per game, waardoor je Gsync niet nodig hebt en ook je input lag stabiel zal blijven (net als bij FPS merk je een iets hogere stabiele input lag, veel minder dan een sterk fluctuerende input lag - voor een stabiele lag kun je compenseren). Tegelijkertijd betaal je bij voor Gsync en lock je jezelf effectief aan een Nvidia kaart. Kortom, de prijs is vrij hoog voor zo'n beperkt voordeel. Een scherm gaat meestal meerdere GPU generaties mee, dus dat is nogal wat. Tot slot, maar dat is heel persoonlijk, vind ik backlight strobe een bijzonder nuttige feature die wél bijna altijd zinvol is, en die is ook niet te combineren met de variable refresh die Gsync gebruikt. Veel van deze argumenten gelden ook voor FreeSync maar de grap is nu juist dat je daar (bijna) geen meerprijs voor neerlegt, dus 'het niet gebruiken ervan' doet minder pijn en daarmee is ook de vendor lock-in niet- of minder- aan de orde.
GTX 1060;3;0.43406611680984497;Lang verhaal maar het mist de nuance waarom G-Sync duurder is: Je betaalt voor de extra chip (laag productie volume, dus duur) welke je wel de beste ervaring oplevert. Later is AMD met LFC gekomen. Op de slide van AMD zelf staat duidelijk het probleem aangegeven. Het is prima om de voor en nadelen van een bepaalde techniek te benoemen maar het komt nu over alsof je denkt dat Nvidia gebakken lucht verkoopt. Voorlopig bied G-sync de beste spelervaring, AMD is hard aan het werk om FreeSync te verbeteren. Zie deze recentelijk slide van dit jaar.
GTX 1060;3;0.4510270953178406;"Niemand heeft wat tegen G-Sync, maar ik denk dat je bent vergeten dat AMD gebruik maakt van FreeSync, wat precies hetzelfde doet. Maar dan op monitoren die al verkrijgbaar zijn voor minder dan 150 euro LG biedt momenteel een 34"" 21:9 Freesyncer aan voor minder dan €900, terwijl een G-Syncer met dezelfde specs nog 1200 moet opleveren. Wellicht was het prijsverschil nog groter toen jij jouw monitor kocht. Daar had je heel wat AMD GPU-power van kunnen kopen En op hoge refreshrates is het sowieso minder opvallend, mocht er een frame missen. Bovendien kan een monitor dan makkelijker overschakelen. De meest nuttige toepassing blijft bij lage framerates, maar op hoge framerates is het natuurlijk ook prima."
GTX 1060;2;0.44949138164520264;Bij hogere refresh rates van je monitor heb je minder tot geen last van de stutters die gsync wegneemt. Dus het verschil in tijd (ms) wanneer een GPU klaar is om een frame te tonen en wanneer de monitor het beeld ververst heeft is met hogere refresh rate een stuk kleiner (1/60, 1/100, 1/144 etc). Dit ligt ook wel aan de persoon hoe gevoelig die er voor is en het hoe hoog het aantal Hz van je monitor is. Maar vanaf 100-120Hz merk ook ik niets meer van de voordelen die gsync zou moeten geven en heb het daarom ook maar uitgezet omdat het ook weer een klein beetje lag/frame-time toevoegt. Edit: Je hebt trouwens OF gsync OF vsync aanstaan, niet allebei tegelijk.
GTX 1060;3;0.3819189667701721;Ik denk dat ik er erg gevoelig voor ben dan, want ik vind het echt heerlijk om naar die smoothness te kijken! Dat weet ik, ik gaf alleen aan dat mijn oude beeldscherm op 60hz met of zonder vsync heel anders aan voelde dan 60hz op mijn nu gsync monitor terwijl dat in feite het zelfde zou moeten zijn toch? ik bedoel 60hz is 60hz.. right?
GTX 1060;2;0.38222745060920715;Vsync wil je eigenlijk altijd uit hebben staan, dat geeft behoorlijk wat lag. Minimaal 30ms meen ik mij te herinneren van een video op youtube dacht ik. Deze was het geloof ik Je hebt nu een 100Hz monitor en dat doet ook al veel voor de smoothness, ook als je maar 40 tot 60FPS in een game haalt. Het is niet alleen gsync. Als je wilt vergelijken moet je je monitor ook op 60Hz zetten of bedoel je dat je dat ook gedaan hebt? Wat je zou moeten vergelijken met je oude monitor is 60Hz zonder en met gsync, 100Hz met en zonder gsync en dan ook alles met lage (+/-40) FPS en hoge (100+) FPS in game. Dan zul je waarschijnlijk merken wanneer je je monitor op 100Hz hebt draaien en 90+ FPS in games haalt dat gsync niet veel meer toevoegt. Misschien zelfs met rond onder de 60FPS in game ook niet. Ik merk het met mijn 144Hz monitor niet, maar ik weet niet of jouw 100Hz ook al genoeg is om niets van gsync te merken.
GTX 1060;1;0.28500181436538696;FreeSync is vooralsnog net zo hard een vendor lock-in als G-Sync..
GTX 1060;3;0.5948655009269714;Ja, maar als dat weinig tot niets kost, is dat natuurlijk niet zo relevant.
GTX 1060;3;0.41454991698265076;intel gaat het ook ondersteunen, en nvidia had het vorig jaar al kunnen actieveren in de 9 serie maar is daar te gierig voor.
GTX 1060;1;0.5500538349151611;Intel zegt al langer dat ze FreeSync gaan ondersteunen, tot nu toe is daar weinig van terecht gekomen. Verder had Nvidia al G-Sync uitgebracht toen AMD met FreeSync op de proppen kwam, en hadden het overgrote gedeelte van de markt in handen. Een concurrerende techniek ondersteunen is dan ook totaal niet logisch. Heeft niks met gierigheid te maken, maar eerder met een solide business plan.
GTX 1060;2;0.3812398910522461;Voor zover ik weet staat Nvidia amd niet toe om freesync te gebruiken, en anders wordt er sowieso wel een hoog bedrag voor gevraagd. Dat zie je o.a. ook terug in de G-Sync monitors. Je kan het moeilijk een vendor lock-in noemen als het een open standaard betreft.
GTX 1060;2;0.31296980381011963;G-Sync vereist integen stelling tot FreeSync custom hardware in de vorm van een FPGA. Dat drijft de prijs enorm op. AMD is tot nu toe de enige vendor die FreeSync ondersteund. Wil je FreeSync gebruiken dan zit je vooralsnog vast aan AMD. Of de standaard nu open of prioritair is verder irrelevant.
GTX 1060;2;0.36196938157081604;In de praktijk misschien nu wel, maar Intel komt er ook mee, en er komen straks nog veel meer schermen bij. Een echte vendor-lock-in zoals g-sync kan ik het niet noemen.
GTX 1060;1;0.7030901312828064;nu het opgenomen is in de VESA standaard verwacht ik niet dat het lang gaat duren. waarmee ze consumenten en met namen hun eigen klanten benadelen. Dat het kan en niet illegaal is wil toch niet zeggen dat je dat zomaar hoeft te accepteren van een bedrijf? dit soort dingen ga je toch niet als makke schapen gewoon slikken? kom op zeg. ze naaien hun eigen klanten, en de markt, en jij gaat ze nog lopen verdedigen ook. verder zit je helemaal niet vast aan AMD met freesync, want je betaalde helemaal niks extra's voor freesync. als je een nvidia koopt heb je dus geen verloren investering, je mist alleen de mogelijkheden van free sync. (wat, opnieuw, de keuze van nvidia is)
GTX 1060;2;0.30284398794174194;natuurlijk zal Intel het ooit wel gaan gaan ondersteunen, maar dat is tot nu toe alleen maar speculeren. Ik zeg dan ook solide en niet ethisch. Nvidia heeft geïnvesteerd in het ontwerpen van een FPGA en de bijbehorende marketing kosten. Ieder weldenkend bedrijf zal de kosten willen terug verdien en winst willen maken op de investering. Wil je echter het Adaptive Sync gedeelte van je monitor gebruiken weer wel.
GTX 1060;3;0.48338693380355835;Nee hoezo? Het primaire doel van een bedrijf is om winst te maken, niet om vriendjes te worden met de consument. (hoewel brand loyalty ook een positief effect kan zijn) Maar misschien moeten we deze discussie elders voort zetten aangezien we behoorlijk off topic gaan. Ik vind deze discussie wel zeer boeiend.
GTX 1060;2;0.36984744668006897;Ik zou beargumenteren dat G-Sync handig is voor iedere game die je met een unlocked framerate (en zonder V-Sync) wilt spelen. Ongeacht de fps. Dat weegt wmb. zwaarder dan ULMB! Wat de DX12 potentie van de RX480 betreft ben ik het met je eens, maar staat nu al vast dat Nvidia de DX12 performance van Pascal niet gaat verbeteren? En hoeveel games gaan async compute hevig implementeren? ULMB, strobing en vendor lock-in: helemaal met je eens! Maar gezien het track record van Nvidia gedurende op zn minst de afgelopen twee jaar lijkt het groene team een redelijke safe bet te zijn op dit moment. Persoonlijk zou ik er niet om rouwen om over drie of vier jaar m'n huidige G-Sync monitor af te schrijven en daarna een mooi 4k OLED paneeltje met Free/G-sync te kopen. Dat zou betekenen dat mn vendor lock-in twee generaties heeft geduurd (970+10XXXX), en dat zie ik niet als een probleem. Eerst mag AMD weleens met mooie high end kaartjes gaan komen though! Dan eventueel spijt krijgen van de vendor lock in en een 10XXXX moeten kop[en.
GTX 1060;1;0.3721267580986023;"Je had jezelf een paar maanden geleden ook 300 euro kunnen besparen op een Freesyncer met dezelfde specificaties, want dat is het prijsverschil tussen die 34"" Predators van Acer met G-Sync of Freesync. Had je alleen daarvan al een redelijk vette kaart kunnen scoren..."
GTX 1060;3;0.43937021493911743;Is het niet zo dat Freesync minder functionaliteit biedt dan G-Sync? Alleen binnen een bepaalde fps range oid?
GTX 1060;3;0.5336535573005676;Dat verschilt voor beide technieken per monitor. Over het algemeen wordt G-Sync wel op meer high-end modellen gezet waar vanzelfsprekend de range groter is, Nvidia stelt daar dan ook eisen aan. Freesync is een open standaard waar AMD dus geen eisen aan kan stellen. Over het algemeen is het verschil niet groot.
GTX 1060;3;0.47742703557014465;Ik snap wat jullie zeggen, maar ik ben niet minder blij met de monitor die jullie de grond in stampen Het is voor mij een weloverwogen beslissing geweest niet met freesync te gaan.. ten eerste wil ik geen AMD kaarten meer, heb ze jaren gehad en ben inmiddels van kamp groen geworden, niks mis met kamp rood overigens, gewoon een voorkeur. daarbij wilde ik meer dan 60hz, en 21:9 aspect ratio. Dan heb je maar 2 smaken, acer predator x34 of asus PG348Q. Qua design vind ik de acer predator X34P het mooist (komt in december pas naar nederland) maar qua built quality van wat we nu weten en harde feiten hebben is de Asus superieur. ik heb dan ook de asus.. enja.. op een beeldscherm van ruim €1400 maakt die €300 voor mij nou ook niet zo veel verschil
GTX 1060;3;0.4458469748497009;Al doende leert men, toch? Zo was het ook een beetje bedoeld als plek in de comments. Je monitor wordt er niet minder om - maar er zijn wel handigere keuzes te maken. De situatie was echter een tijdje geleden nog heel anders als nu, de Gsync/FreeSync keuze was een tijd bijzonder karig en dat is nu niet meer zo. Ik heb zelf ook niet al te lang geleden een 1080p 24 inch 120hz monitor (geen 'sync') aangeschaft en heb nu ook een beetje buyer's remorse (wil eigenlijk 1440p), maar ik denk dat je daar nooit echt vanaf zult zijn ^^
GTX 1060;2;0.31184107065200806;"Je kan 'm in de verkoop gooien en inderdaad 1440p nemen.. ik heb zelf meer dan 6 jaar op 1080p 60hz 24"" gegamed.. je kan dan de individuele pixels zien en dat was ik zat na verloop van tijd.. ik dacht goed ik ga een monitor kopen waarmee ik weer minimaal 5 a 6 jaar mee vooruit kan. Toen wilde ik op ieder vlak verandering. want meer pixels + meer inch vond ik niet een geweldige upgrade. Ik wilde dus: -meer dan 60hz -hogere resolutie dan 1080p -gsync -spectaculaire verandering punt één is gelukt: 60hz = 100hz punt twee is gelukt = 1080p = 1440p (ultrawide, dus geen 2560 maar 3440 breed) punt drie is gelukt punt vier is ook gelukt = curved en een 21:9 aspect ratio"
GTX 1060;2;0.4739589989185333;Ik stamp je monitor niet de grond in hoor! Tis natuurlijk een topding Maar je reactie op Vayra hierboven is natuurlijk nogal nutteloos. Mensen met een G-Sync monitor gaan altijd voor Nvidia en mensen met een Freesyncer gaan voor AMD (en Intels igp's, binnenkort). Net als dat je met zo'n Nvidia 3D-headset ook niet snel voor AMD kiest. Dat is vanzelfsprekend - al zou je ook als Nvidia gamer natuurlijk heel goed een Freesyncer kunnen overwegen omdat dat vaak bijna niks scheel met een normale monitor itt G-Sync. In jouw situatie is een GTX1060 sowieso behoorlijk beperkt, op de resolutie waarop jij speelt met bijbehorende framerates kijk jij eerder naar de 1070 en 1080. De discussie sterft zo nogal af, dat is het meer. Mensen die zich oriënteren kunnen zich beter focussen op de kaart alleen, of op het totaalplaatje en de bijbehorende kosten. Laten we kampen eens buiten beschouwing laten en de boel objectief bekijken, met juiste oriëntatie
GTX 1060;1;0.3024241030216217;Ach, hier op Tweakers zijn er gewoon mensen die om wat voor reden dan ook een bedrijf het zonlicht niet meer gunnen. Wees tevreden met je monitor! Het is een prima ding, en daar kan een ander niks aan veranderen. De GTX960 wordt hier ook elke keer weer met de grond gelijk gemaakt, maar daarbij vergeet men dat het alternatief van AMD op dat moment geen ZeroFan technologie had, een stuk meer verbruikte, en een H.265 decoder miste... Zolang een product in jouw wensenplaatje past, lekker kopen.
GTX 1060;1;0.7174846529960632;En wat als iemand 5000 euro per maand kan sparen en uitgeven? Of als iemand meer dan 100.000 euro 'over' heeft? Je leeft maar één keer en je geld kan je daarna toch niet meenemen. Daarnaast is gamen een goedkope hobby vergeleken met veel andere hobby's, zelfs als je dit soort dure monitoren koopt.
GTX 1060;3;0.3261684477329254;Rare analyse. De GTX 660 Ti was een goede deal ten op zichtte van de GTX 670. Alleen in enkele specifieke titels was er iets te merken van de beperkte bus. Bovendien had je de 660 Ti PE die met zijn factory overvolt 1200-1280 Mhz boost. Die was daarmee sneller dan menig GTX 670. Deze kaart scoorde een 9.7 op TechPowerUp. De x60 serie die ik daarvoor had was de GTX 460 Hawk. Dat is de leukste kaart die ik ooit heb gehad. Stock was dat ding 675 Mhz. MSI liet hem op 780 Mhz draaien. Echter dankzij de overvolting, uniek voor die tijd, kon je hem opschroeven naar 950 Mhz. Daarmee zat je onderhand op GTX 480 performance. Deze kaart scoorde een 9.4 op TechPowerUp. De GTX 260 daarvoor was ook weinig mis mee. Bij launch was hij een stuk duurder maar uiteindelijk heb ik een uitlopend (pre 216 core) model voor 139 euro bij CDromland gehaald. Tot op de dag van vandaag werkt die kaart nog steeds. In mijn ervaring is het juist eerder het tegenovergestelde. Op 1 of andere manier lijken alle 4870's te zijn gesneuveld. Daarnaast is het erg lastig met het legacy driver gezeur. Zelfs 'recente' kaarten als de 6870 zijn legacy tegenwoordig. Nu heb ik niet de illusie dat die GTX 260 en GTX 460 van mij nog echt optimalisaties krijgen (die zijn ook heus wel legacy). Echter je kunt gewoon een nieuwe driver downloaden en vrijwel alle games draaien er nog op (ook op Windows 10). Dit jaar merkte ik voor het eerst dat backwards compatabilty een probleem werd bij de GTX 260 (dx10). The Division en Battleborn weigerden te draaien. Op de 460 ging het wel, op de 6870 deed the devision het ook niet. Dus los van dat je uitspraak niet genuanceerd is lijkt mij eerder het tegenovergestelde waar. In de regel blijven mijn Nvidia kaarten het langer doen en zijn ze ook langer bruikbaar. Dat is in ieder geval de laatste 10 jaar zo geweest.
GTX 1060;1;0.639316737651825;Het gaat er niet om dat de kaarten legacy zijn of niet, het gaat erom dat je de games niet kunt spelen. Zoals dus meest recent The Division met de 6870. Verder doet de E350 (HD 6310 ) bijvoorbeeld geen hardware decoding op YouTube op Win10 via Crimson. De tegenhanger van de 4870 was overigens de gtx 260, niet de 460. De AMD ging toen aan de leiding, dat was de betere kaart. July 2009: ATI Radeon HD 4800 Series 7.03% NVIDIA GeForce GTX 260 2.42% Juni 2010 ATI Radeon HD 4800 Series 8.16% NVIDIA GeForce GTX 260 3.46% Dit jaar: GTX 260: 0.84% 4870: 0.51% Dus ze zijn echt verdwenen...
GTX 1060;1;0.5631022453308105;De 4870 zijn in duizenden mining rigs verdwenen en een martelingsdood gestorven. De 4850 en 4870 waren jarenlang de goto voor home/semipro-mining van bitcoins. Dat zal zich zo snel niet herhalen omdat mining op gpus inmiddels natuurlijk veel te inefficiënt is.
GTX 1060;1;0.32515084743499756;"ik denk eigenlijk niet dat die mining GPU's op de steam hardware survey terecht zijn gekomen (die zullen immers geen steam draaien) maar zoals ik hier onder al aan gaf, hij vergelijkt de ""4800 series"" met enkel de 4870, ipv met zowel de 4850 als de 4870. en dan blijken er ineens 3 keer zo veel 4800 serie kaarten nog in gebruik te zijn."
GTX 1060;2;0.4053473472595215;Dat bedoelde ik ook niet. Mining was de reden dat ze er dit jaar in verhouding met de nvidia inmiddels veel minder zijn.
GTX 1060;1;0.46416473388671875;"je vergelijkt de 4800 SERIE (dus zowel de 4850 als 4870) van toen, met alleen de 4870 nu. ze zijn dus niet verdwenen alleen uitgesplitst. Samen met de 1% van de 4850 zijn er dus nog steeds 2 keer zo veel als van de 260 en hebben ze dus nog gewoon nagenoeg de zelfde verhouding. dat had ik uitgelegd, puur toeval dat de nvidia nog werkt omdat de kaart erna practice de zelfde architectuur heeft gehouden. de e350 is 5 jaar oud, en crimson is daar niet voor geschikt en die GPU staat ook niet bij de supported products van de driver. crimson page zegt het zelf: ""AMD E-Series APUs with Radeon™ R2 Graphics"" de bobcat heeft een HD 6xxx serie en dus geen R2. je hoeft onder windows 10 helemaal geen driver te installeren voor de e350, en dat zal de pagina je vertellen als je hem laat detecteren of handmatig aangeeft welke hardware je precies hebt. dan kom je hier uit namelijk:"
GTX 1060;1;0.45208466053009033;Ik verzin niet hoe AMD die hardware aanmeld. Die verandering heb ik ook niet verzonnen, die heb ik ooit eens opgemerkt omdat ik elk jaar wel een keer op de hardware survey kijk. Het staat je vrij de hele geschiedenis uit te pluizen. Begin eens in 2011, dan zie je dat er ongeveer evenveel 4850's dan 4870's waren. Vergelijk dat met nu NVIDIA GeForce GTX 260 0.84% AMD Radeon HD 4850 0.96% AMD Radeon HD 4870 0.51% Gek he? Dat was nu juist het punt dat ik maakte. Volgens Vayra heb je langer plezier van je AMD videokaart. In werkelijkheid heb je na enkele jaren al 'gezeik' bij AMD. In mijn ervaring zit de probleemloze werking bij Nvidia op circa 8 jaar. Bij AMD is dat circa 4 jaar (dan pak ik de 7000 serie als laatst werkende serie). Als je geen driver installeert dan is YouTube al te veel voor de E-350. Kijk anders eens op het AMD forum als je mij niet geloofd: Verder bleken er ook dependencies te missen bij Crimson en moest ik die handmatig uit de temp mappen bij elkaar sprokkelen.
GTX 1060;2;0.5156278014183044;nee, maar je had er wel even beter op kunnen letten voor je wilde ongefundeerde uitspraken gaat doen. mensen die meer uit gaven aan een GPU zijn ook sneller geneigd te upgraden? verder hadden die 2 kaarten het zelfde PCB de zelfde componenten op geheugen na en was de hd4870 zelfs iets koeler als de hd4850. heel weinig reden dus om het verschil in gebruik nu aan hardware failen te wijden/ ik zie hier zeer zeker geen goed bewijs voor de claim dat AMD kaarten sneller kapot gaan. en mijn ervaring is dat de nvidia in mijn laptop kapot is na 3 jaar matig gebruik met maar heel af en toe een game, en de huidige nvidia in mijn werklaptop niet eens 2 schermen in daisychain kan aansturen zonder elke half uur een driver recovery te moeten doen. terwijl de ATI x700 (2004, met voltmod) nog prima werkt. en van allen AMD/ATI kaarten die ik heb gehad (5) alleen mijn 14 jaar oude 9700pro (2002) is echt kapot is gegaan. en dat is gewoon onzin. voor oudere kaarten zijn gewoon legacy drivers voor te krijgen en mijn vriendin's 5000 serie (2009 dus 7 jaar oud) werkt er prima mee op windows 10. dan kan je wel zeggen dat division nog werkt op een iets oudere nvidia als op AMD maar dat is, nogmaals, puur toeval omdat die nvidia niet officieel is ondersteund, en je probeert hemelbrede conclusies te verbinden aan een moment opname met maar 1 game en maar 1 kaart. tja, kon beter, maar de e350 is aardig niche, en is zeker niet indicatief voor de normale videokaarten. verder draait die e350 hier prima als linux server/media center.
GTX 1060;2;0.5043352246284485;Wat mij betreft verandert het helemaal niets aan de uitspraak. Bij benadering gingen de 260 en 4870 een tijd lang gelijk aan elkaar op. Ineens begonnen de 4870's te dalen. Dat gebeurde nadat die dingen al niet meer te koop waren. Het lijkt me niet dat mining daar een rol bij heeft gespeeld. Immers daarvoor wil je een nieuwe kaart anders ben je bij voorbaat kansloos. Dan hadden we dat eerder moeten zien gebeuren. Verder snap ik je punt dat je aan de laptop illusteert, met n=1 is het lastig uitspraken doen. Maargoed ik heb ondertussen 24 jaar ervaring met hardware en ik ben vrijwel altijd op de hoogte geweest van de ontwikkelingen. Dit is hoe ik er naar kijk. Als ik een uitspraak moet doen of je nu op lange termijn meer aan een Nvidia of aan een AMD hebt dan wint wat mij betreft de eerste. Hetgeen bevestigd wordt doordat ik hier nog o.a. een GTS 250 heb liggen waarmee je gewoon een game als Borderlands 2 op je lan kunt spelen. Werkt 100% zoals verwacht. Vayra impliceerde het tegenovergestelde. De e-350 was superieur aan het Atom alternatief van Intel. Ik was fan en aankoper van het eerste uur. Ik heb hier ook een positieve review geplaatst. Echter vanaf dag 1 was het een bak ellende wat drivers betrof. Ik weet niet hoe snel jij bent ingestapt maar zowel op Linux als Windows was het allemaal zeer problematisch. Zoals ik al zei is het anno 2016 opnieuw problematisch. Ik kreeg hem recentelijk bij een clean Windows 7 install niet normaal geïnstalleerd zonder rare fratsen uit te halen. Als ik de balans opmaak dan heb ik geen spijt van de E-350, de atom was gewoon te traag. Maar ondertussen zit er wel een G4400 in mijn NAS (aanrader overigens, bizar veel sneller met unrarren etc).
GTX 1060;3;0.5077789425849915;Inderdaad mooi om te zien, maar nou was de 980 ook niet heel spannend... een 970 met leuke overklok presteerde al bijna gelijk en kostte veel minder. De prijs/prestatie verhouding nekte de 980. Zelfde verhaal maar dan wat minder sterk met de 770 en 780.
GTX 1060;3;0.2872355282306671;Ik vind het zelfs nog wel interessant als deze kaart echt €280 wordt om 'm in te ruilen voor de GTX1060 tot de Ti kaarten uit komen..
GTX 1060;4;0.44571200013160706;De prijs maakt nu dan toch wel veel uit want als de gemiddelde 1060 kaart op 310-320 of meer zit dan heeft de RX 480 een prijsprestatievoordeel en daarin zit dan ook nog wat meer DX 12 potentieel.
GTX 1060;2;0.5068138837814331;Daarbij komt erbij dat je de 1060 niet in SLI (wat erg jammer is) kan zetten en bij de rx480 eventueel een ander rx480 kan aansluiten. Dus m.a.w. de 480 lijkt me wat meer future-proof. Voor mij betekent het gebrek van SLi support bij de 1060 toch een no-go, helaas.
GTX 1060;1;0.45473602414131165;En Nvidia kan niet mee komen op DX12. Voor de huidige DX11 spellen hebben ze nog voordeel, maar dat is straks n.v.t.
GTX 1060;5;0.46579238772392273;Ik zie nochtans in deze benchmarks dat de 1060 heel goed mee kan komen in DX12.
GTX 1060;2;0.4062243402004242;uhmm de ~280 euro kostende 480 verslaat de 370 euro kostende MSI in doom, hitman en warhammer, en enkel in de nvidia-gesponsorde game tombraider kan de 1060 hem voor blijven. Als we ashes niet nee nemen dan hoort tomb raiders er ook niet in thuis. en dan zouden we ook nog even naar de 4GB 480 kunnen kijken die praktische niks langzamer is maar wel beduidend goedkoper.
GTX 1060;1;0.26500391960144043;Die 370 euro kostte de RX480 op de dag van de release ook ongeveer, dat is heel snel gezakt. Soort early adopter tax Komt nog wel naar beneden die prijs
GTX 1060;1;0.4838675260543823;nee, op de dag van release kon je voor 280 een rx480 8Gb bestellen. enkel de eerste 2 winkels die prijzen naar buiten brachten hadden ze exhortation hoog staan.
GTX 1060;2;0.35036933422088623;In Nederland kun je ATM (vandaag) inderdaad geen 1060 voor adviesprijs kopen, in Engeland en Duitsland hebben ze al wel de juiste prijs (overclockers verkoopt de MSI voor zo'n 238 pond). M'n punt is, die prijs zakt nog wel naar een normaal niveau, dit zijn gewoon de webshops die wat extra winst willen pakken.
GTX 1060;1;0.42608642578125;Op de dag van de launch en enkele dagen later kon je een 219 euro RX 480 kaart kopen welke flashable was naar een RX 480 8GB
GTX 1060;3;0.5573305487632751;De 9 serie van nvidia doet het niet goed in dx12. Maar de 10 serie kan prima mee komen in dx12.
GTX 1060;1;0.5539245009422302;nou nee, ook de 10 serie failt behoorlijk. alleen in de door nvidia gesponsoorde game tombraider kunnen ze AMD voor blijven op het zelfde prijs punt. elke andere low level API game wint AMD met gemak.
GTX 1060;1;0.27607306838035583;Ook een bron?
GTX 1060;2;0.361956387758255;de review? in hitman, warhammer en doom word de beduidend duurdere 1060 verslagen door de 480. alleen in de door nvidia gesponsorde tombraider kan de 10 serie AMD voor blijven. en dan zijn gears of war en ashes nog niet eens meegenomen in deze review, die AMD ook wint. en dan hebben we nog niet eens gekeken naar async, waar het beste dat van de 10 serie gezegt kan worden is dat ze niet langer performance verliezen als het aangezet wordt. alleen in time spy kan nvidia een bescheide winstje laten zien maar timespy heeft dan nauwelijks echte async, enkel context switching, wat nvidia wel kan... met de driver wat je terug ziet in de CPU scores die naar beneden gaan, terwijl geen enkele AMD daar last van heeft.
GTX 1060;3;0.4801604151725769;Hitman en ashes zijn beide games waar AMD qua development erg dichtbij bij staat. Dat is soortgelijk als the witcher op nvidia kaarten. Doom draait overigens alleen echt lekker onder vulkan. In opengl kakken ze keihard in.
GTX 1060;1;0.6580421924591064;Witcher zit vol Propriety Nvidia Gameworks crap. Dat is toch wel ff wat anders dan een van de eerste DX12 games meehelpen met de implementatie ervan. En aangezien AMD al sinds GCN 1.0 zijn architectuur klaar heeft voor een dergelijke API was het logisch om bij hen aan te kloppen. Nvidia geloofd nog steeds heilig in een eeuwenoude architectuur die nu toch echt afgeschreven kan worden. Al zullen ze ongetwijfeld nog wel een tijdje met zakken geld strooien om dat te verlengen. (Zoals het door Nvidia gesponsorde Time Spy en z'n neppe implementatie van async)
GTX 1060;1;0.6058282256126404;Wat een onzin praat je nu. Amd teert juist enorm op de oude gcn technologie en moet het puur van raw power hebben. Nvidia is met enorme compressie technieken bezig geweest om met minder meer te doen. Vandaar dat ze ook zo zuinig zijn.
GTX 1060;1;0.5660563707351685;ROTR zou ik trouwens niet meerekenen voor DX12. Het is in essentie een DX11 spel in een DX12 wrapper
GTX 1060;2;0.37536710500717163;Doom is vulkan officieel niet ondersteund door de 1060. Warhammer dx12 was bugged as hell. Hitman zal ik zo nog even kijken. in mijn ogen een storm in een glas water wat mensen wel vaker last van hebben. /edit alleen de 1060 heeft moeite met hitman. De 1070/1080 tonen prima scores. Even 1 a 2 driver versies wachten. En hij komt gewoon de 480 voorbij zoals de rest.
GTX 1060;3;0.40322554111480713;True, maar ergens ook logisch. Hadden ze SLI ondersteuning gehad kocht niemand nog een 1080 natuurlijk.
GTX 1060;2;0.4204411506652832;Ja want dit is ook met de 960 gebeurt? Sli heeft veel nadelen. En is vrijwel alleen intresant als je in het hoog segment zit. Maar in laag/mid segment is sli onnodig.
GTX 1060;3;0.445907860994339;Multi GPU opstellingen hebben vele nadelen en zijjn zelden aan te raden, of het nu in low of high end segment zit. Maar dat is het punt hier niet. Als goedkoop upgrade pad kan het wel interessant zijn
GTX 1060;3;0.35843968391418457;Uiteraard is sli af te raden. Alleen wil je sneller als een 1080. Is de enige mogelijkheid 2x1080.
GTX 1060;3;0.3214726746082306;Kom even snel uit het jaar 2005 naar 2016 m8. SLI en CFX opstellingen zijn prima voor 99% van alle games. Draai al jaren CFX en dat gaat soepel. Een betere reden om geen SLI aan te bieden zou zijn omdat in de toekomst net zoals bij CFX er geen bridge meer nodig is voor multi-gpu opstellingen. Sterker nog, DX12 heeft gewoon een api waarmee multii-gpu opstellingen out-of-the-box werken zonder driver instellingen. Kijk maar even hier: of bijv. Daaruit maak ik op dat je prima 2 x GTX1060 kan draaien en een behoorlijke performance gain hebt.. zolang het een dx12 game is etc etc. Als de optimalisatie op DX12 niveau gefixed kan worden scheelt dat wel aanzienlijke knaken voor zowel nVidia als AMD.
GTX 1060;3;0.6055955290794373;maar in DX12 (en misschien ook Vulkan?) wordt multi gpu beter ondersteund mogelijks valt dat probleem dus weg en wordt het dus wél interessant (want 2x RX480 is veel goedkoper dan een GTX1080)
GTX 1060;3;0.5357950925827026;De 1060 verbruikt dan weer wat minder en is veel minder luid.
GTX 1060;1;0.2512931227684021;Iedereen weet dat AMD ref. coolers gewoon luid zijn. Maar wie koopt er ook ref.
GTX 1060;1;0.3940497636795044;Idd. Vind prijzen rond de 400 Euro eerlijk gezegd ook geen midrange prijzen. Midrange zou tussen de 250 - 350 Euro max moeten zijn, alles daarboven is geen midrange.
GTX 1060;2;0.5271182060241699;Dat is meer het resultaat van de tekorten. (Al dan niet artificieel) Als ze nou gewoon eens lekker gingen (of als iedereen 3 maanden geduld had) zullen beide kaarten wel een stuk beter geprijsd zijn denk ik. Nu kijk je tegen de gokjes van sales managers aan, die hopen even een 150% marge te halen.
GTX 1060;3;0.4901391565799713;Voor mij ook wel belangrijk, nu ben ik in de basis een console gamer dus met high end videokaarten doe ik niet mee. Toch is mijn 1Gb 7770 toe aan vervanging (om even aan te geven dat hij het voor mij tot nu toe goed deed). De RX 480 was mijn eerste keus, nu na dit lezen is de 1060 ook niet verkeerd maar wel erg prijzig. Hij scoort dus blijkbaar een klein stukje beter (kort door de bocht) maar is bij 80 euro duurder. Hier ga ik er vanuit dat binnen enkele maanden de prijzen genormaliseerd zijn. AMD zet hem in de markt voor 200 en hoger en Nvidia voor 280 en hoger. Voor mij is dan 80 euro een hoop als ik het niet zozeer terugzie. De high end gamer zal toch wel zijn neus ophalen en voor de 1070/1080 gaan. Stroomverbruik vind ik trouwens wel zeer netjes.
GTX 1060;1;0.28063052892684937;1060 aftermarket met 6gb gespot voor €286
GTX 1060;1;0.6097662448883057;voorraad 0, voorraad leverancier 0
GTX 1060;3;0.31404757499694824;Ja en dat dan zal je even moeten wachten. Maar zullen gewoon voor die prijs moeten leveren.
GTX 1060;4;0.2733575105667114;Oplichters zaak? Staat gewoon goed aangeschreven, ook hier op tweakers.
GTX 1060;5;0.41095083951950073;Heb anders nooit problemen mee gehad.
GTX 1060;2;0.39129677414894104;ik zie steeds duidelijker het voordeel van DX12 en Vulkan en het hogere potentieel dat AMD weet te bieden vs. de oudere API. Tuurlijk, Nvidia zal wel met een driver komen om het verschil kleiner te maken maar voelt toch een beetje als vastklampen aan oude techniek (CD en muziek, ook zoiets). Wat mij betreft tijd dat alle gameontwikkelaars zich puur gaan richten op DX12 en vulkan in plaats van het blijven oplappen van DX11 met drivers. Als Nvidia dan ook overstapt op nieuwe architectuur is daar iedere(en) (gamer) mee gebaat
GTX 1060;3;0.36727216839790344;er zijn met directx 12 / vulkan natuurlijk twee verschillende zaken aan de hand, en die moet je eigenlijk wel los van elkaar vergelijken. ten eerste is er het feit dat de api dichter bij de hardware zit. hierdoor zou de overhead voor grafische taken minder moeten zijn dan met directx 11 of opengl, omdat de opdrachten simpelweg door minder lagen heen moeten gaan. je moet hier echter wel rekening bij houden dat als je een zeer snelle cpu gebruikt, deze overhead toch al vrij klein zal zijn. dit effect zal je veel meer merken als je bijvoorbeeld een sterke grafische kaart test op een vrij zwakke cpu. ik verwacht dat dit voordeel vergelijkbaar is op zowel nvidia als amd hardware, aangezien er eigenlijk alleen software lagen tussenuit geknipt wordt, en je voordeel krijgt doordat die zaken niet meer door de cpu heen hoeven. het tweede voordeel staat eigenlijk los van directx 12 of vulkan, maar wordt er door mogelijk gemaakt, en dat is async compute. daarmee kunnen bepaalde taken die voorheen sequentieel uitgevoerd werden, nu simultaan gedaan worden. als een nvidia kaart dan 3200 cores heeft ( die allen iets sneller zijn ), en met directx 11 worden er ook 3000 benut, dan heb je met async compute er maar 200 bij ( zo ' n ~ 6 % laten we zeggen ). nu weet ik dus niet hoe accuraat dit is, en hoe dit in de werkelijkheid echt zit, maar het idee is wel belangrijk. directx 11 vs 12 is dus een compleet ander verhaal dan async compute uit vs async compute aan. en nvidia zou wel eens niet veel sneller kunnen zijn met async compute dan zonder async compute, simpelweg omdat de hardware al volledig benut zou worden. om dit goed te bekijken, zou het dus interessant zijn om verschillende tests te doen. zowel de top - end amd kaart als de top - end nvidia kaart met een vrij slechte cpu testen op directx 11 of opengl vs directx 12 of vulkan zonder async compute. en een andere test ( met een sterke cpu ) op directx 12 of vulkan met en zonder async compute, om dat geval in isolatie te testen. edit : filmpje ondertussen teruggevonden :
GTX 1060;2;0.4636238217353821;Je kan het ook anders zien. AMD heeft nooit alles uit dX11 weten te halen. Nvidia heeft dit wel altijd gekunt. Betreft de nieuwe api's er is nogsteeds bar weinig materiaal om echt goed over te kunnen oordelen. De grote games die er zijn. Zijn vrijwel allemaal door AMD gesponsord (hitman, ashes of singularity, tomb raider).
GTX 1060;2;0.4232255816459656;Dat nVidia achterloopt op DX gebied is nou niet echt schokkend. Dit is altijd zo geweest, vandaar was DX10 zou gestript van functies omdat nVidia hun zaken niet op orde hadden.
GTX 1060;5;0.46816486120224;In DX11 en OGL zijn ze anders wel heer en meester.
GTX 1060;3;0.48796913027763367;Ik wacht wel op de rx480 aftermarket... als de huidige dx12 titels iets doen voorspellen zoals: warhammer,doom en hitman vs tombraider dan lijkt de rx480 aftermarket de sweetspot voor het midden segment is voor dx12 en vulkan titels die het komend jaar en de jaren daaropvolgend uitkomen.
GTX 1060;1;0.3277450203895569;NVIDIA zal vast niet stil gaan zitten als het op Drivers voor DX12 aankomt.
GTX 1060;3;0.3662750720977783;"Klopt, dit is nvidia's software oplossing : Dynamic load balancing & Preemption Dynamic load balancing Preemption ""Even with all of these additions, Pascal still won’t quite match GCN. GCN is able to run async compute at the SM/CU level, meaning each SM/CU can work on both graphics and compute at the same time, allowing even better efficiency."" ""Nonetheless, Pascal is finally offering a solution with hardware scheduled async compute, bringing Nvidia closer to AMD. Either way, with both Nvidia and AMD working on async compute, developers are more likely to take notice and make sure our GPUs are fully utilized."" Source:"
GTX 1060;2;0.33209896087646484;Als ik kijk naar puntje 11. Prestatie-index, zijn jullie volgens mij de aller belangrijkste vergeten in het middensegment, namelijk de prijs/prestatie verhouding. Geen idee of dit bewust is gedaan of domweg een foutje maar bij de RX 480 review stond die grafiek er wel reviews: AMD Radeon RX 480: CrossFire, VR, DX12 en overklok getest Het lijkt me dat je met onderstaande data toch de benodigde doorslaggevende grafieken kan berekenen en van daar uit ook een echt advies kan geven welke kaart je nu dient aan te schaffen? Hint: AMD is hier aan de leiding
GTX 1060;3;0.3666566014289856;Nee want de prijzen zijn nog niet echt stabiel. Ik zou graag de performance per watt grafieken zien. Dan kan je ook echt advies geven over welke kaart je dient aan te schaffen. Hint: NVIDIA is hier aan de leiding.
GTX 1060;2;0.4264613389968872;Dit was nochtans geen probleem bij de 480 launch. De prijs/performance werd zelfs in een nieuwsbericht aangehaald met de 'verkeerde' prijzen. Dit is echt meten met 2 verschillende maten op deze manier. Ik bedoel: kijk naar de launch prijs van bv. de MSI gamging: pricewatch: MSI GeForce GTX 1060 Gaming X 6G 379 kost hij nu aan keiharde euro's vs 275 euro voor de RX 480 8GB. Op basis daarvan kun je gerust stellen dat de prijs/performance bar slecht is, en de gamers die op 1080p wensen te gamen beter de 480 (4gb eig) kunnen aanschaffen. Als je dit in een grafiek giet gaat Nvidia af als een gieter en zijn ze duidelijk de verliezende partij. Performance/Watt is leuk, maar niet beslissend in dit segment (en volgens mij bijna nooit buiten de server markt). Je gaat echt niet honderd euro extra uitgeven om enkele watts te besparen. Koop dan een efficiëntere voeding, die kan je tenminste meenemen naar je volgende systeem.
GTX 1060;5;0.27245455980300903;voor ongeveer die prijs kan je een rx480 kopen of een r9 390X
GTX 1060;1;0.4995978772640228;We hebben dat inderdaad de vorige keer gedaan, maar de prijzen schommelden de eerste dagen enorm, waardoor je die indexen eigenlijk niet goed kon gebruiken. Ook nu nog zien we dat de prijzen erg variabel zijn en - vooral nu er weinig voorraad is - weer omhoog gaan. Vandaar dit keer geen prijs/prestatie-index.
GTX 1060;2;0.3928070366382599;Dan zou ik toch een serieuze waarschuwing verwachten om de kaart uit jullie review (MSI gaming) niet te kopen vanwege de te hoge prijs Momenteel. Het is mogelijk dat hij nog gaat zakken, maar dat geld voor alle kaarten. Daarbij vind ik dat Nvidia hun claims met prijzen 'vanaf' eerlijk gezegd onzin. Je komt het overal tegen in nieuwsberichten en reviews, maar de realiteit is gewoon anders. Dat mag toch vermeld worden in de review, of dat kan alleen bij AMD? Zie ook de minpunten bij de andere grote reviews hier op Tweakers van Foritein en TERW_DAN pricewatch: MSI GeForce GTX 1060 Gaming X 6G Zijn jullie van plan om over 2 weken dan wel een dergelijke vergelijking te publiceren voor de prijs/performance per resolutie? Dan kan het op deze manier recht gezet worden. Nu is het een groot 'happy' verhaaltje maar het pijnpunt, prijzen en beschikbaarheid worden vermeden imo.
GTX 1060;2;0.4523116946220398;Op het moment dat de review online gaat weten wij nog niks, enkel de adviesprijzen van Nvidia en soms een adviesprijs van een fabrikant. Winkels bepalen echter weer zelf wat ze voor een kaart vragen. We zouden de review na publicatie kunnen updaten, maar uit ervaring weten we dat de prijzen behoorlijk schommelen, dus dan zouden we in de dagen na release dagelijks het artikel bij kunnen werken. Daar wordt het niet duidelijker op. Als de custom versies van de GTX1060 en RX480 - toch de versies die de meesten zullen kopen - een beetje goed beschikbaar zijn, zullen we die tegen elkaar afzetten. Tegen die tijd zullen de prijzen hopelijk wat gestabiliseerd zijn zodat je goed kunt zien welke kaart de meeste performance voor je euro's geeft.
GTX 1060;3;0.46940624713897705;De X (performance) blijft gelijk, terwijl de Y (prijs) variabel is, dit moet toch wel een dynamische tabel te verwerken zijn? Met de BBG heb je bijvoorbeeld ook een prijs op moment van publicatie en een prijs real time uit de PW opgehaald. een en ander moet dan toch te koppelen zijn en X per Y is Z. Ik vind het een beetje Tweakers onwaardig om excusen te zoeken om iets niet te doen, dan te kijken wat mogelijk is.
GTX 1060;1;0.31334632635116577;hopelijk komt die er nog bij, want zelfs als je nog maar af gaat op de adviesprijzen, dan zie je de RX480 helemaal naar de kop van het pak schieten.
GTX 1060;3;0.4405989348888397;Mooie kaart en een goede insteek in de huidige markt lijkt me. Echter wat ik niet aan die prijzen snap is hoe nvidia hierin aan het schalen is gegaan. Toen ik mijn GTX770 kocht heb ik daar €322 voor betaald. Toen vond ik dat een extreem dure kaart voor toch wel behoorlijk zo niet hele goede prestaties. De opvolger GTX970 was alweer wat duurder, maar met ~€380 vond ik dit nog begrijpelijk, ook gezien de inflatie ed. Maar op dit moment zie ik de gtx1070 (logische opvolger van de 970) voor >€500, en de GTX1080 is met ruim €800 helemaal onbetaalbaar. De GTX1060 bied dezelfde prestaties als een GTX980 en heeft de prijs van mijn (toen nieuwe) GTX770. Is Nvidia hier nu aan het schuiven met hun producten. Wellicht dat iemand hier een goede uitleg voor kan geven.
GTX 1060;2;0.4563908874988556;Aantal redenen die ik kan bedenken: 1. Adcocaat van de duivel argument: Weinig concurrentie in het high-end segment, Nvidia kan flink verdienen op de early adopters. 2. In a perfect world argument: Nieuw productie procedé van 16nm finfet, hierdoor slechte yields en hoge R&D kosten die terugverdient worden. 3. De euro-dollar is in ongeveer 2 jaar tijd volledig naar beneden gekelderd, dit heeft de europrijzen helaas relatief hard doen stijgen. 4. De 1070 lijkt een logische opvolger van de 970 qua naamgeving maar is dit in verhouding natuurlijk niet. Voordat Pascal uit kwam was je dik €700 kwijt voor een 980 ti. Nu heb je voor ongeveer €500 een 1070 met 980 ti prestaties voor een lager bedrag met minder verbruik. Dit is hoe ik er over denk in ieder geval. De prijzen voor de 1080 vind ik wel belachelijk en die hele 'founders edition' mag ook direct opdoeken. De 1070 zou mooi in balans zijn voor een bedrag van €430 a €450 in mijn ogen. 1440p, waar ik op game, is wel een stuk betaalbaarder geworden dan
GTX 1060;1;0.4234860837459564;Zo werkt dat natuurlijk niet. Als dat de gang van zaken was, zaten we inmiddels op GPU's van €9999, want er zijn meer dan eens nieuwe generaties geweest die de prestaties van de vorige generatie bijna verdubbelden. Dat is wel hoe Nvidia het met Maxwell en Pascal in de markt zet, maar de 1070/1080 zijn gewoon de opvolgers van de 970/980. Naamgeving van zowel de kaart als de chip en de die size laten dit zien. Volgens jouw logica had de 8800 GTX geen voorganger, aangezien zelfs 2 7900 GTX'en in SLI die niet bij hielden. Dan had de 8800 GTX dus makkelijk €1100 moeten kosten. Een GTX 280 had dan wel €1600 ofzo mogen zijn. Een GTX 480 dan weer makkelijk zo'n €3000. Een 1060, 1070 en 1080 volgen gewoon de 960, 970 en 980 op. En dit zijn op hun beurt de opvolgers van de 560 Ti en lager.
GTX 1060;3;0.25706931948661804;"Dit is dus precies wat mijn gevoel ook was. Nvidia plaatst qua prijzen de GTX1070 in de markt als een GTX980, tuurlijk presteren ze nagenoeg hetzelfde omdat de chips beter worden, maar een echt alternatief aan de onderkant komt er dus niet bij. Of gaan ze straks een ""GTX1040"" uitbrengen die de specs van een GTX950 moet vertegenwoordigen? ""De prijzen laten het hier mijn inziens namelijk op lijken"""
GTX 1060;1;0.45018014311790466;Indien de 1060 ietwat genegeerd word en men daadwerkelijk overstapt op de rx480 dan zullen de prijzen terug wat worden gelijkgetrokken imo. Alleen spijtig dat deze laatste vaker dan niet een €40-€60 duurder uitvalt bij de shops die er wat winst op willen pakken. Denk niet dat AMD het zich kan veroorloven om ze op de vingers te tikken en levering kan weigeren. En daarbovenop nu nog een review van een custom OC 1060 waar mensen (nog) geen cijfertjes kunnen zien van een custom rx480.
GTX 1060;3;0.5480086803436279;Allemaal leuk en wel, maar ik ben bang dat die adviesprijs van rond de 280 euro, vanwege de tekorten en generale nederlandse naaierij, eerder iets van 350+ euro gaat worden hier.
GTX 1060;3;0.4263466000556946;De prijs-prestatie verhouding zal inderdaad alles bepalend worden. Nog even wachten en dan de conclusie trekken denk ik dan maar.
GTX 1060;3;0.3054121732711792;Precies wat je zegt. Ik hoor verhalen van 280 euro tot prijzen richting de 400 euro, dat zegt nogal wat over hoe interessant een GTX 1060 is / gaat worden. Daarnaast houdt het bij AMD ook niet over wat beschikbaarheid (zeker aftermarket) of prijzen betreft, dus het is allemaal nog een beetje afwachten. En ook zoals je zegt, het is daardoor heel lastig om nu een conclusie te trekken alvorens we ook nog maar in de glazen bol hebben gekeken wat DX12 enzo betreft, pfff.
GTX 1060;1;0.3669385612010956;Ik zie op Amerikaanse websites dat de prijs tussen de 260 en 320 dollar ( ligt.
GTX 1060;3;0.37136074900627136;Plus import/btw, dan zou 280-350 wel een richtlijn moeten zijn. Prijzen die je nu ook bij caseking ziet staan voor tal van 1060's.
GTX 1060;2;0.5238739848136902;Dat vind ik toch geen mid-range prijzen meer hoor maar wel high-end. De reviewer vind deze prijzen te rechtvaardigen vanwege de goede prestaties, ik niet. De prijzen van de RX480 gaan sowieso lager liggen en meer aanleunen bij de mid-range.
GTX 1060;2;0.4014248549938202;Je geeft het zelf al een beetje aan.. 'gaan liggen'. Het is echt nog afwachten, want op dit moment zie ik ook geen RX 480 kaarten voor minder dan 300 euro leverbaar. Als 280 eur al echt buiten je budget zit zal je nog heel even moeten wachten, de kans dat er spoedig een RX 470 en/of een GTX 1050 zullen volgen lijken mij wel aanwezig.
GTX 1060;4;0.30419841408729553;Interessante opmerking! Ik dacht dat dit voor zij die nVidea prefereren, niet bijzonder relevant was. Is dat veranderd of eerder omdat het nu goed uit lijkt te komen, wat denk jij? Op het gebied van de prijs prestatie verhouding was AMD toch lang koning? [Opgepast, na deze zin volgt een mening.] Voor mij geen nVidea ever Ze verk*er de markt door computer spellen fabrikanten te sponsoren als zij toevallig hun spel afstellen op nVidea only blabla. Op zich geen probleem! - behalve als het gros van de gamers hier door aanneemt dat nVidea per definitie beter is met alle gevolgen van dien.. Daar komt bij dat het de concurrentie belemmerd in de ICT markt.
GTX 1060;5;0.3155335783958435;*NVIDIA
GTX 1060;3;0.333671510219574;Alternate durft gewoon bijna dik 400 euro te vragen azerty doet het iets netter
GTX 1060;1;0.5611858367919922;Alternate is altijd al verschrikkelijk duur.
GTX 1060;1;0.5716603994369507;Ze zijn na mijn idee duurder geworden nadat ze stofzuigers en andere onzin zijn gaan verkopen
GTX 1060;3;0.5433283448219299;Wat mij meteen opvalt is de smalle bus en de beperkte bandbreedte. In de review van techpowerup zie je dat het verschil tussen de GTX 1060 en de R9 290X op 1920x1080 15 procent is in het voordeel van de GTX 960 en op 4k is dat maar 2 procent. Nu lijkt het geheugen wel aardig overklokbaar maar op hoge resoluties zou ik niet zomaar deze kaart aanbevelen.
GTX 1060;2;0.3847498595714569;Dat zegt mogelijk meer over de 290X dan de 1060. Als ik naar mijn benchmark resultaten kijk tussen de nieuwere RX48 en de GTX 1060 (en dan enkel die op hoge instellingen) dan zie ik diezelfde 15-20% winst op 1080p terugkeren op 1440p en 4K. Dat is dus wel met een factory-oc GTX 1060 vs de reference RX 480 (dat is wat ik nu eenmaal heb liggen), maar zonder noemenswaardige memory clock boost, en gezien de onderlinge verhoudingen niet echt een indicatie dat 'de bandbreedte' een echte rol speelt. Daarbij is 4K gaming sowieso behelpen met alles minder dan een GTX 1080, maar het nut van 4K is een andere discussie
GTX 1060;3;0.38595443964004517;"wel lache dat de ""standaard'' rx 480 in sommige bench en games zelf beter is dan de OC versie gtx 1060! voor mij geen rx 480 (heb nu een 390 dus nuteloos) maar zeker wel AMD. en die nvidia prijzen, man man man je bent toch wel gestoord als je dat gaat kopen. dat is het zelfde als een nissan gtr kopen met prestaties van een golf."
GTX 1060;3;0.5069949626922607;Ik vind de conclusie wel heel erg neutraal. Ze mogen daar best uitspreken dat de prijs / perf van een 480 op dit moment gewoon beter is dan die van de GTX 1060.
GTX 1060;5;0.8714947700500488;Wow ik ben meer enthusiast over die amd rx 480. Meer en meer games gaan gebruik maken van de vulkan en dx12. Amd gaat ondertussen ook drivers uitbrengen die hem wat sneller maakt ook nog. Heeft ook een mooie prijskaartje.
GTX 1060;1;0.4900730550289154;En NVIDIA brengt geen Drivers meer uit?
GTX 1060;2;0.38869795203208923;"Jawel. Alleen heeft AMD erop gegokt dat DX12(.1 toch?) het helemaal wordt, en de support hard-coded ingebakken. Dat was (als ik het ontwikkelingsproces goed heb gelezen) veel minder het geval bij Nvidia. Met als gevolg dat Nvdia meer voor softwarematige oplossingen zal kiezen dan AMD. Staat weer tegenover dat Nvidia zuiniger en sneller is (en idd compressie veel beter snapt). Dan blijf je dus zitten met de vraag: Een 219€ AMD 4GB vs een 279€ Nvidia 3GB; wat gaat het worden?"
GTX 1060;2;0.22781780362129211;Tegen de tijd dat er genoeg DX12 Games zijn ben je al lang weer toe aan een Upgrade. DX12 games zijn op 1 hand te tellen, daarvoor zou ik geen RX 480 aan gaan schaffen aangezien de GTX 1060 het wint in alle DX11 titels.
GTX 1060;2;0.31823715567588806;ik weet niet hoe vaak jij een nieuwe kaart koopt, maar als ik nu een kaart koop verwacht ik daar jaren mee te kunnen. als je dat niet kunt kan het al gauw uit om nu wel een duurder model te kopen ( 1080 bijv ), en die dan wel zo lang te bewaren, want vaak vervangen met dit soort kleine perf winsten kan gewoon niet uit. deze generatie is een van de * hoogtepunten * van de afgelopen paar jaar, en daarmee nog steeds veel minder indrukwekkend dan de verbeteringen ervoor. de slechte dx12 resultaten baren me daar wel wat zorgen - de enige benchmark die daar uit de toon springt is rottr, maar die is zeer dubieus omdat daar dx11 gewoon sneller was - oftwel, een compleet zinloze benchmark, want je zou nog op amd nog op nvidia ooit het spel zo draaien. vooral de minimum framerates waren abominable. weet iemand of de laatste patch dit fixed? iig, voor nu is het het veiligste om die hele bench niet seurieus te nemen. er is een rapport van iemand die 40 % (! ) van de power eraf wist te halen. als je dus niet de allernieuwste spellen wilt spelen ( zeg, doom ofzo ), of je gfx kaart snel wilt vervangen ( en dus dx11 belangrijker is dan in mijn verwachting - maar dit is financieel onverstandig ), en weinig compute doet of specifiek weet dat de compute load nvidia vriendelijk is, dan is de 1060 een hele sterke keus. maar als normale consument die een kaart wel jarenlang houdt, of mogelijk wel nu al de nieuwste spellen wil spelen, of veel compute doet is de 1060 gewoon niet heel overtuigend. eerlijk gezegd ben ik wat teleurgesteld in deze generatie kaarten ueberhaupt - de processshrink was behoorlijk gehyped, en ja, ze zijn sneller. maar het stelt niet zo gek veel voor.
GTX 1060;1;0.48915594816207886;Sorry hoor, maar de economische levensduur van elektronica is 3 tot 5 jaar. Als er over 3 jaar geen DX12 games op de markt zijn, zou ik mij toch serieus beginnen afvragen of software development nog wel zin heeft...
GTX 1060;4;0.46571895480155945;"Deze kaart presteert iets beter dan de RX 480 en is ook iets duurder. De verdere prijsontwikkeling zal ""sweetspot"" keuze maken..."
GTX 1060;2;0.4836881756782532;Mijn aanname was/is dat de 'sweetspot' niet relevant is voor veel nVidea fanaten. Het verbaast me dat er - voor mijn gevoel - opeens zo veel aandacht is voor prijs prestatie Wat is jullie idee hieromtrent? [twijfelt intussen enorm of er nu meer of minder aandacht is voor de prijs / prestatie verhouding in vergelijking met voorgaande 'releases']
GTX 1060;2;0.4978347420692444;"Er is meer aandacht voor omdat AMD PR dat zo wil en dat is de enige manier waarmee ze kunnen concurreren. Ze proberen zo de aandacht van de performance per watt verhouding weg te houden want dat is heel nadelig voor hen;"
GTX 1060;2;0.2719927132129669;"Ik denk eerder doordat het komt omdat de 1070/1080 kaarten wel héél erg duur zijn geworden. Leuk en aardig dat je er heel veel voor terugkrijgt, voor de meeste mensen die op 1080P gamen is het gewoon teveel geld. Misschien dat ze dachten bij nVidia ""Apple komt er al jaren meeweg, dus dat moet dan ons ook lukken om te dure producten in de markt te zetten"". Tja ik denk dat ze bedrogen zijn uitgekomen. Hieronder wordt de performance per watt weer eens van stel gehaald, een argument dat met het uitkomen van de 480 toch redelijk makkelijk onderuit te halen is. Die is sterk, en dan ook sterk verbeterd! nvidia's waren zuiniger in het verleden, ook weer niet zo zuinig dat het de vaak forsere meerprijs ruim te verdedigen viel. duurde toch wel een paar jaar eer je die meerprijs terugverdient had. uiteindelijk valt en staat alles met een prijs. uitzonderlijke prestaties voor een dito prijs? tsja dan maar iets minder uitzonderlijk en een vriendelijker prijskaartje,."
GTX 1060;3;0.5580453872680664;Als de prijs rond €280 zal gaan liggen is dit een prima keuze! Alles daarboven maakt de kaart minder aantrekkelijk.
GTX 1060;3;0.46117910742759705;Ik was bijna vergeten dat de 1060 er ook nog aan zat te komen. Ik wacht in ieder geval nog even. De 1060 is een leuke opvolger voor mijn 970, de prestatiewinst is niet heel groot maar het kost ook weinig extra. Tenminste, als de prijzen normaal worden en niet nog lange tijd zo achterlijk hoog blijven als bij de 1070/1080. Fast Sync is een leuke bonus en ook het lagere geluidsniveau is bij mij meer dan welkom. Voor die paar euro extra die het (zou moeten gaan) kosten... Geduld zal worden beloond.
GTX 1060;3;0.45085781812667847;Van wat ik momenteel meekrijg ziet het er niet al te goed uit. In het buitenland zijn de eerste prijzen al binnen en ze liggen wederom net als bij de 1070 en 1080 ver boven de adviesprijs . Dit is normaal voor de aller eerste samples natuurlijk, maar ze zijn ook bij hun grotere broers nog steeds niet omlaag Gains in Vulkan zijn tot nog toe ook zeer minimaal wat jammer is omdat dat wel echt goed geoptimaliseerde software is waar we uiteindelijk heen moeten. Het is best wel ironisch dat AMD's drivers beter zijn dan die van Nvidia met een tig keer zo klein research team en budget. AMD laat echter ook sterk op zich wachten, de meeste custom 480 kaarten komen waarschijnlijk pas rond 2e week augstus uit. Wel heb ik heel veel hoop voor deze kaarten, de huidige limieten op de 480 zijn voornamelijk power en cooling omdat de reference editie ook figuurlijk blowt. Zou ook zeker geen reference 480 halen tenzij je echt wil crossfiren ofzo. Ik denk dat deze strijd bepaald gaat worden door hoe goed de custom 480's zullen gaan overclocken, omdat de 1060 al bijna tot zijn limiet gepushed is, en tevens of de 1060 geen hevige prijs stijgingen gaat ondervinden zoals momenteel zeer waarschijnlijk is. Helaas niet zeer optimistisch op basis van het recente verleden met Nvidia...
GTX 1060;2;0.4813295900821686;Als je op dit moment een GTX 1060 koopt moet je toch ook wel leip zijn. Je kan voor +/- €400 zelfs meestal minder een GTX 980ti kopen. Deze is toch wel een tandje sneller dan de 1060 maar gebruikt wel een stuk meer stroom. In mijn ogen is het geen goede zet om op dit moment een GTX 1060 te kopen. De RX 480 is qua waar voor je geld wel een stuk beter als komende week de custom gekoelde RX 480's uitkomen. Trouwens nog cheers naar Nvidia voor op de lunchday meteen custom gekoelde kaarten aan te bieden!
GTX 1060;1;0.3901718854904175;Wat is er gebeurd bij de total warhammer dx12 test? De 980ti staat onder de 970. Lijkt me dat hier iets is misgegaan. Ook in doom onder of vrijwel gelijk aan de 980.
GTX 1060;1;0.8760749697685242;Kijk eens naar de adviesprijzen van de vorige kaarten GTX 1060: $249 GTX 960: $199 GTX 760: $249 GTX 660: $229 GTX 560: $199 GTX 460: $229 ($199 voor 768MB) GTX 260 core 216: $279 Als je gaat corrigeren voor inflatie dan is de GTX 1060 eerder goedkoper dan duurder geworden. Dat de euro prijs zoveel hoger uitvalt kan Nvidia niets aan doen en heeft ook helemaal niets met de positionering van de kaart te maken. De hogere prijs volgt 100% uit de euro dollar koers. Als jij op 20 September 2014 (GTX 980) een euro aan de Amerikanen gaf dan kreeg je daar 1.2829 dollars voor terug. Je koopt dus met 28% korting van Nvidia. Als jij vandaag een euro aan de Amerikanen geeft dan krijg je daar 1.102 voor terug, slechts 10% korting. De afgelopen weken is dit zelfs richting de 0 gedaald. Voor de mensen die denken, maar die GTX 260 was toch veel goedkoper? Dat klopt op 16 september 2008 was de euro dollar koers 1,4792. Toen kreeg je dus maar liefst 48% korting.
GTX 1060;4;0.5175046920776367;"Prima kaart zo te zien. Uitgebreide review ""duimpje"". Is het een optie het prestatie verschil tussen 2560x1440 / 3440x1440 ook in kaart te brengen in aankomende reviews? Dus 3440 ook testen (verschil in cijfers ~3,7m pixels vs ~5m pixels zou op 26% verschil uitkomen). Voor mij voorlopig nog geen reden om te upgraden met m'n 970 sli opstelling."
GTX 1060;2;0.4766691029071808;De resultaten voor WQHD en UHD bij Far Cry 4 en GTA5 lijken verwisseld te zijn: alle kaarten halen namelijk nu hogere FPS op UHD dan op WQHD...... [Edit: van de auteur kreeg ik in een andere draad een reactie op mijn opmerking]: Die resultaten kloppen gewoon: de framerate ligt hoger op 4k omdat we daarbij de grafische settings omlaag draaien, anders houd je nauwelijks nog speelbare framerates over, waardoor je de videokaarten niet onderling kunt vergelijken.
GTX 1060;3;0.3504398763179779;De heren van Digital Foundry komen tot een vergelijkbare conclusie: Het is dus echt stuivertje wisselen tussen de twee. RX480 wordt namelijk niet ala slam dunk verslagen, maar is redelijk gelijkwaardig te noemen. De uiteindelijke keuze gaat dus liggen in wat je belangrijker zult vinden. Crossfire en 8 GB ram, of iets betere performance, significant lager stroomverbruik en betere featureset? Concluderend kan je in ieder geval zeggen dat beide kaarten ongelofelijk veel performance bieden voor hun geld. Ik vind dan ook dat zowel kamp Groen als kamp Rood daar een applaus voor mogen krijgen
GTX 1060;5;0.29092589020729065;+1
GTX 1060;1;0.39099010825157166;Die Timespy Async benchmark is nutteloos want Timespy maakt geen gebruikt van Async.
GTX 1060;5;0.3331018090248108;Nou, dat wordt een jaartje wachten voor hij op de officiële adviesprijs niveau terechtkomt bij webshops. Ik vind de videokaarten tegenwoordig loeiduur geworden voor gewone midrange kaarten vergeleken met vroeger.
GTX 1060;3;0.5069648027420044;Goede prestaties en stroom verbruik maar toch ben ik blij om een twee hands msi 980 te hebben gekocht voor een goede prijs. Ik had mijn twijfels maar het feit dat deze duurder is dan verwacht en geen SLI wordt ondersteund is wel een min punt.
GTX 1060;2;0.31954196095466614;Een gfx kaart van €400 die 'mid-range' genoemd wordt. Wat verlang ik terug naar de tijd dat mid-range nog betaalbaar was, zo'n €150-250 (het Radeon HD6850 / GTX460 tijdperk). Onder de €150 was in dit geval dan daadwerkelijk 'budget' te noemen.
GTX 1060;3;0.4050362706184387;Je bent hierin niet de enigste
GTX 1060;3;0.32683226466178894;Hoe dan ook, ik zie van beide kampen nog geen midrange prijsjes. Laten we hopen dat daar eerst eens wat aan gaat veranderen als de beschikbaarheid wat beter wordt.
GTX 1060;2;0.5515208840370178;Zie prijzen tussen 330 en 400 euro. Als Nvidia concurrerend wil zijn met Amd, zullen die prijzen heel erg omlaag moeten. Anders verliest Nvidia in deze prestatie klasse. Ik vind de prijzen namelijk nogal hoog. Voor een paar tientjes meer heb je een 980(ti) en die presteert toch echt een stuk beter. Het zal nog jaren duren voor VR een beetje gemeengoed wordt, dus daarvoor hoef je deze kaart niet te kopen. Helemaal omdat deze kaart niet krachtig genoeg is voor een goede VR belevenis
GTX 1060;1;0.4713653326034546;Weer geen temperatuur metertjes... tis me wat met jullie !
GTX 1060;4;0.3605518341064453;Op voorraad blijkbaar
GTX 1060;3;0.36216995120048523;Waarom vergelijke jullie altijd een reference model van 980ti met een custom 1070/1080? Het is logisch dat een reference een stuk trager is dan een custom en die scoort ook wat minder. Voor nieuwe pascal kaarten pakken jullie dan wel de beste uit om te testen. Uit jullie benchmarks blijkt dat een MSI Gaming X 1070 zo'n 18800 scoort voor Fire Strike, terwijl mijn 980ti over 19500 kwam. (ik spreek hier over een stock Gigabyte Xtreme Gaming). Persoonlijk vind ik dat het beter zou zijn om ofwel reference ofwel custom kaarten te vergelijken voor een review, anders denken de mensen dat een 1070 zoooo veel beter is dan 980 ti, wat helemaal niet klopt. Gr
GTX 1060;1;0.41064804792404175;Is mijn MSI Radeon R9 390 nu afgeschreven?
GTX 1060;2;0.46787142753601074;Waarom zou ik deze kopen als ik voor 300 euro ook een GTX 970 G1 kan kopen met, ongeveer, de zelfde prestatie's als een 980, die dan weer, ongeveer, overeenkomt met de 1060? Oke, hij heeft een nieuwere chip, bladiebladiebla, maar ik vind het, nu, mijn geld nog niet waard.
GTX 1060;3;0.39349228143692017;Omdat de 1060 goedkoper is dan de 970 en beter performt?
GTX 1060;4;0.3943122625350952;Mooi kaartje hopen dat het niet te duur word hier in NL. edit: ik zie al wat prijzen het valt wel mee zo te zien vanaf 299,-
GTX 1060;5;0.5364181995391846;Een tip voor mensen die 2e hands niet schuwen (via tweakers.net vraag en aanbod): Een 290x kost tweedehands +/- 200e en presteert bijna op het 390x niveau.
GTX 1060;1;0.8026118278503418;Ja hoor eens, ik ga niet tweedehands een 290 halen als ik voor enkele tientjes meer een nieuwe 480 haal, en die ook nog eens zuiniger is... Da's zonde van mijn geld.
GTX 1060;1;0.5453810095787048;Een rx480 is echt niet net zo snel als een 290x en kost 50€ meer...
GTX 1060;2;0.3262144923210144;ik beweer dan ook niet dat ie sneller is, wel dat je voor een paar tientjes meer, een nieuwe zuinigere GPU in huis haalt. Leuk dat de 290 sneller is, op 1080P ga ik er echt niets van merken.
GTX 1060;3;0.31635868549346924;ikzelf ga waarschijnlijk tot oktober/november wachten, voordat ik een nieuwe GPU ga halen. aangezien ik mijn 3570k wel stabiel moet overclocken en dat nog niet echt gelukt is op 4.4 ghz.. (ivy bridge is ondanks de waterkoeling *antec kuhler 620* best nogwel warm in de zomer *69-77 graden*) denk ik dat ik in november eens ga kijken voor een 1070 in de hoop dat deze iets in prijs zakt (450 +-) want ik heb altijd 'mid-range' gedraaid.. en dat is toch altijd NET weer zeuren op 1080p ultra na een jaar of 2-3 (draai nu een 660ti.. omdat mijn systeem uit 2013 komt) dus een wat snellere/future proof 1070 is meer dan welkom
GTX 1060;3;0.34363842010498047;Enig idee of de release van de 1060 effect zal hebben op de prijs van de 1070?
GTX 1060;1;0.5080324411392212;ga er maar niet vanuit... de enige dingen die effect hebben op prijzen van high end kaarten.. - andere high end kaarten (de 490 ergens in 2016/2017) - price drops van de betreffende kaart (zal dus nooit gebeuren bij Nvidia zolang ze marktleider zijn) - tijd (en die hebben ze nog genoeg, reken maar een maand of 6-7) - aanbod: hoeveel winkels verkopen ze.. met welke winst marge? *winkel A zegt 520 euro ipv 490, dus steekt zo 30 euro in zn achterzak* zolang winkel B dan niet zegt.. wij verkopen hem voor 485.. zal de prijs NOOIT dalen. - hoeveel aanbod is er van kaarten (soorten/voorraad) *als de voorraad minimaal is heb je HELE hoge prijs.. is de voorraad stabiel.. dan heb je een 'iets' minder hoge prijs (520-550 gaat dan naar 495-510)
GTX 1060;2;0.39683496952056885;Ik begrijp eigenlijk al dat gezeur niet. Gezeur dat de rx480 toch voordeliger zou zijn.... Dan vraag ik me toch af op welk vlak dat die rx in het voordeel is. De 1060 heeft dezefde advies prijs als de RX, ok vandaag is de 1060 te verkrijgen in belgie vanaf 329 maar de RX is in belgie ook aan 319 euro te verkrijgen dus prijsverschil is belachelijk klein en de 1060 is dan nog sneller dus hoe is die RX dan in zijn voordeel ?
GTX 1060;1;0.385724812746048;Waarom geen 3dmark score van de AMD RX 480 4GB? Of krijgen die bijna dezelfde score in 3dmark?
GTX 1060;2;0.5331038236618042;Vond ik ook al vreemd. Ik denk dat het ram verschil tussen 4-6gb momenteel nog weinig uitmaakt voor de prestaties? Maar prijsverschil is nog groter vergeleken met de 4gb vs de 6gb.
GTX 1060;3;0.5065271854400635;Interessant, ik heb een oude 780 en de 30% hogere prestaties, schat ik zo , zijn wel leuk voor 279 wat de kaart hier in Duitsland kost. Verzending is gratis, terugsturen eventueel ook, dus even proberen maar. Goede review jammer dat er geen overlock info in zit.
GTX 1060;1;0.27026233077049255;Waar is de tijd gebleven dat men nog een overclock test toevoegde bij de review..
GTX 1060;3;0.394454687833786;"Ehhh ok reference 1060 advies is 289$ en reference RX 480 8GB is 239$. Dus 50 dollar verschil, Dat is pak hem beet 50 euro met huidige wisselkoers. RX480 8GB gebruikt wel een ""hele"" 30 watt meer. Ok even rekenen dus. 0.22 cent per kWh dus je moet 227 kWh meer verstoken met de rx 480 8GB om gelijk te zijn aan prijs. hij gebruikt 30 watt meer dus 1kWh is 33 uur. dus om die 227 kWh te halen moet je uhm 7491 uur gamen. Als je ongeveer 4 uur per dag gamed is dat 1872 dagen ... oftewel yup 5 jaar ... denk niet dat die 30 watt dus erg relevant is als je het mij vraagt lol."
GTX 1060;3;0.43624135851860046;Ik heb me net een nieuwe LG ultrawide (34UC88B) aangeschaft. Dit om mijn twee 1080p schermen te vervangen door één 3440x1440. Ik ben helemaal geen keiharde gamer dus ik zal er wel tevreden mee zijn om zo af en toe eens Crysis 3 te spelen en mijn bureau zal er opgeruimder door uitzien. Ik heb nog wel een GTX660 van enkele jaren oud, die het bij Crysis 3 echt nog wel goed doet. Ik zat te wachten op de 1060, om deze te kunnen vergelijken met de RX480. Als ik zo naar het stroomverbruik kijk, dan trekt de RX wel wat meer en wordt ook een stuk warmer waardoor de ventilator ook weer meer geluid maakt. De 1070 is naar het budget gekeken te duur, dus ik zit nu weer te twijfelen tussen de RX480 en de GTX 1060. Misschien nog wat wachten tot de prijzen zakken.
GTX 1060;1;0.5055238604545593;1060 kost toch echt meer dan een 480.
GTX 1060;1;0.5658653974533081;Het rode kamp is weer een desillusie rijker .... ondanks het prijsvoordeel ......
GTX 1060;3;0.47187942266464233;Als iemand die geen kleur kiest maar in tegenstelling tot vroeger alleen nog maar voor prijs prestatie kiest ben ik enigszins teleur gesteld in kamp rood en is de 480 niet echt de killer van wat we er van verwacht hadden(al is de kaart zeker niet slecht) en is de 1060 toch wel heel erg aan de prijs voor een kaartje in het midrange-segment. Moeilijke keuze. Ik denk dat ik nog even een paar maandjes wacht tot de prijzen wat zakken.
GTX 1060;3;0.5618016123771667;Opzich is de 1060 wel sneller maar ondersteund geen SLI. Als je dus snellere gameprestaties wil hebben zal je meteen moeten upgraden naar een 1070 of 1080 en da's best prijzig. Met de RX 480 kan je voor nog geen €300 een 2e kaart erbij zetten, komen de prestaties in de buurt van de 1080 en 't kost minder. Voor mij maakt dat de keuze vrij makkelijk.
GTX 1060;1;0.42361268401145935;Kan iemand mij vertellen waarom de 980Ti van de 980 verliest op Doom (Vulkan)?
GTX 1060;2;0.5460565090179443;Jammer genoeg geeft dit nu wel een vertekend beeld. aangezien de RX480 alleen getest is als reference kaart en de 1060 meteen als aftermarket kaart met een dikke koeler + OC. Nu is er dus helaas geen goed beeld van de prestaties van beide reference/FE kaarten. De aftermarket versie van de RX480 zou volgens ASUS 20% beter presteren dan de reference versie(dit zou betekenen dat hij beter presteert dan de 1060). Ik wacht de RX480 aftermarket af, met de daarbij behorende prijzen + die van de 1060 om vervolgens een keuze te maken. Ben nu al zo lang aan het wachten, er kan nog wel een week of wat bij op edit: Het klopt inderdaad dat de reference 1060 wel getest is en die is zeker beter dan de reference 480. Over enkele weken zullen we zien wat de aftermarket RX480 doet.
GTX 1060;3;0.3682918846607208;Het valt mee, tweakers heeft twee 1060 kaarten tegelijk getest, de reference versie en de msi OC versie. Dus je kunt de reference met de 480 (reference) vergelijken. Die msi OC versie hoort eigenlijk niet thuis in deze review aangezien ze alleen met reference kaarten hebben vergeleken.
GTX 1060;2;0.4263289272785187;Excuses. ik heb blijkbaar erg slordig/snel gekeken en dat dus gemist Forgive me
GTX 1060;3;0.2783726453781128;Dan kijk je toch gewoon naar de reference GTX1060, die ook getest is?
GTX 1060;4;0.548357367515564;De founders edition is ook meegenomen in de tests, dus er is wel degelijk een directe vergelijking mogelijk met de rx480 reference.
GTX 1060;1;0.284280925989151;Hier puur voor de info een mooi vergelijk van de eerste benchmarks van de Saphire Rx 480 nitro vs de 1060. de cijfers liegen er niet om. Ze liggen ontzettend dicht bij elkaar! Dit zijn de cijfers waar ik op doelde met mijn reactie!
GTX 1060;1;0.38944631814956665;AMD all the way, meer dan 3 jaar geleden een 290 (350 euro) gekocht, overclock erop en hij presteert in Dx12 / vulkan games net of nog beter als de 980TI, een videokaart die 600 euro koste tot een recent. Dat terwijl het de tegenhanger was van de 780, die toen ongeveer even snel was. De 780 TI (ik zie de nivdia CEO nog smilend op het podium staan met ze 290 killer) wordt volledig weg geblazen inmiddels en is onderhand low end, terwijl een 290 in DX12 gewoon nog high end is.
GTX 1060;1;0.6544740200042725;DIT, het begint nu wel extremere vormen aan te nemen (mensen klagen zelfs op de Nvidia forums). Ik was altijd fan van nvidia met de 480 en 580 etc.. maar nu is het echt over met die malafide praktijken.. als het moet haal zo een GTX 1080, maar ik stop gewoon geen geld meer in het kluisje van de Nvidia CEO die 16miljoen netto per jaar verdient.... stond als ik mij niet vergis zelfs in de FORBIS van een van de best verdiende......... en geeneens ff excuus met die GTX 970 3,5 gb .. Tel daar nog bij op het ''gimpen'' van drivers (kijk hoe slecht de 780/titan 1 nu is!!!) .. en het feest is compleet. Waarom zou ik hier nog duizenden euros in pompen????? Dan maar gameworks uit xD en die geintjes met de monopolie op Physx.. en geen open source willen ondersteunen.
GTX 1060;1;0.48807013034820557;ik deel jouw mening van harte. Terwijl ik toch in de meeste gevallen voor nvidia zal kiezen, ....maar wel tweedehands. Never nooit dat ik de hoofdprijs ga betalen voor deze mafkezen.
GTX 1060;1;0.402926504611969;Waarom een '-1' voor deze reacties, wat mis ik?
GTX 1060;5;0.3370729386806488;dat zijn de mensen die de hoofdprijs hebben betaald nieuw in de winkel
GTX 1060;1;0.45628857612609863;Waar haal je die 100euro ? Hier in Belgie staat de 1060 voor 329 te koop en de RX 480 voor 319 dus 10 euro verschil. Beetje research voor je reactie geeft is geen overbodige luxe, lijkt het dan minder dom.
GTX 1060;1;0.643722414970398;480 8GB is gewoon voor 275 te krijgen. de 330 is dus 55 euro meer. ofwel 20%. en je krijg nog 2 GB minder geheugen ook. en als we kan kijken naar alle nieuwe games (met de nieuwe API's) dan verliest de 1060 ze gewoon allemaal behalve de ene door nvidia gesponsorde game.
GTX 1060;1;0.5592036843299866;Raar dat jij +1 krijgt en ik -1. Puur omdat ik negatief reageer op de reviewer?
GTX 1060;3;0.3048414885997772;In de pricewatch hier staat ie anders voor 380-390-400. Dus daar haal ik die.
GTX 1060;3;0.3350074291229248;Amai duurder dan bij ons dus.
GTX 1060;1;0.26476526260375977;Nee hoor, als je wil kan je er nu eentje bestellen.
GTX 1060;3;0.36388006806373596;bestellen ja. maar dat kan altijd met een paper launch. geleverd krijgen binnen afzienbare tijd, daar gaat het om.
GTX 1060;1;0.5363289713859558;Even snel gekeken, maar volgensmij ga je sowieso voor half aug. niks krijgen.
GTX 1050 Ti;1;0.6053471565246582;welke prijzen worden in de review nou met elkaar vergeleken?? nvidia 1050 ti 155,- amd 470 210,-? euhhhh wat? Hier wat detail data: als je voor de een een introductie prijs hanteert , dan graag ook voor de ander. en dan ook graag in dollars want die valuta hanteren zowel amd als nvidia 1050 ti nvidia introductie prijs 139,- dollar 470 amd introductie 179 169 dollar als je voor de een een pricewatch prijs hanteert, dan graag ook voor de ander 1050 ti staat in pricewatch voor 225,- 470 staat in pricewatch voor 204,- Het scheelt 30 dollar mocht je zowel AMD als Nvidia geloven. Het scheelt -21 euro als je pricewatch mag geloven. Het scheelt 49 euro als je de 470 in nederland haalt en een 1050ti uit duitsland Het scheelt 39 euro als je de 470 in duitsland haalt en een 1050ti uit duitsland etc. om nou de duurste en de goedkoopste naast elkaar te zetten in een review leg je toch de verkeerde vergelijking neer?
GTX 1050 Ti;2;0.3595983386039734;Op dit moment staat er 1 kaart bij 1 winkel in de PW, dat lijkt me een wat weinig om conclusies uit te trekken. De prijzen in de review zijn 210 euro voor de RX470, dat was de goedkoopste in de PW op het moment van het schrijven van het artikel. Nvidia geeft een adviesprijs op van 125 euro voor de GTX 1050 en 155 euro voor de 1050 Ti. Ik heb ze gisteren nog aan de lijn gehad en ze hebben me bezworen dat er genoeg voorraad is, zodat er geen gekke prijsstijgingen ontstaan, zoals bij de introductie van de 1070 en 1080 het geval was. De prijs van de MSI GTX 1050 Ti in de review (155eu) heb ik van MSI doorgekregen, net als de prijs voor de gtx 1050 gaming x (145eu) en de gtx 1050ti gaming x (175eu). Dat zijn de prijzen waarop de conclusie van de review gebaseerd is. De meeste lezers van de review zullen in Nederland en België wonen, dus lijkt het me nuttig om europrijzen te hanteren en daarnaast met de huidige prijs van de kaarten te rekenen en niet met een introductieprijs van een aantal maanden terug.
GTX 1050 Ti;1;0.6834084987640381;Dit is een uitleg die kant noch wal raakt. Je blijft introductie prijzen vergelijken met actuele prijzen, terwijl als er een ding wat de laatste maanden ons heeft geleerd is dat introductieprijzen ver van de werkelijkheid staan. Kies een of het andere, of benoem alles zoals je in eerdere reviews wel hebt gedaan. Ook wordt in de conclusie de resultaten van DX12, niet voor de eerste keer, volledig genegeerd. Daarin is de 460 duidelijk sneller dan de reguliere 2GB 1050. Tevens mis ik de 4GB versie van de 460. Wat nu de review incompleet maakt.
GTX 1050 Ti;2;0.4550245702266693;De 460 en 470 zijn beide getest. AMD heeft hier in zijn huidige generatie nog geen kaart tussen gepositioneerd, anders hadden we deze natuurlijk ook meegenomen. De 4GB versie van de 460 hebben wij helaas niet liggen, maar dat zal ook niet zó veel uitmaken op deze resoluties. Kijk bijvoorbeeld naar de R9 285, deze heeft ook maar 2GB beschikbaar, maar presteert wel degelijk beter dan de 460. Met welke kaarten en of prijzen had je dan liever dat we deze kaarten vergelijken? Op moment dat de review gepubliceerd wordt zijn er logischerwijs nog geen winkel prijzen beschikbaar.
GTX 1050 Ti;2;0.31971630454063416;Gewoon appels met appels vergelijken. Niet appels met peren. Dus introductieprijs met introductieprijs, adviesprijs met adviesprijs, actuele prijs met actuele prijs. Introductieprijs met actuele prijs vergelijken slaat als een tang op een varken. Er wordt hierboven ook vermeldt dat er genoeg voorraad zou zijn, met als enige bron WC Eend. Als ik bij de overkant van de grote plas kijk, Newegg, zeker niet de kleinste, en daar zit alles al op backorder. Voorraad mn neus, Er mag dan nu wel wat voorraad in de grote magazijnen liggen, dat zal niet lang duren. Amerika blijft de grootste afzetmarkt en Europa zal achteraan moeten sluiten bij het aanvullen van die voorraad. 285 vergelijken met de 460? Als je het over appels en peren hebt... 2GB vs 4GB, tuurlijk maakt dat uit voor het complete beeld, 1050 wordt tegenover de 460 gepositioneerd en de 4GB variant is nu niet te vergelijken.
GTX 1050 Ti;2;0.41128358244895935;Wel over appels en peren praten, maar vervolgens zelf niet met feiten maar met voorspellingen komen. Nogmaals, ten tijde van een introductie kunnen wij niet anders dan het over een introductieprijs of een adviesprijs vanuit een fabrikant hebben, omdat er simpelweg geen andere prijzen beschikbaar zijn. Kom in je volgende reactie gewoon eens met feiten, in plaats van fabels.
GTX 1050 Ti;1;0.7172902226448059;"Er wordt toch bedoelt dat er actuele prijzen vergeleken worden met een msrp? Dat is onder geen omstandigheden eerlijk, en de fabrikant bellen om te vragen of ze wel genoeg voorraad hebben kan daar nooit tegenover staan. Natuurlijk zeggen ze dat, iedereen zou dat zeggen. Dat is dus juist een voorspelling in plaats van een feit, amd zei ook genoeg voorraad te hebben en die kaarten raakten ook enorm snel uit voorraad. En een ""fok"" achtige reactie wordt ook nooit gewaardeerd..."
GTX 1050 Ti;1;0.29031726717948914;Uiteraard, en dat begrijp ik ook zeker. Mijn vraag is alleen wat er dan wél verwacht wordt. Ten tijde van publicatie, op moment van verlopen embargo, zijn er nog geen winkelprijzen bekend, en kan daar dus ook geen oordeel over zijn. Om dat op te vangen hebben we MSI een belletje gegeven en naar hun adviesprijzen gevraagd.
GTX 1050 Ti;5;0.35579413175582886;Gast, hou toch je mond dicht en lees wat hij zegt.
GTX 1050 Ti;3;0.4131326377391815;Maar wat zou dit dan betekenen, is het gunstiger of ongunstiger voor aankoop van een 1050 kaart? En dan voor op dit moment maar ook naar verwachting over 3 maand?
GTX 1050 Ti;2;0.4103706181049347;Dat niet alleen. Ik snap nog steeds niet dat tweakers.net deze kaarten met een high end processor test. (Terwijl CPU's belangrijker zijn aan het worden in games) En waarschijnlijk niet de processor die iemand kiest om met deze videokaarten te gebruiken. Waarom geen 4 profielen (testsystemen) op gameperformantie kiezen? I3, i5, i7 en AMD? Ok, het is makkelijkerer op deze manier. Maar niet echt realistisch. Overal word iedere videokaart getest met een high end processor. Niet echt realistisch voor het merendeel? Specs voor deze test: Moederbord: Gigabyte GA-X99-Ultra Gaming Processor: Intel Core i7-6950X @ 4,2GHz Geheugen: Crucial Ballistix Elite BLE4C8G4D30AEEA 32GB ddr4-3000 Ssd: OCZ RD400 1TB nvme m2 Dan kan je beter besparen op de rest van de onderdelen en meer investeren in de videokaart! Edit: verbetering in de profielen die mogelijk zouden zijn
GTX 1050 Ti;3;0.4761861264705658;Dit doen ze echter om te laten zien welke kaart echt beter is om bottleneck te voorkomen van de cpu tijdens het gamen. Als je bijvoorbeeld een I3 of fx6300 zou gaan gebruiken tijdens het gamen en er komt een intensief stukje voor de cpu zullen de frames bij beide gpu's droppen naar vrijwel hetzelfde terwijl de ene gpu op 70% zit qua gebruik en de andere op 55% zit qua gebruik van de gpu. dan is de gpu die op 55% qua gebruik zit sneller wat je niet terugziet in de scores. Daarnaast zou je dan zeggen dat dit dan gelijk zou zijn aan synthetische benchmarks. Echter is dit het ook niet gezien je hier ook uit kan halen welke grafische kaart beter is voor welke game. Zo zie je bijvoorbeeld dat in farcry de r9 285 slomer is dan elke 1050 terwijl hij de r9 285 bij dragon age sneller is dan elke 1050.
GTX 1050 Ti;3;0.518447995185852;Maar het zou ook zo kunnen zijn dat een bepaalde videokaart ervoor zorgt dat de processor minder bottlenecked. Door betere drivers bijvoorbeeld. Of DX12/Vulkan.
GTX 1050 Ti;1;0.691965639591217;Dat idee voegt totaal niets toe aan de resultaten. Het enige wat je dan krijgt is dat bepaalde games met elke kaart dezelfde FPS gaan neerzetten, puur omdat je cpu limited bent. De conclusie van elke test wordt vervolgens dat het niet uitmaakt welke gpu je koopt boven de x euro. Dat is dan vervolgens te kort door de bocht want in game X kan het weer heel anders zijn en met resolutie Y is het ook weer een ander verhaal. De reden dat we al sinds het begin van pc hardware met de snelste cpu's testen is dat dit de enige manier is om zo eerlijk mogelijk wat over de prestaties van de gpu te zeggen. Op diezelfde manier heb je laren lang tests op 800x600 gezien met de snelste gpu van de planeet om zo wat te kunnen zeggen over de cpu performance.
GTX 1050 Ti;2;0.38526293635368347;Maar zo geef je de comunity van tweakers wel een realistische kijk op zijn prestaties binnen een game. Ipv het hoogst haalbare. En dat het meer en meer de cpu is die games de nodige fps geeft met de gpu.
GTX 1050 Ti;2;0.49373844265937805;Eerst lees je een GPU-review (waarin de bottleneck bij de GPU wordt gelegd), daarna een CPU-review (waarbij de bottleneck bij de CPU wordt gelegd). Dan weet je hoeveel frames je ongeveer krijgt met elke combinatie GPU+CPU. Je mist dan alleen factoren die van allebei afhangen (CPU-overhead veroorzaakt door een minder efficiënte GPU) maar dat effect zal klein zijn. Ga je realistische scenario's benchmarken, dan weet je juist veel minder, je favoriete combinatie moet maar net getest zijn. Als je alle combinaties gaat testen ben je tien jaar verder en het voegt weinig toe t.o.v. de simpelere GPU- en CPU-benchmarks. Al met al is de huidige manier van reviewen juist het gericht meten van nuttige gegevens, jouw voorgestelde aanpak meet veel vagere dingen en je kan minder met je resultaten.
GTX 1050 Ti;3;0.33051782846450806;Het werkt toch twee kanten op? Als jij die kaart koopt en je verwacht 60 fps en je haalt 40 fps dan weet je meteen wat het je oplevert als je de cpu upgrade.
GTX 1050 Ti;4;0.3550754189491272;Helemaal mee eens. Het zou nu best zo kunnen zijn dan iemand met een AMD of een goedkope Core i3 of Core i5. Denkt dat hij/zij een bepaalde game op High kan slepen op een GTX1050(Ti). Want dan mogelijk niet het geval is omdat de CPU dit niet aan kan. Vandaar moet je IMO low end GPU's met low end CPU's, Midrange met midrange CPU's en Highend moet Highend CPU's testen. Het zal een uitzondering zijn dat iemand met een Core i3 een GTX 1080 of iets dergelijks als Videokaart koopt.
GTX 1050 Ti;1;0.290677934885025;Ik ben het hier niet mee eens. Je wil toch de videokaart vergelijken? Met deze setup weet je zeker dat alle behaalde frames per seconde gehaald zijn door de GPU, en niet door een combinatie van processor/GPU. En alle prestatieverschillen zitten in de GPU, en niet ergens anders.
GTX 1050 Ti;4;0.27087169885635376;En koop jij een i7-6950x met deze videokaarten? Of koop je een betaalbare cpu met deze videokaarten? Realistisch blijven. Snap da het een goede vergelijking is tussen de videokaart zelf. Maar waarom geen 4 verschillende testsysteem, i3, i5, i7 en AMD om alle videokaarttesten op uit te voeren. Zodat het voor ons, tweakers, makkelijker vergelijkbaar blijft? Maar deze fps is misschien 50% minder met een i3 Al zal het nooit exact uw processor zijn, maar dan heb je betere kansen dat het aan uw verwachtingen voldoet. En daar draaien reviews toch om? Het beste met je comunity voorhebben
GTX 1050 Ti;2;0.4597751200199127;Nee, maar dat is ook niet de bedoeling van de test. Het gaat om uitsluiten van factoren die performance beinvloeden, maar die niet van de GPU zelf komen. M.a.w., je wilt er voor zorgen dat de GPU het traagste deel van het systeem is. Alleen dan kan je iets zinnigs zeggen over de prestatie van een GPU. Ik zal het proberen uit te leggen met een auto analogie: We willen twee auto's vergelijken welke meer performance voor zijn prijs levert. Als test laten we ze rijden op een gesloten circuit met dezelfde weersomstandigheden en dezelfde coureur. De rondetijden zeggen dan wat over de onderlinge prestaties van de wagen. We kunnen dezelfde auto's ook testen op een willekeurige woensdagmiddig in de binnenstad, en timen hoelang het duurt om van A naar B te komen. het resultaat zegt zeer weinig over de auto zelf, maar heel veel over de omgevingsfactoren (verkeer, weer, chauffeur). Dat laatste is waar jij naar op zoek bent. Prima, maar dat is niet waar de test voor gedraaid wordt. @Weijlander legt verder prima uit waarom.
GTX 1050 Ti;1;0.7617470622062683;Je koopt toch niet je hele PC met tien componenten op basis van de review van één component? Je kijkt ook nog naar een CPU-benchmark. En als je boottijden wil weten zoek je reviews van de SSD. Goh, een Ti1050 kan 60FPS halen op mijn resolutie, maar die i3 maar 30. Misschien toch maar even een gebalanceerdere setup bedenken. Als de gebenchmarkte componenten niet de bottleneck zijn, weet je dit dus niet - ja, in de review haalt 'ie 30FPS, maar waar ligt dat aan? Geen idee. Oh, we hadden nog wat traag geheugen in de kast liggen en dat hebben we maar gebruikt, daarom is hij zo traag. Heel realistisch, en ook heel zinloos bij een benchmark. Zo nu en dan komen er nog wel benchmarks voorbij waarbij de gebenchde componenten per ongeluk wél de bottleneck zijn. Conclusie: ze halen állemaal exact 73 FPS, dus geen idee welke beter is. Hoe helpt dát? Om dat te voorkomen neem je totaal overdreven hardware voor de rest van de PC.
GTX 1050 Ti;2;0.444499135017395;Nee. Als je een review zoekt van een videokaart, dan wil je weten hoe de videokaart presteert. Niet bij welke game een combinatie van hardware beter werkt. Als je wil weten welke CPU je nodig hebt voor een bepaalde game dan zoek je een benchmark op van CPU's. Je onderzoekt elke variabele afhankelijk van elkaar.
GTX 1050 Ti;3;0.42778822779655457;Wat denk je zelf. Laten we even iedere keer alle testen uitgebreid draaien met 6 verschillende hardware configuraties en dat met 10 verschillende grafische kaarten. Dat nog eens keer de 8 testen die gedraaid worden. Oftewel 480 verschillende test resultaten. Lijkt me niet heel handig. Op de huidige manier heb je voor iedere kaart gewoon de beste test resultaten die er uit de grafische kaart gehaald kunnen worden. Zo weet je misschien niet of jou configuratie het haald maar je weet wel wat ongeveer de prijs/prestatie is per kaart en wat er bij jou aardig aan zal sluiten. Bij alle verschillende configuraties is het ook maar de vraag of deze sowieso voor die van jou geldt. Dus ten opzichte van het werk zou dat nauwelijks van toegevoegde waarde zijn. Verder on topic ben ik erg blij met deze review. Gezien het feit dat de ti gaming editie nauwelijks een meerwaarde heeft tenopzichte van een gewone ti ga ik in dit geval daar voor. Of die RX470 moet binnen nu en heel kort drastisch omlaag gaan.
GTX 1050 Ti;1;0.37492474913597107;"Als ik hierboven in de tekst lees: "".........waardoor gamen met een simpele oem-pc ineens een optie wordt."" Ik vraag me dan af of je de resultaten uit deze test nog kunt gebruiken. Mocht de CPU de bottleneck worden dan is gamen met een simpele oem-pc nog steeds geen optie."
GTX 1050 Ti;3;0.36859723925590515;"Dat is waar, alleen betekent een ""simpele OEM pc"" niet per definitie altijd dat het ook een budgetpc is. Kan ook een betaalbare i5 desktop zijn van de Mediamarkt etc, waarin dan een vrij magere GT..30/40 of geen enkele losse videokaart zit."
GTX 1050 Ti;5;0.286332905292511;Helemaal gelijk, zo weet je als lezer nog niet wat die kaart voor jou kan betekenen. Het zou veel realistischer en inzichtelijker zijn om de kaarten met een i3 en i5 of een fx6300 en a10 te vergelijken. Meer on topic zat aan een rx 460 te denken als nieuwe htpc kaart, maar denk dat het een 1050Ti wordt.
GTX 1050 Ti;2;0.4195409119129181;En het probleem is dat bijna iedere review-site zich er aan schuldig maakt. Zelfde testsysteem voor alle videokaarten. Maar niet stilstaan dat low-end videokaarten ook wel eens in een low-end systeemconfiguatie zitten. De kans voor tweakers.net om dit te veranderen.
GTX 1050 Ti;3;0.32482805848121643;Ik snap dat altijd hetzelfde systeem wordt gebruikt, dat maakt het vergelijken van de kaarten makkelijk. Het is nou eenmaal een vergelijking, niemand heeft precies hetzelfde systeem, dus niemand kan aan deze getallen zien welke prestaties te verwachten zijn. Het is echter wel een leuk idee om de belangrijkste volgende schakel, de processor, ook te variëren. Als je één populaire CPU van vorig jaar neemt, of het bijbehorende instapmodel (net als deze kaart), dan krijg je een extra dimensie aan info en dat is wel praktisch voor de eindgebruiker: wij, de lezers. Dan krijg je voor de kaart in kwestie een extra balkje per grafiek of 1 pagina voor de vergelijking tussen testsysteem 1 en 2 met alleen de kaart in kwestie. I.p.v. alleen de CPU te veranderen, kun je er ook voor kiezen om het hele systeem te veranderen, behalve de videokaart. Dan neem je dus instap CPU, RAM, HDD, SSD, etc. en kijk je wat de kaart doet op zo'n systeem. Als lezer kun je met die twee ijkpunten je eigen te verwachten prestaties een beetje intrapoleren en bovendien zie je hoe dezelfde kaart het doet in een ander systeem. Je ziet meteen of bepaalde games of software meer eisen van de kaart, of toch afhankelijk zijn van de rest van het systeem.
GTX 1050 Ti;1;0.5537195205688477;Het is geen platform review maar een GPU review. Wat jij vraagt is een CPU review. Tweakers is beperkt in tijd. Om een GPU review, gecombineerd met een CPU review te doen kost gewoonweg belachelijk veel tijd. Ik snap niet dat na zoveel jaar GPU reviews hier nog over gestruikeld wordt. Als je bv. een Pentium zou gebruiken, zouden alle GPU's in sommige spelletjes gewoon dezelfde snelheid neerzetten omdat het spel dan op een CPU bottleneck stoot. Als je een probleem hebt met het feit dat een i7 6950x zoveel kost dan kan ik je zeggen dat een i5 6600 in spelletjes even snel is (als dat je iets of wat gerust stelt).
GTX 1050 Ti;2;0.3661031424999237;Ik begrijp wat je bedoeld. Zoals velen zeggen doen ze dit om bottlenecks te voorkomen zodat ze puur de prestaties van de videokaart kunnen meten. Het is wel heel slordig dat ze deze uitleg niet duidelijk erin vermelden ten tijde van de review en dat er er gewoon maar vanuit gaan dat iedereen dit wel begrijpt. Deze lakse houding zie je heel vaak bij tweakers terug komen en dit doet de review geen gunst. Zo zie je deze lakse houding ook al een aantal jaren terug in huj zogenaamde budget midrange etc builds van pcl en laptop guide. Hele lakse houding in de introductie en er maar vanuit gaan dat alles wel begrepen wordt. Ik zou zeggen kijk eens naar je Internationale concurrent zoals techdeals, Linus of hardwarecanucks etc etc en dan zie je precies waarom zei gezien worden als een autoriteit op dit gebied.
GTX 1050 Ti;2;0.299133837223053;Wij testen in deze review videokaarten. Geen games, geen CPU's en ook geen complete systemen. Om de prestaties van deze videokaarten te vergelijken wil je dus een systeem waar je met de rest van de componenten niet tegen bottlenecks aan loopt. We schrijven ook regelmatig benchmark stukken over games, daar testen we hoe goed die specifieke game op verschillende hardware draait. Hierbij testen we meestal ook of het loont om bij die specifieke game een andere CPU te gebruiken.
GTX 1050 Ti;2;0.37841176986694336;Gezond verstand en tweakers gaan al jaren niet meer samen. Je punt is correct deze test hardware slaat nergens op als je kaarten in deze klasse test. Tja, misschien heeft tweakers wel niets anders, in dat geval zijn ze misschien wel zielig. In ieder ander geval niet professioneel en of reel bezig.
GTX 1050 Ti;3;0.5062291026115417;Het gat tussen de 1050 en 1060 vind ik wel vrij groot
GTX 1050 Ti;5;0.29539793729782104;Daarom de 1050 Ti.
GTX 1050 Ti;1;0.44649264216423035;toch nog rond de 39% slechtere prestaties
GTX 1050 Ti;3;0.6605367064476013;Ja maar wel 47% goedkoper, dus in verhouding meer back for the buck. (deze MSI 1050 Ti van € 155,- vergeleken met een MSI/Gigabyte 1060 van € 290,-)
GTX 1050 Ti;3;0.5180352330207825;Ja maar stel je wil alleen nvidia en verwacht betere prestaties dan 1050ti maar wil wel de nieuwste generatie en de 1060 vind je te duur
GTX 1050 Ti;4;0.40473437309265137;Vind het erg jammer dat Tweakers WEER niet de 75w Geforce GTX 950 (GM206-251 GPU) noemt en mee test, dan zit je met een GPU met de zelfde aantal cores en wattage. Zo als de KFA2 GeForce GTX 950 2GB OC LP en Asus GTX950-2G en de Asus MINI-GTX950-2G, en ook EVGA heeft er meerdere. Erg leuke grafische kaart voor 1080P de Geforce GTX 1050 Ti, een stuk sneller dan de Geforce GTX 950, en ook veel sneller dan de Geforce GTX 960, in sommige spellen zelfs bijna 2x zo snel als de Geforce GTX 960, ben benieuwt wanneer eindelijk de Low profile versies uitkomen, duurde ook erg lang met de GeForce GTX 950.
GTX 1050 Ti;3;0.395865261554718;Heel gaaf zon review op het nieuwe superhighend systeem, maar zon kaart koop je niet als je zon super highend systeem hebt, had mij fijn geleken als tweakers ook op een budget systeem hadden getest, waar deze kaart more likely voor gekocht gaat worden. verder wel een goede review natuurlijk.
GTX 1050 Ti;2;0.3917156457901001;Dat kan, maar Tweakers test hier wat de prestatie is onafhankelijk van het systeem... immers zullen er amper/geen bottlenecks te vinden zijn omdat de rest van het systeem veel sneller is. Op een budgetsysteem zal je dus afhankelijk van de 10.000 configuratiemogelijkheden een andere prestatie halen die niet veel zeggen over de daadwerkelijke maximale prestatie van de grafische kaart. Testen met een budget/midrange onderdelen kan dus onbetrouwbare/niet erg nuttige resultaten geven die onderling niet per se iets bewijzen. Dan zeg je eigenlijk meer over het systeem als totaalplaatje dan over puur de grafische kaart alleen. En ik begon over dat het kan. Het is natuurlijk wel interessant om voor een ander systeem ook de prestatie te zien, maar dat kost waarschijnlijk wel enorm veel tijd om zowel op verschillende reso te benchmarken, op verschillende systemen en alle kaarten (niet alleen deze nieuwe) op evt. een nieuwe driver. Heb nog even gekeken. Een iets minder extreem i7 systeem en toch veel benchmarks vind je o.a. hier: Techpowerup: MSI GTX 1050 Ti Gaming X Enige wat ik wel graag zou willen zien is het verbruik van alleen de kaart zelf en niet van het hele systeem. Techpowerup doet dit bijvoorbeeld altijd al, dus het moet niet onmogelijk zijn om dit te meten. Aangezien men alles test m.b.t. de kaart, vind ik het verbruik ook interessant als dit enkel de kaart betreft. Het totaalverbruik is immers van zoveel afhankelijk, lopende processen, hardwaresamenstelling, overkloks, evt. 3th party chips die niksdoen en ook verbruiken etc. Dat zal bij dit X99 zoveel meer zijn dan bij een i3 natuurlijk.
GTX 1050 Ti;4;0.3904748857021332;Voor eenn kwart van de prijs ben je ook bottleneck vrij en test je realistisch
GTX 1050 Ti;1;0.42122960090637207;Tweakers vind dat niet nodig, en een uitleg waarom overbodig. Voor een fatsoenlijke review: www.anandtech.com
GTX 1050 Ti;1;0.7390304803848267;Wat een onzin, alsof een uitleg verplicht is. En er is hier al meermaals een reactie gegeven. Dat die of niet door jou zijn gezien of niet in jouw straatje vallen is wat anders. Ik zal je op weg helpen, zie bijv. de reactie van Ch3cker. En wat is er met die link naar Anandtech? Die komt niet op een review uit, dus wat is je punt?
GTX 1050 Ti;2;0.3337755799293518;Je hebt gelijk, voor een echte review moet je hier zijn: Zie voor mijn gelijk mijn eerste post. En je bent inderdaad niets verplicht, maar ik neem aan dat je een review niet voor niets schrijft.
GTX 1050 Ti;1;0.32178276777267456;*bang* for the buck
GTX 1050 Ti;3;0.2585737705230713;Oeps, soortement van typo, of eerder kortsluiting in mijn hoofd!
GTX 1050 Ti;2;0.4123643636703491;Als ik Digital Foundry mag geloven is de TI model zelfs 60% trager dan de 1060.
GTX 1050 Ti;3;0.25304436683654785;Ik denk dat hij dat bedoelt, de RX 470 zit daar precies tussen, nVidia doet er verstandig aan om dat gat daar te laten. Zo kunnen mensen die iets meer willen dan een 1050Ti en dus eigenlijk op een RX470 uitkomen, meteen gaan twijfelen over een 1060.
GTX 1050 Ti;5;0.26937344670295715;nvidia heeft de 1060 3gb direct tegenover de RX470 geplaatst. Deze heeft minder cuda cores als de normale 1060 en had dus ook eigenlijk 1050ti moeten heten.
GTX 1050 Ti;3;0.4018331468105316;Er is ook nog een 1060 met 3GB en iets minder cores (1152 voor de 3GB vs 1280 voor de 6GB), maar die hebben we nog niet getest helaas
GTX 1050 Ti;3;0.32726576924324036;Denk je dat je nog de mogelijkheid gaat krijgen om die te testen? Of is dat niet te voorspellen?
GTX 1050 Ti;4;0.2846148908138275;
GTX 1050 Ti;3;0.3410883843898773;Dat is inderdaad een goeie. Thanks. Vuistregel voor mij: Vind ik geen benchmarks van hetgeen wat ik wil? > op hardware.info kijken .
GTX 1050 Ti;3;0.5394679307937622;Kan misschien nog wel een keer tussendoor, maar bij de volgende review kijken we eerst naar wat snellers .
GTX 1050 Ti;3;0.22810344398021698;1080ti of Vega dus
GTX 1050 Ti;3;0.40954989194869995;Vraag mij af of dat tegenwoordig een aanrader wordt, het feit dat je met veel kaarten al ziet dat 2gb gewoon te weinig is zegt genoeg. Voor een kaart die zo snel is als de GTX1060 is het wenselijk dat je meer dan 3gb geheugen hebt, daarom jammer van de rare geheugenbus configuratie, 4/8gb was een betere keuze geweest ivm een 256bits bus. Je zal dat gebrek aan geheugen niet echt direct terugzien in average fps maar wel in minimum fps en microstuttering. De GTX970 heeft tenminste 3.5gb geheugen (met extra 512mb traag).
GTX 1050 Ti;2;0.4675213694572449;Begrijpelijk, voor simpele games zie ik een GTX 1060 als te veel kracht. Stel je speelt CS:GO, een willekeurige MOBA en af en een toe een RPG dan is een 105Ti al genoeg kracht om dat te draaien. In 1080p zou ik een GTX 1060 overwegen als ik spellen zoals The Witcher en Battlefield zou spelen op 60fps.
GTX 1050 Ti;3;0.37288275361061096;Naar mijn mening wordt de lat hier in de reacties wel énorm hoog gelegd - tot overdrijven aan toe. 'Als je een beetje fatsoenlijk op 1080p wilt gamen heb je toch wel echt een 1060 nodig' 'Bij simpele games is een 1050TI wel genoeg' Ik speel CS:GO, League, en af en toe een (iets oudere) RPG, op mijn Intel Iris 6100. Een waardeloze integrated kaart, en dat gaat helemaal prima. Dankzij de reacties lijkt het nu bijna net alsof bijvoorbeeld een 970 nu al enorm low-end is, waar je met geluk nog 60fps haalt op games van drie jaar geleden. Verder lijkt de 1050TI mij persoonlijk enorm interessant!
GTX 1050 Ti;3;0.6359132528305054;Ze hebben wel deels gelijk. Op fullhd met settings op high is inderdaad een 980 of 1060 wel al snel gewenst als je de 60fps wilt halen. Alles onder de 970 heeft het nu wel moeilijker. Veel serieuze cs go spelers willen die game ook met hige framerates spelen trouwens.
GTX 1050 Ti;4;0.39350083470344543;Ik game zelfs nog op een GTX275! De zogenaamde e-sports titels draaien hier nog perfect op, op de highest settings (CS:GO). Zit nu wel met een volledig geupgraded PC van een tijd geleden, omdat ik wat meer kracht wilde voor videobewerking (van een i5-650, 8GBDDR3 naar een i7-4790K, 16GBDDR3) en de videokaart deed het nog steeds prima op de spellen die ik speelde, dus had ik deze nog niet geupgrade. De meest nieuwe game die ik speel was the Witcher 2 en voor de rest had ik de PS3 nog. Maar goed, het was leuk geweest en ik ben nu in de markt om een nieuwe videokaart aan te schaffen. Vooral ook omdat mijn videokaart af en toe tijdens een game uitvalt en mijn hele systeem vast zit. Dus zit nu te twijfelen tussen een 1060-6GB asus strix en een 1070-8GB asus strix kaart. En uiteraard ga ik deze ook weer heel wat jaren gebruiken totdat deze mankementen gaat vertonen. Maar ja, geloof me, als een GTX275 896MB videokaart van een behoorlijk aantal generaties geleden de e-sports titels kan spelen, dan zal dat met een 1050 of 1050ti zeker heel soepel verlopen!
GTX 1050 Ti;2;0.4368232190608978;Ik vond mijn oude 670GTX niet eens meer voldoende voor de meeste games een anderhalf jaar terug. CS GO en wat esports zullen vast draaien. Maar Overwatch bijvoorbeeld wat nu een populaire esports titel is zal al performance problemen tonen.
GTX 1050 Ti;2;0.4025561809539795;Ik heb de 960/2GB, de oudere broer van de 1050 en 1050ti en ik kan je vertellen dat ik tot op heden bijna alle steamspellen kon spelen op 1080p/ultra. En de 960 is blijkbaar minder snel dan de 1050 en 1050ti: Metro2033, zelfs Soma, dota2, cs:go, civilatizion, etcetcetc. Enige spel wat op ultra niet lekker liep was the witcher 2, maar ach, dan meer low/medium gfx. Qua spel merk je er niets van.
GTX 1050 Ti;3;0.38334208726882935;ik verbaasde me ook een beetje over de 960 benchmark ... linus review zei iets dat de 960 net sneller was dan beide. Iemand nog andere voorbeelden ?
GTX 1050 Ti;2;0.39729344844818115;Als je 1080p/ultra niet prettig loopt, dan maakt 5 fps meer of minder niet veel uit. Als je nu een 960 hebt, dan is die ongeveer gelijk aan die 1050 of die 1050ti. Er is wel verschil maar kans is aannemelijk dat je het niet merkt. Als de game zuigt in 1080p/ultra met een 960, dan is de kans aannemelijk dat die ook zuigt met een 1050 of een 1050ti. Gewoon jaartje doorsparen voor een 1070ti, die zullen ze eventueel ook nog wel op de markt brengen.
GTX 1050 Ti;3;0.440245658159256;Idd daar zijn het te marginale verschillen voor. Wel tov 460 lijkt me de prestatie winst groot genoeg als je zou moeten kiezen om bv 25 euro meer uit te geven voor de 1050. Maar de 1070ti zal niet gauw in de 'confectie' systemen in de winkels terecht komen. NVIDIA lijkt wel weer de cashcow met de 1050(ti) in huis te hebben en hun marges zijn vaak al zoveel beter als bij AMD kaarten. Ik heb al een 960 dus dat maakt het iig makkelijk om gewoon nog een jaar of twee te wachten
GTX 1050 Ti;3;0.5288990139961243;Benchmarks op hardware.info tonen dat de gtx960 4GB net iets trager is dan de 1050ti. Ik vind het jammer dat de 960 4GB vaak buiten beschouwing wordt gelaten. Van zodra iets meer dan 2GB nodig is dan krijg je een gans ander beeld vd resultaten.
GTX 1050 Ti;2;0.49330535531044006;Maar het veranderd waarschijnlijk niets. Heel dat meutje lowbudget nvdiakaarten speelt hetzelfde prima op 1080p/medium en 1080p/ultra af. En de kans is zeer aannemelijk dat ze bij dezelfde highend games en settings gaan kwakkelen, 5 fps meer of minder maakt niet veel uit.
GTX 1050 Ti;3;0.2363279163837433;"Helemaal mee eens. Ik zou mijn 760 oc graag vervangen door een 1050 x4 deze is betaalbaar en draait de meeste games ""soepel"" op hoge instellingen dikke prima toch ? 400 euro + gaat mij gewoon wat te ver voor een videokaart"
GTX 1050 Ti;4;0.2981503903865814;"Benchmarks zijn inderdaad over het algemeen op de maximale settings, als je een paar settings aanpast die maar amper effect hebben op de visuele kwaliteit, kan je al gauw 10-20% extra prestaties krijgen, laat staan als je op high of medium speelt, wat er in veel moderne games nog steeds erg degelijk uitziet. Ik houd wel van eyecandy en speel met een RX 470 op 1080p, maar mijn laptop met een R9 m290x (prestatieniveau vergelijkbaar met een RX 460), kan al mijn games ook gewoon spelen op 1080p met 60fps, alleen dan op lagere settings. Mensen overdrijven zo ontzettend over wat voor gpu je ""nodig hebt""."
GTX 1050 Ti;1;0.32969003915786743;Als ik het zo lees moet ik snel mijn gtx960 verkopen. Die levert nog minstens 125 op, en als ik voor dat bedrag een 1050 kan krijgen is dat een gratis upgrade.
GTX 1050 Ti;3;0.4338548481464386;"Dat ligt eraan; tenzij ik iets over het hoofd zie, zijn er geen GTX960's meegenomen in de test met 4GB memory. Wat dan wel weer opvallend is aangezien het verschil tussen 2GB en 4GB juist bij deze serie kaarten al een aanzienlijk verschil opleverde (en zeker op hogere resoluties)."
GTX 1050 Ti;3;0.6174715161323547;Naja, die 4gb versie van de gtx 960 presteert niet veel beter in de praktijk dan de 2gb vanwege de krappe geheugenbus. Dus denk dat t verschil niet veel anders zal.zijn
GTX 1050 Ti;3;0.6084201335906982;Het ligt een beetje aan het type spel. Omdat ik openworld games wel leuk vind is volgens 4 gb daar echt wel een voordeel... zelfs met een 128 bit data bus. Simpel omdat er minder tussen het 'langzame' geheugen op het moederbord en het geheugen op de gpu kaart gecommuniceerd hoeft te worden. Dan heb je het trouwens wel over behoorlijke grafische settings die dat weer kunnen verstieren. Het krappe geheugenbus gaat over de communicatie tussen de gpu chip en het geheugen op de videokaart. Die GPU chip moet net zo goed wachten als de data nog van het moederbord gehaald worden.
GTX 1050 Ti;2;0.4764769375324249;Het gaat er om dat 2GB te weinig is en je dan swapping krijgt met systeem ram. De bus vd 1050ti en 960 zijn beide 128 bits dus het verschil in snelheid is klein. Op hw.info zijn trouwens benchmarks vd 4GB 960 en die blijkt iets trager te zijn dan de 1050ti.
GTX 1050 Ti;3;0.5088791251182556;Dan moet je inderdaad wel snel zijn, want omdat het nu voor 125 op de V&A staat betekend t niet dat je dat er voor gaat krijgen als deze kaarten volop verkrijgbaar zijn
GTX 1050 Ti;1;0.46781986951828003;Of dat iemand nu op V&A 125 betaalt voor die kaarten Ik hou het een beetje in de gaten, en de advertenties voor 100 zijn zo weg, die van 125 staan een tijdje, en wie weet zakt de verkoper dan wel voordat ie weg gaat.
GTX 1050 Ti;4;0.38963666558265686;haha idd een goed iedee
GTX 1050 Ti;3;0.4546411335468292;Mja, het verschil op 1080p/ultra in farcry4 was niet groot. Ik heb zelf ook een 960/2GB en mijn mening is dat de 1050/1050ti mij niet veel upgrade potentieel biedt . Ik spaar liever door voor een 1070 of een 1070ti, mits die er komt.
GTX 1050 Ti;3;0.3208150565624237;Als je echt serieus wil gamen @ 1080p moet je volgens mij toch naar een RX470 of GTX1060 kopen, die kaarten zijn wat duurder maar dan zit je safe voor de komende jaren. Dit is mijn inziens een geval van de gierigheid bedriegt de wijsheid. Zeker geen slechte kaart voor de casual gamer die lichte games draait maar toch net iets te traag voor het serieuze werk.
GTX 1050 Ti;2;0.35933825373649597;'Serieus' gamen vind ik een rare aanname. Is het enkel serieus met zware anti aliasing en de grootst mogelijke textures? De Tweakers benchmarks geven imo een vertekend beeld omdat de games op zo'n hoge settings worden getest. Op HWinfo moet je maar eens naar de normal-high benchmarks kijken, de kaart haalt daar makkelijk 60-100 fps in full hd en kan er zelfs flink wat in wqhd draaien aan 60 fps.
GTX 1050 Ti;3;0.5464982390403748;Met serieus gamen doel ik op games als BF1, Project Cars, COD enz.., dus games die je online speelt en waar je minimum 60fps moet voor halen. Deze kaart zal dat zeker doen momenteel mits wat aanpassingen aan de grafische instellingen, maar wat gebeurt er volgend jaar als de eerste ports van de PS4 Pro en de Xbox One Scorpio verschijnen? Ik vind dat deze kaart net iets te traag is om ze te kopen als dat net de games zijn die je wilt spelen, voor een beetje meer heb je iets dat meer toekomst biedt.
GTX 1050 Ti;5;0.25340232253074646;Volgend jaar koop je voor een soortgelijk bedrag een kaart welke de huidige 1060 weer wegblaast. De oude verkoop je dan voor 50-70 euro minder. Zo kan je elk jaar goedkoop upgraden. Nu iets kopen wat je niet nodig hebt is niet altijd efficient. Met deze kaart kan ik een Skylake game systeem samenstellen die prijs/performance subliem in elkaar zit voor rond de 480 euro. (i3-6100, 1050Ti, 8GB).
GTX 1050 Ti;3;0.4008098840713501;Aan de andere kant @1080 en ultra/high is ook niet voor iedereen nodig. Normal/high @1080p is best ook wel speelbaar. Zoals Tweakers goed aanhaalt, zijn deze tests gemaakt voor de allernieuwste, meest krachtige GPU's te doen zweten. Dan zit je bij de 1050 niet op de juiste plaats. #imho
GTX 1050 Ti;3;0.3688153028488159;Deze kaart zal ook niet bedoeld zijn voor de hardcore gamer. Met deze kaarten kan je in principe een game computer in elkaar zetten die qua prijs in de buurt komt van een console (als je randapparatuur zoals scherm etc niet meetelt). Gezien de enorme populariteit van games zoals CSGO, Dota, LoL en recentelijk overwatch maakt dat een mooie instap, zoals in de conclusie al genoemd wordt is hier nog best een 'prestatiegat'. De nieuwste en mooiste games kan je dan wel niet voluit draaien, maar ze zijn zeker te spelen als je de graphics een stukje terugdraait. *edit : Ik ben het wel met je eens dat een 1060 een safe bet is voor de lange termijn, de verschillen in games gebaseerd op DX12 en Vulcan zijn aanzienlijk, en daar gaan er alleen maar meer van komen.
GTX 1050 Ti;2;0.35005471110343933;Zou het niet slimmer zijn om te zeggen: Koop dan gewoon een 2dehands HD7970 of zoiets (100eur of minder)... Als je dan toch voor budget kijkt. Nee denk je niet?
GTX 1050 Ti;3;0.30315807461738586;Weet je hoe oud die kaart al is?
GTX 1050 Ti;2;0.34883806109428406;Want de leeftijd van een videokaart bepaalt of een game speelbaar is? Handig, kan je elke keer dat er een nieuwe graka uitkomt je graka weggooien, is immers oud. Waarom nog benchmarks draaien dan? Als iemand wil gamen op fhd met 60fps en dat lukt nog met de huidige kaart, waarom dan upgraden? Hangt allemaal maar net af van je eisen en van de games die je speelt lijkt mij zo
GTX 1050 Ti;1;0.36534684896469116;Ja, de 6XXX lijn krijgt namelijk geen driver-updates meer van AMD en BF1 vereist een minimale driverversie boven die is gereleased voor de 6XXX
GTX 1050 Ti;3;0.4060462713241577;Leuk.. Maar sinds wanneer is de v HD7970 een kaart in de 6xxx lijn? Dat is nml weer een generatie ouder. Overigens is de aangehaalde kaart door AMD ook nog eens rebranded uitgebracht, als de 280x. In principe een kaart die als volgende generatie weggezet is door AMD. Van wat ik begreep wordt alles GCN gewoon nog supported door AMD qua driverupdates gezien het feit dat deze ook DX12/Mantle API doen. De 6xxx serie kaarten die je aanhaalt zijn inderdaad geen GCN. Ik kan mij voorstellen dat er een moment komt dat inderdaad de grens verlegd wordt naar bijv. GCN 1.2 voor specifieke nieuwere games, maar goed, dat verandert niets aan de discussie dat het 100% afhangt van de specifieke game en de eisen van de gebruiker. Vrijwaard je natuurlijk niet om inderdaad rekening te houden met minimale systemeneisen voor bepaalde games - maar dat is m.i. niet helemaal alleen voorbehouden aan graka's en is zeker handig om te gaan doen als je voor een budget build gaat lijkt mij.
GTX 1050 Ti;3;0.4867711365222931;Voor prestaties en toekomstige DX12 moet je deze GTX 1050Ti nu niet echt verkiezen boven een tweedehands GTX 970 die dra in dezelfde prijsklasse valt. Goed voor systeembouwers die garantie willen verlenen op een budget bakje. ivm oude kaarten: Mijn HD7970 Ghz edition voldoet prima. Want mijn 8 jaar oud high color gammut Dell scherm met resolutie van 1920x1200 wil maar niet stuk. Balen maar ja, dat gebeurt soms als je degelijk materiaal aankoopt. Iemand bood me een GTX 970 aan voor 150 Euro vorige week maar ik heb bedankt want als ik upgrade wil ik iets voelbaar echt veel sneller (de opvolger van de GTX1080 of de 1080 aan 150 Euro)
GTX 1050 Ti;2;0.3913896083831787;Als ik een nieuwe pc in elkaar zet doe ik dat persoonlijk liever met nieuwere componenten. Denk aan garantie, lagere TDP, nieuwere architectuur, om maar iets te noemen. Het is een keuze, 2e hands is natuurlijk ook prima. En zoals in andere comments ook al aangehaald wordt, de mensen uit deze doelgroep zullen zulke computertjes niet vaak zelf in elkaar gaan zetten maar dit laten doen, of compleet prefab bestellen. Er zullen op deze manier prima game-pctjes komen voor een prikkie. Gat in de markt wat mij betreft.
GTX 1050 Ti;2;0.5862031579017639;Ben ik het niet mee eens. De benchmarks zijn vrijwel allemaal hele hoge settings genomen én bij flink demanding games. Op maximale settings spelen is voor veel gamers helemaal geen must. Deze kaart gaat vrijwel alle games op een medium-high setting op 1080p/60fps kunnen draaien. De 1050 ti is naar mijn idee de ideale budgetkaart voor 1080p. Een 1060 kost je al bijna het dubbele (de 3GB VRAM versie bottlenekt teveel tegenover 4GB) en is voor veel gamers een overkill. Tevens zijn ze ideaal voor portable ITX kasten en ook 75 watt tegenover 120 watt is echt wel wat waard. Uiteraard geldt dit enkel wanneer men zich aan de adviesprijzen houdt. Ik zie nu een 1050ti voor 225 euro in de Pricewatch staan, dat gaat natuurlijk nergens over.
GTX 1050 Ti;1;0.4928921163082123;Dat zijn de enige 1050's die te krijgen zijn momenteel dus waarschijnlijk via parallelimport binnengekomen en nu woekerprijzen vragen. Goeie reden om nooit meer iets bij dat bedrijf te kopen.
GTX 1050 Ti;2;0.3396543264389038;De 1060 3Gb heeft toch een 192 bits databus tov de 1050(Ti) een 128 bits databus? Hoezo zou er bij de eerstgenoemde dan eerder een bottleneck zijn ?
GTX 1050 Ti;2;0.3885256052017212;Het gaat niet om de snelheid, maar hoeveelheid. Een game als GTA V gebruikt al snel meer dan 3GB VRAM. Dat heeft veel te maken met de huidige consoles, die 8GB GDDR5 aan boord hebben. Omdat deze games leunen op de VRAM van de PS4 en Xbox One, doen (helaas) veel PC ports dat ook. Daarom is het aan te raden om op dit moment toch minimaal voor 4GB te gaan.
GTX 1050 Ti;2;0.48408693075180054;Ah je gebruikt de term bottleneck dus niet erg handig in dit geval. Jij hebt het over de inhoud van de gehele fles en dat bij de 1060 die fles eerder overstroomt. Snap je het verschil ? Open world spellen lijken me zeker op hoge resoluties idd zeker de eersten die 3 Gb een beperking gaan vinden. (Het was de reden dat ik toch voor de 4Gb versie van de 960 ben gegaan... want 2 Gb is nog sneller vol) Maar voldoet de 1060 3Gb sowieso dan wel voor die hogere resoluties ? Ik zou dan eerder de 8Gb 480 of zelfs een 1070 aanraden?
GTX 1050 Ti;2;0.5553984045982361;"""Daar komt bij dat de 1050 (Ti) in principe geen extra voeding nodig heeft, waardoor gamen met een simpele oem-pc ineens een optie wordt"" Misschien heb je geen connector nodig, zo'n kaart trekt toch nog tot 75w uit het PCI-E slot. Een gemiddelde OEM PC is echt niet in staat om nog 50w extra aan te bieden, de marges van die systemen zijn al veel te krap. Die conclusie is imho een beetje snel getrokken"
GTX 1050 Ti;5;0.2885604202747345;Ik zou zeggen: koop er dan meteen een nieuwe voeding bij. Heb je ook weer 5 jaar garantie op.
GTX 1050 Ti;1;0.7090386748313904;Oh, dat moet je sowieso doen Mijn punt is dat dit 'selling point' totaal irrelevant hebt. Je moet toch een andere voeding kopen wil je 'm in een OEM-systeem schroeven. En als je dat doet, kan je net zo goed voor de RX470 gaan, mocht je dat als tegenhanger beschouwen.
GTX 1050 Ti;3;0.639396071434021;Okee, maar vaak zit er al een oude videokaart in die je kan vervangen en dat scheelt dan weer. Ik heb zelf overigens in de afgelopen jaren een aantal keer een videokaart in een OEM-PC erbij geprikt en dat ging steeds zonder problemen. Dat waren wel simpele kaarten, de GT 730 en de GT 740 als ik me goed herinner.
GTX 1050 Ti;3;0.6120339035987854;Wel oppassen wat voor stekkers het moederbord gebruikt, niet alle fabrikanten gebruiken 24pins en niet alle voedingen hebben 20pins aansluitingen.
GTX 1050 Ti;5;0.25518596172332764;75W is 75W uit een PCIe slot. Geen enkele OEM gebruikt voedingen die maar 50W kunnen leveren aan de PCIe bus, dan kunnen ze aan de lopende band moederborden gaan vervangen.
GTX 1050 Ti;2;0.4453258216381073;Je hebt zelf kunnen zien dat de techbench met een overgeklokte 10-core (!) processor en de 1050ti nog geen 150W verbruiken tijdens een stresstest. Toegegeven, alleen de videokaart wordt gestrest en de de cpu blijft idle, maar stel dat je een wat oudere quadcore of gewoon een i3 hebt, kom je met een gaminglast echt niet boven de 200W verbruik aan hoor. De meeste oem-voedingen kunnen dat zeker wel leveren. Bovendien wordt tijdens de energietest ook nog eens het vermogen van het gehele systeem gemeten. De voeding heeft een efficiëntie van ongeveer 90%, dus je mag ook nog eens 10% van het gemeten verbruik afhalen, aangezien dat niet het vermogen is dat van de voeding wordt gevraagd. Conclusie: De voeding van je oem-bakkie moet wel héél brak zijn om een 1050 Ti niet aan te kunnen. Trek je kast open en kijk naar het stickertje op de zijkant van de voeding. Staat daar een getal hoger dan 200? Dan kan hij de kaart in principe gewoon aan.
GTX 1050 Ti;2;0.3965694308280945;Als het getal op de 12v-rail dermate hoog is, geef ik je gelijk.
GTX 1050 Ti;2;0.3430347442626953;Voor een complete systeembouwer voor bv de Mediamarkt-klant scheelt het toch een kabeltje trekken en een voeding die die extra 12V rail niet hoeft te hebben. (En die voedingen zonder zo'n rail hebben ze toch al groot ingekocht) Bij elkaar maar een paar euro waarschijnlijk, maar kijk maar eens bij oa eerder genoemde winkel en je ziet daar vooral systeempjes met maximaal een Geforce 950 staan. Een beetje tweakers koopt die dingen daar toch niet dus is het simpelweg een andere markt van voornamelijk onwetenden... ik bedoel een Z170 moederbord oid zullen de complete systemen verkopers er ook zelden instoppen. Maar het is een wel heel erg lukratieve markt gezien het succes van de 750ti
GTX 1050 Ti;3;0.37345412373542786;Nu nog in een low profile versie, liefst met single slot IO connectoren. Dat de koeler op zich wat breder is, is niet zo erg, maar ik heb maar 1 IO slot. Het is misschien wel een niche markt, maar sinds de 750Ti is er geen alternatief meer, en er is toch de nodige vraag voor.
GTX 1050 Ti;5;0.386796772480011;Ben wel benieuwd hoe de 750ti presteert tov de 1050 serie
GTX 1050 Ti;3;0.31152060627937317;Ik zag deze vandaag : Edit : Deze is toch informatief? Precies wat spokje zich afvraagt ?
GTX 1050 Ti;1;0.6146088242530823;Bedankt!
GTX 1050 Ti;1;0.7165171504020691;Maar ik vraag me in dit geval dus echt af waarom ik hiet met '0' beoordeeld wordt. Verkeerde bron ofzo?
GTX 1050 Ti;1;0.36316484212875366;Geen idee. Ik zag later wel dat in de eerste 3DMark test, de 750ti er gewoon bij staat. Maar dat is alleen in de eerst twee testen. Maar ik was erg blij met je filmpje, alleen mag ik je niet modereren. Dit is de scorekaart: +1 On-topic 1 0 Off-topic / Irrelevant 2 Slechts 2 mensen zijn met het verkeerde been uit bed gestapt gister
GTX 1050 Ti;1;0.5569707155227661;Idd ik weet dat jij het niet was Dat zal het zijn. Dan laat ik me niet uit het veld slaan door deze '0'
GTX 1050 Ti;3;0.33370861411094666;'0' is ook prima. '-1' zou ik pas boos worden
GTX 1050 Ti;1;0.5391559600830078;Ik was niet boos, maar teleurgesteld Neh ik wou gewoon weten waarom... jeeh nu +1
GTX 1050 Ti;5;0.7291463017463684;haha awesome!
GTX 1050 Ti;5;0.4008987247943878;ik zit inderdaad ook te wachten op een low profile.. ik heb een standaard hp pro 3120 sff die ik helemaal wil gaan upgraden en het lijkt me geweldig om hier een 1050 in te zien zitten
GTX 1050 Ti;1;0.311256468296051;Het zou mooi zijn als de 1050 in een low profile editie uitkomt. Dan heb ik voor weinig geld een kaart die hardwarematig H265 kan decoderen en mijn HTPC'tje ook een simpele game kan laten spelen, zonder dat ik meteen de hele bende (mobo, proc, installatie) moet upgraden. Iemand die weet of dat er aan zit te komen?
GTX 1050 Ti;2;0.3658922016620636;TPD laat het toe, dus zou eerlijk gezegd niet weten waarom niet.
GTX 1050 Ti;3;0.429889976978302;Kan je er niet gewoon een zalman koelertje opzetten (of een ander merk/type)? Die is wel 2 sloten dik, maar heeft geen 2 sloten aan de achterzijde nodig. Het zijn nog mooie koelertjes ook.
GTX 1050 Ti;3;0.39643195271492004;Zit nu toch wel te twijfelen om mijn RX460 te gaan upgraden... In een game zoals ets 2 kan ik net niet op high/60fps gamen. En mijn sapphire houdt er ook wel van om geluid te maken, en warm te worden.
GTX 1050 Ti;3;0.4640938639640808;ik heb de 1070 en ik heb alles max (en scaling op 400% op 2560x1440p) en haal 60-80 fps. ligt er aan wat voor settings je gebruikt. e rx 460 zou op 60fps op een mindere kwaliteit moeten draaien
GTX 1050 Ti;2;0.3249523341655731;Wacht iig nog een maandje! .... ben je nog ruim voor de '13e maand prijsstijgingen' en dan zijn de prijzen van de nieuwe kaarten vast ook wat gezakt. Zelfs misschien voor de al wat langere uit zijnde kaarten omdat deze natuurlijk de nieuwe concurentie gaan voelen. ^_^
GTX 1050 Ti;2;0.2989509105682373;is een SLI mogelijk bij de 1050 TI?
GTX 1050 Ti;2;0.3854422867298126;"Nope, Nvidia vindt SLI iets voor highend kaarten; de GTX 1060 deed al geen sli, dus de 1050 helaas ook niet."
GTX 1050 Ti;1;0.28393009305000305;1070 en hoger en max twee kaarten. Daar houdt het op.
GTX 1050 Ti;5;0.6448607444763184;Nu heb ik geen verstand van videokaarten maar gebruik op dit moment een Sapphire Radeon HD 7950 3GB GDDR5 With Boost Wil iemand zo vriendelijk zijn om misschien te vertellen of deze kaarten interessant zijn qua prestatiewinst omdat de prijzen dat zo te zien zeker zijn. Met het gevaar dat ik natuurlijk hartelijk word uitgelachen. Dank!
GTX 1050 Ti;3;0.6231152415275574;Qua prestatiewinst voor jou geen interessante kaart, de 7950 zit rond het niveau van de R9 285, alleen heb jij 1GB vram meer en een bredere geheugenbus. Het zou dus geen upgrade maar een sidegrade zijn Deze nieuwe kaarten zijn echter wel een stukje efficiënter.
GTX 1050 Ti;5;0.5276634097099304;Dank je!
GTX 1050 Ti;3;0.4319605827331543;Destijds een goede aankoop gedaan. Die kaart gaat al wel wat jaartjes mee.
GTX 1050 Ti;5;0.3874744772911072;Ik vertrouw dan ook op de Best Buy artikelen van Tweakers en luister naar adviezen van Tweakers
GTX 1050 Ti;3;0.4137747585773468;Een 7970 staat gelijk aan een 280X, die beter is dan de 285 die is ge-rebrand is als de 380. De 380 is net wat beter dan de 960. Zo kun het bekijken omdat geoverklokte 7950s bijna even goed zijn als 7970s. Dus je kunt jouw kaart vergelijken met de 960 in deze review, en dan kun je er ook nog een paar FPS bij op tellen (3-6). EDIT: Vergeet deze uitleg, ik zie net dat er een R9 285 wordt gebruikt in de benchmarks die inderdaad rond de 7950 zit .
GTX 1050 Ti;5;0.6205578446388245;Dank!
GTX 1050 Ti;3;0.676997721195221;"Mooie review, maar ik mis eigenlijk de GTX960 4G in het hele verhaal; je geeft aan dat de 1050 ti een stuk sneller is dan de 960, maar daar bedoel je volgens mij alleen de 2G variant van de 960. Ik ben wel benieuwd hoe de 960 4G zich houdt ten opzichte van de 1050 TI."
GTX 1050 Ti;3;0.4528033137321472;Het hangt natuurlijk een beetje van de kloksnelheid van de 1050 Ti en de 960 af. Maar om onze benchmarkresultaten te gebruiken: daarin is de GTX 960 is ongeveer tien tot twintig procent langzamer is dan de 1050Ti in de meeste benchmarks, behalve in die benchmarks waarin de 2GB de bottleneck is, zoals in Tomb Raider, Hitman, Total War en Doom. Zou het geheugen geen bottleneck vormen (wat bij de 4GB-versie het geval zou zijn) dan verwacht ik dat het verschil nog steeds tien tot twintig procent is (maar geen 120% oid zoals bij de 2GB-kaarten).
GTX 1050 Ti;2;0.46945086121559143;Dat zou inderdaad wel logisch zijn, maar ik had hem er toch wel tussen verwacht. In ieder geval lijkt het erop dat het nu geen goed idee meer is om een 2GB kaart te kopen.
GTX 1050 Ti;3;0.5176487565040588;Lijkt me wel een leuke kaartje voor m'n HTPC. Mits deze in LP ook nog een keer uitkomt.
GTX 1050 Ti;1;0.41061148047447205;Ik hoop er ook op. MSI maakt zo maar even 10 (!) varianten, maar niet één LP of single slot. Hoewel het er de uitgewezen kaart voor is. Gemiste kans opnieuw.
GTX 1050 Ti;3;0.4365779161453247;Wat vreemd dat de AMD kaarten niet zoveel last lijken te hebben aan het gebrek aan geheugen(de 2G edities) bij de games. Deze doen vrolijk mee in het 4GB rijtje.
GTX 1050 Ti;2;0.4644780457019806;AMD heeft over het algemeen beter geheugen management als het gaat om geheugen gebruik, bij Nvidia zie je de trend dat ze meer kunnen met de beschikbare geheugenbandbreedte. Er is overigens in andere reviews ook een patroon zichtbaar met 2gb kaarten in combinatie met de nieuwe api's (DX12, Vulkan), ze presteren daar vaak slechter. Dit komt omdat er blijkbaar niet meer echte geoptimaliseerd wordt voor 2gb kaarten door game ontwikkelaars bij de nieuwere api's. Dit kan je goed zien bij kaarten zoals de R9 285/R9 380 2gb vs de R9 380(x) 4gb en de GTX960 2gb vs GTX960 4gb. Het verschil in prestaties is soms enorm.
GTX 1050 Ti;3;0.46471136808395386;Jammer dat de normale TI versie niet de semipassieve koeler heeft van de gaming-X. Een van de redenen om een kaart te kopen zonder 6 pins is juist de geluidsproductie. (Voor mij ten minste)
GTX 1050 Ti;5;0.5505518913269043;Yep, volledig mee eens. Ik zou graag een niet overgeklokte 1050 Ti versie willen kopen zonder 6 pins, maar wel met 4GB en die mooie semi-passieve koeler.
GTX 1050 Ti;1;0.44234898686408997;Ik zie nu net de eerste prijsvermelding: pricewatch: Asus EX-GTX1050TI-4G € 225,- haha, zelfs duurder dan een aantal goedkoopste GTX 1060 3GB modellen.
GTX 1050 Ti;1;0.40286198258399963;Hehehe, pfff, die gekke Informatique ook. Al is ie daar nog € 6,- goedkoper dan hun goedkoopste GTX 1060 3GB.
GTX 1050 Ti;1;0.45540860295295715;€125 en €155 Als dat de nederlandse MSRP is, zal het in de praktijk nog wel duurder worden ook. Zit je zo weer op de €150 en €180 die de 950 en 960 nu kosten, met dus een minimale verbetering in bang/buck...
GTX 1050 Ti;3;0.5993636250495911;Je kan dan ook beter voor een RX 470 gaan . Als je zat te kijken naar een RX 460 dan is de GTX 1050 wel waarschijnlijk de betere keuze.
GTX 1050 Ti;1;0.35130971670150757;Als mijn pcie niet 75 watt aankan omdat dit geen gen3 is. Kan ik deze kaart dan alsnog aansluiten doormiddel van de voedingsstekker vanaf mijn voeding?
GTX 1050 Ti;5;0.443629652261734;Let op dat PCI-e 1.0 en 2.0 (x16) ook tot 75 watt kunnen leveren.
GTX 1050 Ti;1;0.4806939959526062;Lijkt me (aan de huidige prijs) een totaal overbodige kaart. Ze zal snel moeten dalen in prijs om relevant te worden. Zo vind ik het volgende in de pricewatch : GTX 1060 vanaf 268 euro. GTX 1050 TI vanaf 229 euro. Prijsverschil van 39 euro. Zelfs wanneer je een krap budget hebt kan je bijna niet verantwoorden om dan voor de 1050TI te gaan.
GTX 1050 Ti;3;0.34277957677841187;De 1050 Ti vind je, zeker in Duitsland, al vanaf 150 € bv: (wel 20€ verzendkosten bv. bij die shop dus 170€) Prijzen in Nederland en Belgie zullen ook nog wat zakken (je waarschijnlijk wel nog een beetje geduld moeten hebben).
GTX 1050 Ti;1;0.4765012264251709;1050ti 225,- RX470 242,- bron alternate. dus voor 17 euro meer 30% meer preformance.. voorlopig nog geen 1050ti
GTX 1050 Ti;1;0.37569722533226013;Tja, dat heb je met alle prijzen. De eerste shops die ze aanbieden vragen onredelijk hoge prijzen om te profiteren van mensen die niet kunnen wachten tot de prijzen dalen tot normale bedragen. De adviesprijs van de 1050 Ti zou omgerekend 155 euro bedragen. Wellicht zal het nog weken duren vooraleer de prijzen in de buurt komen.
GTX 1050 Ti;2;0.30166125297546387;Vraag me af of een 1050Ti betere FPS op full HD heeft in een oudere bak met een A10-5800K APU...
GTX 1050 Ti;3;0.4714807868003845;Denk het wel.
GTX 1050 Ti;5;0.48571303486824036;Zier er goed uit, thanks!
GTX 1050 Ti;5;0.5055179595947266;Een van de 1050 kaarten is zeker interessant, ik wil er niet teveel aan uitgeven en ik denk dat deze veel beter presteerd dan mijn gt610
GTX 1050 Ti;4;0.47442391514778137;Leuke kaart voor weinig, niks mis mee in deze tijden
GTX 1050 Ti;3;0.3211262822151184;Zou Overwatch 1920X1080 alles op high te spelen zijn?
GTX 1050 Ti;3;0.3512133061885834;ja, want de GTX 750 ti kan dat al
GTX 1050 Ti;1;0.32485437393188477;Vraag me af hoe ze de laptop versie gaan afstellen
GTX 1050 Ti;3;0.34857824444770813;Jammer geen BF1? was ik eigenlijk wel erg benieuwd naar
GTX 1050 Ti;5;0.2563166618347168;LTT heeft een video
GTX 1050 Ti;3;0.28767240047454834;Ik heb 2 jaar en 2 maanden geleden een AMD HD7950 3gb met arctic accelero twin turbo 2 gekocht voor 95€. Dat was nadat alle bitcoinminers hun kaarten dumpten. Na zoveel tijd beginnen er eindelijk kaarten te komen die deze deal kunnen evenaren. In high end is er inmiddels wel een behoorlijk prestatiegat geslagen maar voor mijn gevoel gaat die ontwikkeling in low/mid end heel langzaam. Kaarten zoals uit deze review halen het nu bijna in
GTX 1050 Ti;5;0.38465726375579834;Nu kunnen ook de laatste budget console lovers overstappen op de PC :-)
GTX 1050 Ti;3;0.4752938747406006;Het is wel verwarrend, in de inleiding geven jullie aan ook een 1060 3gb mee te testen, maar in het overzicht is deze in een keer veranderd in een 6gb uitvoering. Wat toch wel degelijk een andere kaart is kwa snelheid en prijs. Welke hebben jullie nu getest ?
GTX 1050 Ti;1;0.5180166959762573;Hoe houd deze kaart zich in een gemiddeld systeem? Ik snap dat je benchmarks wil maken in een zo snel mogelijk systeem maar welke gek heeft er nou zo'n duur systeem en propt daar deze kaart in? Misschien zou tweakers eens een onderzoek moeten doen wat het gemiddelde systeem is wat mensen thuis hebben staan bijvoorbeeld een i5 2500 met 8GB geheugen (ik noem maar wat) en de prestaties dan testen.
GTX 1050 Ti;3;0.427523672580719;Een i5 en i7 maakt niet zo veel verschil in benchmarks om alles te vervangen
GTX 1050 Ti;1;0.5030862092971802;Als ik GTA V draai met 4x msaa, dan is mijn gemiddelde framerate nog wel te doen. De dikke framedrops en het gestotter echter niet. Doorgaans werd er een minimale, gemiddelde en maximale framerate aangegeven. Waarom zijn jullie hier mee gestopt? Dat zegt echt zoveel meer over de speelbaarheid, dan een gemiddelde framerate. Want de 1050 (non-ti) is met deze grafische instellingen dus niet bruikbaar als ik de benchmarks zie. En bij de 1050 ti vraag ik me ook sterk af, of het in de categorie 'speelbaar' geplaatst kan worden. Zelfs op normal of medium, tikken ze net de 60fps aan. Als dat regelmatig naar 40 of lager dropt, dan is het niet speelbaar imo. Deze low-end categorie is echt zwaar overbodig. Kunnen ze niet beter oudere modellen in deze prijscategorie laten vallen. Een stuk logischer dan kaartjes uitbrengen, waar je niet eens fatsoenlijk op 1080p kan gamen. Ja games als CS/LoL/Dota2 zullen het prima doen, maar die kun je ook op een goedkope laptop van amper €400 prima spelen.
GTX 1050 Ti;2;0.40669766068458557;"""Doorgaans werd er een minimale, gemiddelde en maximale framerate aangegeven. Waarom zijn jullie hier mee gestopt? Dat zegt echt zoveel meer over de speelbaarheid, dan een gemiddelde framerate."" Een meer dan terechte klacht! Juist de dips bepalen hoe speelbaar het is, het gaat dan natuurlijk niet alleen om de diepte van die dips maar ook om hoelang het duurt. Misschien is dat een reden waarom ze het niet meer weergeven, soms heb je een hele lage dip die je niet merkt in de gameplay, het duurt zo kort (tijdens het laden of zo) dat FRAPS het niet eens weergeeft. Toch wel doen, eventueel met de kanttekening hoe je het wel of niet merkt aan de gameplay en dit goed onderbouwen."
GTX 1050 Ti;2;0.3865733742713928;Ik zou toch echt een nieuwe 1050(ti) boven een tweedehands 960 kiezen (maar ik heb al een mooie 960 en ben nog wel een tijdje tevreden denk ik) Wees voorzichtig met tweedehands te kopen... Wat heb ik al veel lui idiote overclocks zien doen... Altijd een risico dat bij een tweedehands kaart het geval is en dat kan veel later tot problemen leiden.
GTX 1050 Ti;2;0.3902432918548584;Bij de buren is de 4gb versie van de 960 in 3dmark wel sneller dan de 1050 ti : Aangezien Linustech ook al over dat de 960 beter presteert als de 1050 ti... Gebruikt Tweakers een 'langzaam' model ? Herstel : Alleen in 3d mark Firestrike oeps en een paar spellen op 'lage' resolutie.
GTX 1050 Ti;1;0.397438108921051;"Wat een onzin word hier weer gebracht door tweakers: 'Ook wat prijs betreft zijn Nvidia's kaarten interessant; de GTX 1050 is met 125 euro even duur als de RX 460, maar wel sneller' Als je dan toch eerlijk advies wilt geven moet je er wel bij zeggen dat nvidia vrijwel in elke dx11 game hetzelfde of beter presteert maar dat het bij dx12 titels weer andersom is. (1050 vs 460) Geen interessante kaart als je graag de nieuwe titels wilt spelen 'dx12' je ziet overal vrijwel hetzelfde beeld amd presteert beter in de dx12 titels nvidia op dx11 ook @ guru3d hebben ze dat waar genomen. In elke dx12 titel waarbij de 460 is opgenomen in de grafiek presteert deze gelijk aan de 1050 of beter. Wat mij betreft een goede zet geweest van amd om zich meer op dx12 te richten"
GTX 1050 Ti;3;0.5068079233169556;Waarom staat de GTX 1060 3GB niet tussen de benchmarks ? De vergelijking met de 6GB vind ik immers niet heel relevant. Bij TechPowerUp word deze wel vergeleken.
GTX 1050 Ti;1;0.4667879045009613;Op dit moment is de Sapphire RX 470 4GB Nitro+ aan 185€ te koop op amazon.fr. Dus ik zou niet weten waarom iemand voor een GTX 1050 ti van 160 à 170 € zou gaan, lijkt me een no-brainer. Ik heb dan ook net zelf die RX 470 besteld
GTX 1050 Ti;2;0.43982383608818054;Is het de moeite een 960 4gb te vervangen door een 1050 Ti? (bv. met uiteindelijk 20 € verlies als je je 960 verkoopt via V&A) Ik twijfel maar denk het is de moeite niet echt waard...
GTX 1050 Ti;2;0.4445511996746063;Nee, tenzij je 10-20% betere prestaties de moeite waard vind om daar extra voor in te leggen. Je kan beter kijken naar een RX470 die een heel stuk sneller is. Mochten de prijzen naar 150 euro dalen dan zou ik het nog niet doen, daarentegen als de RX470 eindelijk in prijs daalt naar 170-185 euro dan is dat wel een mooie stap vooruit. Om meteen de vraag van een boel mensen te beantwoorden, niet upgraden (sidegraden) naar een GTX1050Ti als je in bezit bent van de volgende kaarten: GTX960, GTX770, GTX680, HD7950, R9 280, R9 285, R9 380 en zeker niet als je een snellere kaart hebt zoals een GTX780, 280x, 380x. De enige reden om te sidegraden is voor een specifieke feature (HDMI 2.0? en in ieder geval niet VR is want daar is de kaart te traag voor ) die je wilt gaan gebruiken of de lagere power consumptie.
GTX 1050 Ti;2;0.4988923966884613;AMD Polaris is voor mij momenteel wel geen optie, DXVA2 (in het bijzonder met HEVC) heeft nog bugs zoals ik hier en daar in de gespecialiseerde fora lees (gebruik die PC ook als HTPC / SatTV en steeds meer en meer UHD zenders zijn beschikbaar die HEVC 10 bit gebruiken als Codec). Gamen doe ik vooral racegames (Project Cars / F1 2016) en een beetje Fifa op 1080p (op mn 4k scherm native spelen is voor mij niet nodig, 1080p is genoeg voor mij) (en daarvoor is de 960 voldoende vind ik) HDMI 2.0 kan de 960 ook al btw. Na meer benchmarks bekeken te hebben kan ik enkel als argument bedenken: minder verbruik (alhoewel dat is ook geen wereld), cleanere kast (want geen PEG) en voor bv. uiteindelijk ca. 20 € verschil tussen verkoop/aankoop (meer zou ik te veel vinden, ik baseer me op prijzen uit Duitsland) zou ik een vernieuwing van garantie krijgen (die is bijna afgelopen)...maar kdenk het is de moeite niet waard. (en de performance boost is waarschijnlijk nauwelijks merkbaar)
GTX 1050 Ti;4;0.4514891505241394;Gewoon lekker bij je GTX960 blijven dus . Ik kan hier overigens prima 4k HEVC 10bits afspelen, om te testen gebruik ik de testfiles van Dit met een cpu load van 4-5% op een oude i5-4670k, RX480 icm Win7, MPC-HC met de interne LAV filters waar ik HEVC en 4k heb aangevinked. Gebruikte files 1080p: jellyfish-10-mbps-hd-hevc-10bit.mkv jellyfish-40-mbps-hd-hevc-10bit.mkv Voor 4k: jellyfish-140-mbps-4k-uhd-hevc-10bit.mkv jellyfish-300-mbps-4k-uhd-hevc-10bit.mkv Zullen wellicht appart geconfigureerde bestanden zijn die niet werken?
GTX 1050 Ti;2;0.4819474220275879;Met de prijzen van de 1050TI nee de prijzen worden weer is omhoog gedrukt om tekijken wat een '' gek '' ervoor geeft, wat je wel zou kunnen doen is je oude 960 verkopen en een 470 4 gig halen of 480 8 gig of 1060 6 gig. Want dan heb je teminste een '' echte '' upgrade en als je voor de 480 of 1060 gaat kiezen nvidia heeft met somige spellen in dx12 slechtere fps dan in dx11. Dus als je bevoorbeeld een amd fx cpu hebt of een intel xeon etc hebt zou ik voor amd gpu gaan want dx12 api helpt dat all je cores beter gebruikt worden en de load beter verspreid wordt) ( Heb een 8350 met een r9 290x zelfs met 8350 op stock en gpu lichte overclock kan ik bf1 met 60+fps op ultra spelen cpu load van alle cores zitten tussen de 40-70% en met dx11 is de load niet zo goed verspreid ) ( marja 144hz freesync dus gewoon lekker op high zetten x] En ja dat is ook het probleem als je wat meer '' budget/bangbuck '' wil hebben freesync of g-sync monitoren zoek het verschil in prijs maar is xD
GTX 1050 Ti;3;0.24742726981639862;Wat ik me af vraag ( wat ook destijds een reden voor 660gtx was) kan je met deze kaart gebruik maken van nvidia game stream (naar bijvoorbeeld R. pi2 of handhelds / laptops) ?
GTX 1050 Ti;2;0.48620957136154175;Het testen van games op ultra instellingen heeft weinig zin. In de meeste games is het visuele verschil tussen ultra en very high niet erg groot. Ik zou het liefst testen op medium instellingen en de op één na hoogste instelling zien. Bij budget kaarten is dan gelijk te zien of 60 fps haalbaar is en bij high-end kaarten is te zien of 144 fps haalbaar is. Nu liggen de testresultaten vaak te dicht bij elkaar om er iets zinnigs over te zeggen. In ieder geval prettig dat er in deze review niet alleen op ultra is getest.
GTX 1050 Ti;2;0.43751978874206543;Het zal me niks verbazen dat AMD binnenkort met een RX 465 zal komen. Deze zal dan net iets betere prestaties als de 1050 ti hebben en net iets goedkoper zijn. Net als dat ze dat toendertijd met de R9 285 hebben gedaan. Alleen sloeg AMD toen met de prijs de plank volledig mis en kwamen ze hier veel te laat mee. De 285 was langzamer dan de 280X maar wel bijna net zo duur. Niet lang daarna kwam AMD toen met de R9 380.
GTX 1050 Ti;5;0.5996780395507812;Juist in die budgetklasse moet je echt geen Nvidia nemen. FreeSync/G-Sync is een must zogauw je een nieuwe monitor koopt en mensen die een 1050 Ti kopen gaan niet een G-Sync monitor kopen terwijl ze FreeSync er gratis bij krijgen.
GTX 1050 Ti;1;0.5199201703071594;Waarom zou je een gloednieuw low-end model kopen ipv een 1 of 2 jaar ouder high-end model voor dezelfde prijs?
GTX 1050 Ti;3;0.28754401206970215;Omdat je heel makkelijk een 2e of 3e generatie i5 2e-hands OEM computer kan upgraden naar een game-PC, voor weinig.
GTX 1050 Ti;3;0.33021432161331177;Wat hebben mensen op tweakers het veel over dingen al 4K gaming, Freesync en andere dure fratsen. Full HD is voor mij op alle vlakken iig nog ruim voldoende. Maar tja ik had een paar jaar geleden nog een beeldbuis TV
GTX 1050 Ti;3;0.34446316957473755;Vergelijkbaar met R7 370?
GTX 1050 Ti;4;0.24703587591648102;Mijn vader heeft een pc met een i5 2500, 8gb ram en een hd 6450 (een oudere dell vostro pc). Ik wil de hd 6450 verwisselen met deze: videokaart. De voeding is op het moment 350 Watt. Zou ik de videokaart in de pc kunnen doen zonder de voeding te hoeven veranderen? Alvast Bedankt.
GTX 1050 Ti;3;0.39415794610977173;Dat lijkt mij prima gaan. Ik zelf heb diverse Dell Optiplex 780 en 790 modellen voorzien van GTX 750 Ti kaarten zonder 6-pins aansluiting en dat werkt allemaal prima, al een jaar of twee. De lichtste voeding in zo'n systeem was 240W, de zwaarste 265W allebei met een 12V - 17A rail.
GTX 1050 Ti;3;0.48513269424438477;Oke Bedankt!
GTX 1050 Ti;3;0.32824769616127014;Ik denk dat je met de non ti versie tov de 6450 al een flinke stap voorwaarts zal maken?
GTX 1050 Ti;5;0.24769817292690277;Ja dat klopt maar ik heb nooit op die pc gegamed en nu krijg ik de pc pas. Dan gooi ik er liever meteen een goede videokaart in zodat ik ook de triple-A games kan spelen.
GTX 1050 Ti;2;0.3978293538093567;Oh, kreeg niet goed door dat de pc voor jou was
GTX 1050 Ti;2;0.4496215879917145;Het blijft nvidia . Niet te vertrouwen als het om performance gaat na +- 2 jaar drivers Nvidia is altijd duurkoop. Dus en duur, en na 2 jaar gegimpt met performance drop van 30%, waar AMD kaarten juist stukken sneller worden. Het zijn toch dingen waar reviewers liever over zwijgen, logisch. Daarnaastheb je echt wel een 290X nodig om op ultra te draaien bij 1080p. Zeker met al die console porten.
GTX 1050 Ti;3;0.5781553983688354;Voor zover ik kan zien liggen de prestaties een beetje rond de gtx 770. Welke meer stroom verbruikt, maar tweedehands voor minder dan 100 euro al over de toonbank gaat.
GTX 1050 Ti;5;0.4698992669582367;Eindelijk, DE update voor mijn GTX 750 Ti! Goede review, nu weet ik het zeker!
GTX 1050 Ti;1;0.6217889785766602;Tweakers gebruikt een ultra highend systeem met een cpu van ruim 1700 euro om deze kaartjes te testen. Volstrekt onrealistisch. Natuurlijk word de bottleneck kaart getrokken. Onzin, je bent ook al bottlenek vrij voor een kwart van de de prijs. Wat overblijft is desinteresse. Echt iets wat je je wel kunt veroorloven als je tweakers heet, NOT. De reviews hier zijn leuk, zo kun je nog eens lachen. Maar je koop er op beslissen kun je beter ergens anders doen
GTX 1050 Ti;3;0.3716036081314087;Mag ik hieruit concluderen dat het basissysteem (ex GPU) minimaal 50 watt verbruikt op het moment van de stresstest? Ik stel deze vraag omdat ik mij afvraag of deze kaart ook prima zijn werk zal doen in een PCI-E 2.0 slot die maximaal 75 Watt (toch?) kan leveren. In oudere systemen kan deze videokaart best nog wel een boost geven, maar als de stroomtoevoer een bottleneck vormt dan is dat wel iets om over na te denken om te kiezen voor een variant met een extra stroom connector.
GTX 1050 Ti;1;0.6723135709762573;RIP AMD Hype... ze hebben op sommige prijspunten nog steeds interessante opties. Bv de 470. Maar de lukratieve complete systemen markt zijn ze met de 460 maar zo kort de interessantste optie geweest dat ze daar niet veel gewonnen hebben op NVIDIA denk ik. Maar de grootste flop van AMD is toch wel de 4gb versie van de 480... jaja onder de 200 dollar. Hierbij reik ik ze vast de bullshit award van 2016 uit.
GTX 1050 Ti;3;0.49099957942962646;Die gaat volgens mij toch echt naar de 1070 en de 1080 maar ik ben het wel met je eens dat de kaart iets te duur is geworden, met name buiten de USA en de aftermarketkaarten.
GTX 1050 Ti;2;0.5046549439430237;Ik heb voor een tweede systeem een RX 470 gekocht (uit nieuwsgierigheid), maar als ik eerlijk ben was de GTX 1060 3GB een betere oplossing geweest. De aangekondigde prijsverlaging door AMD zal helpen, maar ik denk dat AMD fans de prijzen opdrijven. Er is een kleine groep mensen die alleen maar AMD koopt en producenten (MSI, Asus etc.) weten dat. Producenten en retailers concurreren harder met elkaar als het om NVIDIA producten gaat in het budget segment. AMD kan hier weinig aan doen.
GTX 1050 Ti;2;0.45800480246543884;O? De 470 leek mij juist wel aantrekkelijk? Waarom was de GTX1060 3Gb een betere oplossing geweest? Je maakt me nieuwsgierig Ik denk dat de prijzen nog zo hoog liggen van de beschikbaarheid van oa de ram chips en de slechte marges die iig AMD bij de 480 in de adviesprijs had meegenomen. De 470 lijkt haast een noodgreep geweest om toch iets te leveren met iets mindere ram chips die wel ongeveer aan het beloofde prijspunt voldoet. De review versies en de zeldzame 1ste batch van de 4Gb 480 waren stiekem 8Gb omdat de kleinere ramchips zelfs niet beschikbaar waren. Daarmee AMD die bullshit award 2016 toch echt glansrijk binnen gesleept.
GTX 1050 Ti;5;0.2723003029823303;Waarom? Ze hebben gewoon een competitief aanbod met voor- en nadelen en voor veel budgetgamers is de gratis FreeSync een mooie bonus zo gauw ze een nieuwe monitor kopen. Binnenkort (een paar maanden) komen de Vega-kaarten uit en dan doet AMD dus weer in alle segmenten mee.
GTX 1050 Ti;3;0.2694014608860016;"Laten we eerst even de echte prijzen afwachten. Voorlopig zitten alle nieuwe GPU's ruim boven de advies prijs. Een detail wat in de ""goede"" journalistiek van tweakers in de conclusie even voor het gemak werd vergeten."
GTX 1050 Ti;3;0.2539338767528534;.
GTX 1050;1;0.6053471565246582;welke prijzen worden in de review nou met elkaar vergeleken?? nvidia 1050 ti 155,- amd 470 210,-? euhhhh wat? Hier wat detail data: als je voor de een een introductie prijs hanteert , dan graag ook voor de ander. en dan ook graag in dollars want die valuta hanteren zowel amd als nvidia 1050 ti nvidia introductie prijs 139,- dollar 470 amd introductie 179 169 dollar als je voor de een een pricewatch prijs hanteert, dan graag ook voor de ander 1050 ti staat in pricewatch voor 225,- 470 staat in pricewatch voor 204,- Het scheelt 30 dollar mocht je zowel AMD als Nvidia geloven. Het scheelt -21 euro als je pricewatch mag geloven. Het scheelt 49 euro als je de 470 in nederland haalt en een 1050ti uit duitsland Het scheelt 39 euro als je de 470 in duitsland haalt en een 1050ti uit duitsland etc. om nou de duurste en de goedkoopste naast elkaar te zetten in een review leg je toch de verkeerde vergelijking neer?
GTX 1050;2;0.3595983386039734;Op dit moment staat er 1 kaart bij 1 winkel in de PW, dat lijkt me een wat weinig om conclusies uit te trekken. De prijzen in de review zijn 210 euro voor de RX470, dat was de goedkoopste in de PW op het moment van het schrijven van het artikel. Nvidia geeft een adviesprijs op van 125 euro voor de GTX 1050 en 155 euro voor de 1050 Ti. Ik heb ze gisteren nog aan de lijn gehad en ze hebben me bezworen dat er genoeg voorraad is, zodat er geen gekke prijsstijgingen ontstaan, zoals bij de introductie van de 1070 en 1080 het geval was. De prijs van de MSI GTX 1050 Ti in de review (155eu) heb ik van MSI doorgekregen, net als de prijs voor de gtx 1050 gaming x (145eu) en de gtx 1050ti gaming x (175eu). Dat zijn de prijzen waarop de conclusie van de review gebaseerd is. De meeste lezers van de review zullen in Nederland en België wonen, dus lijkt het me nuttig om europrijzen te hanteren en daarnaast met de huidige prijs van de kaarten te rekenen en niet met een introductieprijs van een aantal maanden terug.
GTX 1050;1;0.6834084987640381;Dit is een uitleg die kant noch wal raakt. Je blijft introductie prijzen vergelijken met actuele prijzen, terwijl als er een ding wat de laatste maanden ons heeft geleerd is dat introductieprijzen ver van de werkelijkheid staan. Kies een of het andere, of benoem alles zoals je in eerdere reviews wel hebt gedaan. Ook wordt in de conclusie de resultaten van DX12, niet voor de eerste keer, volledig genegeerd. Daarin is de 460 duidelijk sneller dan de reguliere 2GB 1050. Tevens mis ik de 4GB versie van de 460. Wat nu de review incompleet maakt.
GTX 1050;2;0.4550245702266693;De 460 en 470 zijn beide getest. AMD heeft hier in zijn huidige generatie nog geen kaart tussen gepositioneerd, anders hadden we deze natuurlijk ook meegenomen. De 4GB versie van de 460 hebben wij helaas niet liggen, maar dat zal ook niet zó veel uitmaken op deze resoluties. Kijk bijvoorbeeld naar de R9 285, deze heeft ook maar 2GB beschikbaar, maar presteert wel degelijk beter dan de 460. Met welke kaarten en of prijzen had je dan liever dat we deze kaarten vergelijken? Op moment dat de review gepubliceerd wordt zijn er logischerwijs nog geen winkel prijzen beschikbaar.
GTX 1050;2;0.31971630454063416;Gewoon appels met appels vergelijken. Niet appels met peren. Dus introductieprijs met introductieprijs, adviesprijs met adviesprijs, actuele prijs met actuele prijs. Introductieprijs met actuele prijs vergelijken slaat als een tang op een varken. Er wordt hierboven ook vermeldt dat er genoeg voorraad zou zijn, met als enige bron WC Eend. Als ik bij de overkant van de grote plas kijk, Newegg, zeker niet de kleinste, en daar zit alles al op backorder. Voorraad mn neus, Er mag dan nu wel wat voorraad in de grote magazijnen liggen, dat zal niet lang duren. Amerika blijft de grootste afzetmarkt en Europa zal achteraan moeten sluiten bij het aanvullen van die voorraad. 285 vergelijken met de 460? Als je het over appels en peren hebt... 2GB vs 4GB, tuurlijk maakt dat uit voor het complete beeld, 1050 wordt tegenover de 460 gepositioneerd en de 4GB variant is nu niet te vergelijken.
GTX 1050;2;0.41128358244895935;Wel over appels en peren praten, maar vervolgens zelf niet met feiten maar met voorspellingen komen. Nogmaals, ten tijde van een introductie kunnen wij niet anders dan het over een introductieprijs of een adviesprijs vanuit een fabrikant hebben, omdat er simpelweg geen andere prijzen beschikbaar zijn. Kom in je volgende reactie gewoon eens met feiten, in plaats van fabels.
GTX 1050;1;0.7172902226448059;"Er wordt toch bedoelt dat er actuele prijzen vergeleken worden met een msrp? Dat is onder geen omstandigheden eerlijk, en de fabrikant bellen om te vragen of ze wel genoeg voorraad hebben kan daar nooit tegenover staan. Natuurlijk zeggen ze dat, iedereen zou dat zeggen. Dat is dus juist een voorspelling in plaats van een feit, amd zei ook genoeg voorraad te hebben en die kaarten raakten ook enorm snel uit voorraad. En een ""fok"" achtige reactie wordt ook nooit gewaardeerd..."
GTX 1050;1;0.29031726717948914;Uiteraard, en dat begrijp ik ook zeker. Mijn vraag is alleen wat er dan wél verwacht wordt. Ten tijde van publicatie, op moment van verlopen embargo, zijn er nog geen winkelprijzen bekend, en kan daar dus ook geen oordeel over zijn. Om dat op te vangen hebben we MSI een belletje gegeven en naar hun adviesprijzen gevraagd.
GTX 1050;5;0.35579413175582886;Gast, hou toch je mond dicht en lees wat hij zegt.
GTX 1050;3;0.4131326377391815;Maar wat zou dit dan betekenen, is het gunstiger of ongunstiger voor aankoop van een 1050 kaart? En dan voor op dit moment maar ook naar verwachting over 3 maand?
GTX 1050;2;0.4103706181049347;Dat niet alleen. Ik snap nog steeds niet dat tweakers.net deze kaarten met een high end processor test. (Terwijl CPU's belangrijker zijn aan het worden in games) En waarschijnlijk niet de processor die iemand kiest om met deze videokaarten te gebruiken. Waarom geen 4 profielen (testsystemen) op gameperformantie kiezen? I3, i5, i7 en AMD? Ok, het is makkelijkerer op deze manier. Maar niet echt realistisch. Overal word iedere videokaart getest met een high end processor. Niet echt realistisch voor het merendeel? Specs voor deze test: Moederbord: Gigabyte GA-X99-Ultra Gaming Processor: Intel Core i7-6950X @ 4,2GHz Geheugen: Crucial Ballistix Elite BLE4C8G4D30AEEA 32GB ddr4-3000 Ssd: OCZ RD400 1TB nvme m2 Dan kan je beter besparen op de rest van de onderdelen en meer investeren in de videokaart! Edit: verbetering in de profielen die mogelijk zouden zijn
GTX 1050;3;0.4761861264705658;Dit doen ze echter om te laten zien welke kaart echt beter is om bottleneck te voorkomen van de cpu tijdens het gamen. Als je bijvoorbeeld een I3 of fx6300 zou gaan gebruiken tijdens het gamen en er komt een intensief stukje voor de cpu zullen de frames bij beide gpu's droppen naar vrijwel hetzelfde terwijl de ene gpu op 70% zit qua gebruik en de andere op 55% zit qua gebruik van de gpu. dan is de gpu die op 55% qua gebruik zit sneller wat je niet terugziet in de scores. Daarnaast zou je dan zeggen dat dit dan gelijk zou zijn aan synthetische benchmarks. Echter is dit het ook niet gezien je hier ook uit kan halen welke grafische kaart beter is voor welke game. Zo zie je bijvoorbeeld dat in farcry de r9 285 slomer is dan elke 1050 terwijl hij de r9 285 bij dragon age sneller is dan elke 1050.
GTX 1050;3;0.518447995185852;Maar het zou ook zo kunnen zijn dat een bepaalde videokaart ervoor zorgt dat de processor minder bottlenecked. Door betere drivers bijvoorbeeld. Of DX12/Vulkan.
GTX 1050;1;0.691965639591217;Dat idee voegt totaal niets toe aan de resultaten. Het enige wat je dan krijgt is dat bepaalde games met elke kaart dezelfde FPS gaan neerzetten, puur omdat je cpu limited bent. De conclusie van elke test wordt vervolgens dat het niet uitmaakt welke gpu je koopt boven de x euro. Dat is dan vervolgens te kort door de bocht want in game X kan het weer heel anders zijn en met resolutie Y is het ook weer een ander verhaal. De reden dat we al sinds het begin van pc hardware met de snelste cpu's testen is dat dit de enige manier is om zo eerlijk mogelijk wat over de prestaties van de gpu te zeggen. Op diezelfde manier heb je laren lang tests op 800x600 gezien met de snelste gpu van de planeet om zo wat te kunnen zeggen over de cpu performance.
GTX 1050;2;0.38526293635368347;Maar zo geef je de comunity van tweakers wel een realistische kijk op zijn prestaties binnen een game. Ipv het hoogst haalbare. En dat het meer en meer de cpu is die games de nodige fps geeft met de gpu.
GTX 1050;2;0.49373844265937805;Eerst lees je een GPU-review (waarin de bottleneck bij de GPU wordt gelegd), daarna een CPU-review (waarbij de bottleneck bij de CPU wordt gelegd). Dan weet je hoeveel frames je ongeveer krijgt met elke combinatie GPU+CPU. Je mist dan alleen factoren die van allebei afhangen (CPU-overhead veroorzaakt door een minder efficiënte GPU) maar dat effect zal klein zijn. Ga je realistische scenario's benchmarken, dan weet je juist veel minder, je favoriete combinatie moet maar net getest zijn. Als je alle combinaties gaat testen ben je tien jaar verder en het voegt weinig toe t.o.v. de simpelere GPU- en CPU-benchmarks. Al met al is de huidige manier van reviewen juist het gericht meten van nuttige gegevens, jouw voorgestelde aanpak meet veel vagere dingen en je kan minder met je resultaten.
GTX 1050;3;0.33051782846450806;Het werkt toch twee kanten op? Als jij die kaart koopt en je verwacht 60 fps en je haalt 40 fps dan weet je meteen wat het je oplevert als je de cpu upgrade.
GTX 1050;4;0.3550754189491272;Helemaal mee eens. Het zou nu best zo kunnen zijn dan iemand met een AMD of een goedkope Core i3 of Core i5. Denkt dat hij/zij een bepaalde game op High kan slepen op een GTX1050(Ti). Want dan mogelijk niet het geval is omdat de CPU dit niet aan kan. Vandaar moet je IMO low end GPU's met low end CPU's, Midrange met midrange CPU's en Highend moet Highend CPU's testen. Het zal een uitzondering zijn dat iemand met een Core i3 een GTX 1080 of iets dergelijks als Videokaart koopt.
GTX 1050;1;0.290677934885025;Ik ben het hier niet mee eens. Je wil toch de videokaart vergelijken? Met deze setup weet je zeker dat alle behaalde frames per seconde gehaald zijn door de GPU, en niet door een combinatie van processor/GPU. En alle prestatieverschillen zitten in de GPU, en niet ergens anders.
GTX 1050;4;0.27087169885635376;En koop jij een i7-6950x met deze videokaarten? Of koop je een betaalbare cpu met deze videokaarten? Realistisch blijven. Snap da het een goede vergelijking is tussen de videokaart zelf. Maar waarom geen 4 verschillende testsysteem, i3, i5, i7 en AMD om alle videokaarttesten op uit te voeren. Zodat het voor ons, tweakers, makkelijker vergelijkbaar blijft? Maar deze fps is misschien 50% minder met een i3 Al zal het nooit exact uw processor zijn, maar dan heb je betere kansen dat het aan uw verwachtingen voldoet. En daar draaien reviews toch om? Het beste met je comunity voorhebben
GTX 1050;2;0.4597751200199127;Nee, maar dat is ook niet de bedoeling van de test. Het gaat om uitsluiten van factoren die performance beinvloeden, maar die niet van de GPU zelf komen. M.a.w., je wilt er voor zorgen dat de GPU het traagste deel van het systeem is. Alleen dan kan je iets zinnigs zeggen over de prestatie van een GPU. Ik zal het proberen uit te leggen met een auto analogie: We willen twee auto's vergelijken welke meer performance voor zijn prijs levert. Als test laten we ze rijden op een gesloten circuit met dezelfde weersomstandigheden en dezelfde coureur. De rondetijden zeggen dan wat over de onderlinge prestaties van de wagen. We kunnen dezelfde auto's ook testen op een willekeurige woensdagmiddig in de binnenstad, en timen hoelang het duurt om van A naar B te komen. het resultaat zegt zeer weinig over de auto zelf, maar heel veel over de omgevingsfactoren (verkeer, weer, chauffeur). Dat laatste is waar jij naar op zoek bent. Prima, maar dat is niet waar de test voor gedraaid wordt. @Weijlander legt verder prima uit waarom.
GTX 1050;1;0.7617470622062683;Je koopt toch niet je hele PC met tien componenten op basis van de review van één component? Je kijkt ook nog naar een CPU-benchmark. En als je boottijden wil weten zoek je reviews van de SSD. Goh, een Ti1050 kan 60FPS halen op mijn resolutie, maar die i3 maar 30. Misschien toch maar even een gebalanceerdere setup bedenken. Als de gebenchmarkte componenten niet de bottleneck zijn, weet je dit dus niet - ja, in de review haalt 'ie 30FPS, maar waar ligt dat aan? Geen idee. Oh, we hadden nog wat traag geheugen in de kast liggen en dat hebben we maar gebruikt, daarom is hij zo traag. Heel realistisch, en ook heel zinloos bij een benchmark. Zo nu en dan komen er nog wel benchmarks voorbij waarbij de gebenchde componenten per ongeluk wél de bottleneck zijn. Conclusie: ze halen állemaal exact 73 FPS, dus geen idee welke beter is. Hoe helpt dát? Om dat te voorkomen neem je totaal overdreven hardware voor de rest van de PC.
GTX 1050;2;0.444499135017395;Nee. Als je een review zoekt van een videokaart, dan wil je weten hoe de videokaart presteert. Niet bij welke game een combinatie van hardware beter werkt. Als je wil weten welke CPU je nodig hebt voor een bepaalde game dan zoek je een benchmark op van CPU's. Je onderzoekt elke variabele afhankelijk van elkaar.
GTX 1050;3;0.42778822779655457;Wat denk je zelf. Laten we even iedere keer alle testen uitgebreid draaien met 6 verschillende hardware configuraties en dat met 10 verschillende grafische kaarten. Dat nog eens keer de 8 testen die gedraaid worden. Oftewel 480 verschillende test resultaten. Lijkt me niet heel handig. Op de huidige manier heb je voor iedere kaart gewoon de beste test resultaten die er uit de grafische kaart gehaald kunnen worden. Zo weet je misschien niet of jou configuratie het haald maar je weet wel wat ongeveer de prijs/prestatie is per kaart en wat er bij jou aardig aan zal sluiten. Bij alle verschillende configuraties is het ook maar de vraag of deze sowieso voor die van jou geldt. Dus ten opzichte van het werk zou dat nauwelijks van toegevoegde waarde zijn. Verder on topic ben ik erg blij met deze review. Gezien het feit dat de ti gaming editie nauwelijks een meerwaarde heeft tenopzichte van een gewone ti ga ik in dit geval daar voor. Of die RX470 moet binnen nu en heel kort drastisch omlaag gaan.
GTX 1050;1;0.37492474913597107;"Als ik hierboven in de tekst lees: "".........waardoor gamen met een simpele oem-pc ineens een optie wordt."" Ik vraag me dan af of je de resultaten uit deze test nog kunt gebruiken. Mocht de CPU de bottleneck worden dan is gamen met een simpele oem-pc nog steeds geen optie."
GTX 1050;3;0.36859723925590515;"Dat is waar, alleen betekent een ""simpele OEM pc"" niet per definitie altijd dat het ook een budgetpc is. Kan ook een betaalbare i5 desktop zijn van de Mediamarkt etc, waarin dan een vrij magere GT..30/40 of geen enkele losse videokaart zit."
GTX 1050;5;0.286332905292511;Helemaal gelijk, zo weet je als lezer nog niet wat die kaart voor jou kan betekenen. Het zou veel realistischer en inzichtelijker zijn om de kaarten met een i3 en i5 of een fx6300 en a10 te vergelijken. Meer on topic zat aan een rx 460 te denken als nieuwe htpc kaart, maar denk dat het een 1050Ti wordt.
GTX 1050;2;0.4195409119129181;En het probleem is dat bijna iedere review-site zich er aan schuldig maakt. Zelfde testsysteem voor alle videokaarten. Maar niet stilstaan dat low-end videokaarten ook wel eens in een low-end systeemconfiguatie zitten. De kans voor tweakers.net om dit te veranderen.
GTX 1050;3;0.32482805848121643;Ik snap dat altijd hetzelfde systeem wordt gebruikt, dat maakt het vergelijken van de kaarten makkelijk. Het is nou eenmaal een vergelijking, niemand heeft precies hetzelfde systeem, dus niemand kan aan deze getallen zien welke prestaties te verwachten zijn. Het is echter wel een leuk idee om de belangrijkste volgende schakel, de processor, ook te variëren. Als je één populaire CPU van vorig jaar neemt, of het bijbehorende instapmodel (net als deze kaart), dan krijg je een extra dimensie aan info en dat is wel praktisch voor de eindgebruiker: wij, de lezers. Dan krijg je voor de kaart in kwestie een extra balkje per grafiek of 1 pagina voor de vergelijking tussen testsysteem 1 en 2 met alleen de kaart in kwestie. I.p.v. alleen de CPU te veranderen, kun je er ook voor kiezen om het hele systeem te veranderen, behalve de videokaart. Dan neem je dus instap CPU, RAM, HDD, SSD, etc. en kijk je wat de kaart doet op zo'n systeem. Als lezer kun je met die twee ijkpunten je eigen te verwachten prestaties een beetje intrapoleren en bovendien zie je hoe dezelfde kaart het doet in een ander systeem. Je ziet meteen of bepaalde games of software meer eisen van de kaart, of toch afhankelijk zijn van de rest van het systeem.
GTX 1050;1;0.5537195205688477;Het is geen platform review maar een GPU review. Wat jij vraagt is een CPU review. Tweakers is beperkt in tijd. Om een GPU review, gecombineerd met een CPU review te doen kost gewoonweg belachelijk veel tijd. Ik snap niet dat na zoveel jaar GPU reviews hier nog over gestruikeld wordt. Als je bv. een Pentium zou gebruiken, zouden alle GPU's in sommige spelletjes gewoon dezelfde snelheid neerzetten omdat het spel dan op een CPU bottleneck stoot. Als je een probleem hebt met het feit dat een i7 6950x zoveel kost dan kan ik je zeggen dat een i5 6600 in spelletjes even snel is (als dat je iets of wat gerust stelt).
GTX 1050;2;0.3661031424999237;Ik begrijp wat je bedoeld. Zoals velen zeggen doen ze dit om bottlenecks te voorkomen zodat ze puur de prestaties van de videokaart kunnen meten. Het is wel heel slordig dat ze deze uitleg niet duidelijk erin vermelden ten tijde van de review en dat er er gewoon maar vanuit gaan dat iedereen dit wel begrijpt. Deze lakse houding zie je heel vaak bij tweakers terug komen en dit doet de review geen gunst. Zo zie je deze lakse houding ook al een aantal jaren terug in huj zogenaamde budget midrange etc builds van pcl en laptop guide. Hele lakse houding in de introductie en er maar vanuit gaan dat alles wel begrepen wordt. Ik zou zeggen kijk eens naar je Internationale concurrent zoals techdeals, Linus of hardwarecanucks etc etc en dan zie je precies waarom zei gezien worden als een autoriteit op dit gebied.
GTX 1050;2;0.299133837223053;Wij testen in deze review videokaarten. Geen games, geen CPU's en ook geen complete systemen. Om de prestaties van deze videokaarten te vergelijken wil je dus een systeem waar je met de rest van de componenten niet tegen bottlenecks aan loopt. We schrijven ook regelmatig benchmark stukken over games, daar testen we hoe goed die specifieke game op verschillende hardware draait. Hierbij testen we meestal ook of het loont om bij die specifieke game een andere CPU te gebruiken.
GTX 1050;2;0.37841176986694336;Gezond verstand en tweakers gaan al jaren niet meer samen. Je punt is correct deze test hardware slaat nergens op als je kaarten in deze klasse test. Tja, misschien heeft tweakers wel niets anders, in dat geval zijn ze misschien wel zielig. In ieder ander geval niet professioneel en of reel bezig.
GTX 1050;3;0.5062291026115417;Het gat tussen de 1050 en 1060 vind ik wel vrij groot
GTX 1050;5;0.29539793729782104;Daarom de 1050 Ti.
GTX 1050;1;0.44649264216423035;toch nog rond de 39% slechtere prestaties
GTX 1050;3;0.6605367064476013;Ja maar wel 47% goedkoper, dus in verhouding meer back for the buck. (deze MSI 1050 Ti van € 155,- vergeleken met een MSI/Gigabyte 1060 van € 290,-)
GTX 1050;3;0.5180352330207825;Ja maar stel je wil alleen nvidia en verwacht betere prestaties dan 1050ti maar wil wel de nieuwste generatie en de 1060 vind je te duur
GTX 1050;4;0.40473437309265137;Vind het erg jammer dat Tweakers WEER niet de 75w Geforce GTX 950 (GM206-251 GPU) noemt en mee test, dan zit je met een GPU met de zelfde aantal cores en wattage. Zo als de KFA2 GeForce GTX 950 2GB OC LP en Asus GTX950-2G en de Asus MINI-GTX950-2G, en ook EVGA heeft er meerdere. Erg leuke grafische kaart voor 1080P de Geforce GTX 1050 Ti, een stuk sneller dan de Geforce GTX 950, en ook veel sneller dan de Geforce GTX 960, in sommige spellen zelfs bijna 2x zo snel als de Geforce GTX 960, ben benieuwt wanneer eindelijk de Low profile versies uitkomen, duurde ook erg lang met de GeForce GTX 950.
GTX 1050;3;0.395865261554718;Heel gaaf zon review op het nieuwe superhighend systeem, maar zon kaart koop je niet als je zon super highend systeem hebt, had mij fijn geleken als tweakers ook op een budget systeem hadden getest, waar deze kaart more likely voor gekocht gaat worden. verder wel een goede review natuurlijk.
GTX 1050;2;0.3917156457901001;Dat kan, maar Tweakers test hier wat de prestatie is onafhankelijk van het systeem... immers zullen er amper/geen bottlenecks te vinden zijn omdat de rest van het systeem veel sneller is. Op een budgetsysteem zal je dus afhankelijk van de 10.000 configuratiemogelijkheden een andere prestatie halen die niet veel zeggen over de daadwerkelijke maximale prestatie van de grafische kaart. Testen met een budget/midrange onderdelen kan dus onbetrouwbare/niet erg nuttige resultaten geven die onderling niet per se iets bewijzen. Dan zeg je eigenlijk meer over het systeem als totaalplaatje dan over puur de grafische kaart alleen. En ik begon over dat het kan. Het is natuurlijk wel interessant om voor een ander systeem ook de prestatie te zien, maar dat kost waarschijnlijk wel enorm veel tijd om zowel op verschillende reso te benchmarken, op verschillende systemen en alle kaarten (niet alleen deze nieuwe) op evt. een nieuwe driver. Heb nog even gekeken. Een iets minder extreem i7 systeem en toch veel benchmarks vind je o.a. hier: Techpowerup: MSI GTX 1050 Ti Gaming X Enige wat ik wel graag zou willen zien is het verbruik van alleen de kaart zelf en niet van het hele systeem. Techpowerup doet dit bijvoorbeeld altijd al, dus het moet niet onmogelijk zijn om dit te meten. Aangezien men alles test m.b.t. de kaart, vind ik het verbruik ook interessant als dit enkel de kaart betreft. Het totaalverbruik is immers van zoveel afhankelijk, lopende processen, hardwaresamenstelling, overkloks, evt. 3th party chips die niksdoen en ook verbruiken etc. Dat zal bij dit X99 zoveel meer zijn dan bij een i3 natuurlijk.
GTX 1050;4;0.3904748857021332;Voor eenn kwart van de prijs ben je ook bottleneck vrij en test je realistisch
GTX 1050;1;0.42122960090637207;Tweakers vind dat niet nodig, en een uitleg waarom overbodig. Voor een fatsoenlijke review: www.anandtech.com
GTX 1050;1;0.7390304803848267;Wat een onzin, alsof een uitleg verplicht is. En er is hier al meermaals een reactie gegeven. Dat die of niet door jou zijn gezien of niet in jouw straatje vallen is wat anders. Ik zal je op weg helpen, zie bijv. de reactie van Ch3cker. En wat is er met die link naar Anandtech? Die komt niet op een review uit, dus wat is je punt?
GTX 1050;2;0.3337755799293518;Je hebt gelijk, voor een echte review moet je hier zijn: Zie voor mijn gelijk mijn eerste post. En je bent inderdaad niets verplicht, maar ik neem aan dat je een review niet voor niets schrijft.
GTX 1050;1;0.32178276777267456;*bang* for the buck
GTX 1050;3;0.2585737705230713;Oeps, soortement van typo, of eerder kortsluiting in mijn hoofd!
GTX 1050;2;0.4123643636703491;Als ik Digital Foundry mag geloven is de TI model zelfs 60% trager dan de 1060.
GTX 1050;3;0.25304436683654785;Ik denk dat hij dat bedoelt, de RX 470 zit daar precies tussen, nVidia doet er verstandig aan om dat gat daar te laten. Zo kunnen mensen die iets meer willen dan een 1050Ti en dus eigenlijk op een RX470 uitkomen, meteen gaan twijfelen over een 1060.
GTX 1050;5;0.26937344670295715;nvidia heeft de 1060 3gb direct tegenover de RX470 geplaatst. Deze heeft minder cuda cores als de normale 1060 en had dus ook eigenlijk 1050ti moeten heten.
GTX 1050;3;0.4018331468105316;Er is ook nog een 1060 met 3GB en iets minder cores (1152 voor de 3GB vs 1280 voor de 6GB), maar die hebben we nog niet getest helaas
GTX 1050;3;0.32726576924324036;Denk je dat je nog de mogelijkheid gaat krijgen om die te testen? Of is dat niet te voorspellen?
GTX 1050;4;0.2846148908138275;
GTX 1050;3;0.3410883843898773;Dat is inderdaad een goeie. Thanks. Vuistregel voor mij: Vind ik geen benchmarks van hetgeen wat ik wil? > op hardware.info kijken .
GTX 1050;3;0.5394679307937622;Kan misschien nog wel een keer tussendoor, maar bij de volgende review kijken we eerst naar wat snellers .
GTX 1050;3;0.22810344398021698;1080ti of Vega dus
GTX 1050;3;0.40954989194869995;Vraag mij af of dat tegenwoordig een aanrader wordt, het feit dat je met veel kaarten al ziet dat 2gb gewoon te weinig is zegt genoeg. Voor een kaart die zo snel is als de GTX1060 is het wenselijk dat je meer dan 3gb geheugen hebt, daarom jammer van de rare geheugenbus configuratie, 4/8gb was een betere keuze geweest ivm een 256bits bus. Je zal dat gebrek aan geheugen niet echt direct terugzien in average fps maar wel in minimum fps en microstuttering. De GTX970 heeft tenminste 3.5gb geheugen (met extra 512mb traag).
GTX 1050;2;0.4675213694572449;Begrijpelijk, voor simpele games zie ik een GTX 1060 als te veel kracht. Stel je speelt CS:GO, een willekeurige MOBA en af en een toe een RPG dan is een 105Ti al genoeg kracht om dat te draaien. In 1080p zou ik een GTX 1060 overwegen als ik spellen zoals The Witcher en Battlefield zou spelen op 60fps.
GTX 1050;3;0.37288275361061096;Naar mijn mening wordt de lat hier in de reacties wel énorm hoog gelegd - tot overdrijven aan toe. 'Als je een beetje fatsoenlijk op 1080p wilt gamen heb je toch wel echt een 1060 nodig' 'Bij simpele games is een 1050TI wel genoeg' Ik speel CS:GO, League, en af en toe een (iets oudere) RPG, op mijn Intel Iris 6100. Een waardeloze integrated kaart, en dat gaat helemaal prima. Dankzij de reacties lijkt het nu bijna net alsof bijvoorbeeld een 970 nu al enorm low-end is, waar je met geluk nog 60fps haalt op games van drie jaar geleden. Verder lijkt de 1050TI mij persoonlijk enorm interessant!
GTX 1050;3;0.6359132528305054;Ze hebben wel deels gelijk. Op fullhd met settings op high is inderdaad een 980 of 1060 wel al snel gewenst als je de 60fps wilt halen. Alles onder de 970 heeft het nu wel moeilijker. Veel serieuze cs go spelers willen die game ook met hige framerates spelen trouwens.
GTX 1050;4;0.39350083470344543;Ik game zelfs nog op een GTX275! De zogenaamde e-sports titels draaien hier nog perfect op, op de highest settings (CS:GO). Zit nu wel met een volledig geupgraded PC van een tijd geleden, omdat ik wat meer kracht wilde voor videobewerking (van een i5-650, 8GBDDR3 naar een i7-4790K, 16GBDDR3) en de videokaart deed het nog steeds prima op de spellen die ik speelde, dus had ik deze nog niet geupgrade. De meest nieuwe game die ik speel was the Witcher 2 en voor de rest had ik de PS3 nog. Maar goed, het was leuk geweest en ik ben nu in de markt om een nieuwe videokaart aan te schaffen. Vooral ook omdat mijn videokaart af en toe tijdens een game uitvalt en mijn hele systeem vast zit. Dus zit nu te twijfelen tussen een 1060-6GB asus strix en een 1070-8GB asus strix kaart. En uiteraard ga ik deze ook weer heel wat jaren gebruiken totdat deze mankementen gaat vertonen. Maar ja, geloof me, als een GTX275 896MB videokaart van een behoorlijk aantal generaties geleden de e-sports titels kan spelen, dan zal dat met een 1050 of 1050ti zeker heel soepel verlopen!
GTX 1050;2;0.4368232190608978;Ik vond mijn oude 670GTX niet eens meer voldoende voor de meeste games een anderhalf jaar terug. CS GO en wat esports zullen vast draaien. Maar Overwatch bijvoorbeeld wat nu een populaire esports titel is zal al performance problemen tonen.
GTX 1050;2;0.4025561809539795;Ik heb de 960/2GB, de oudere broer van de 1050 en 1050ti en ik kan je vertellen dat ik tot op heden bijna alle steamspellen kon spelen op 1080p/ultra. En de 960 is blijkbaar minder snel dan de 1050 en 1050ti: Metro2033, zelfs Soma, dota2, cs:go, civilatizion, etcetcetc. Enige spel wat op ultra niet lekker liep was the witcher 2, maar ach, dan meer low/medium gfx. Qua spel merk je er niets van.
GTX 1050;3;0.38334208726882935;ik verbaasde me ook een beetje over de 960 benchmark ... linus review zei iets dat de 960 net sneller was dan beide. Iemand nog andere voorbeelden ?
GTX 1050;2;0.39729344844818115;Als je 1080p/ultra niet prettig loopt, dan maakt 5 fps meer of minder niet veel uit. Als je nu een 960 hebt, dan is die ongeveer gelijk aan die 1050 of die 1050ti. Er is wel verschil maar kans is aannemelijk dat je het niet merkt. Als de game zuigt in 1080p/ultra met een 960, dan is de kans aannemelijk dat die ook zuigt met een 1050 of een 1050ti. Gewoon jaartje doorsparen voor een 1070ti, die zullen ze eventueel ook nog wel op de markt brengen.
GTX 1050;3;0.440245658159256;Idd daar zijn het te marginale verschillen voor. Wel tov 460 lijkt me de prestatie winst groot genoeg als je zou moeten kiezen om bv 25 euro meer uit te geven voor de 1050. Maar de 1070ti zal niet gauw in de 'confectie' systemen in de winkels terecht komen. NVIDIA lijkt wel weer de cashcow met de 1050(ti) in huis te hebben en hun marges zijn vaak al zoveel beter als bij AMD kaarten. Ik heb al een 960 dus dat maakt het iig makkelijk om gewoon nog een jaar of twee te wachten
GTX 1050;3;0.5288990139961243;Benchmarks op hardware.info tonen dat de gtx960 4GB net iets trager is dan de 1050ti. Ik vind het jammer dat de 960 4GB vaak buiten beschouwing wordt gelaten. Van zodra iets meer dan 2GB nodig is dan krijg je een gans ander beeld vd resultaten.
GTX 1050;2;0.49330535531044006;Maar het veranderd waarschijnlijk niets. Heel dat meutje lowbudget nvdiakaarten speelt hetzelfde prima op 1080p/medium en 1080p/ultra af. En de kans is zeer aannemelijk dat ze bij dezelfde highend games en settings gaan kwakkelen, 5 fps meer of minder maakt niet veel uit.
GTX 1050;3;0.2363279163837433;"Helemaal mee eens. Ik zou mijn 760 oc graag vervangen door een 1050 x4 deze is betaalbaar en draait de meeste games ""soepel"" op hoge instellingen dikke prima toch ? 400 euro + gaat mij gewoon wat te ver voor een videokaart"
GTX 1050;4;0.2981503903865814;"Benchmarks zijn inderdaad over het algemeen op de maximale settings, als je een paar settings aanpast die maar amper effect hebben op de visuele kwaliteit, kan je al gauw 10-20% extra prestaties krijgen, laat staan als je op high of medium speelt, wat er in veel moderne games nog steeds erg degelijk uitziet. Ik houd wel van eyecandy en speel met een RX 470 op 1080p, maar mijn laptop met een R9 m290x (prestatieniveau vergelijkbaar met een RX 460), kan al mijn games ook gewoon spelen op 1080p met 60fps, alleen dan op lagere settings. Mensen overdrijven zo ontzettend over wat voor gpu je ""nodig hebt""."
GTX 1050;1;0.32969003915786743;Als ik het zo lees moet ik snel mijn gtx960 verkopen. Die levert nog minstens 125 op, en als ik voor dat bedrag een 1050 kan krijgen is dat een gratis upgrade.
GTX 1050;3;0.4338548481464386;"Dat ligt eraan; tenzij ik iets over het hoofd zie, zijn er geen GTX960's meegenomen in de test met 4GB memory. Wat dan wel weer opvallend is aangezien het verschil tussen 2GB en 4GB juist bij deze serie kaarten al een aanzienlijk verschil opleverde (en zeker op hogere resoluties)."
GTX 1050;3;0.6174715161323547;Naja, die 4gb versie van de gtx 960 presteert niet veel beter in de praktijk dan de 2gb vanwege de krappe geheugenbus. Dus denk dat t verschil niet veel anders zal.zijn
GTX 1050;3;0.6084201335906982;Het ligt een beetje aan het type spel. Omdat ik openworld games wel leuk vind is volgens 4 gb daar echt wel een voordeel... zelfs met een 128 bit data bus. Simpel omdat er minder tussen het 'langzame' geheugen op het moederbord en het geheugen op de gpu kaart gecommuniceerd hoeft te worden. Dan heb je het trouwens wel over behoorlijke grafische settings die dat weer kunnen verstieren. Het krappe geheugenbus gaat over de communicatie tussen de gpu chip en het geheugen op de videokaart. Die GPU chip moet net zo goed wachten als de data nog van het moederbord gehaald worden.
GTX 1050;2;0.4764769375324249;Het gaat er om dat 2GB te weinig is en je dan swapping krijgt met systeem ram. De bus vd 1050ti en 960 zijn beide 128 bits dus het verschil in snelheid is klein. Op hw.info zijn trouwens benchmarks vd 4GB 960 en die blijkt iets trager te zijn dan de 1050ti.
GTX 1050;3;0.5088791251182556;Dan moet je inderdaad wel snel zijn, want omdat het nu voor 125 op de V&A staat betekend t niet dat je dat er voor gaat krijgen als deze kaarten volop verkrijgbaar zijn
GTX 1050;1;0.46781986951828003;Of dat iemand nu op V&A 125 betaalt voor die kaarten Ik hou het een beetje in de gaten, en de advertenties voor 100 zijn zo weg, die van 125 staan een tijdje, en wie weet zakt de verkoper dan wel voordat ie weg gaat.
GTX 1050;4;0.38963666558265686;haha idd een goed iedee
GTX 1050;3;0.4546411335468292;Mja, het verschil op 1080p/ultra in farcry4 was niet groot. Ik heb zelf ook een 960/2GB en mijn mening is dat de 1050/1050ti mij niet veel upgrade potentieel biedt . Ik spaar liever door voor een 1070 of een 1070ti, mits die er komt.
GTX 1050;3;0.3208150565624237;Als je echt serieus wil gamen @ 1080p moet je volgens mij toch naar een RX470 of GTX1060 kopen, die kaarten zijn wat duurder maar dan zit je safe voor de komende jaren. Dit is mijn inziens een geval van de gierigheid bedriegt de wijsheid. Zeker geen slechte kaart voor de casual gamer die lichte games draait maar toch net iets te traag voor het serieuze werk.
GTX 1050;2;0.35933825373649597;'Serieus' gamen vind ik een rare aanname. Is het enkel serieus met zware anti aliasing en de grootst mogelijke textures? De Tweakers benchmarks geven imo een vertekend beeld omdat de games op zo'n hoge settings worden getest. Op HWinfo moet je maar eens naar de normal-high benchmarks kijken, de kaart haalt daar makkelijk 60-100 fps in full hd en kan er zelfs flink wat in wqhd draaien aan 60 fps.
GTX 1050;3;0.5464982390403748;Met serieus gamen doel ik op games als BF1, Project Cars, COD enz.., dus games die je online speelt en waar je minimum 60fps moet voor halen. Deze kaart zal dat zeker doen momenteel mits wat aanpassingen aan de grafische instellingen, maar wat gebeurt er volgend jaar als de eerste ports van de PS4 Pro en de Xbox One Scorpio verschijnen? Ik vind dat deze kaart net iets te traag is om ze te kopen als dat net de games zijn die je wilt spelen, voor een beetje meer heb je iets dat meer toekomst biedt.
GTX 1050;5;0.25340232253074646;Volgend jaar koop je voor een soortgelijk bedrag een kaart welke de huidige 1060 weer wegblaast. De oude verkoop je dan voor 50-70 euro minder. Zo kan je elk jaar goedkoop upgraden. Nu iets kopen wat je niet nodig hebt is niet altijd efficient. Met deze kaart kan ik een Skylake game systeem samenstellen die prijs/performance subliem in elkaar zit voor rond de 480 euro. (i3-6100, 1050Ti, 8GB).
GTX 1050;3;0.4008098840713501;Aan de andere kant @1080 en ultra/high is ook niet voor iedereen nodig. Normal/high @1080p is best ook wel speelbaar. Zoals Tweakers goed aanhaalt, zijn deze tests gemaakt voor de allernieuwste, meest krachtige GPU's te doen zweten. Dan zit je bij de 1050 niet op de juiste plaats. #imho
GTX 1050;3;0.3688153028488159;Deze kaart zal ook niet bedoeld zijn voor de hardcore gamer. Met deze kaarten kan je in principe een game computer in elkaar zetten die qua prijs in de buurt komt van een console (als je randapparatuur zoals scherm etc niet meetelt). Gezien de enorme populariteit van games zoals CSGO, Dota, LoL en recentelijk overwatch maakt dat een mooie instap, zoals in de conclusie al genoemd wordt is hier nog best een 'prestatiegat'. De nieuwste en mooiste games kan je dan wel niet voluit draaien, maar ze zijn zeker te spelen als je de graphics een stukje terugdraait. *edit : Ik ben het wel met je eens dat een 1060 een safe bet is voor de lange termijn, de verschillen in games gebaseerd op DX12 en Vulcan zijn aanzienlijk, en daar gaan er alleen maar meer van komen.
GTX 1050;2;0.35005471110343933;Zou het niet slimmer zijn om te zeggen: Koop dan gewoon een 2dehands HD7970 of zoiets (100eur of minder)... Als je dan toch voor budget kijkt. Nee denk je niet?
GTX 1050;3;0.30315807461738586;Weet je hoe oud die kaart al is?
GTX 1050;2;0.34883806109428406;Want de leeftijd van een videokaart bepaalt of een game speelbaar is? Handig, kan je elke keer dat er een nieuwe graka uitkomt je graka weggooien, is immers oud. Waarom nog benchmarks draaien dan? Als iemand wil gamen op fhd met 60fps en dat lukt nog met de huidige kaart, waarom dan upgraden? Hangt allemaal maar net af van je eisen en van de games die je speelt lijkt mij zo
GTX 1050;1;0.36534684896469116;Ja, de 6XXX lijn krijgt namelijk geen driver-updates meer van AMD en BF1 vereist een minimale driverversie boven die is gereleased voor de 6XXX
GTX 1050;3;0.4060462713241577;Leuk.. Maar sinds wanneer is de v HD7970 een kaart in de 6xxx lijn? Dat is nml weer een generatie ouder. Overigens is de aangehaalde kaart door AMD ook nog eens rebranded uitgebracht, als de 280x. In principe een kaart die als volgende generatie weggezet is door AMD. Van wat ik begreep wordt alles GCN gewoon nog supported door AMD qua driverupdates gezien het feit dat deze ook DX12/Mantle API doen. De 6xxx serie kaarten die je aanhaalt zijn inderdaad geen GCN. Ik kan mij voorstellen dat er een moment komt dat inderdaad de grens verlegd wordt naar bijv. GCN 1.2 voor specifieke nieuwere games, maar goed, dat verandert niets aan de discussie dat het 100% afhangt van de specifieke game en de eisen van de gebruiker. Vrijwaard je natuurlijk niet om inderdaad rekening te houden met minimale systemeneisen voor bepaalde games - maar dat is m.i. niet helemaal alleen voorbehouden aan graka's en is zeker handig om te gaan doen als je voor een budget build gaat lijkt mij.
GTX 1050;3;0.4867711365222931;Voor prestaties en toekomstige DX12 moet je deze GTX 1050Ti nu niet echt verkiezen boven een tweedehands GTX 970 die dra in dezelfde prijsklasse valt. Goed voor systeembouwers die garantie willen verlenen op een budget bakje. ivm oude kaarten: Mijn HD7970 Ghz edition voldoet prima. Want mijn 8 jaar oud high color gammut Dell scherm met resolutie van 1920x1200 wil maar niet stuk. Balen maar ja, dat gebeurt soms als je degelijk materiaal aankoopt. Iemand bood me een GTX 970 aan voor 150 Euro vorige week maar ik heb bedankt want als ik upgrade wil ik iets voelbaar echt veel sneller (de opvolger van de GTX1080 of de 1080 aan 150 Euro)
GTX 1050;2;0.3913896083831787;Als ik een nieuwe pc in elkaar zet doe ik dat persoonlijk liever met nieuwere componenten. Denk aan garantie, lagere TDP, nieuwere architectuur, om maar iets te noemen. Het is een keuze, 2e hands is natuurlijk ook prima. En zoals in andere comments ook al aangehaald wordt, de mensen uit deze doelgroep zullen zulke computertjes niet vaak zelf in elkaar gaan zetten maar dit laten doen, of compleet prefab bestellen. Er zullen op deze manier prima game-pctjes komen voor een prikkie. Gat in de markt wat mij betreft.
GTX 1050;2;0.5862031579017639;Ben ik het niet mee eens. De benchmarks zijn vrijwel allemaal hele hoge settings genomen én bij flink demanding games. Op maximale settings spelen is voor veel gamers helemaal geen must. Deze kaart gaat vrijwel alle games op een medium-high setting op 1080p/60fps kunnen draaien. De 1050 ti is naar mijn idee de ideale budgetkaart voor 1080p. Een 1060 kost je al bijna het dubbele (de 3GB VRAM versie bottlenekt teveel tegenover 4GB) en is voor veel gamers een overkill. Tevens zijn ze ideaal voor portable ITX kasten en ook 75 watt tegenover 120 watt is echt wel wat waard. Uiteraard geldt dit enkel wanneer men zich aan de adviesprijzen houdt. Ik zie nu een 1050ti voor 225 euro in de Pricewatch staan, dat gaat natuurlijk nergens over.
GTX 1050;1;0.4928921163082123;Dat zijn de enige 1050's die te krijgen zijn momenteel dus waarschijnlijk via parallelimport binnengekomen en nu woekerprijzen vragen. Goeie reden om nooit meer iets bij dat bedrijf te kopen.
GTX 1050;2;0.3396543264389038;De 1060 3Gb heeft toch een 192 bits databus tov de 1050(Ti) een 128 bits databus? Hoezo zou er bij de eerstgenoemde dan eerder een bottleneck zijn ?
GTX 1050;2;0.3885256052017212;Het gaat niet om de snelheid, maar hoeveelheid. Een game als GTA V gebruikt al snel meer dan 3GB VRAM. Dat heeft veel te maken met de huidige consoles, die 8GB GDDR5 aan boord hebben. Omdat deze games leunen op de VRAM van de PS4 en Xbox One, doen (helaas) veel PC ports dat ook. Daarom is het aan te raden om op dit moment toch minimaal voor 4GB te gaan.
GTX 1050;2;0.48408693075180054;Ah je gebruikt de term bottleneck dus niet erg handig in dit geval. Jij hebt het over de inhoud van de gehele fles en dat bij de 1060 die fles eerder overstroomt. Snap je het verschil ? Open world spellen lijken me zeker op hoge resoluties idd zeker de eersten die 3 Gb een beperking gaan vinden. (Het was de reden dat ik toch voor de 4Gb versie van de 960 ben gegaan... want 2 Gb is nog sneller vol) Maar voldoet de 1060 3Gb sowieso dan wel voor die hogere resoluties ? Ik zou dan eerder de 8Gb 480 of zelfs een 1070 aanraden?
GTX 1050;2;0.5553984045982361;"""Daar komt bij dat de 1050 (Ti) in principe geen extra voeding nodig heeft, waardoor gamen met een simpele oem-pc ineens een optie wordt"" Misschien heb je geen connector nodig, zo'n kaart trekt toch nog tot 75w uit het PCI-E slot. Een gemiddelde OEM PC is echt niet in staat om nog 50w extra aan te bieden, de marges van die systemen zijn al veel te krap. Die conclusie is imho een beetje snel getrokken"
GTX 1050;5;0.2885604202747345;Ik zou zeggen: koop er dan meteen een nieuwe voeding bij. Heb je ook weer 5 jaar garantie op.
GTX 1050;1;0.7090386748313904;Oh, dat moet je sowieso doen Mijn punt is dat dit 'selling point' totaal irrelevant hebt. Je moet toch een andere voeding kopen wil je 'm in een OEM-systeem schroeven. En als je dat doet, kan je net zo goed voor de RX470 gaan, mocht je dat als tegenhanger beschouwen.
GTX 1050;3;0.639396071434021;Okee, maar vaak zit er al een oude videokaart in die je kan vervangen en dat scheelt dan weer. Ik heb zelf overigens in de afgelopen jaren een aantal keer een videokaart in een OEM-PC erbij geprikt en dat ging steeds zonder problemen. Dat waren wel simpele kaarten, de GT 730 en de GT 740 als ik me goed herinner.
GTX 1050;3;0.6120339035987854;Wel oppassen wat voor stekkers het moederbord gebruikt, niet alle fabrikanten gebruiken 24pins en niet alle voedingen hebben 20pins aansluitingen.
GTX 1050;5;0.25518596172332764;75W is 75W uit een PCIe slot. Geen enkele OEM gebruikt voedingen die maar 50W kunnen leveren aan de PCIe bus, dan kunnen ze aan de lopende band moederborden gaan vervangen.
GTX 1050;2;0.4453258216381073;Je hebt zelf kunnen zien dat de techbench met een overgeklokte 10-core (!) processor en de 1050ti nog geen 150W verbruiken tijdens een stresstest. Toegegeven, alleen de videokaart wordt gestrest en de de cpu blijft idle, maar stel dat je een wat oudere quadcore of gewoon een i3 hebt, kom je met een gaminglast echt niet boven de 200W verbruik aan hoor. De meeste oem-voedingen kunnen dat zeker wel leveren. Bovendien wordt tijdens de energietest ook nog eens het vermogen van het gehele systeem gemeten. De voeding heeft een efficiëntie van ongeveer 90%, dus je mag ook nog eens 10% van het gemeten verbruik afhalen, aangezien dat niet het vermogen is dat van de voeding wordt gevraagd. Conclusie: De voeding van je oem-bakkie moet wel héél brak zijn om een 1050 Ti niet aan te kunnen. Trek je kast open en kijk naar het stickertje op de zijkant van de voeding. Staat daar een getal hoger dan 200? Dan kan hij de kaart in principe gewoon aan.
GTX 1050;2;0.3965694308280945;Als het getal op de 12v-rail dermate hoog is, geef ik je gelijk.
GTX 1050;2;0.3430347442626953;Voor een complete systeembouwer voor bv de Mediamarkt-klant scheelt het toch een kabeltje trekken en een voeding die die extra 12V rail niet hoeft te hebben. (En die voedingen zonder zo'n rail hebben ze toch al groot ingekocht) Bij elkaar maar een paar euro waarschijnlijk, maar kijk maar eens bij oa eerder genoemde winkel en je ziet daar vooral systeempjes met maximaal een Geforce 950 staan. Een beetje tweakers koopt die dingen daar toch niet dus is het simpelweg een andere markt van voornamelijk onwetenden... ik bedoel een Z170 moederbord oid zullen de complete systemen verkopers er ook zelden instoppen. Maar het is een wel heel erg lukratieve markt gezien het succes van de 750ti
GTX 1050;3;0.37345412373542786;Nu nog in een low profile versie, liefst met single slot IO connectoren. Dat de koeler op zich wat breder is, is niet zo erg, maar ik heb maar 1 IO slot. Het is misschien wel een niche markt, maar sinds de 750Ti is er geen alternatief meer, en er is toch de nodige vraag voor.
GTX 1050;5;0.386796772480011;Ben wel benieuwd hoe de 750ti presteert tov de 1050 serie
GTX 1050;3;0.31152060627937317;Ik zag deze vandaag : Edit : Deze is toch informatief? Precies wat spokje zich afvraagt ?
GTX 1050;1;0.6146088242530823;Bedankt!
GTX 1050;1;0.7165171504020691;Maar ik vraag me in dit geval dus echt af waarom ik hiet met '0' beoordeeld wordt. Verkeerde bron ofzo?
GTX 1050;1;0.36316484212875366;Geen idee. Ik zag later wel dat in de eerste 3DMark test, de 750ti er gewoon bij staat. Maar dat is alleen in de eerst twee testen. Maar ik was erg blij met je filmpje, alleen mag ik je niet modereren. Dit is de scorekaart: +1 On-topic 1 0 Off-topic / Irrelevant 2 Slechts 2 mensen zijn met het verkeerde been uit bed gestapt gister
GTX 1050;1;0.5569707155227661;Idd ik weet dat jij het niet was Dat zal het zijn. Dan laat ik me niet uit het veld slaan door deze '0'
GTX 1050;3;0.33370861411094666;'0' is ook prima. '-1' zou ik pas boos worden
GTX 1050;1;0.5391559600830078;Ik was niet boos, maar teleurgesteld Neh ik wou gewoon weten waarom... jeeh nu +1
GTX 1050;5;0.7291463017463684;haha awesome!
GTX 1050;5;0.4008987247943878;ik zit inderdaad ook te wachten op een low profile.. ik heb een standaard hp pro 3120 sff die ik helemaal wil gaan upgraden en het lijkt me geweldig om hier een 1050 in te zien zitten
GTX 1050;1;0.311256468296051;Het zou mooi zijn als de 1050 in een low profile editie uitkomt. Dan heb ik voor weinig geld een kaart die hardwarematig H265 kan decoderen en mijn HTPC'tje ook een simpele game kan laten spelen, zonder dat ik meteen de hele bende (mobo, proc, installatie) moet upgraden. Iemand die weet of dat er aan zit te komen?
GTX 1050;2;0.3658922016620636;TPD laat het toe, dus zou eerlijk gezegd niet weten waarom niet.
GTX 1050;3;0.429889976978302;Kan je er niet gewoon een zalman koelertje opzetten (of een ander merk/type)? Die is wel 2 sloten dik, maar heeft geen 2 sloten aan de achterzijde nodig. Het zijn nog mooie koelertjes ook.
GTX 1050;3;0.39643195271492004;Zit nu toch wel te twijfelen om mijn RX460 te gaan upgraden... In een game zoals ets 2 kan ik net niet op high/60fps gamen. En mijn sapphire houdt er ook wel van om geluid te maken, en warm te worden.
GTX 1050;3;0.4640938639640808;ik heb de 1070 en ik heb alles max (en scaling op 400% op 2560x1440p) en haal 60-80 fps. ligt er aan wat voor settings je gebruikt. e rx 460 zou op 60fps op een mindere kwaliteit moeten draaien
GTX 1050;2;0.3249523341655731;Wacht iig nog een maandje! .... ben je nog ruim voor de '13e maand prijsstijgingen' en dan zijn de prijzen van de nieuwe kaarten vast ook wat gezakt. Zelfs misschien voor de al wat langere uit zijnde kaarten omdat deze natuurlijk de nieuwe concurentie gaan voelen. ^_^
GTX 1050;2;0.2989509105682373;is een SLI mogelijk bij de 1050 TI?
GTX 1050;2;0.3854422867298126;"Nope, Nvidia vindt SLI iets voor highend kaarten; de GTX 1060 deed al geen sli, dus de 1050 helaas ook niet."
GTX 1050;1;0.28393009305000305;1070 en hoger en max twee kaarten. Daar houdt het op.
GTX 1050;5;0.6448607444763184;Nu heb ik geen verstand van videokaarten maar gebruik op dit moment een Sapphire Radeon HD 7950 3GB GDDR5 With Boost Wil iemand zo vriendelijk zijn om misschien te vertellen of deze kaarten interessant zijn qua prestatiewinst omdat de prijzen dat zo te zien zeker zijn. Met het gevaar dat ik natuurlijk hartelijk word uitgelachen. Dank!
GTX 1050;3;0.6231152415275574;Qua prestatiewinst voor jou geen interessante kaart, de 7950 zit rond het niveau van de R9 285, alleen heb jij 1GB vram meer en een bredere geheugenbus. Het zou dus geen upgrade maar een sidegrade zijn Deze nieuwe kaarten zijn echter wel een stukje efficiënter.
GTX 1050;5;0.5276634097099304;Dank je!
GTX 1050;3;0.4319605827331543;Destijds een goede aankoop gedaan. Die kaart gaat al wel wat jaartjes mee.
GTX 1050;5;0.3874744772911072;Ik vertrouw dan ook op de Best Buy artikelen van Tweakers en luister naar adviezen van Tweakers
GTX 1050;3;0.4137747585773468;Een 7970 staat gelijk aan een 280X, die beter is dan de 285 die is ge-rebrand is als de 380. De 380 is net wat beter dan de 960. Zo kun het bekijken omdat geoverklokte 7950s bijna even goed zijn als 7970s. Dus je kunt jouw kaart vergelijken met de 960 in deze review, en dan kun je er ook nog een paar FPS bij op tellen (3-6). EDIT: Vergeet deze uitleg, ik zie net dat er een R9 285 wordt gebruikt in de benchmarks die inderdaad rond de 7950 zit .
GTX 1050;5;0.6205578446388245;Dank!
GTX 1050;3;0.676997721195221;"Mooie review, maar ik mis eigenlijk de GTX960 4G in het hele verhaal; je geeft aan dat de 1050 ti een stuk sneller is dan de 960, maar daar bedoel je volgens mij alleen de 2G variant van de 960. Ik ben wel benieuwd hoe de 960 4G zich houdt ten opzichte van de 1050 TI."
GTX 1050;3;0.4528033137321472;Het hangt natuurlijk een beetje van de kloksnelheid van de 1050 Ti en de 960 af. Maar om onze benchmarkresultaten te gebruiken: daarin is de GTX 960 is ongeveer tien tot twintig procent langzamer is dan de 1050Ti in de meeste benchmarks, behalve in die benchmarks waarin de 2GB de bottleneck is, zoals in Tomb Raider, Hitman, Total War en Doom. Zou het geheugen geen bottleneck vormen (wat bij de 4GB-versie het geval zou zijn) dan verwacht ik dat het verschil nog steeds tien tot twintig procent is (maar geen 120% oid zoals bij de 2GB-kaarten).
GTX 1050;2;0.46945086121559143;Dat zou inderdaad wel logisch zijn, maar ik had hem er toch wel tussen verwacht. In ieder geval lijkt het erop dat het nu geen goed idee meer is om een 2GB kaart te kopen.
GTX 1050;3;0.5176487565040588;Lijkt me wel een leuke kaartje voor m'n HTPC. Mits deze in LP ook nog een keer uitkomt.
GTX 1050;1;0.41061148047447205;Ik hoop er ook op. MSI maakt zo maar even 10 (!) varianten, maar niet één LP of single slot. Hoewel het er de uitgewezen kaart voor is. Gemiste kans opnieuw.
GTX 1050;3;0.4365779161453247;Wat vreemd dat de AMD kaarten niet zoveel last lijken te hebben aan het gebrek aan geheugen(de 2G edities) bij de games. Deze doen vrolijk mee in het 4GB rijtje.
GTX 1050;2;0.4644780457019806;AMD heeft over het algemeen beter geheugen management als het gaat om geheugen gebruik, bij Nvidia zie je de trend dat ze meer kunnen met de beschikbare geheugenbandbreedte. Er is overigens in andere reviews ook een patroon zichtbaar met 2gb kaarten in combinatie met de nieuwe api's (DX12, Vulkan), ze presteren daar vaak slechter. Dit komt omdat er blijkbaar niet meer echte geoptimaliseerd wordt voor 2gb kaarten door game ontwikkelaars bij de nieuwere api's. Dit kan je goed zien bij kaarten zoals de R9 285/R9 380 2gb vs de R9 380(x) 4gb en de GTX960 2gb vs GTX960 4gb. Het verschil in prestaties is soms enorm.
GTX 1050;3;0.46471136808395386;Jammer dat de normale TI versie niet de semipassieve koeler heeft van de gaming-X. Een van de redenen om een kaart te kopen zonder 6 pins is juist de geluidsproductie. (Voor mij ten minste)
GTX 1050;5;0.5505518913269043;Yep, volledig mee eens. Ik zou graag een niet overgeklokte 1050 Ti versie willen kopen zonder 6 pins, maar wel met 4GB en die mooie semi-passieve koeler.
GTX 1050;1;0.44234898686408997;Ik zie nu net de eerste prijsvermelding: pricewatch: Asus EX-GTX1050TI-4G € 225,- haha, zelfs duurder dan een aantal goedkoopste GTX 1060 3GB modellen.
GTX 1050;1;0.40286198258399963;Hehehe, pfff, die gekke Informatique ook. Al is ie daar nog € 6,- goedkoper dan hun goedkoopste GTX 1060 3GB.
GTX 1050;1;0.45540860295295715;€125 en €155 Als dat de nederlandse MSRP is, zal het in de praktijk nog wel duurder worden ook. Zit je zo weer op de €150 en €180 die de 950 en 960 nu kosten, met dus een minimale verbetering in bang/buck...
GTX 1050;3;0.5993636250495911;Je kan dan ook beter voor een RX 470 gaan . Als je zat te kijken naar een RX 460 dan is de GTX 1050 wel waarschijnlijk de betere keuze.
GTX 1050;1;0.35130971670150757;Als mijn pcie niet 75 watt aankan omdat dit geen gen3 is. Kan ik deze kaart dan alsnog aansluiten doormiddel van de voedingsstekker vanaf mijn voeding?
GTX 1050;5;0.443629652261734;Let op dat PCI-e 1.0 en 2.0 (x16) ook tot 75 watt kunnen leveren.
GTX 1050;1;0.4806939959526062;Lijkt me (aan de huidige prijs) een totaal overbodige kaart. Ze zal snel moeten dalen in prijs om relevant te worden. Zo vind ik het volgende in de pricewatch : GTX 1060 vanaf 268 euro. GTX 1050 TI vanaf 229 euro. Prijsverschil van 39 euro. Zelfs wanneer je een krap budget hebt kan je bijna niet verantwoorden om dan voor de 1050TI te gaan.
GTX 1050;3;0.34277957677841187;De 1050 Ti vind je, zeker in Duitsland, al vanaf 150 € bv: (wel 20€ verzendkosten bv. bij die shop dus 170€) Prijzen in Nederland en Belgie zullen ook nog wat zakken (je waarschijnlijk wel nog een beetje geduld moeten hebben).
GTX 1050;1;0.4765012264251709;1050ti 225,- RX470 242,- bron alternate. dus voor 17 euro meer 30% meer preformance.. voorlopig nog geen 1050ti
GTX 1050;1;0.37569722533226013;Tja, dat heb je met alle prijzen. De eerste shops die ze aanbieden vragen onredelijk hoge prijzen om te profiteren van mensen die niet kunnen wachten tot de prijzen dalen tot normale bedragen. De adviesprijs van de 1050 Ti zou omgerekend 155 euro bedragen. Wellicht zal het nog weken duren vooraleer de prijzen in de buurt komen.
GTX 1050;2;0.30166125297546387;Vraag me af of een 1050Ti betere FPS op full HD heeft in een oudere bak met een A10-5800K APU...
GTX 1050;3;0.4714807868003845;Denk het wel.
GTX 1050;5;0.48571303486824036;Zier er goed uit, thanks!
GTX 1050;5;0.5055179595947266;Een van de 1050 kaarten is zeker interessant, ik wil er niet teveel aan uitgeven en ik denk dat deze veel beter presteerd dan mijn gt610
GTX 1050;4;0.47442391514778137;Leuke kaart voor weinig, niks mis mee in deze tijden
GTX 1050;3;0.3211262822151184;Zou Overwatch 1920X1080 alles op high te spelen zijn?
GTX 1050;3;0.3512133061885834;ja, want de GTX 750 ti kan dat al
GTX 1050;1;0.32485437393188477;Vraag me af hoe ze de laptop versie gaan afstellen
GTX 1050;3;0.34857824444770813;Jammer geen BF1? was ik eigenlijk wel erg benieuwd naar
GTX 1050;5;0.2563166618347168;LTT heeft een video
GTX 1050;3;0.28767240047454834;Ik heb 2 jaar en 2 maanden geleden een AMD HD7950 3gb met arctic accelero twin turbo 2 gekocht voor 95€. Dat was nadat alle bitcoinminers hun kaarten dumpten. Na zoveel tijd beginnen er eindelijk kaarten te komen die deze deal kunnen evenaren. In high end is er inmiddels wel een behoorlijk prestatiegat geslagen maar voor mijn gevoel gaat die ontwikkeling in low/mid end heel langzaam. Kaarten zoals uit deze review halen het nu bijna in
GTX 1050;5;0.38465726375579834;Nu kunnen ook de laatste budget console lovers overstappen op de PC :-)
GTX 1050;3;0.4752938747406006;Het is wel verwarrend, in de inleiding geven jullie aan ook een 1060 3gb mee te testen, maar in het overzicht is deze in een keer veranderd in een 6gb uitvoering. Wat toch wel degelijk een andere kaart is kwa snelheid en prijs. Welke hebben jullie nu getest ?
GTX 1050;1;0.5180166959762573;Hoe houd deze kaart zich in een gemiddeld systeem? Ik snap dat je benchmarks wil maken in een zo snel mogelijk systeem maar welke gek heeft er nou zo'n duur systeem en propt daar deze kaart in? Misschien zou tweakers eens een onderzoek moeten doen wat het gemiddelde systeem is wat mensen thuis hebben staan bijvoorbeeld een i5 2500 met 8GB geheugen (ik noem maar wat) en de prestaties dan testen.
GTX 1050;3;0.427523672580719;Een i5 en i7 maakt niet zo veel verschil in benchmarks om alles te vervangen
GTX 1050;1;0.5030862092971802;Als ik GTA V draai met 4x msaa, dan is mijn gemiddelde framerate nog wel te doen. De dikke framedrops en het gestotter echter niet. Doorgaans werd er een minimale, gemiddelde en maximale framerate aangegeven. Waarom zijn jullie hier mee gestopt? Dat zegt echt zoveel meer over de speelbaarheid, dan een gemiddelde framerate. Want de 1050 (non-ti) is met deze grafische instellingen dus niet bruikbaar als ik de benchmarks zie. En bij de 1050 ti vraag ik me ook sterk af, of het in de categorie 'speelbaar' geplaatst kan worden. Zelfs op normal of medium, tikken ze net de 60fps aan. Als dat regelmatig naar 40 of lager dropt, dan is het niet speelbaar imo. Deze low-end categorie is echt zwaar overbodig. Kunnen ze niet beter oudere modellen in deze prijscategorie laten vallen. Een stuk logischer dan kaartjes uitbrengen, waar je niet eens fatsoenlijk op 1080p kan gamen. Ja games als CS/LoL/Dota2 zullen het prima doen, maar die kun je ook op een goedkope laptop van amper €400 prima spelen.
GTX 1050;2;0.40669766068458557;"""Doorgaans werd er een minimale, gemiddelde en maximale framerate aangegeven. Waarom zijn jullie hier mee gestopt? Dat zegt echt zoveel meer over de speelbaarheid, dan een gemiddelde framerate."" Een meer dan terechte klacht! Juist de dips bepalen hoe speelbaar het is, het gaat dan natuurlijk niet alleen om de diepte van die dips maar ook om hoelang het duurt. Misschien is dat een reden waarom ze het niet meer weergeven, soms heb je een hele lage dip die je niet merkt in de gameplay, het duurt zo kort (tijdens het laden of zo) dat FRAPS het niet eens weergeeft. Toch wel doen, eventueel met de kanttekening hoe je het wel of niet merkt aan de gameplay en dit goed onderbouwen."
GTX 1050;2;0.3865733742713928;Ik zou toch echt een nieuwe 1050(ti) boven een tweedehands 960 kiezen (maar ik heb al een mooie 960 en ben nog wel een tijdje tevreden denk ik) Wees voorzichtig met tweedehands te kopen... Wat heb ik al veel lui idiote overclocks zien doen... Altijd een risico dat bij een tweedehands kaart het geval is en dat kan veel later tot problemen leiden.
GTX 1050;2;0.3902432918548584;Bij de buren is de 4gb versie van de 960 in 3dmark wel sneller dan de 1050 ti : Aangezien Linustech ook al over dat de 960 beter presteert als de 1050 ti... Gebruikt Tweakers een 'langzaam' model ? Herstel : Alleen in 3d mark Firestrike oeps en een paar spellen op 'lage' resolutie.
GTX 1050;1;0.397438108921051;"Wat een onzin word hier weer gebracht door tweakers: 'Ook wat prijs betreft zijn Nvidia's kaarten interessant; de GTX 1050 is met 125 euro even duur als de RX 460, maar wel sneller' Als je dan toch eerlijk advies wilt geven moet je er wel bij zeggen dat nvidia vrijwel in elke dx11 game hetzelfde of beter presteert maar dat het bij dx12 titels weer andersom is. (1050 vs 460) Geen interessante kaart als je graag de nieuwe titels wilt spelen 'dx12' je ziet overal vrijwel hetzelfde beeld amd presteert beter in de dx12 titels nvidia op dx11 ook @ guru3d hebben ze dat waar genomen. In elke dx12 titel waarbij de 460 is opgenomen in de grafiek presteert deze gelijk aan de 1050 of beter. Wat mij betreft een goede zet geweest van amd om zich meer op dx12 te richten"
GTX 1050;3;0.5068079233169556;Waarom staat de GTX 1060 3GB niet tussen de benchmarks ? De vergelijking met de 6GB vind ik immers niet heel relevant. Bij TechPowerUp word deze wel vergeleken.
GTX 1050;1;0.4667879045009613;Op dit moment is de Sapphire RX 470 4GB Nitro+ aan 185€ te koop op amazon.fr. Dus ik zou niet weten waarom iemand voor een GTX 1050 ti van 160 à 170 € zou gaan, lijkt me een no-brainer. Ik heb dan ook net zelf die RX 470 besteld
GTX 1050;2;0.43982383608818054;Is het de moeite een 960 4gb te vervangen door een 1050 Ti? (bv. met uiteindelijk 20 € verlies als je je 960 verkoopt via V&A) Ik twijfel maar denk het is de moeite niet echt waard...
GTX 1050;2;0.4445511996746063;Nee, tenzij je 10-20% betere prestaties de moeite waard vind om daar extra voor in te leggen. Je kan beter kijken naar een RX470 die een heel stuk sneller is. Mochten de prijzen naar 150 euro dalen dan zou ik het nog niet doen, daarentegen als de RX470 eindelijk in prijs daalt naar 170-185 euro dan is dat wel een mooie stap vooruit. Om meteen de vraag van een boel mensen te beantwoorden, niet upgraden (sidegraden) naar een GTX1050Ti als je in bezit bent van de volgende kaarten: GTX960, GTX770, GTX680, HD7950, R9 280, R9 285, R9 380 en zeker niet als je een snellere kaart hebt zoals een GTX780, 280x, 380x. De enige reden om te sidegraden is voor een specifieke feature (HDMI 2.0? en in ieder geval niet VR is want daar is de kaart te traag voor ) die je wilt gaan gebruiken of de lagere power consumptie.
GTX 1050;2;0.4988923966884613;AMD Polaris is voor mij momenteel wel geen optie, DXVA2 (in het bijzonder met HEVC) heeft nog bugs zoals ik hier en daar in de gespecialiseerde fora lees (gebruik die PC ook als HTPC / SatTV en steeds meer en meer UHD zenders zijn beschikbaar die HEVC 10 bit gebruiken als Codec). Gamen doe ik vooral racegames (Project Cars / F1 2016) en een beetje Fifa op 1080p (op mn 4k scherm native spelen is voor mij niet nodig, 1080p is genoeg voor mij) (en daarvoor is de 960 voldoende vind ik) HDMI 2.0 kan de 960 ook al btw. Na meer benchmarks bekeken te hebben kan ik enkel als argument bedenken: minder verbruik (alhoewel dat is ook geen wereld), cleanere kast (want geen PEG) en voor bv. uiteindelijk ca. 20 € verschil tussen verkoop/aankoop (meer zou ik te veel vinden, ik baseer me op prijzen uit Duitsland) zou ik een vernieuwing van garantie krijgen (die is bijna afgelopen)...maar kdenk het is de moeite niet waard. (en de performance boost is waarschijnlijk nauwelijks merkbaar)
GTX 1050;4;0.4514891505241394;Gewoon lekker bij je GTX960 blijven dus . Ik kan hier overigens prima 4k HEVC 10bits afspelen, om te testen gebruik ik de testfiles van Dit met een cpu load van 4-5% op een oude i5-4670k, RX480 icm Win7, MPC-HC met de interne LAV filters waar ik HEVC en 4k heb aangevinked. Gebruikte files 1080p: jellyfish-10-mbps-hd-hevc-10bit.mkv jellyfish-40-mbps-hd-hevc-10bit.mkv Voor 4k: jellyfish-140-mbps-4k-uhd-hevc-10bit.mkv jellyfish-300-mbps-4k-uhd-hevc-10bit.mkv Zullen wellicht appart geconfigureerde bestanden zijn die niet werken?
GTX 1050;2;0.4819474220275879;Met de prijzen van de 1050TI nee de prijzen worden weer is omhoog gedrukt om tekijken wat een '' gek '' ervoor geeft, wat je wel zou kunnen doen is je oude 960 verkopen en een 470 4 gig halen of 480 8 gig of 1060 6 gig. Want dan heb je teminste een '' echte '' upgrade en als je voor de 480 of 1060 gaat kiezen nvidia heeft met somige spellen in dx12 slechtere fps dan in dx11. Dus als je bevoorbeeld een amd fx cpu hebt of een intel xeon etc hebt zou ik voor amd gpu gaan want dx12 api helpt dat all je cores beter gebruikt worden en de load beter verspreid wordt) ( Heb een 8350 met een r9 290x zelfs met 8350 op stock en gpu lichte overclock kan ik bf1 met 60+fps op ultra spelen cpu load van alle cores zitten tussen de 40-70% en met dx11 is de load niet zo goed verspreid ) ( marja 144hz freesync dus gewoon lekker op high zetten x] En ja dat is ook het probleem als je wat meer '' budget/bangbuck '' wil hebben freesync of g-sync monitoren zoek het verschil in prijs maar is xD
GTX 1050;3;0.24742726981639862;Wat ik me af vraag ( wat ook destijds een reden voor 660gtx was) kan je met deze kaart gebruik maken van nvidia game stream (naar bijvoorbeeld R. pi2 of handhelds / laptops) ?
GTX 1050;2;0.48620957136154175;Het testen van games op ultra instellingen heeft weinig zin. In de meeste games is het visuele verschil tussen ultra en very high niet erg groot. Ik zou het liefst testen op medium instellingen en de op één na hoogste instelling zien. Bij budget kaarten is dan gelijk te zien of 60 fps haalbaar is en bij high-end kaarten is te zien of 144 fps haalbaar is. Nu liggen de testresultaten vaak te dicht bij elkaar om er iets zinnigs over te zeggen. In ieder geval prettig dat er in deze review niet alleen op ultra is getest.
GTX 1050;2;0.43751978874206543;Het zal me niks verbazen dat AMD binnenkort met een RX 465 zal komen. Deze zal dan net iets betere prestaties als de 1050 ti hebben en net iets goedkoper zijn. Net als dat ze dat toendertijd met de R9 285 hebben gedaan. Alleen sloeg AMD toen met de prijs de plank volledig mis en kwamen ze hier veel te laat mee. De 285 was langzamer dan de 280X maar wel bijna net zo duur. Niet lang daarna kwam AMD toen met de R9 380.
GTX 1050;5;0.5996780395507812;Juist in die budgetklasse moet je echt geen Nvidia nemen. FreeSync/G-Sync is een must zogauw je een nieuwe monitor koopt en mensen die een 1050 Ti kopen gaan niet een G-Sync monitor kopen terwijl ze FreeSync er gratis bij krijgen.
GTX 1050;1;0.5199201703071594;Waarom zou je een gloednieuw low-end model kopen ipv een 1 of 2 jaar ouder high-end model voor dezelfde prijs?
GTX 1050;3;0.28754401206970215;Omdat je heel makkelijk een 2e of 3e generatie i5 2e-hands OEM computer kan upgraden naar een game-PC, voor weinig.
GTX 1050;3;0.33021432161331177;Wat hebben mensen op tweakers het veel over dingen al 4K gaming, Freesync en andere dure fratsen. Full HD is voor mij op alle vlakken iig nog ruim voldoende. Maar tja ik had een paar jaar geleden nog een beeldbuis TV
GTX 1050;3;0.34446316957473755;Vergelijkbaar met R7 370?
GTX 1050;4;0.24703587591648102;Mijn vader heeft een pc met een i5 2500, 8gb ram en een hd 6450 (een oudere dell vostro pc). Ik wil de hd 6450 verwisselen met deze: videokaart. De voeding is op het moment 350 Watt. Zou ik de videokaart in de pc kunnen doen zonder de voeding te hoeven veranderen? Alvast Bedankt.
GTX 1050;3;0.39415794610977173;Dat lijkt mij prima gaan. Ik zelf heb diverse Dell Optiplex 780 en 790 modellen voorzien van GTX 750 Ti kaarten zonder 6-pins aansluiting en dat werkt allemaal prima, al een jaar of twee. De lichtste voeding in zo'n systeem was 240W, de zwaarste 265W allebei met een 12V - 17A rail.
GTX 1050;3;0.48513269424438477;Oke Bedankt!
GTX 1050;3;0.32824769616127014;Ik denk dat je met de non ti versie tov de 6450 al een flinke stap voorwaarts zal maken?
GTX 1050;5;0.24769817292690277;Ja dat klopt maar ik heb nooit op die pc gegamed en nu krijg ik de pc pas. Dan gooi ik er liever meteen een goede videokaart in zodat ik ook de triple-A games kan spelen.
GTX 1050;2;0.3978293538093567;Oh, kreeg niet goed door dat de pc voor jou was
GTX 1050;2;0.4496215879917145;Het blijft nvidia . Niet te vertrouwen als het om performance gaat na +- 2 jaar drivers Nvidia is altijd duurkoop. Dus en duur, en na 2 jaar gegimpt met performance drop van 30%, waar AMD kaarten juist stukken sneller worden. Het zijn toch dingen waar reviewers liever over zwijgen, logisch. Daarnaastheb je echt wel een 290X nodig om op ultra te draaien bij 1080p. Zeker met al die console porten.
GTX 1050;3;0.5781553983688354;Voor zover ik kan zien liggen de prestaties een beetje rond de gtx 770. Welke meer stroom verbruikt, maar tweedehands voor minder dan 100 euro al over de toonbank gaat.
GTX 1050;5;0.4698992669582367;Eindelijk, DE update voor mijn GTX 750 Ti! Goede review, nu weet ik het zeker!
GTX 1050;1;0.6217889785766602;Tweakers gebruikt een ultra highend systeem met een cpu van ruim 1700 euro om deze kaartjes te testen. Volstrekt onrealistisch. Natuurlijk word de bottleneck kaart getrokken. Onzin, je bent ook al bottlenek vrij voor een kwart van de de prijs. Wat overblijft is desinteresse. Echt iets wat je je wel kunt veroorloven als je tweakers heet, NOT. De reviews hier zijn leuk, zo kun je nog eens lachen. Maar je koop er op beslissen kun je beter ergens anders doen
GTX 1050;3;0.3716036081314087;Mag ik hieruit concluderen dat het basissysteem (ex GPU) minimaal 50 watt verbruikt op het moment van de stresstest? Ik stel deze vraag omdat ik mij afvraag of deze kaart ook prima zijn werk zal doen in een PCI-E 2.0 slot die maximaal 75 Watt (toch?) kan leveren. In oudere systemen kan deze videokaart best nog wel een boost geven, maar als de stroomtoevoer een bottleneck vormt dan is dat wel iets om over na te denken om te kiezen voor een variant met een extra stroom connector.
GTX 1050;1;0.6723135709762573;RIP AMD Hype... ze hebben op sommige prijspunten nog steeds interessante opties. Bv de 470. Maar de lukratieve complete systemen markt zijn ze met de 460 maar zo kort de interessantste optie geweest dat ze daar niet veel gewonnen hebben op NVIDIA denk ik. Maar de grootste flop van AMD is toch wel de 4gb versie van de 480... jaja onder de 200 dollar. Hierbij reik ik ze vast de bullshit award van 2016 uit.
GTX 1050;3;0.49099957942962646;Die gaat volgens mij toch echt naar de 1070 en de 1080 maar ik ben het wel met je eens dat de kaart iets te duur is geworden, met name buiten de USA en de aftermarketkaarten.
GTX 1050;2;0.5046549439430237;Ik heb voor een tweede systeem een RX 470 gekocht (uit nieuwsgierigheid), maar als ik eerlijk ben was de GTX 1060 3GB een betere oplossing geweest. De aangekondigde prijsverlaging door AMD zal helpen, maar ik denk dat AMD fans de prijzen opdrijven. Er is een kleine groep mensen die alleen maar AMD koopt en producenten (MSI, Asus etc.) weten dat. Producenten en retailers concurreren harder met elkaar als het om NVIDIA producten gaat in het budget segment. AMD kan hier weinig aan doen.
GTX 1050;2;0.45800480246543884;O? De 470 leek mij juist wel aantrekkelijk? Waarom was de GTX1060 3Gb een betere oplossing geweest? Je maakt me nieuwsgierig Ik denk dat de prijzen nog zo hoog liggen van de beschikbaarheid van oa de ram chips en de slechte marges die iig AMD bij de 480 in de adviesprijs had meegenomen. De 470 lijkt haast een noodgreep geweest om toch iets te leveren met iets mindere ram chips die wel ongeveer aan het beloofde prijspunt voldoet. De review versies en de zeldzame 1ste batch van de 4Gb 480 waren stiekem 8Gb omdat de kleinere ramchips zelfs niet beschikbaar waren. Daarmee AMD die bullshit award 2016 toch echt glansrijk binnen gesleept.
GTX 1050;5;0.2723003029823303;Waarom? Ze hebben gewoon een competitief aanbod met voor- en nadelen en voor veel budgetgamers is de gratis FreeSync een mooie bonus zo gauw ze een nieuwe monitor kopen. Binnenkort (een paar maanden) komen de Vega-kaarten uit en dan doet AMD dus weer in alle segmenten mee.
GTX 1050;3;0.2694014608860016;"Laten we eerst even de echte prijzen afwachten. Voorlopig zitten alle nieuwe GPU's ruim boven de advies prijs. Een detail wat in de ""goede"" journalistiek van tweakers in de conclusie even voor het gemak werd vergeten."
GTX 1050;3;0.2539338767528534;.
RX 7900 XTX;3;0.43480736017227173;Aan die prijzen zijn er niet veel goeie redenen meer om voor NVidia te kiezen. Wordt ook interessant om te zien wat NVidia gaat doen met prijzen en nog te lanceren kaarten nu algemene vraag lager ligt en AMD een volwaardig alternatief is. Voor de 30 serie is AMD eigenlijk een beter alternatief omdat daar geen absurd lage geheugen configuraties aangeboden worden. NVidia staat met hun 10GB en minder kaarten met de broek op de enkels.
RX 7900 XTX;5;0.30566951632499695;(linux) drivers, is en blijft (ook na jaren) voor mij -de- reden om voor groen te gaan. hieronder komen reacties van kamp rood, dat dit 'al lang niet meer is' blijf het [amd] proberen, maar nvidia (icm linux) blijf voor mij beste prestatie geven minste koppijn geven
RX 7900 XTX;1;0.2868382930755615;"Vroeger was dit inderdaad de consensus, dat Nvidia veel betere Linux-drivers biedt. Maar dat is al enkele jaren niet meer zo. Open een willekeurige Reddit-thread over dit onderwerp en je zult zien dat vrijwel iedereen je AMD adviseert vanwege de ondersteuning die ingebakken is in de kernel en dus juist minder koppijn geeft (theoretisch) dan de proprietary drivers van Nvidia. Nu moet ik zeggen dat ik zelf ook al heel lang Nvidia combineer met Linux (Fedora momenteel) en dat ik in mijn specifieke situatie nauwelijks problemen ervaar, en dat ik vermoed dat men op fora een beetje is doorgeschoten in het bashen van Nvidia. Maar ik geloof gerust dat de ondersteuning van AMD flink is verbeterd sinds mijn laatste GPU-aankoop. Voor mijn volgende videokaart zou ik AMD beslist overwegen... Enfin, mijn punt is: zo zwart-wit is het volgens mij niet. Er zijn nog steeds goede redenen om voor een groene kaart te kiezen, NVENC om maar iets te noemen. Maar 'minder gedoe' is nou juist een argument dat in mijn ervaring meestal door Radeon-eigenaren wordt gebruikt. ;-)"
RX 7900 XTX;3;0.3627561628818512;Op het gebied van NVENC zou deze generatie AMD kaarten ook een gigantische inhaalslag hebben moeten maken. Helaas dat reviewers dat zelden bekijken.
RX 7900 XTX;2;0.5160173773765564;EposVox heeft al wat info, het ziet er helaas niet goed uit. Nog steeds slechter dan Nvidia en Intel. En bovendien neemt het veel te veel van de GPU in beslag, bij streaming niet ideaal.
RX 7900 XTX;4;0.48043063282966614;Ziet er prima uit, met wel nog wat ruimte voor verbetering
RX 7900 XTX;5;0.74887615442276;En laat dat nou net zeer interessant zijn voor mensen zoals ik, die hun computer vooral gebruiken voor videobewerking en niet zozeer voor gaming. In combinatie met Adobe Premiere Pro presteren Nvidia videokaarten traditioneel fors beter dan hun AMD equivalenten. Het prestatieverschil daarbij is groter dan bij de meeste games.
RX 7900 XTX;1;0.43046262860298157;Wat is de naam van de tegenhanger van AMD ?
RX 7900 XTX;2;0.4002768099308014;VCE. Wil je het weten voor hardware encoding van video of realtime encoding voor streaming? Voor encoding zou nu hardwarematig AV1 en HEVC t/m 8K 60fps supported moeten zijn. Voor streaming zie ik ook de term AMF. edit: Lijkt er op dat alleen AV1 nu 8K 60 doet, en HEVC beperkt is tot 8K 48. Beetje verrassend en ook misleidend in de AMD presentaties. Lijkt er op dat er een speciale chip voor AV1 is toegevoegd en dat daardoor dat nu extreem goed presteert, maar HEVC niet in dezelfde mate verbeterd is.
RX 7900 XTX;3;0.33506327867507935;Ik streamde inderdaad vandaar de vraag, maar is vrij duidelijk zo!
RX 7900 XTX;4;0.37089961767196655;In mijn ervaring zijn Linux gebruikers wel een beetje bevooroordeeld ten opzichte van AMD door die semi open source drivers. Nu is er zeker veel over te zeggen, ik heb AMD en het integreert gewoon beter, Wayland is ook erg fijn, ze hebben geen ongelijk. Maar je zag mensen al AMD aanraden toen fglrx nog maar pas dood was en de nieuwe drivers nog steeds allerlei mankementen hadden. Dat was in mijn optiek nog wel voorbarig. Maar tegenwoordig heeft AMD wel duidelijk mijn voorkeur vanwege de drivers. Nvidia heeft wel functies, zoals CUDA, DLSS, raytracing, daar loopt AMD echt achter, maar de basics zijn bin AMD tegenwoordig ook wel echt beter. Dat begon met de 5000 serie toch serieus te worden, en sinds de 6000 serie al helemaal.
RX 7900 XTX;5;0.6397565603256226;Zelfde ervaring hier. Ben echt blij van Nvidia afgestapt te zijn, eerst een 5500xt gehad, nu een 6700XT en alles werkt out of the box, geen proprietary drivers. Ideaal als je een rolling release gebruiken bent of af en toe voor de lol zelf een kernel compileert. Een verademing.
RX 7900 XTX;3;0.3830205500125885;En nog afgezien van rolling releases of kernel compilaties, het integreert gewoon fijner. Ik heb een 165Hz display met Freesync (eerder was dat 144Hz zonder Freesync), plus 2 60Hz schermen. Op Nvidia zouden de zijpanelen flink wat last hebben van tearing. En dan had ik Freesync kunnen vergeten. Hier werkt dat zoveel beter. Wat alleen niet fijn werkt is de combinatie 1080P en 1440P, waarbij je ook eigenlijk iets van desktop scaling zou willen gebruiken, maar ik heb ook een soortgelijk probleem op het werk waar ik wél Windows gebruik, en daar werkt dat nóg slechter.
RX 7900 XTX;1;0.4858613610267639;Oh ja, multi-monitor... Ja voor dat soort dingen zijn proprietary drivers gewoon shit.
RX 7900 XTX;1;0.627787172794342;Jawel, het is nog steeds zo dat AMD drivers slechter zijn. LINUX geeft dit ook duidelijk aan in zijn video, terwijl hij dik voor AMD gaat nu.
RX 7900 XTX;1;0.4606952369213104;Deze thread gaat over Linux. Ik heb de video niet bekeken, maar aangezien het Linus is neem ik aan dat het over de Windows driver gaat, een totaal andere dus. Linus heeft afgelopen jaar ook een reeks gemaakt waarin hij zgn probeerde over te stappen naar Linux, laten we het erop houden dat dit geen succes was. Vandaar mijn aanname.
RX 7900 XTX;3;0.3213234841823578;Volgens mij haal je wat namen door elkaar...
RX 7900 XTX;5;0.3106626868247986;Ja….auto correctie iPhone haha Linus Je weet wat ik bedoel
RX 7900 XTX;1;0.7002450227737427;WTF!? Linux drivers van Nvidia zijn een ramp... security bugs die al in jaren niet gepatcht zijn, gebonden aan bepaalde kernel versies, closed source etc... Je ziet zo geen dergelijke verhalen over AMD : AMd drivers op Linux zitten in elke distro bij en hebben geen extra werk nodig. Die dingen werken out-of-the-box, super stabiel en zijn open source. En dat is al jaren zo. Linux ondersteuning is waarom Nvidia er bij mij niet inkomt...
RX 7900 XTX;3;0.42166152596473694;Redelijk recent kwam er wel een belofte op beterschap uit kamp groen: nieuws: Nvidia maakt zijn gpu-drivers voor Linux open source beschikbaar
RX 7900 XTX;2;0.48206937313079834;"Je koopt resultaten geen beloftes als je slim bent. Ik draai zelf nu al jaren AMD (6800 & 6900XT naast een 2060 mobile en een 1080). De drivers zijn enorm veel beter geworden, ze waren waardeloos. Ze zijn nu meer dan prima. Ik prefereer ze inmiddels boven die van Nvidia. maar dat is persoonlijk. Dus ik ga niemand tegenspreken over de ""betere drivers"". Wat ik leuk en enorm vervelend tegelijk vindt is dat AMD-kaarten vaak een ja na release een stuk beter zijn dan op moment van release. Mijn oude 5700XT was toen ik hem kocht soms wel 25% zwakker dan een jaar later. Mijn 6800 en 6900XT zijn ook aanzienlijk krachtiger nu dan een jaar eerder als ik de benchmarks van 1,5 jaar geleden moet geloven. Mijn verwachting is dat AMD dat deze generatie ook weel zal hebben, maar dat is GEEN reden om deze kaarten nu te komen. Als ze je NU aanstaan, dan kan je de trekker overhalen. Want de belofte dat het beter gaat worden is niks waard."
RX 7900 XTX;2;0.41489124298095703;Zijn er goede benchmarks te vinden waarbij de resultaten aantonen dat de 5700XT of een vergelijkbare kaart dergelijke sprongen heeft gemaakt, simpelweg door drivers? Ik geloof best dat in sommige games een 25% gelukt is, maar verwacht zelf dat dat dan ook komt doordat de drivers of de ondersteuning vanuit de game makers in het begin zo slecht waren.
RX 7900 XTX;2;0.31732749938964844;Daar heeft het ook mee te maken. Maar ik heb zonder veel moeite een artikeltje weten te vinden waar de 5700XT een 10% verbetering ogenschijnlijk kreeg in 2019 (Bron: Daarnaast heeft AMD natuurlijk ook wat voordeel met het feit dat nagenoeg alle games met hun RDNA-architectuur in gedachte worden gemaakt (Xbox & PS). vervolgens wordt er op PC natuurlijk veel voor Nvidia geoptimaliseerd omdat een enorm deel van de markt op die kaarten draait, maar dan nog.
RX 7900 XTX;1;0.38252729177474976;"de titel van dat artikel luidt zie het woord "" should "", dat betekent hier "" zou moeten "". het artikel bevat de volgende tekst. daar staat dus dat de 19. 7. 1 drivers 10 % langzamer zijn dan de drivers daarvoor, met een rx 590. dat betekent dus, dat amd met de 19. 7. 1 drivers, blijkbaar de eerste drivers die de rx 5700xt en 5700 ondersteunen, dingen heeft moeten doen waardoor de drivers in het algemeen 10 % langzamer worden voor ( sommige van ) de bestaande / oudere kaarten. als dit artikel betrouwbaar zou zijn ( ik heb sterk mijn twijfels maar daarover zo meer ), dan bewijst dit artikel exact het tegenovergestelde van wat je beweert : de bestaande kaarten ( in dit geval de rx 590 ) gaat dus 10 % in performance achteruit. nu wil je misschien zeggen, ja maar de schrijver beweert dat dit goed nieuws zou kunnen zijn, zie : ik zit niet goed genoeg in de logica / formal fallacies ( om gelijk te zeggen welke fallacies het hier betreft, maar minstens 1 zal hier flink van toepassing zijn. het klopt niet, het is niet logisch. en natuurlijk games waarbij de game / driver combinatie specifiek slecht was, waardoor je inderdaad die 15 % performance boost haalt. maar alle fabrikanten brengen nieuwe en over het algemeen betere drivers uit, waar wat winst mee valt te behalen. en het wordt telkens aangetoond wanneer er grondige reviews worden uitgevoerd, dat het uiteindelijk maar weinig aan de verhoudingen tussen verschillende videokaarten verandert in die zin, dat je de benchmarks die nu gedaan zijn over 2 jaar in grote lijnen en wat betreft de onderlinge verhoudingen ( voornamelijk over meerdere games heen ), nog uitstekend zullen voldoen, indien we de resultaten van amd en nvidia uit het verleden mogen gebruiken als voorspelling."
RX 7900 XTX;4;0.48175686597824097;Prima!~Mijn ervaringen met mijn 5700XT waren positief over de 2 jaar dat ik het draaide en het werd in de game die in die tijd veel speelde (Destiny 2 en dat is een game die niet bijzonder goed met AMD werkt) werden de prestaties over die 2 jaar beter, toen ik begon was het 65-70 FPS en tegen het einde 80-85 FPS. Of dit optimalisaties van Bungo waren of AMD heb ik niet in detail bij gehouden. Het viel me wel op dat het tussen drivers kon verschillen. Maar goed, dit is N=1 en meer een anecdote dan waarheid. Dus... doe ermee wat je wilt. Ik vind het mooi en vervelend tegelijk dat AMD dit gedrag vertonen. Want het vertelt mij dat de drivers nooit af zijn, zoals ze dit bij Nvidia wel zijn. Want double digit prestatie verbeteringen krijgen is natuurlijk leuk, maar het laat achterstallig werk zien.
RX 7900 XTX;2;0.4027363061904907;Huh? Ik game al sinds de GeForce 8800GTX exclusief op nVidia, maar heb de afgelopen 24 jaar altijd AMD kaarten voor mijn Linux systeem gekocht omdat elke nVidia kaart mij op Linux hoofdpijn bezorgde. nVidia was altijd wel flink sneller, maar wat koop ik daar voor als hun drivers crashes veroorzaken die een complete harde rest eisen waarbij de volgende boot een eeuwigheid duurt vanwege de filesystem check? AMD heeft in die 24 jaar minder crash problemen gehad en belangrijker, meestal geen reboot nodig maar de driver (automatisch) opnieuw opstarten en klaar.
RX 7900 XTX;1;0.5205349922180176;Hoezo bezorgde AMD je geen hoofdpijn? Tegenwoordig zijn de drivers wel fijn, maar ik kan me nog wel fglrx herinneren. Had je echt helemaal niets aan. Dat was pas hoofdpijn, en was was de reden dat ik wel een decennium Nvidia heb gebruikt op Linux. Pas dit jaar gebruik om weer AMD.
RX 7900 XTX;2;0.4255349934101105;Voor R300 en R400 gebruikte ik de OS drivers. 3D was heel traag, maar de drivers heel stabiel. Zoals ik al zei gamen deed ik er niet op, linux machines zijn voor (voornamelijk c++) software development. De AMD proprietary fglrx heb ik derhalve nooit gebruikt. Veel interessante games waren er indertijd overigens ook niet op Linux en Proton/Wine was ook niet echt bestaand/viable, dus ik heb nooit het idee gehad dat ik wat miste. Als je 3D modelling software gebruikte dan kan ik me voorstellen dat je het worstellen met de nVidia drivers voor lief nam, daar was AMD toen niet echt geschikt voor.
RX 7900 XTX;1;0.5491917729377747;Sorry maar dit is toch wel echt lijnrecht het tegenovergestelde van mijn ervaring. bij het selecteren van een laptop voor linux is echt mijn voornaamste, leidende criterium dat er GEEN nvidia gpu in zit. Ik weiger OOIT nog een seconde van mijn tijd in Nvidia op Linux te steken Mijn huidige laptop heeft een quadro M1200 en er is maar 1 distributie die uberhaupt succesvol boot out of the box en dat is Pop_OS. Alle andere distributies die ik heb geprobeerd, en die lijst is LANG, hebben showstoppende problemen met nvidia. Echt om gek van te worden gewoon. 99% van de distributies haalt na een verse installatie werkelijk nieteens de desktop, de meesten nieteens de login manager Daarentegen zitten de AMD GPU drivers tegenwoordig gewoon in de kernel gebakken. Dus ik weet niet hoe lang het geleden is dat je amd hebt geprobeerd, maar minstens 5 jaar denk ik. Zelfs debiab stable heeft ondertussen een nieuw genoege kernel Vandaar ook de welbekende video van linus torvalds waarin hij nvidia vervloekt en zijn middelvinger rechtstreeks tegen de camera opsteekt
RX 7900 XTX;4;0.5218971967697144;Geen problemen hier met Ubuntu 22.04 en 22.10 (beide geprobeerd) op een Dell XPS 9510 met een RTX 3050ti. Ook blijkt dat Nvidia samen Ubuntu werkt voor het server GPU geweld (kwam ik tegen bij het zoeken naar de driver . Als je voor de Nvidia driver hebt gekozen kunt je door nvidia-settings in de terminal te typen het settings grafische programma gwaarmee je grafisch zaken (power/anti-alias etc). kunt regelen en zie je op hoeveel MHz het geheugen en de GPU werkt. Voor mij goed genoeg!
RX 7900 XTX;1;0.6718441843986511;ben je nu aan het trollen ? Nvidia is een ramp op Linux met de proprietary drivers. Daarnaast zoals andere al aangeven zitten de AMD drivers in de Kernel ingebakken plug en play. Voor mij nooit maar dan ook nooit Nvidia het werkt gewoon niet goed op Linux. Daarnaast nu steamdeck, Ps5 en Xbox gebruikt maakt van AMD architectuur is het maar een kwestie van tijd tot dat alle nieuwe games echt in het voordeel van team rood gaan vallen. Je kan tegen optimalisatie niet genoeg invoeren op hardware. Je ziet het al bij Modern Warfare.
RX 7900 XTX;1;0.3857836127281189;Dat word al jaren geroepen sinds het begin PS4 dat games beter draaien op AMD. COD is daar niet echt een voorbeeld van want het is zelfs een Nvidia sponsored game geloof ik want de game heeft DLSS en Nvidia reflex en is maar 1 uitschieter. Je zou al jaren lang moeten zien dat games met AMD hardware sneller zou zijn dan vergelijkbare videokaarten van Nvidia. Ik vind dit tegenwoordig meer een mythe.
RX 7900 XTX;3;0.4261760115623474;Game heeft ook gewoon FSR, Fidelity CAS en XESS.... Daarnaast ben ik zelf juist vaker van DLSS aan het stappen omdat het toch echt wel een lagere kwaliteit beeld is en uiteraard altijd met artifacts.
RX 7900 XTX;3;0.5272451043128967;Je ervaring is valide, maar ik had eigenlijk andersom verwacht omdat de AMDGPU driver in de mainline kernel zit.
RX 7900 XTX;1;0.26951220631599426;Linux en Nvidia is een regelrechte nachtmerrie. Weet je zeker dat je niet op Windows zit?
RX 7900 XTX;1;0.34620392322540283;Toegegeven, ik had laatst wat problemen met de pro diver, maar dat is niets vergelijken met wat ik met Nvidia op Ubuntu heb meegemaakt. En om Phoronix the quoten:
RX 7900 XTX;3;0.43955498933792114;Blij dat je goede ervaringen hebt met Linux en Nvidia. Echter, mijn eigen ervaring wijkt hier toch redelijk sterk vanaf. Integratie van de drivers in de kernel zoals bij de concurrentie (Intel iGPU drivers en AMD) was voor mij toch steeds volledig pijnloos op Manjaro en Fedora. Met Nvidia ook, toch op Manjaro, altijd last om in de TTY te proberen raken als de desktop env vastliep. Om nog maar te zwijgen van wat er gebeurt met je systeem eens Nvidia de support voor jouw kaart laat vallen (Desalniettemin kudos voor de mensen achter Nouveau). Maar ik kan natuurlijk ook enkel uit persoonlijke ervaring spreken.
RX 7900 XTX;1;0.5322799682617188;De NVidia linux drivers zijn naar mijn ervaring allesbehalve vlekkeloos. Op zowel Arch als Ubuntu serieuze problemen met tearing (vooral bij scrollen) gehad totdat ik enkele opties heb aangepast in het NVidia control center. Ook werkte hibernation niet meer tussen april en begin deze maand op Arch (zwart scherm bij resume) terwijl dat op een ander system zonder NVidia gpu niet het geval was. Ik ga hoogstwaarschijnlijk een AMD GPU kopen waarbij het driver fiasco van NVidia een grote rol speelt.
RX 7900 XTX;3;0.3490563631057739;Ik snap wel een beetje waar je vandaan komt want ik ben ook jarenlang steeds weer bij NVidia teruggekeerd. Toch ben ik afgelopen BF overgestapt naar rood omdat de drivers van AMD (meer) open source zijn. Het is nog wat te vroeg voor grote conclusies maar ik moet zeggen dat het dusver vlekkeloos gaat. Dan bedoel ik niet alleen dat games lekker lopen maar ook userland ervaring zoals het omgaan met mijn ingewikkelde en flexibel multi-monitor setup. Out-of-the-box een betere ervaring dan ik de afgelopen 10 jaar met nvidia heb gehad Daar moet ik wel bij zeggen dat ik die kaart pas kort heb en alleen nog maar een paar games heb gespeeld. Met kerst wil ik ik een HPC/AI-projectje doen. Mijn beleving is dat die wereld erg op NVidia is gericht. Momenteel heb ik echter het vertrouwen dat dit ook goed gaat werken. Dusver mijn anecdata.
RX 7900 XTX;2;0.3595048189163208;Het is juist andersom, AMD heeft betere drivers voor Linux.
RX 7900 XTX;3;0.4045525789260864;Nee hoor, je moet het zelf weten. Echter de opensource drivers van AMD zijn fantastisch en zeer stabiel onder Linux. De opensource versie van nVidiab is daarentegen lachwekkend als het niet zo triest zou zijn. Dat is voor mij de redenom AMD in mijn setup te hebben en houden.
RX 7900 XTX;2;0.35392090678215027;""" vroeger "" was het toch zo dat je zo ongeveer intel + nvidia moest hebben ook maar een kans te maken om spellen met wine te draaien? ik ben pas sinds vorig jaar 100 % overgestapt op linux ( en dus ook gaan gamen met wine ), en ik heb eigenlijk nooit iets anders gehad dan intel + nvidia, maar er ook nooit problemen mee gehad. ik moet wel zeggen dat ik altijd een half jaar tot een jaar achter de feiten aan loop : ik ben zelden of nooit een early adopter van hardware. ook nu heeft mijn ( oude ) i7 - 6700k met gtx 1070 geen problemen onder debian 11. ( ik draai dan wel kde op xorg ; ik ga pas over op wayland als debian er de standaard van maakt. ) wel staat er een nieuwe computer op het programma, na de release van debian 12 bookworm. die computer komt er daarom waarschijnlijk ergens in juni - augustus. die gaat een ryzen 7950 in eco mode bevatten ( want ik wil per se 16 "" normale "" cores in dit systeem ), en mogeljk / waarschijnlijk een radeon 7000 - serie videokaart. het enige waar ik een beetje mee zit is het volgende : als ik de open source driver wil gebruiken, dan is daarvoor linux 6. 0 nodig ( vandaag is 6. 1 lts uitgekomen, dus ik ga ervan uit dat die in debian 12 terechtkomt ), en mesa 22. 2 ( die zit al in bookworm ; 22. 3 zit al in sid, dus ik ga ervan uit dat debian 12 mesa 22. 3 gaat bevatten ). voor mijn gevoel is dat een beetje te kort dag, omdat ik niet verwacht dat debian bugs gaat fixen of backporten, behalve security fixes ; als iets niet goed werkt in de rx 7000 serie kaarten en daarvoor een nieuwe kernel + nieuwe mesa nodig is, dan zal het niet werken voor 2 jaar lang, behalve als er ( heel snel ) een nieuwere kernel + mesa in backports terecht komt. hoe makkelijk / lastig is het tegenwoordig om de proprietary amd - driver te installeren? de nvidia driver is... eh... niet makkelijk, als je secure boot nodig hebt, maar op een nieuw systeem gaat dat mogeljik gewoon uit staan omdat het alleen linux gaat draaien. mijn desktopcomputer krijgt geen windows meer. nooit meer."
RX 7900 XTX;3;0.5598829388618469;Ik begrijp niet zo goed waarom.je nieuwe hardware met Debian wilt combineren. Neem dan een distro die de kernels wat sneller volgt, en dat moet ook geen Arch zijn, zit nog wel wat tussen.
RX 7900 XTX;5;0.543692409992218;Ik koop al 10 jaar lang enkel radeon en heb altijd op HD of nu 4K ultra kunnen spelen en nooit problemen gehad met drivers of spel/ stability issues. Kiezen voor het groene kamp is gewoon fanboi zijn en je geld over de balk smijten.
RX 7900 XTX;3;0.27275365591049194;Je bedoelt de oude fireglx elende of de huidige amdgpu driver? Die oude was brak, die nieuwe is top.
RX 7900 XTX;2;0.5318211317062378;Bij de RTX 4080: DLSS3.0, Raytracing performance, CUDA, power efficiency. vs een 20% hogere prijs. Het prijsverschil verdien je terug met 3-5,5 uur per dag (5 jaar lang) gamen in stroom verbruik... (Het is weird om Nvidia als energie 'zuinig' neer te zetten!) Persoonlijk zou voor mij het prijsverschil de genoemde features waard zijn om toch voor Nvidia te gaan. Maar ik sla deze generatie even over. Wellicht wanneer de energie prijzen weer zijn gedaald naar normale niveau's dat ik weer eens ga kijken naar een PC build met een losse kaart. Tot dan zal ik hier primair zeer energie zuinige AMD CPUs gebruiken met intergrated graphics + cloudgaming (naast de Steam Deck, welke ook AMD gebruikt).
RX 7900 XTX;1;0.44583484530448914;"En dan hebben we het nog niet over idle verbruik, want schijnbaar lukt het AMD alweer niet om fatsoenlijke drivers te hebben bij multi-monitor setups: (iemand linkte deze in AMD topic op GoT). En zelfs als je zegt: ""Maakt mij niet uit, heb toch maar één monitor"", je gaat je dan wel afvragen wat er verder nog meer aan de hand is met hun drivers. Verder ook geen idee wat het probleem is van een kaart zonder 12GB+ geheugen. Volgens mij loopt mijn 3070 nog niet vol."
RX 7900 XTX;2;0.47124814987182617;Schijnbaar lukt het Nvidia niet eens een fatsoenlijke driver te leveren bij een single monitor setup: Om dan meteen te gaan afvragen wat er verder allemaal mis is met de drivers gaat mij een beetje ver, maar ik begrijp waarom je nog een 30 series kaart hebt want de 40 series kan jij niet vertrouwen. Het geheugen is belangrijk voor hogere resoluties en nieuwere spellen, de 8 gb van de 3070 gaan makkelijk gevuld worden over een paar jaar. Erop bezuinigen bij een dure kaart is gewoon planned obsolescence. Met meerdere monitoren zou ik zeker ook even wachten of dit issue weggaat.
RX 7900 XTX;1;0.4821544587612152;De spellen die over een paar jaar makkelijk de 8GB geheugen vullen, gaan toch niet op hoge settings draaien op mijn 3070. En op lagere settings past dat vast weer wel binnen de 8GB. Een specifieke bug bij bepaalde moederborden als geen drivers geinstalleerd zijn, is natuurlijk iets wat wat makkelijker door QA kan glippen dan 100W+ verbruiken gewoon standaard bij een multi-monitor setup (en iets soortgelijks bij een Youtube videotje). En bugs kunnen er altijd zijn, liefst niet al teveel, maar het zal helaas nooit nul zijn. Maar dit verbruik is schaar ik daar niet onder, het is niet zozeer een bug als gewoon heel slecht geoptimaliseerde drivers. En was dit nou een enkel dingetje, dan was ik het met je eens hoor dat de stap naar slechte drivers groot is. Maar laten we wel wezen, dit is het zoveelste deel in AMDs drivers die het slecht doen bij meerdere monitoren qua verbruik en/of bij monitoren die niet op 60Hz draaien. En algemeen is één van de grote problemen van AMD al een tijdje langer de drivers. (Tot mijn 3070 was het merendeel van mijn kaarten AMD. Na de 5700XT had ik daar voor voorlopig wel genoeg van).
RX 7900 XTX;4;0.3929961025714874;Het is niet alleen multimonitor verbruik, maar kennelijk ook bij het afspelen van video's, zoals van Youtube. Genoeg VRAM kan een punt zijn als je graag exorbitante texture mods draait zonder verdere optimalisatie.
RX 7900 XTX;2;0.39642494916915894;Definitie van multimonitor setup is heel breed en wordt nooit terdegen uitgezocht door review sites. Ik had dit probleem voorheen ook met mijn 6800XT, na het plaatsen van 2 *4K op 144hz refresh sync was het probleem weg, voorheen was het een FullHD+QHD met 60-144hz combo. je geeft aan alsof als er 1 probleem is het vol met problemen is en nvidia er geen enkele heeft helaas is soms zelf wat onderzoek doen moeilijk tegenwoordig en is het leuker om een issue gewoon te blijven linken.
RX 7900 XTX;3;0.5052107572555542;I got three... Ouch!
RX 7900 XTX;2;0.4094336926937103;5.5 uur per dag gamen gedurende 5 jaar. Nu niet echt een gebruikelijk scenario.
RX 7900 XTX;2;0.5387763977050781;Inderdaad. En dat moeten dan ook games zijn die je videokaart zwaar aan het werk zetten. Als je dan tussendoor ook nog wel es wat minder vereisende spellen speelt klopt de berekening ook al niet meer.
RX 7900 XTX;4;0.31502440571784973;Bij gelijkblijvende energieprijzen.
RX 7900 XTX;1;0.5771655440330505;DLSS3 = stront tussen je frames net zoals tvs van 10 geleden dat deden. Nee dank je, ik hoef geen interpolatie. RT: zoals Jelle in de video zegt: leuk om even aan te zetten, maar uiteindelijk zet je het uit, want visueel is het verschil erg klein, en de impact op fps is enorm (en die impact wordt niet kleiner: bij de 4000 serie verlies je net zoals 2 generaties terug in de 2000 serie ongeveer 50% van je framerate). CUDA: wordt in geen enkele game gebruikt, dus voor de meeste consumenten. Uitermate irrelevant. Zoals ik op vorige artikelen al zei: het is erg vreemd hoe mensen deze punten telkens weer opnoemen terwijl de realiteit aantoont dat dat slechts niche toepassing zijn die de gemiddelde consument niet gebruikt. Uiteindelijk draait het oh altijd om raster performance, tegen de tijd dat RT er echt toe doet is je 4000 serie kaart echt niet meer relevant.
RX 7900 XTX;3;0.29978641867637634;Raytracing, betere professional software support etc zijn toch zeer sterke redenen. Die 10GB is overigens helemaal geen probleem.
RX 7900 XTX;3;0.40620195865631104;Nee dat is wat individueel relevant is. Dus voor de een, de enigste keuze en voor de ander heeft meer keuze.
RX 7900 XTX;1;0.47619011998176575;Verkondig het dan niet als een feit.
RX 7900 XTX;2;0.42068588733673096;Wellicht als games het enige zijn wat je doet op je PC maar wanneer je bijvoorbeeld bezig bent met 3d, renderen of AI dan is er eigenlijk geen enkele serieuze keuze buiten nVidia. Vrijwel alle software/tools zijn geoptimaliseerd voor nVidia en wanneer je bijvoorbeeld de CUDA cores in wil zetten voor andere taken/berekeningen dan biedt nVidia een heel scala aan gratis tools.
RX 7900 XTX;2;0.4955081343650818;Helaas komen deze kaarten vanwege hun buitensporig idle verbruik in een multi-monitor setup bij voorbaat al niet meer in aanmerking: Het is te hopen dat AMD hier nog een driver update voor uitbrengt. Echter is, voor zover ik weet, de R390/R9 Fury generatie de laatste die niet een bovensporig verbruik hiervoor met zich meebracht. @Trygve Omdat zeker onder Tweakers vaak met meerdere beeldschermen wordt gewerkt, is het misschien een goed idee even snel een twee monitor aan het systeem te hangen om dit ook expliciet in de Tweakers reviews mee te nemen. Een idle verbruik van 80W wanneer je een extra beeldscherm aan je GPU hebt hangen, zal afgezien van de hogere stroomrekening ook niet meer passief te koelen zijn.
RX 7900 XTX;3;0.41887789964675903;Bij laptops kan je switchen naar IGPU, jammer dan het bij desktops niet kan..
RX 7900 XTX;2;0.35285645723342896;Laptops hebben hier dan ook een hardware onderdeel voor die het ingebouwde beeldscherm kan switchen tussen de igpu en dgpu. Die zogenaamd fysiek de connectie om. Meestal als ze een hdmi poort hebben voor externe monitoren zit die wel direct op de dgpu aangesloten zonder mogelijkheid om die via de igpu aan te sturen. Dat ben ik gaan onderzoeken toen ik me afvroeg waarom mijn hdmi poort het niet deed als ik in linux mijn dgpu op disabled had staan. Het is blijkbaar heel normaal
RX 7900 XTX;2;0.4641452729701996;Als ik het destijds goed gelezen heb is dat wel een AM5 feature. Helaas hebben zowel Tweakers als Techpowerup op AM4 of Intel getest. Ik hoop in ieder geval dat het een oplossing kan bieden of dat AMD middels de drivers het verbruik de kop in kan drukken.
RX 7900 XTX;5;0.4591715633869171;Je kunt zover ik weet met 13th gen ook gewoon je monitor op het moederbord aansluiten met een dgpu erin en dat ie die gewoon gebruikt. Vorige week ook een paar keer geprobeerd en dat ging prima.
RX 7900 XTX;1;0.43957701325416565;Ja maar dan draait hij altijd op je igpu, en nooit op je dgpu zover ik weet. Dus niet dat je dan desktop werk op igpu doet en spellen op de dgpu.
RX 7900 XTX;1;0.4754151403903961;Nee hoor, had in de games gewoon fps zoals met de videokaart zelf.
RX 7900 XTX;1;0.6018384099006653;Dit doet mijn GTX 1080 default ook gewoon. Als je een tweede monitor aansluit weigert hij naar idle state te gaan en blijft hij 1200-1700 MHz draaien en warm worden. Ik heb ooit een of ander tooltje geinstalleerd die specifiek hiervoor is gemaakt waarin je kunt customizen bij welke 2D en 3D load de kaart uit idle mag komen Dus ik ben het zeker met je eens dat een multimonitor idle test zeer nuttig zou zijn, maar ik denk niet dat de resultaten per se zo zullen zijn als jij schetst Dat het niet passief te koelen is maakt mij niet uit. Ik zet die 0 fan mode per definitie uit. Gewoon altijd minimaal 5% dutycycle. Daar merk je in geluid helemaal niets van, maar het scheelt je sowieso meer dan 10 graden idle. Ik test het met iedere nieuwe gpu altijd even. in de avond als het stil is thuis zet ik even met het handje de duty cycle steeds 1% hoger totdat ik hem kan horen veranderen. Dan zet ik hem weer 1 of 2% terug en dat wordt het minimum percentage Om dezelfde redenatie hebben bequiet voedingen bijvoorbeeld ook geen semipassieve fan, maar draait hij gewoon heel langzaam bij lage load
RX 7900 XTX;3;0.48012876510620117;Maar als je gpu idle 80W verbruikt zal een minimale snelheid mogelijk ook niet afdoende zijn. Afgezien daarvan dat zoiets een serieus verschil maakt op de jaarrekening als je je PC ook voor kantoorwerk gebruikt zo’n 1600 uur op jaarbasis. Dat scheelt gerust een kWh of 100 even snel gerekend. Mijn ervaringen met tools om verbruik van AMD kaarten te drukken zijn met m’n R9 niet erg positief. Die wordt bij de geringste aanpassing overigens al instabiel. Vandaar dat een officiële correctie voor mij wel wenselijk is. Maar dat terzijde. Het is wel een goed alternatief en goede tip
RX 7900 XTX;2;0.44731035828590393;Misschien even kijken of anderen er al een oplossing voor hebben. Ik had er ook last van. Mijn GTX 1080 liep 50-60 watt idle. Nvidia inspector geinstalleerd, multi display power saver aangezet en hoppa, 13 watt. Stukken beter. 80 watt is voor de gemiddelde gpu koeler piece of cake. Een 6800-6800XT bijvoorbeeld trekt meer dan 250 watt onder load. Een schamele 80 watt kunnen ze prima geruisloos koelen. De meeste aardige kwaliteit AIB kaarten zijn al bijna onhoorbaar tijdens het gamen. Dan kunnen ze echt wel op minimaal toerental 80 watt aan.
RX 7900 XTX;2;0.40611332654953003;staat wel niet bij met welke multimonitor setup dit getest is en in hoeverre deze monitors op elkaar afgesteld zijn. op mijn 6800XT had ik dat probleem ook tot ik 2 * monitor met zelfde resolutie (4k) en refresh rate (144hz) eropzet en het was opgelost... weet dus niet of het aan refresh rate ligt of aan resolutie. het is wel een pak hoger verbruik dan de 6000 serie idle met dit probleem...
RX 7900 XTX;2;0.4777821898460388;Als dat de oplossing is, dan worden Nvidia GPU's ineens relatief een flink stuk goedkoper. Daarnaast is er, zeker voor niet-gaming doeleinden, vaak sprake van een asynchrone setup natuurlijk. Is het niet vanwege het gebruiken van een oudere monitor als secundair scherm, dan wel omdat de ene in landscape en de ander in portrait modus staat. Maar je hebt gelijk, die nuance dient wel gemaakt te worden, er zijn mensen waar dat natuurlijk de situatie is.
RX 7900 XTX;3;0.4524793028831482;combinatie van oude monitor, kleuren afstelling en verschillende pixel size ga je je enkel toch maar irriteren op de verschillen en het windows ongemak....
RX 7900 XTX;3;0.383937269449234;Dat is geen normale OCD maar OC’D OCD
RX 7900 XTX;2;0.5072572231292725;Dit doet mijn 1070 ook anders. Al is daar een Quick fix voor. Maar loopt hij warm te draaien en hoge clocks terwijl hij niks doet. Dus denk AMD het wel fixt met een drivers.
RX 7900 XTX;4;0.364163339138031;Interessant, want mijn 1070 doet dat niet. Ik zit nu in dual monitor set-up en hij verbruikt ongeveer 10 watt en de gpu clocksnelheid is 139mhz. Ik heb hier ook nooit problemen mee gehad
RX 7900 XTX;3;0.46291813254356384;Met de huidige prijzen is dat imo zeker wel iets dat ze even aan de test of iig aan de conclusie zouden moeten toevoegen.
RX 7900 XTX;5;0.3457639217376709;Komt snel een update voor, is al aangekondigd.
RX 7900 XTX;1;0.24268148839473724;Dit is een driver issue en bevestigd door AMD. Hier wordt aan gewerkt en wordt door middel van een patch opgelost. Zie de review van LTT op YouTube.
RX 7900 XTX;2;0.43691691756248474;Haha in mijn appartement gaat de kachel ook al amper aan. Stadsverwarming verbruik is bij on bijzonder laag. Dit jaar is het wel lang leven de vega 64, de gratis elektrische kachel!
RX 7900 XTX;2;0.5656667947769165;Ondanks dat de prijs prestatie verhouding qua Raster Prima is heeft AMD wat mij betreft wel een beetje de plank mis geslagen. Ze laten mooie presentaties zien met 50-70% prestatie winst vervolgens is het eerder 35-40% en worden die hogere prestaties alleen enigszins met RT gehaald. In het verleden waren AMD presentaties nog te geloven en kreeg je ook echt wat je in die slides zag. Maar nu zijn ze dus gigantisch aan het cherry picken geweest en komt het resultaat niet in de van de beloofde waardes in de presentatie. Helaas gaan ze dus ook met dit soort dingen meer de kant van nVidia op. Erg jammer want je hebt hier helemaal niets aan. Consumenten komen er toch wel achter als ze maar een review lezen. Een beetje jammer want op deze manier verpest je het vooral voor jezelf. Voortaan dus ook bij AMD niets meer van de marketing slides geloven. Dan over het resultaat. Een erg magere winst tov de 6950XT gezien dit een hele nieuwe generatie is. Waar de 6900XT en 6950XT het nVidia in raster moeilijk konden maken lukt AMD dat nu nog helemaal niet. Sommige games doen het heel goed en dan zie je de potentie van de architectuur maar meestal komt dat niet naar boven. Of dat nu de Drivers zijn of toch wat nadelen van de eerste keer met chiplets werken zal de tijd leren. Er gaan ook geruchten over een eventuele N31+ / respin die mogelijk hoger kan clocken. Of dat waar is zal de tijd ook leren maar deze resultaten vallen toch wel tegen na alle leaks en eigenlijk ook als je naar de spec's en het productie process kijkt. Ik had verwacht dat die kaart toch wat hoger zou gaan clocken. Pas als N32 uit komt kunnen we zien of er inderdaad wat mis was met de N31 opzet waardoor die minder hoog clockt of dat RNDA3 toch minder potent is dan gehoopt. Al met al is het wel een upgrade maar qua prijs prestatie verhouding gaan we er niet heel veel op vooruit.
RX 7900 XTX;2;0.3225991129875183;"AL hun presentaties waren dan ook ""tot"" en ze halen dat gewoon. Misschien wou je het gewoon geloven maar dit was niet anders dan elke keer ervoor. De 6950XT is een pak boven de 7900 xtx als je appelen met appelen vergelijk is qua prijs de 7900 xtx de 6900xt van vorige generatie. EN daar zie je 50-100% winst mee in 4k/ultra wat heel mooi is qua winst tegen een gelijke prijs bij release. vergelijken met de enorm veel duurdere 4090 is beetje absurd, de 7900 XTX scoort een pak beter dan de 4080 in raster en kost een pak minder. Als je dat niet goed genoeg vind ..."
RX 7900 XTX;2;0.41527754068374634;klopt maar dit is wel marketing ten top natuurlijk tot 70 % average maar 31 - 38 % halen afhankelijk van de site die je leest. dat is wel heel wat anders dan het beeld van 50 - 70 % wat ze schetsen in de presentatie want daar laten ze geen lagere scores zien. de rdna2 gpu ' s konden waar maken wat ze lieten zien in de slides en deden dat consistent in het merendeel van de games. maar hier is het andersom. dan is het af en toe een uitschieter waar de prestaties goed zijn. de drivers of de chiplet strategie zijn duidelijk nog niet op orde. ik neig nu nog wat meer naar driver issues en hopelijk kunnen ze de prestaties wat opkrikken. maar voorheen schetste amd een realistisch beeld in presentaties van wat je gemiddeld kon verwachten. dat is nu overduidelijk voorbij. ik draag amd al jaren lang een warm hart toe maar dit soort dingen keur ik gewoon niet goed. ik snap niet waarom ze zich tot het niveau van nvidia moeten verlagen. vergeet niet dat in tegenstelling tot de cpu markt amd in de gpu markt al lang niet meer zo dichtbij is gekomen als met rdna2. ze hadden minimaal moeten zorgen dat het prestatie gat gelijk zou blijven. en liever wat inlopen / de 4090 verslaan want er komt nog een 4090 ti. vervolgens kunnen ze de 4080 maar net aan. dus de 4080 ti, 4090 en 4090 ti zullen allemaal sneller zijn dan de 7900xtx ( tenzij amd een wonder driver kan uitbrengen ). en dat is een duidelijke achteruitgang. ook qua performance per watt doen ze het momenteel niet goed ook daar val rdna3 tegen tov rdna2 ja het is een vooruitgang maar niet zo goed als je van een 5nm product mag verwachten. en mogelijk zijn de chiplets daar wel de oorzaak. maar je ziet dat nvidia hier echt voordeel heeft met hun monolithic 4nm design. ik ben in ieder geval klaar met amd free passes geven omdat ze de underdog zijn. ze staan er nu goed voor. hebben meer geld dan ooit. en vragen ook veel hogere prijzen voor hun producten. dan mag je ook wat kritischer zijn en er meer van verwachten.
RX 7900 XTX;2;0.48594269156455994;"neen dat is niet correct : de 70 % was specifiek in 1 benchmark, de rest van de cherry picked benchmarks gaven tot 50 %. toen ik die zag dacht ik "" mooi dus kan je ergens tot zoveel verwahcten in bepaalde games "" als je dan denkt dat het in alle games 50 - 70 % is dan maak je jezelf gewoonweg iets wijs. enkel als je specifiek dezelfde settings overnam en dat was het in andere games in veel gevallen minder. hier hebben we geen settings en zelfs geen specifiek getal gewoon "" tot x % "" ( wat ze de vorige keer ook zo vermelde trouwens maar dan met fps ). neen bepaalde games, maar idd met rt kom je op nog hogere waardes tov de 6900 ' s, en wat ze kosten in de vs doet er weinig toe. hie kost een 6900xt nog steeds 850 - 900 euro je moet vergelijken met wat er nu te koop is. de 4080 is duurder en in wat voor de meeste mensen belangrijk is : raster, trager en duurder. dat je dan nog veel meer prestatie vraagt tegen een log lagere prijs is een beetje absurd. vergeleken met wat nvidia uitbracht is deze kaart gewoon beter geprijsd qua performance zelfs tegen de 4090. het geld ligt niet in het high end segment, dat ligt vooral daaronder. high end is vooral om aandacht te krijgen en de mensne dan te doen geloven dat de lagere kaarten even goed zijn. 4080 en 4090 zijn absurd hoog geprijsd, dat amd daar niet mee meedoet is goed ( en logisch ze zijn de underdog en moeten in princiepe wel ) dit een flop noemen omdat ze alle achterstand niet kunnen wegwerken gaat veel te ver en weeral gaat eerder over dat je jezelf onrealistische verwachtingen aangepraat hebt dan iets anders. gezien de huidige prijzen en gezien de prestaties vind ik het alvast een goede kaart van amd. de voornaamste concurent is hun eigen 6800 lijn, tegen dat die volleldig uitverkoicht is zal de opvolger in de 7x00 lijn daar ongetwijfeld zijn en bouwt die verder op deze prijs / prestaties."
RX 7900 XTX;2;0.5078128576278687;als het nu 50 % was geweest was het ook prima. maar het is vaker veel minder tot zo laag als 15 - 20 %. gemiddeld is het slechts 31 - 38 % afhankelijk van de review dat is ver onder de 50 %. daar deden ze ook geen vergelijking met een andere kaart als ik me niet vergis. maar de cijfers die ze lieten zien kwamen overeen met de reviews. alleen was de geteste scene soms wel anders waardoor je andere waardes had maar dat lag veel meer in lijn met de werkelijkheid dan de 50 % die ze hier lieten zien en bijna nooit gehaald wordt. 50 % is al cherry picked met de huidige drivers. we weten nog niet wat ze hier gaan kosten. het zou mij niet verbazen als de aib versies richting de 1200 - 1300 euro gaan we zullen het straks zien. want wat je zelf ook aanhaalt de prijs in amerika is niet de prijs hier. dat ze daar een 7900 xtx voor 999 kunnen kopen is leuk maar wij moeten in euro ' s heel veel meer gaan betalen. en stroomverbruik is zeker belangrijk. het is niet slecht maar had gewoon veel beter kunnen zijn. qua value for money ga je er tov rdna2 niet op vooruit. en qua idle verbruik en geluid productie ga je er op achteruit. ik vind dat best dingen om over na te denken. misschien is dat voor sommige mensen niet belangrijk. maar voor mij persoonlijk is dat wel belangrijk. ik ga er overigens wel een testen ( mits hij rond de 1100 euro blijft ) omdat ik de afgelopen jaren ervaringen topic ' s op tweakers gemaakt heb met core clock / power scaling tests en dat wil ik deze generatie ook doen. ik weet alleen niet of ik de kaart uiteindelijk ga houden of dat ik weer terug ga naar een 6800xt of 6950xt. hopelijk kan amd de nodige zaken via de drivers verbeteren. en vooral de games die nu weinig winst laten zien wat verder opkrikken. anders is een 2e hands 6800xt voor 525 - 575 gewoon interessanter gezien de huidige nadelen bij in ieder geval de referentie kaarten. ( maar dat is mijn persoonlijke mening ).
RX 7900 XTX;2;0.4287000894546509;"Omdat je andere games erbij haalt dan die AMD liet zien. Nogmaals dat komt omdat je droomede van 70% beter overal wat imho absurd is. Zo een sprongen maken generaties niet, niet amd en niet nvidia. Gezien wat nvidia deed is wat amd deed gelijkaardig, beter dan de 4080, minder dan de 4090 met als verschil dat bij beide ook de prijs naarboven ging en bij AMD niet. Jawel hoor en ook met de ""up to"". Hebben ze nu iets harder ge cherrypicked ? Waarschijnlijk wel, dan afronden en met de grafieken die je toonde en sommige beginnen te dromen dat AMD tegen de helft van de prijs een 4090 op de mark zet. Ik kan je zeggen dat dit nooit zal gebeuren . Ik vergelijk wat we momenteel kunnen vergelijken, en idd gezien een 4080 hier al 1400-1500 euro kost zal de 7900xtx ergens tussen de 1100-1300 vallen vermoed ik. Hoe dichter tegen de 1400 hoe onaantrekelijker de kaart natuurlijk word. Stroom verbruik is echt geen issue, 3080 vebruikt ook dubbel zoveel in idle als een 6800 geen haan die ernaar kraaide. Idle zonder meerdere monitors verbruikt de 7900 xtx trouwens veel minder tov de 4080/4090 : 11wat tegen 14/22 watt. Ziende dat 2-4% multimonitor is maakt dit weinig uit, en zoals al gezegd waarschijnlijk een bug die men wel oplost . Kost per frame is de 7900xtx 15% lager dan de 6950xt en is de 4080 12% duurder tov de 3080 idle verbuik is (die bug niet meegerekend) een paar watt hoger dan de vorige generatie en nog steeds lager dan wat nvidia heeft (daar ben je fout in) . Geluid is lager dan bij de 6950 xt en luider dan de 6900/6800 maar de temeprturen zijn beter, ongetwijfeld als de de fans wat terugdraait je op iets gelijkaardig uitkomt als de vorige generatie. En idd iets om bij na te denken als je deze koopt maar valt al bij al echt wel mee imho."
RX 7900 XTX;2;0.4695018231868744;Ik heb het ook niet zo zeer over 70% maar vooral die 50% die wordt ook gewoon bijna niet gehaald. HWU heeft behoorlijk wat games getest. TPU ook en daar zie je hetzelfde zelfs die 50% is gewoon vaak niet haalbaar. Nee met de 50-70% terwijl het gemiddelde op de 35% ligt. Stop er dan ook gewoon een paar games van rond de 35% in om een realistisch beeld te krijgen. Nu hebben ze alleen de beste resultaten gepakt. En dat is cherry picking ja. Dat ze de resultaten van 20% niet laten zien kan ik mij nog iets bij voorstellen al was dat wel beter geweest dan ben je gewoon eerlijk over je product. En dan zou niemand teleurgesteld zijn bij de reviews. Je kan dan ook prima de 50-70% resultaten laten zien dan krijg je gewoon een beter totaal plaatje. Nu krijg je alleen het rooskleurige plaatje. 1165-1199 is de daadwerkelijke prijs geworden. Ik kraai wel naar het idle verbruik. Ik zal een van de weinige zijn. Al heb ik meer mensen gezien die het belangrijk vinden. Multi monitor @ 85 watt moet wel echt opgelost gaan worden tho. Zou kunnen de 6950XT kan stock erg luid zijn die cooler is identiek aan die van de 6900XT en die koelers kunnen prima met 300 watt omgaan maar bij meer zijn ze niet zo stil meer. Het verschilt wel een beetje van kaart tot kaart. Sommige hadden hogere hot spot temps en die fan's waren dan ook een stuk luider. Je kan die resultaten zien in het Navi 2X ervaringen topic ik heb echt veel 6800XT, 6900XT en een paar 6950XT kaarten getest. Des al niet te min is een luidere fan tov de 6900XT jammer. En gezien de temps hadden ze de fan curve ook iets aan kunnen passen maar dat geef je zelf ook al aan. Ik snap alleen niet waarom AMD dat dan zelf niet doet. Want ze weten na vele jaren hoe belangrijk mensen geluidproductie vinden.
RX 7900 XTX;2;0.35148096084594727;CODMW2 : +45% slide : 50% Legion (vind enkel met raytacing terug) : +55% slide (zonder RT) : 50% Cyberpunk +59% slide: +70% Metro exodus : +55% slide : +50% ... Zonder twijfel kan je met de juiste settings die waardes van AMD verkrijgen. Zijn deze titels representatief? Tja, heel populaire recente games en natuurlijk pakt AMD er de beste uit, dat is toch echt maar logisch. Ze hadden zelfs kunnen beweren sneller te zijn dan de 4090 want dat gebeurt ook heel af en toe. Het is dan toch echt aan jou dat jij ervan maakt dat deze een gemidelde zijn terwijl AMD dat nooit gezegd heeft. Wat dus 250 euro minder is dan nvidia (of 4080 is 22% duurder), voor betere performance in rasterization kan je toch echt geen slecht resultaat noemen voor algemeen gamen met deze kaart. Als over idle : nogmaals dat is louter voor multi monitor, alle andere gevallen vebruikt die veel minder dan een 4080 laat staan 4090 wat volgens jou het verschil dus nog veel groter zal maken qua prijs. Zelfs voor geluid en waremte: als je die links hierboven ziet is het duidelijk dat AMD hun kaarten agressiever koelt om legere temperaturen te krijgen tegen meer lawaai. Brengt die temperatuur naar iets gelijkaardig die 6800/6900 heeft en qua lawaai zullen die niet verschillen. Logisch ze verstoken ongeveer evenveel en de koelers zijn heel gelijkaaardig. Dat de formfactor hetzelfde is en niet enorm absurd groot en je niet moet gaan klooien met andere stroomkabels zijn enkel voordelen voor AMD.
RX 7900 XTX;2;0.36672329902648926;dit zijn echt de beste resultaten. zullen we er nog een paar bij gaan pakken? en dit zijn ook geen onpopulaire titels. ik ben het met mlid eens. het zou goed kunnen dat ze verwacht hadden in meer games die 50 - 70 % te kunnen halen maar toch tegen te veel issues aan gelopen zijn. want zoals gezegd is de range groot waar de 6950xt tov de nvidia kaarten heel consistent is qua prestaties gaat de nieuwe 7900 serie alle kanten op. dus iets zit duidelijk niet lekker. maar in het verleden deden ze dit : en ja dit was ook rooskleuriger want rage mode stond aan en sam ook waardoor de radeon kaarten wat sneller waren. nvidia had toen nog geen rebar. maar toen de reviews uit waren klopte de positionering wel : je moet wel even naar de avg fps kijken maar het werd waar gemaakt. de slides waren te geloven. ik ben het helemaal met mild eens hij denkt er hetzelfde over als ik : amd heeft een maand terug gewoon een te mooi plaatje laten zien. en om eerlijk te zijn zo gek was het niet om 50 - 70 % te verwachten. maar als je dan een standaard gebruiker hebt die normaal nvidia had en dan amd een keer probeert. die vervolgens herrie hoort, slechte prestaties ziet in sommige spellen die hij of zij speelt of crashes krijgt. zal dan denken nvidia was echt beter laat ik maar weer terug gaan. ik heb het al te vaak gelezen. vooral ten tijden van de 5700xt waar blackscreen een probleem waren. dit is natuurlijk minder erg. maar wil je mensen echt over halen. en in jou kamp houden zul je het echt heel goed moeten doen. en ik wil dat ook graag zien. ik wil graag dat de markt beter in balans gaat komen dat nvidia niet 80 % of hoger in handen heeft want dat is niet goed voor ons als consument. maar dan is het jammer als er van dit soort dingen mis gaan. en sommige van deze dingen zoals de afstelling van de fan ' s of coilwhine hadden voorkomen of verder beperkt kunnen worden tijdens het ontwikkel proces. dus ik zeg dit juist van uit passie niet om maar zonder goede onderbouwing amd zwart te maken.
RX 7900 XTX;1;0.4244144558906555;"dit was als commentaar op "" ze halen zelfs niet de 1. 5 - 1. 7 "" dat doen ze dus wel. ja er zijn andere titels die minder winst laten zien dat veranderd niks dat we ze toonde wel klopt volgesnde reviews. zou misschien kunnen, kan je moeilijk nu rekening mee houden. is pure speculatie. bevat ook veel "" up to "" dat is iets wat amd en nvidia altijd gebruiken. en zoals aangetoond die 1. 5 en 1. 7 klopt gewoon. dat was het wel, gemiddeld 60 % winst zou bijna ongeevenaard zijn, en dan nog eens bakken goedkoper dan nvidia? sorry men zegt altijd als iets te mooi is om waar te zijn... de performance winst van de 7000 series is on par met winsten ervoor en zelfs een pak hoger in rt. je blijft praten over teleurstellingen, wat uitmaakt is wat mensen kopen en in dat opzicht is voor deze prijs de 7900 xtx gewoon de betere keuze. het is dat of ultra high end gaan met compleet van de pot gerukte prijzen. dat het gemiddelde maar tegen de 40 % is en niet 50 % is maakt darin weinig uit, in rt klopt dit wel trouwens daar was amd gewoon correct. is beetje absurd, amd weet zelf wel gerust hoe hun bedrijf te runnen. err is weinig reden voor amd om zichzelf te benadelen. tegen dat deze kaarten in mensen hun handen zitten zijn die handvol bugs er heus wel uit, reviewers zijn soms daarin veel te voortvarend en zoeken gewoon issues om toch maar aandacht te krijgen. het kabel debacle van nvidia is het beste voorbeeld, uiteindelijk was het gewoon user error nadat reviewers mensen bang lagen te maken dat hun huis ging afbranden. het is en blijft gewoon momenteel de beste kaart in dat prijs segment "" het had beter gekund "" tja het kan altijd beter, geen enkele kaart is ooit perfect. doen alsof dit zo eenvoudig is is gewoon verkeerd. soit het onderste segment is het belangrijkste we zullen zien wat amd en nvidia daarin brengen."
RX 7900 XTX;2;0.450834721326828;We gaan het er duidelijk niet eens over worden. Onze meningen verschillen gewoon en dat mag. Ik vind het zelf een beetje onnodig om @ launch zo veel bugs in de kaart te hebben zitten. Zeker als de kaart dan ook nog eens 1000 euro kost. Dat geld ook voor nVidia. Die power connector problemen zouden ook niet mogen gebeuren die had ook beter ontworpen moeten worden.
RX 7900 XTX;2;0.4823334813117981;"Idd die ""bugs"" zijn veelal iets persoonlijk voor jou en maken weinig uit voor de meeste of komen zelfs niet veelvuldig voor, performance en prijs is daar dat is het belangrijkste voor de meeste."
RX 7900 XTX;5;0.3546141982078552;En nu worden de xtx aangeboden voor 1.499 bij Alternate 😂
RX 7900 XTX;5;0.24069362878799438;Yeah winkels gaan nu aan het scalpen omdat ze populair zijn
RX 7900 XTX;3;0.4360993504524231;Het is volgens hardware unboxed de beste price to performance op MSRP op dit moment. Bizar dat een flagship deze titel heeftil, ze verleiden mij bijna tot koop. Maar daar waren ze er ook al negatief over de kaart. Ik ben juist onder de indruk dat mn voorspelling daarover klopte. Alleen energyverbruik valt beetje tegen. Dat was ook echt beter voorspelt vs 4080. Ben benieuw hoe ze downscalen in lagere wattages.
RX 7900 XTX;5;0.47788164019584656;Er zijn altijd negatieve zaken op te merken, er zijn weinig kaarten die perfect zijn. Het feit blijft dat high end is dit de beste kaart die er momenteel is voor de meeste gamers.
RX 7900 XTX;1;0.37739866971969604;Waar heeft u het over??
RX 7900 XTX;1;0.5282890200614929;Welk stuk bedoel je? en wat is er niet duidelijk?
RX 7900 XTX;1;0.7657938003540039;Die prijzen beginnen ook gewoon echt absurd te worden, ik ga niet meer beginnen aan NVidea. Ik heb heel lang getwijfeld om mijn 1080 Ti te gaan upgraden naar een 3/4000-series of de 7900 XTX, uiteindelijk vanwege de kosten maar niet gedaan i.v.m. de hogere prijzen tegenwoordig. Uiteindelijk heb ik halverwege November een mooie 6900 XT kunnen bemachtigen bij Azerty voor slechts 731 euro. En ook de prijs van deze kaart fluctueert alweer rond de 1000 euro: uitvoering: ASUS TUF Gaming Radeon RX 6900 XT (Hoe snel kan het gaan met de prijzen).
RX 7900 XTX;1;0.9131137728691101;Gewoon niets meer kopen. Dat is de enige manier om de prijzen weer normaal te krijgen. Zolang ik geen kaart meer kan kopen dat tussen de 200 en 300 euro kost waarmee je redelijk een nieuw spel kan spelen, game ik niet meer op de PC. Dan gaan we wel over op een Xbox of steamdeck
RX 7900 XTX;3;0.3129662275314331;U vraagt, wij draaien. De RX 6600, daar kan je prima nieuwe games op spelen. Voor slechts €269 is die voor jou. En met een beetje geluk krijg je met de nieuwe generatie nog meer bang voor je buck. Desalniettemin ben ik het volledig met je eens: er zijn prima andere opties dan een nieuwe GPU kopen. Ik kan nog steeds alles spelen met mijn 1070 en die is tweedehands €175. Heck, met een goede APU kan je ook de tijd overbruggen.
RX 7900 XTX;2;0.6080358624458313;Mijn 1070 recent overclocked en nu draait ie de meeste games op 1440p weer prima rond de 60fps. Zou het wat beter kunnen, sure, maar ik vind het de investering nog echt niet waard. Wellicht als de 7700 of 7600 uit komt en niet te veel kost...
RX 7900 XTX;1;0.4137742221355438;En dan te bedenken dat het gaat om de adviesprijs hè. Leuk om te lezen dat jij ook eentje bent die nog net een 6900XT voor een zacht prijsje heeft kunnen bemachtigen: het is mij ook in november gelukt, namelijk 699€(!!!!) bij Megekko. Misschien was die prijs wel een fout van een algoritme want na mijn bestelling klom hij nagenoeg direct weer richting de 800€. Edit: betreft die van AsRock.
RX 7900 XTX;2;0.35734567046165466;"Nvidia.. Ik zie veel comments hier over de hoge prijzen van de 4xxx Nvidia kaarten, maar let wel: je gaat met een dergelijk kaart waarschijnlijk wel 3 a 4 maal zo lang doen als je ooit met een videokaart hebt gedaan. let me explain; DLLS 3.0 heeft frame interpolatie. Oftewel; in 2030 zul je ( hoe zwaar de games ook worden ) nog steeds op 120+ frames kunnen gamen. 4K zal nog heel lang het maximale mainstream blijven. Al zakt de native framerate naar 24 frames, interpolatie lost dat gewoon op, en interpoleerd met gemak naar 120+ frames. TV's doen dat al jaren. Ik ben echt een enorme liefhebber van hoge framerates, en heb juist doormiddel van frame interpolatie vele games kunnen spelen die ik anders nooit zou hebben gespeeld op mijn PS4 en Switch. Mijn LG tv bijvoorbeeld interpoleerd naar 120 frames, en dat is echt brilliant. Enigste nadeel van mijn LG TV is dat het Lag geeft, en wat beeld anomalies, maar dat heb ik er prima voor over. ( games als RDR2, TLOU, Tomb Raider etc zijn prima te spelen met een klein beetje extra lag ) De DLLS 3.0 techniek van Nvidia gaat dat echt veel beter doen. ( neem ik aan ) Dat betekend dat ( als je met de eventuele nadelen van interpolatie kan leven ), je oneindig door kan gaan met een dergelijke kaart. Ik heb mijn keuze al gemaakt, en ga voor een 4080/4090. ( heb huidig een 3080 ). Het verbaasd mij dat deze techniek uberhaupt zo lang op zich heeft laten wachten. Ik heb het jaren geleden al voorspeld dat deze techniek naar GPU's zou komen. En nu, met de huidige AI mogelijkheden, ben ik er van overtuigd dat dit echt de oplossing voor langdurig gebruik van GPU's gaat brengen. Lag-free that is.."
RX 7900 XTX;4;0.4070769250392914;Erg nette performance. Op RT na vallen beide kaarten me enorm mee. Tenzij Nvidia het licht ziet en de 4080 wat normaler gaat prijzen kies ik (en denk ik velen met mij) dit keer gewoon voor AMD.
RX 7900 XTX;5;0.48087772727012634;Herkenbaar. Ik zit zelf met een 2080 TI en wil graag wat soepeler op 4k spelen maar een speciale ATX 3.0 voeding, een power connector die problemen kan geven het het idiote bedrag wat Nvidia durft te vragen skip ik die. Ik ben heel benieuwd naar de prijzen iig!
RX 7900 XTX;3;0.43169453740119934;Je hebt geen ATX 3.0 voeding nodig voor de RTX 4x00. Wel een verloopkabeltje.
RX 7900 XTX;3;0.5674500465393066;moeten niet nee maar gezien de hoeveelheid stroom dat ding trekt is het wel handig om je PSU te helpen met info over de attached hardware. Daarnaast wil je niet het risico lopen met een 2200 euro kostende videokaart.
RX 7900 XTX;2;0.31394627690315247;Volgens mij blijkt nergens uit dat dit nodig zou zijn
RX 7900 XTX;3;0.3642212152481079;Het is ook geen eis nee. dat heb ik nooit gezegd. Wel zijn er al tal van voorbeelden van afgefikte kabels dus voorzichtigheid is wel gepast.
RX 7900 XTX;2;0.42811715602874756;Voorzichtig zijn kan zeker geen kwaad, maar voor zover ik zie zijn er ook met ATX 3.0-voedingen smeltproblemen. Ik wil maar zeggen: er lijkt me niks dag zo'n nieuwe voeding zelfs maar nuttig maakt
RX 7900 XTX;5;0.3713255226612091;xtx vandaag bij alternate voor de vriendenprijs van 1499 😂
RX 7900 XTX;3;0.41212770342826843;Ik sta op het punt een MSI 4080 af te rekenen voor 1500. Gezien de prijzen van een 6900XT etc. zal een 7900XT(X) toch niet veel goedkoper uitvallen lijkt me?
RX 7900 XTX;1;0.5449605584144592;Neem dit met een korrel zout gezien de instabiele Euro, maar ga als maximale prijzen hier vanuit: De adviesprijs van de RX 7900 XT komt uit op 899 Amerikaanse dollar, de RX 7900 XTX krijgt een adviesprijs van 999 dollar. Omgerekend naar euro's is dat met de huidige wisselkoers inclusief btw momenteel 1110 euro voor de RX 7900 XT en 1233 euro voor de RX 7900 XTX.
RX 7900 XTX;3;0.5300113558769226;Het gaat op dit moment wel weer wat beter met de euro, omgerekend is de 7900XT €1036 en de 7900XTX €1151. Nog steeds niet goedkoper dan een 6900XT, maar niet zoveel duurder.
RX 7900 XTX;5;0.47839027643203735;1499 alternate vandaag 😂 ze mogen hem houden..
RX 7900 XTX;1;0.5224242210388184;"Wat je noem je ""veel goedkoper""? Is 300 euro goedkoper veel? Als je een 6950XT voor 899 op voorraad kan krijgen, dan lijkt 1200 euro voor een 7900XT(X) prima haalbaar. Ik zou als ik jou was een paar dagen wachten tot je de echte prijzen gaat zien. En of Nvidia of de winkels nog iets aan de prijs van de 4080 gaat doen. Want die is echt te hoog voor wat ie levert en winkels komen er niet vanaf. Dus die prijs zal echt wel gaan zakken."
RX 7900 XTX;5;0.2419336438179016;De 6900xt kan je voor 850 euro halen..
RX 7900 XTX;1;0.5396453738212585;En van de week voor 50 euro meer de 6950XT.
RX 7900 XTX;1;0.3017394542694092;Ook ver boven advies dan, toch?
RX 7900 XTX;1;0.4839319586753845;Nee? Adviesprijs voor de verlaging was 1000 euro
RX 7900 XTX;1;0.4869624078273773;Ik ben geen Google pagina. Nog los van dat je originele comment helemaal niet over adviesprijzen ging.
RX 7900 XTX;3;0.33876490592956543;700 dollar. Voor het gemak: 700euro+belasting, dan zit je er ongeveer.
RX 7900 XTX;1;0.6464839577674866;Er is voor NL zelfs nog geen officiële verlaging van de 6900XT of 6950XT, die kosten bij AMD nog altijd €1163 en €1280 respectievelijk ( ), wat als je dit terugrekent naar dollars excl btw, laat zien dat AMD voor NL nog altijd $999 als MSRP voor de 6900XT reference hanteert en $1099 voor de 6950XT, in plaats van de verlaagde MSRP's die je in de US ziet. Terwijl wanneer AMD de MSRP's officieel zou hebben aangepast voor NL, je die veranderding daar normaliter direct ziet.
RX 7900 XTX;1;0.4990468919277191;@SHiNeye Wat probeer je te doen? Eerst ging het nog over de prijs vs de 4080, dat argument ben je kwijt, begin je maar over de adviesprijs, dat argument ben je kwijt, begin je maar over latere adviesprijzen, ook daar heb je geen gelijk? Bestel gewoon wat je wilt dat is je goed recht en dat is jou zaak, maar vervolgens je keuze hier proberen te verantwoorden met loze argumenten (en met welk doel?) heeft niemand wat aan.
RX 7900 XTX;3;0.3665408194065094;Gaat me erom dat ik me niet kan voorstellen dat een 7900XT(X) voor veel goedkoper zal worden dan een 4080.
RX 7900 XTX;5;0.2990112900733948;op AMD website is de 7900 XT weer op voorraad voor 1050
RX 7900 XTX;1;0.3332327604293823;Ah dat is geen verkeerde prijs.
RX 7900 XTX;3;0.41947466135025024;De 7900xtx gaat echt voor een stuk minder over de toonbank morgen
RX 7900 XTX;1;0.4263724982738495;Ben benieuwd. De 4080 kan altijd nog retour. Hopelijk test iemand de 4080 vs 7900XT incl DLSS, want dat zou ik sowieso aanzetten.
RX 7900 XTX;1;0.3710283041000366;Als FSR net zo goed kan upscalen als DLSS wil ik die vergelijking best zien ja. Heb tot op heden alleen DLSS gebruikt, en heb begrepen dat FSR nog niet op het niveau van DLSS zit. En hoezo spammen? Discussie voeren is in jouw ogen spammen?
RX 7900 XTX;1;0.3398294746875763;In dat artikel wordt toch ook gezegd dat FSR nog niet op het niveau is van DLSS..
RX 7900 XTX;3;0.323510080575943;Dat klopt maar het is duidelijk dat je op zoek bent naar een Nvidia kaart, dat doorziet @d3x prima en daar komt de ergernis dan ook vandaan dat iemand zogenaamd geïnteresseerd komt doen. Ik had hiervoor de 3080 en een 6900XT en gezien ik geen upscaling wil gebruiken heb ik juist de AMD gehouden. En voor de prijzen die ik verwacht ga ik vanmiddag hopelijk een 7900XTX op de kop kunnen tikken en later misschien nog een 4080 dan kan ik ze weer vergelijken en heb ik weer hetgeen het beste is voor mij
RX 7900 XTX;4;0.2846148908138275;
RX 7900 XTX;3;0.5513821244239807;Is nog afwachten. Voor mij is het ray tracing performance gat te groot dus vooralsnog blijf ik bij nvidia.
RX 7900 XTX;2;0.30922824144363403;Hoe relevant is Ray Tracing nog, nu AMD samenwerkt met Epic Games voor de Unreal engine en daar de techniek Lumen bij ingebakken zit?
RX 7900 XTX;1;0.7319632768630981;Ray tracing is überhaupt een niche binnen een niche. Dure videokaarten van meer dan pak 'em beet 600 euro is al een niche opzich, volgens mij minder dan 5% van de gamers op steam bijvoorbeeld heeft zo'n kaart. Vervolgens is van die groep die ook echt met raytracing aan speelt ook weer een kleine groep. De meeste die 800 euro voor een videokaart hebben betaald willen ook graag 100fps behalen in de spellen die ze spelen.
RX 7900 XTX;3;0.43308258056640625;Lumen heeft een high-quality mode waar ze wel hardware Ray Tracing gebruiken. Dus ik zou juist zeggen het hangt af of je quality op max wil hebben of niet. Zelfde met de 'next-gen' Witcher 3 update, zie ook Als je de de software ray-tracing van Luman van plan bent te gebruiken, dan is wel compute belangrijk, iets wat veel reviewers nog niet goed testen.
RX 7900 XTX;3;0.3924287259578705;Interessant, ik wist niet dat er ook een hardware RT modus was van Lumen, dank.
RX 7900 XTX;3;0.4907322824001312;Dat hangt van de toepassing af. Lumen's voordeel, minder krachtige hardware nodig, begint nu wel een een beetje te vervagen met deze generatie kaarten. En als je bijvoorbeeld grafisch werk doet, is Ray Tracing toch echt beter. En de Unreal Engine is natuurlijk niet de enige engine. Het aantal spellen dat Ray Tracing ondersteunt is gewoonweg hoger.
RX 7900 XTX;3;0.4010511040687561;Nou ja, dan moet je die kaarten wel kunnen of willen betalen natuurlijk, dus vervagen doet het voordeel m.i. niet, maar het klopt dat UE niet de enige engine is. Dan kun je afwegen of je de games op die andere engines erg belangrijk vindt of niet.
RX 7900 XTX;1;0.6128146052360535;Ik word helemaal niet warm van RT.
RX 7900 XTX;3;0.7218179702758789;Best wel relevant.. Ik heb o.a. Control en Cyberpunk 2077 met Raytracing aan gespeeld.. Ziet er echt wel een stuk mooier en realistischer uit.. Vooral de reflecties in glas zijn een zeer duidelijk verschil..
RX 7900 XTX;2;0.5233884453773499;Het ziet er uiteraard veel mooier uit, maar die games zijn sowieso al niet in Unreal Engine gemaakt. Ik had het echter specifiek over UE's Lumen omdat die techniek m.i. goed genoeg in de buurt komt van RT zonder de enorme performance hit en bijkomstige stookkosten.
RX 7900 XTX;5;0.5902824401855469;Dit is de realiteit! uitvoering: Sapphire AMD Radeon RX 7900 XTX 24GB
RX 7900 XTX;5;0.702434778213501;Ziet er goed uit AMD. Top. Ben benieuwd wat Nvidia zijn reactie is. Zeker omdat de XTX soms echt op de hielen van 4090 zit, en soms zelfs sneller. en dat voor de voor de helft van het geld (RT even uitgesloten)
RX 7900 XTX;3;0.2675885260105133;Ik denk dat je de 4080 bedoelt? De 4090 steekt overal met kop en schouders boven uit (prijskaartje is er ook naar). Dan is dat nog steeds een hele goede prestatie van AMD
RX 7900 XTX;3;0.47070273756980896;Heb je wel naar alle benchmarks gekeken? genoeg benchmarks waar ze boven de 4090 uitsteken. Dus als de 7900 serie stuk beter betaalbaar is dan word het voor de gemiddelde tweaker een makkelijke keuze
RX 7900 XTX;3;0.3544376492500305;Ik zie van de 10 games alleen COD hogere FPS halen dan de rest
RX 7900 XTX;3;0.39917823672294617;Volgens mij pakt ie de 4090 op lagere resolutie wat vaker, maar er is denk ik niemand die een 4090 koopt om op 1440p te gaan gamen
RX 7900 XTX;2;0.437995582818985;Tenzij je echt voor één spel gaat, kijk je toch naar gemiddelden? En als er genoeg benchmarks zijn waar hij boven de 4090 uitsteekt, en hij gemiddeld op ongeveer 4080 niveau uit komt, betekend het dus dat er ook genoeg benchmarks zijn waar de 4080 er ruim bovenuit steekt. (En dat lijkt één spel te zijn die getest is, maar overall is het gewoon in de buurt van een 4080 zonder ray tracing, meestal iets erboven).
RX 7900 XTX;3;0.4810764491558075;En dan is het nog niets niet eens een full die. Zeker knap van Nvidia. Maar ik denk dat AMD het tog wat slimmer doet kwa prijs prestatie verhouding . Ik wil graag een 4090 maar mijn max is tog echt wel 1500.
RX 7900 XTX;3;0.2992742359638214;Ik doelde op AMD die het goed doet - aangepast
RX 7900 XTX;3;0.3833727240562439;Ik ook. Alleen we kunnen de ongelooflijke prestaties van de 4090 natuurlijk niet negeren. Prijs moet alleen normaal.
RX 7900 XTX;2;0.511064350605011;Het zouden pas ongelooflijke prestaties zijn als er een normale prijs aan zou zitten Dat die prijs omlaag gaat kun je op je buik schrijven. Van de 4080 waarschijnlijk wel, omdat ie dezelfde prijs prestatie van de 4090 heeft, maar nu veel te zware concurentie van AMD krijgt. Maar de fans blijven toch wel 4090s kopen, omdat het verschil met AMD zodanig is dat ze het voor zichzelf kunnen rechtvaardigen toch de snelste kaart te kopen. Ook al is ie belachelijk duur. En omdat de prijs/prestatie gelijk is bij de 4080 is het makkelijker te rechtvaardigen de snelste kaart te kopen. In het verleden moest je voor 10% extra prestatie 50% extra prijs betalen. Dan dachten mensen wel twee keer na. Maar als 10% extra prestatie 10% extra prijs is, dan word dat sneller gedaan. Uiteindelijk is het meestal niet zo is, dat die 10 of 20% de doorslag geeft. Het is meer of iemand het waard vind om dat bedrag uit te geven. Niet of ze dat bedrag ook daadwerkelijk hebben.
RX 7900 XTX;1;0.6261849403381348;Ik geloof er geen reet van dat die kaart goed verkoopt laat je vooral hypen door Nvidia. Ik ben ook een grote fanboy van Nvidia en ik koop hem dus echt niet voor 2k. En volgens mij is er al een pricedrop van Nvidia van 5% ofzo geweest. In deze economie heeft niet iedereen zoveel geld over voor een videokaart. AMD heeft al het licht gezien met hun nieuwe processors en een enorme pricedrop ingevoerd. En ik verwacht hetzelfde van Nvidia. En zoniet mijn 3090 op water draait lekker op 2000mhz stabiel🫡
RX 7900 XTX;3;0.36565327644348145;Die 5% was volgens mij (meer) prijscorrectie dan prijsverlaging.
RX 7900 XTX;1;0.5216016173362732;Ja en het exorbitante verbruik ook niet. 😉
RX 7900 XTX;3;0.5458782315254211;Tja. Het is persoonlijk maar ik zie RT wel echt als een belangrijke feature voor next gen games. Voor rasterized only op hoge resoluties volstaat de vorige generatie vaak ook nog wel.
RX 7900 XTX;4;0.4864300489425659;Ideale kaarten voor e-sports waarbij hoge refresh rates nodig zijn. Echter 4k ultra met raytracing dan zit je bij nvidia beter. En de efficiëntie van de 4090 is uitstekend. A plaque tale: requiem trekt 580 watt uit de muur stock 2800mhz@1100mv, maar zodra ik ga knijpen naar 2500mhz@850mv zakt dit naar 430 watt! Uiteraard lever ik 5-10fps in maar dit is nog steeds sneller dan de 7900XTX
RX 7900 XTX;2;0.4475215971469879;Maar tegen welke prijs. De 4090 is gewoon te duur.
RX 7900 XTX;3;0.2779829204082489;Iets is te duur als je een product koopt en er helemaal geen gebruik van maakt. Met de korte en koude dagen maak ik er nu enorm veel gebruik van, dus ik vind mijn geld goed besteed en heb er plezier van. Ik kan begrijpen dat het voor meerdere mensen buiten budget valt en te duur wordt bestempelt.
RX 7900 XTX;3;0.5827513933181763;Iets duur vinden heeft niet standaard te maken met dat het buiten budget valt. Je kan ook iets duur vinden omdat je normaal 600-800 euro betaald voor een high end kaart en dat het nu 1500-2500 euro is. Mijn budget is prima, alleen zegt mijn hoofd zoiets van....tot hier en niet verder.
RX 7900 XTX;3;0.4156314730644226;price/performance/verbruik vind ik een belangrijkere maatstaaf dan max fps. Dat is voor iedereen natuurlijk anders. Plus het feit dat Nvidia denk nog steeds pre crypocrash prijzen te kunnen vragen. Dan is het gewoon een principe kwestie
RX 7900 XTX;4;0.5081532001495361;Goede business case voor een tandarts
RX 7900 XTX;3;0.5368122458457947;Nette review. En inderdaad een fijne mooie nieuwe lancering. Voor de redactie echter wel de vraag . . . in kader van 1 is geen check ik natuurlijk ook andere benchmarks: Kunnen jullie bij de testen specifiek de instellingen zetten? Raytracing wel aan, niet aan, DLSS / FSR wel of niet aan, wel of geen SAM / RBAR? Vind het namelijk opvallen dat sommige benchmark cijfers een redelijke afwijking hebben met andere reviews.
RX 7900 XTX;2;0.3024541735649109;Rbar staat bij ons altijd aan. Rtx of niet staat vrij duidelijk in de naam van de benchmarks. We hebben fsr en dlss uit staan tenzij expliciet aangegeven. Voor deze tests niet gebruikt. Verder is het ook belangrijk op te merken dat onze systemen draaien op 5500 all cores, altijd. De e-cores zijn uitgeschakelt. In sommige games is dat een voordeel, in sommige een nadeel. Het geheugen draait op 7200MT/s. Ook wat sneller dan de meeste publicaties volgens mij.
RX 7900 XTX;3;0.4877794682979584;Hmmmm interessant. Dank voor de extra toelichting. Viel mij op dat in sommige gevallen de max fps zowel 30 negatief als positief uitvallen.
RX 7900 XTX;3;0.5582082867622375;Tenzij je de exacte instellingen weet van de games en het systeem blijft het toch altijd lastig om te vergelijken.
RX 7900 XTX;5;0.262753963470459;Goeie vraag, ik dacht hetzelfde.
RX 7900 XTX;3;0.42452332377433777;Best jammer dat usb-c(video) maar niet wil doorslaan bij desktops en alleen van toepassing bij laptops.
RX 7900 XTX;3;0.5180385112762451;Wat voor voordeel brengt het dan voor desktops? Hooguit zou je dan de monitor als USB hub kunnen gebruiken, maar dan gaat je controller wel over de videokaart heen (qua bandbreedte lijkt me dat ook niet ideaal). Als je het wil gebruiken om de monitor van voeding te voorzien, dan zouden de videokaarten met nog meer stroom aansluitingen moeten komen om dit aan te kunnen.
RX 7900 XTX;3;0.434063196182251;Ik sluit er regelmatig een portable monitor op aan en baal er wel van, de Intel iGPU heeft het gelukkig wel, het is een kleine moeite om erop te bouwen, ze hebben ook een audiochip op gebouwd dan kan usb-c ook gezien dat de standaard gaat worden bij steeds meer monitoren.
RX 7900 XTX;3;0.3443548381328583;De 7900XT (en ook de 7900 XTX) heeft wel een usb-c aansluiting (ik dacht dat je bedoelde enkel usb-c aansluitingen).
RX 7900 XTX;3;0.5944229364395142;Een fijne review om te lezen en gezien te hebben. Echter snap ik echt niet waarom er niet voor een AMD systeem gekozen is i.v.m. Smart Access Memory (kan niet worden geactiveerd op Intel systeem). Als ik dit aanzet kan het in sommige spellen een enorme boost geven van 0-50%. Kan toch echt wel een verschil maken. Over Ray Tracing ben ik het persoonlijk met de reviewer eens. Ik vind de performance hit (nog) te groot voor de visuele uplift die het nu brengt. Neemt niet weg, dat het in de toekomst steeds beter en mooier zal worden en blijven doorontwikkelen wel gewenst is.
RX 7900 XTX;1;0.3459101617336273;Waarom zou je op een Intel geen SAM kunnen aanzetten? Ik heb bij mijn Intel systeem met AMD 6900XT gewoon Re-Size BAR Support. Dat is volgens mij namelijk hetzelfde als SAM.
RX 7900 XTX;2;0.3817698061466217;Je kan het inderdaad activeren in de bios van Intel. Echter van wat ik overal lees heb je smart acces memory alleen als je combinatie Radeon / Ryzen hebt (althans dat zegt AMD). Als je Alt + R drukt en naar prestaties en dan naar afstemmen gaat staat SAM onderaan dan rood en het vinkje aan?
RX 7900 XTX;3;0.2455073744058609;Yup. Misschien een manier om klanten AMD CPU's te kopen.
RX 7900 XTX;4;0.5093337297439575;Bedankt voor je aanvulling, want het wordt inderdaad anders gebracht online (zelfs bij LTT tijdens de review van de 7900 xtx). Fijn dat het ook kan worden geactiveerd op een Intel platform aangezien het soms flinke verbeteringen geeft.
RX 7900 XTX;5;0.6031951308250427;Klopt, het is bij mij ook echt merkbaar. Zowel in games als in synthetische benchmarks zoals timepsy.
RX 7900 XTX;5;0.5468804836273193;Inderdaad ik merkte na het activeren van RBAR in de bios en daarmee SAM, dat sommige spellen nog beter liepen. Echt een hele fijne kaart en alles draait super vloeiend. Ik wist niet dat het ook invloed had op synthetische benchmarks. Net de normale Timespy gedraaid en heb een graphics score van 22000. Mag ik vragen wat jij haalt?
RX 7900 XTX;2;0.35279592871665955;SAM ofwel resize bar is geen AMD exclusieve feature maar hebben hooguit weer onder de aandacht gebrachr dat het bestaat. Het zit namelijk al vrij lang in de pcie spec.
RX 7900 XTX;4;0.4247595965862274;Goed om te weten en bedankt voor de aanvulling. Als ik wat reviews bekijk lijken de AMD GPU's er gemiddeld meer baat bij te hebben dan Nvidia, maar dat maakt voor deze reviews niet meer uit. Er vanuit gaande dat dit nu standaard bij reviewers aan staat.
RX 7900 XTX;5;0.35826802253723145;Jep. Bij ons staat het standaard aan. We controleren regelmatig of het niet per ongeluk uit is gegaan
RX 7900 XTX;1;0.4128377437591553;Dan heb je een andere review gekeken dan ik. LTT zegt duidelijk dat AMD met hun marketing slides iedereen wil laten geloven alsof het alleen maar bij AMD cpus werkt, maar Linus geeft zelf ook aan dat het ook met Intel werkt.
RX 7900 XTX;3;0.3132918179035187;Is er een energie zuinige kaart wat makkelijk games op ultra kan spelen op 1440 resolutie? Heb momenteel een 1080 TI
RX 7900 XTX;1;0.3826448619365692;Je kan volgens mij gewoon een RTX 4090 downvolten.
RX 7900 XTX;1;0.5989675521850586;Ja klopt maar waarom zou ik een hele dure kaart kopen, undervolten en eigenlijk een onderpresterende 4090 willen hebben? Die kaarten kosten meer dan 2000 euro..
RX 7900 XTX;5;0.37760961055755615;"Als je energie zuinig wil zijn is dit denk ik de beste optie. Nu snap ik dat dit voor bijna niemand het geld waard is. Maar je kan het zelfde doen met alle andere kaarten. Ze zijn dan of zuiniger dan je RTX1080 TI of presteren beter. Zonder undervolten heb je denk ik gewoon pech ;-)"
RX 7900 XTX;5;0.2762593626976013;Omdat een 4090 op 250 watt veel betere performance per watt en veel stiller is dan een goedkopere kaart.
RX 7900 XTX;1;0.3706739544868469;Waarom zou je een 4090 willen hebben voor 1440P?
RX 7900 XTX;4;0.31864896416664124;Raytracing op hoge framerates
RX 7900 XTX;1;0.3717119097709656;Welke kaart zou ik dan nemen voor 1440p 165hz?
RX 7900 XTX;5;0.24118585884571075;6800xt/6900xt
RX 7900 XTX;1;0.3934994041919708;Ik lees op meerdere review sites dat ze veel last hadden van coil whine. Hoe zat dat bij jullie?
RX 7900 XTX;3;0.714959442615509;Best heftig aanwezig in veel gevallen. Is wel een beetje fps afhankelijk. Boven de 100 fps was het vaak duidelijk aanwezig.
RX 7900 XTX;1;0.6024584770202637;Ik ben benieuwd of dat op den duur wegtrekt. Mijn R9 Fury had het namelijk ook significant. Maar na een half jaartje ofzo was er niks meer van te horen. Nooit gedacht dat een GPU ingespeeld moet worden.
RX 7900 XTX;1;0.37673813104629517;"Coilwhine is bij mij (Ik had toendertijd een Geforce GTX 970) ""verdwenen"" toen ik een andere PSU had aangeschaft. En eigenlijk ben ik nooit meer een kaart tegen gekomen dat Coilwhine had. Sindsdien heb ik een GTX 1070, RX 580, Vega 56, RTX 2070, RX 5700X, RX 6700XT en RX 6800XT gehad."
RX 7900 XTX;3;0.5563618540763855;Het is niet altijd het geval. Mijn RNDA2 reference kaarten hebben het altijd gehouden. Wat wel hielp was undervolten want dan werd de toon veel minder hoog en scherp en hij werd ook minder luid. En daar naast FPS cappen want hoe hoger de FPS hoe luider en aangezien mijn monitor toch maar 144Hz is hoeft hij ook niet hoger te komen. Op die manier had ik er weinig last van. Maar het verschil een beetje van kaart tot kaart. De een heeft het erger dan de ander ookal zijn het bv alle drie reference 6800 XT's.
RX 7900 XTX;3;0.421945184469223;Mijn 6900XT Red Devil doet na iets meer dan een jaar ook nog steeds coilwhine. Hangt erg van het spel af, ook van de FPS, maar in mijn ervaring ook niet altijd. Ik game namelijk op 60fps4k. En daar heb ik ook nog bij best veel spellen coilwhine. Ik heb mijn GPU bios well op silent gezet. Maar volgens mij maakt dat niet super veel uit. Ik speel de laatste tijd wel wat Civilization VI, daar met max settings 4k60fps heeft de kaart ook nog best wat coilwhine, pitch veranderd naarmate de zoom en waar je naar kijkt. Is irritant, zeker zonder koptelefoon op. Maar goed wat doe je eraan. Volgens mij doet bijna elke moderne kaart het wel tot een behaalde hoogte.
RX 7900 XTX;1;0.6741968393325806;Voor prijzen die ook ruim boven de €1000,- zullen liggen gaan deze videokaarten qua verkopen net zo hard floppen als de RTX 4080 nu al doet.. Weinig mensen zullen een AMD videokaart kopen voor zoveel geld.. En gelukkig maar, want dit moeten we als consumenten gewoon niet meer pikken na ruim 2 jaar ellende met videokaartprijzen.. De tijd van enorme vraag is nu de Covid19 crisis en de crypto mining hype voorbij zijn toch echt voorbij.. Dat moet alleen nog even doordringen bij Nvidia en ook AMD die nog steeds denken met dit soort veel hogere prijzen weg te komen.. Deze AMD kaarten hadden maximaal 700-800 euro moeten kosten, waar de RTX 4080 rond de 800-900 had moeten zitten.. Ze zijn nu allemaal rond de 400-500 euro te duur..
RX 7900 XTX;3;0.4394235908985138;4080 is een pak duurder voor een pak minder performance. Denk dat deze beter gaan verkopen dan 4080 hoor, eerder in lijn met de 4090 omdat die qua prijs/performance overeen komen.
RX 7900 XTX;2;0.45368126034736633;3-5% meer performance noem ik geen pak meer. Reken je RT mee dan is de RTX 4080 gemiddeld wel een pak meer sneller en wat zuiniger. AMD is deze keer echt niet nagekomen wt ze hadden beloofd. 55-70 % halen ze niet dat ligt rond de 30-40 %
RX 7900 XTX;3;0.6081491112709045;Ik verwacht nog wel wat hogere fps bij optimalisatie. Paar games vallen echt beetje buiten de boot.
RX 7900 XTX;3;0.5115710496902466;Ik verwacht ook nog wel optimalisaties. Al .oet ik zeggen dat Nvidia ook nog zit met driver overhead. De RTX 4090 word nooit helemaal benut, ik weet niet of dat ook voor de RTX 4080 geldt. De RX 7000 serie presteerd ook zeker niet slecht en heeft een betere prijs prestatie verhouding.
RX 7900 XTX;1;0.6935046911239624;Het gaat toch echt nooit meer 100% worden wat het was hoor. We zitten al 2 jaar met een inflatie van +10%.
RX 7900 XTX;1;0.6313797831535339;1. De inflatie is pas sinds juli 2022 boven de 10%, en dan vooral alleen maar vanwege gestegen energieprijzen.. 2. Dat is geen excuses voor een nog veel hogere stijging van de adviesprijzen van videokaarten.. Vooral Nvidia maakt het erg bont met hun RTX 4080.. Met een adviesprijs van €1469,-.. De RTX 3080 had een adviesprijs van €719,-.. Dat is dus meer dan een verdubbeling, meer dan 100% zogenaamde inflatie.. De RTX 4080 had hooguit rond de €900 moeten kosten, en die AMD kaarten nog daaronder..
RX 7900 XTX;1;0.48093628883361816;Hoe berekent AMD de FLOPS? Ik snap niet helemaal hoe ze aan die 61,6 TFLOPS komen. Het is immers streamprocessors x kloksnelheid x 2 / 1.000.000. Dan kom ik bij de 7900 XTX uit op 28,26 TFLOPS. Nu heeft RDNA3 net als Nvidia tegenwoordig per core een extra set integer/floatingpoint processors, maar dan nog kom ik niet verder dan 56,52 TFLOPS. Bij de 7900 XT kom ik uit op 21,5/43 TFLOPS, en niet op 52 TFLOPS.
RX 7900 XTX;1;0.37061503529548645;"Tweakers heeft de ""gameclock"" opgelijst. Da's zover ik begrijp de gegarandeerde basissnelheid. Daarnaast is er nog het opportunistisch klokken naar een ""boostclock"". Die is 2500MHz voor de XTX en 2400MHz voor de XT. Komt het dan wel uit?"
RX 7900 XTX;3;0.4573669731616974;Ze worden aardig aantrekkelijk maar ik ben benieuwd wat de RTX 4070 TI gaat presteren en kosten....
RX 7900 XTX;1;0.4402170181274414;3090 voor 800/900. oftewel teveel geld voor een xx70 klasse kaart.
RX 7900 XTX;1;0.6065467596054077;Ik ben van plan morgen een 7900xtx (reference model / Founders edition?) te halen. Maar waar komen deze beschikbaar? Shop van AMD zelf? Notebooksbilliger? Begreep dat hij om 3u smiddags wordt gelaunched? Ik lees hier dat ie voor €1160,- verkocht gaat worden. Waar komt die informatie vandaan? Nog tips om er een te kunnen scoren?
RX 7900 XTX;2;0.3489764928817749;Zou je dat echt doen? Veel klachten over coil whine, zou je denk it beter n boardpartnerkaartje kunnen kopen
RX 7900 XTX;4;0.400851845741272;Performance ziet er goed uit, zeker $ per frame. Zijn er tips of is er al informatie waar deze kaarten morgen beschikbaar zullen zijn in Nederland. Daarnaast al iets bekend over de prijzen in de EU (ook niet onbelangrijk) ?
RX 7900 XTX;2;0.35574862360954285;Zal zeker wel rond de 1200 euro zitten, megekko, azerty etc. Zitten allemaal nog rond de 900 euro met de 6900xt kaarten
RX 7900 XTX;1;0.41275614500045776;Toch had een ieder hier op 25-11 jl. een 6800XT kunnen kopen voor €599
RX 7900 XTX;1;0.5374112725257874;zal dan wel bij een onbetrouwbare winkel zijn geweest , ik heb dat niet gezien
RX 7900 XTX;1;0.2689436078071594;Gewoon via Megekko de PowerColor Red Dragon....zie PW dip op 25-11-'22
RX 7900 XTX;2;0.34402430057525635;dan was zeker een kaart met 3 pci-e power connecters , ik zocht alleen naar kaart met max 2
RX 7900 XTX;1;0.47667816281318665;Nee gewoon met 2x 8-pin voeding...kan je ook zien als je dit type even door Google had getrokken...🤪
RX 7900 XTX;2;0.3527849614620209;die heb ik gemist helaas, had ook 2 zag ik zojuist
RX 7900 XTX;1;0.3844676911830902;Waarom kan Radeon wel hun kaarten dun houden en Nvidia niet? Gemakzucht?
RX 7900 XTX;2;0.3879130780696869;De geruchten zijn dat nvidia veel hoger wou klokken maar daarop teruggekomen is (mogelijks omdat ze nu al enorm veel stroom vreten). Maar de koelers ed waren al hierop ontworpen.
RX 7900 XTX;3;0.37933874130249023;En daarnaast is het in zijn algemeenheid een afweging tussen geluidsproductie en afmetingen. Zolang het in mijn kast past heb ik liever een grotere kaart die stiller is, dan extra loze ruimte in mijn kast en meer lawaai. Tot je dat andere PCI slot natuurlijk wil gebruiken
RX 7900 XTX;1;0.6268401741981506;ZO een enorme koeler en PCB kosten geld hoor, iets wat nvidia liever in hun zak steekt. :-)
RX 7900 XTX;1;0.5314859747886658;Idd in mij tweede slot zit een M2 ssd adapter. Die is bij zo'n dikke kaart niet meer bruikbaar.
RX 7900 XTX;1;0.424643874168396;Iemand al enig idee welke webshops de kaarten komen?
RX 7900 XTX;3;0.3199693262577057;ik gok Megekko en azerty die vanaf vanmiddag wel wat hebben liggen.
RX 7900 XTX;3;0.2546863257884979;Heeft iemand ook enig idee hoe laat?
RX 7900 XTX;1;0.29215148091316223;15:00 volgens megekko zelf
RX 7900 XTX;5;0.7225822806358337;Zeer mooie ontwikkeling van AMD. Als mijn huidige RTX 3060 aan vervanging toe is, kijk ik zeker serieus naar AMD, maar ook naar Intel.
RX 7900 XTX;2;0.3939821124076843;EUR 1.160,- voor de XTX is lager dan ik verwacht had. Ik ging eigenlijk uit van EUR 1.250,-. Hopen dat de Board partners geen flinke marges erover heen gooien, dan koop ik er wellicht morgen één, indien verkrijgbaar uiteraard haha
RX 7900 XTX;1;0.7488188147544861;Realiteit!!!! 1.758,76 € Absoluut benieuwd wat de board partners gaan doen? Ik niet meer, heb gewacht voor deze ondermaatse kaart en toch maar voor de concurrent gekozen.
RX 7900 XTX;1;0.5547013282775879;Als ik jouw link volg krijg ik hem voor €1.163,38, dus waar je die 1758,76 vandaan haalt?
RX 7900 XTX;4;0.38357430696487427;Gezien de nieuwe kaarten beter presteren op 4K in verhouding to 1080p, verwacht ik dat we nog prestatieverbeteringen gaan zien met driver updates in de toekomst. In elk geval kunnen beide kaarten de concurrentie met de 4080 aan, en dit met een significant lagere prijs en een kleiner verbruik. Zeker dus geen misser van AMD, ondanks dat de verwachtingen wel hoger waren. Zeker gezien Nvidia met de 4090 op prestatievlak dan, een betere concurrent op de markt gebracht dan verwacht.
RX 7900 XTX;2;0.446706086397171;Ik hoop vooral dat de prijs/performance verhouding doortrekt naar de mid-end RX7xxxx series en dat deze kaarten goed beschikbaar zijn. Anders is het een paper launch met adviesprijzen die weer totaal niet realistisch zijn. Het zal vast nog even duren voordat ze iets dergelijks aankondigen. Is maar is die overstock aan 68xx / 69xx kwijtraken.
RX 7900 XTX;2;0.5078996419906616;Een beetje wat ik verwacht had qua prijs/performance. Altijd zijn er weer mensen die optimistisch hopen dat AMD compleet Nvidia wegvaagt. Mijn gok was altijd geweest dat AMD qua ruwe prijs/performance een 10-15% beter dan Nvidia gaat zitten. Immers ze zijn de underdog, dus ze moeten een reden geven voor mensen om AMD te kopen, tegelijk als ze teveel goedkoper gaan zitten, dan zal Nvidia hun prijzen bijstellen, en kost dat ze beide winstmarge. Los van claims over dat AMD betere marges zou hebben (lijkt mij heel lastig te bepalen zonder toegang te hebben tot hele gevoelige informatie voor beide partijen), gaat AMD echt geen prijzenoorlog tegen Nvidia beginnen. Daar hebben ze het geld niet voor. En als we zeggen dat de 4080 vergelijkbaar is met de 7900XTX (iets mindere rasterperformance, veel betere raytracing), dan is op adviesprijzen gebaseerd de 7900XTX dus 17% goedkoper voor dezelfde performance. Net buiten mijn 10-15% gok . Uiteraard had ik meer gehoopt, maar niet meer verwacht. Ook ondanks de klachten van het rode kamp over het energieverbruik van Nvidia, het is niks beter bij AMD, zelfs wat slechter. (En ook hier, de klachten over Nvidia waren terecht wat mij betreft, maar je hebt dezelfde over AMD dus).
RX 7900 XTX;3;0.2788821756839752;Ik zit mij te verbazen over de energieconsumptie en de effectieve prijs-per-uur dat je zit te gamen. Dat begint inmiddels serieuze vormen aan te nemen met de huidige energieprijzen. Met een fannatieke dagelijkse game-hobby kan het richting de 30-60 euro per maand aan stroom gaan met de hoge huidige prijzen. Doe je dat over een periode van drie jaar bijvoorbeeld dan is dat in potentie meer dan 2.000,- alleen aan stroomkosten. Het loont dus om niet alleen de meest effeciente kaart te kiezen, maar ook te gaan werken met FPS-limitatie of andere undervolt opties om relatief zo veel mogelijk uit een systeem te krijgen zonder dat alles op de 100% prestatie draait. We zien immers dat je voor een beetje prestatieverlies al veel energie kunt besparen. En in de winter vol gas, zodat je gameroom geen verwarming nodig heeft - dat dan weer wel
RX 7900 XTX;3;0.4072152078151703;Prijs/prestatie is hij gelijk aan de 6800XT: Ik vind eigenlijk dat een nieuwe generatie meer bang voor buck moet geven. Maar ja.
RX 7900 XTX;4;0.2487512081861496;"@Trygve Is het een idee om bij GPU reviews ook een mooie perf/prijs scatter te maken zoals jullie bij de 12400 review deden? Dat zouden jullie met PW prijzen (mediane prijs per chip) zelfs dynamisch kunnen maken. Zo'n prijs/perf scatter maakt gelijk duidelijk welke kaarten een ""goede deal"" zijn. review: Intel Core i5 12400 - Goedkope Alder Lake zet 5600X buitenspel"
RX 7900 XTX;3;0.5653418302536011;Binnenkort komt er weer een gpu-vergelijking met prijs/performance, en voor reviews wil ik kijken of dat ook kan terugkomen. Dynamische prijzen opnemen in die grafieken is momenteel helaas wat lastiger om te doen, wie weet kan dat in de toekomst wel gebeuren.
RX 7900 XTX;2;0.4958743453025818;Goed idee. Maar in plaats van een mediane prijs per GPU, stem ik voor de laagste prijs per GPU. De goedkoopste GPU zal de snelheid halen waarmee de GPU in de benchmarks staat. Een duurdere GPU is sneller en dat komt niet overeen met de benchmarks.
RX 7900 XTX;4;0.4196022152900696;Precies. Zo was het tot een aantal jaren geleden ook, maar nu lijkt het wel alsof je simpelweg tevreden moet zijn met performance/prijs. Idiote trend!
RX 7900 XTX;1;0.4985025227069855;Ik denk dat Nvidia nog in crypto prijzen denkt en AMD prijst gewoon naar Nvidia's prijzen. Ik verwacht dat net zoals de 4080, dat deze AMD kaarten slecht zullen verkopen. Eenzelfde prijs/performance als de vorige generatie is geen upgrade. Het is wachten op een flinke prijsdaling of de volgende generatie over 2 jaar. Helaas.
RX 7900 XTX;3;0.5355994701385498;Goed om te zien dat AMD qua raw power goed mee komt. Voor contentproductie blijft het helaas treurig wat betreft encoder/decoder support. Apple, Nvidia en Intel lopen daar zowel op GPU als CPU vlak flink op voor. AV1 zit er nu in ieder geval wel alvast in, maar ik heb nog geen goeie vergelijkingen gezien en het is nog even afwachten hoe AV1 opgepakt gaat worden als vervanger van H.264/H.265.
RX 7900 XTX;2;0.47037991881370544;Ray power meekomen? voor raytracing schiet het gewoon te kort. Voor rasterized volstaat een 3000 serie nvidia of 6000 serie radeon ook nog wel.
RX 7900 XTX;3;0.3644125163555145;Raytracing is natuurlijk zeer specialistisch proces. Niet zo interessant vanuit productiviteit perspectief en beperkt vanuit gaming. In productiviteit testresultaten die de raw compute power van deze modellen weergeven geven een indrukwekkend beeld. Wanneer OpenCL ondersteund wordt gaan ze de 4090 in aantal toepassingen voorbij.
RX 7900 XTX;3;0.480033814907074;Hoe bedoel je beperkt vanuit gaming? Vrijwel alle nieuwe grote titels ondersteunen het. Het is echt wel een step up qua immersie.
RX 7900 XTX;3;0.4759807586669922;Hangt er vanaf welke ervaring je gevoelig voor bent. Ik ben veel gevoeliger voor hoge texturen en FPS dan iets realistischer licht.
RX 7900 XTX;3;0.6005641222000122;Ik ook, maar voor mij is het en/en. De 4090 biedt eindelijk 120hz plus met raytracing op 1440p . Voor competitive gaming is dat qua responsetijd niet voldoende, maar voor immersive singleplayer games wel
RX 7900 XTX;5;0.7804800271987915;Nou, top. Puur genieten. Andere mensen kunnen 1000,- besparen en hun ideale ervaring nu krijgen.
RX 7900 XTX;3;0.44782477617263794;Comp settings is vrijwel alles op low mits mid/high voor latency zorgen
RX 7900 XTX;2;0.42322883009910583;Ik was ontzettend fan van Ray Tracing. Echter moet ik bekennen dat ik het wow effect nooit gehad heb. Ja het is leuk qua weerspiegelingen maar eerlijk gezegd is het zo minimaal om nu te spreken over game changers.
RX 7900 XTX;2;0.4291899800300598;Waar ik erg benieuwd naar ben is hoe de koeling van deze kaarten zich houdt (ook de aftermarket versies) in een kast met een 90 graden geroteerd moederbord. Ik heb nog een oude Silverstone FT02 staan. Een dijk een van een kast, maar videokaarten uit de vorige generatie (zowel AMD als Nvidia) bleken niet overweg te kunnen met de hangende GPU-configuratie. Ook bij nieuwere cases zoals de Alta F1 bleek dat probleem te spelen. Als dat ook met de huidige generatie weer zo is, zie ik me helaas genoodzaakt om een nieuwe kast te kopen...
RX 7900 XTX;2;0.33381402492523193;Waarom zou dat uitmaken hoe ze hangen? Tenzij de fans alleen horizontaal fatsoenlijk kunnen lopen, maar dat lijkt mij apart.
RX 7900 XTX;3;0.4134785830974579;Het is jammer dat AMD niet ver genoeg mee kan komen. Geruchten gaan dat ze snel met een refresh komen omdat er 2 zaken vanuit AMD niet als belangrijk werden bepaald om zoveel mogelijk winst te halen. De chiplets clocken veel lager dan origineel de bedoeling was, want het ontwerp zou 3 ghz+ zijn en nu komen ze tot 2.5 ghz. Een 2e is dat ze meer chiplets kunnen toevoegen om meer cores te maken welke ze nog niet gedaan hebben. De 7900 XTX heeft nu 96 compute units t.o.v 80 van de vorige gen. Geruchten zeggen dat er een 120 C.U variant uit gaat komen die ergens rond de 3ghz zou halen. Wel is het zo dat Nvidia nu een zeer solide GPU heeft en deze goed weet uit te melken. Als de prijs echt rond de 1159€ zou zijn en de goedkoopste 4080 nu 1399 dan is er nog een verschil van 240€ welke de 7900 XTX toch interessant kan maken. In gewoon rester is de 7900XTX iets sneller, de 4080 is raytracing een stuk beter. AMD heeft de helft meer geheugen, maar Nvidia de betere drivers voor Windows. Ik moet zeggen hoewel AMD zeker een goede videokaart heeft gebouwd valt die toch wel achter op Nvidia. De 4090 heeft geen tegenstander en ze kunnen ook nog de 4090 TI uitbrengen. Hopelijk komt AMD snel met een geüpgrade versie die meer kan meekomen. Nu heeft Nvidia in het topsegment vrijspel. En mensen die zeggen dat het maar weinig kaarten zijn die zo verkocht worden, er zijn al ruim 100K 4090 verkocht wereldwijd zelfs na een paar dagen. AMD moet bij blijven. Wel zal de huidige 3000 serie en 6000 serie kaarten een stuk lager in prijs moeten worden om interessant te blijven.
RX 7900 XTX;1;0.40665706992149353;"In de testverantwoording staan de GTX 1080, RTX 2080 en RX 5700 XT. Ik dacht: ""Mooi, kan iedereen die de afgelopen jaren de scalper- en mining-waanzin heeft overgeslagen deze nieuwe kaarten vergelijken met wat men nog heeft draaien, top!"". Om vervolgens die kaarten nergens meer tegen te komen? Klopt de testverantwoording niet, of zijn de tabellen met resultaten te klein? Afgekapt op 10 resultaten?"
RX 7900 XTX;5;0.4902341365814209;Hier ben ik ook erg benieuwd naar, heb hier nu de RX 5700 XT en ben voorzichtig aan het kijken naar een upgrade. Zou het verschil in de 6800 (XT) en deze nieuwe generatie best eens in zo'n mooi vergelijk willen zien.
RX 7900 XTX;5;0.5230346918106079;Staat erbij nu.
RX 7900 XTX;5;0.6603482961654663;Gracias!
RX 7900 XTX;1;0.640900731086731;Ik heb de redacteur een berichtje gestuurd. De 5700 XT is sowieso getest.
RX 7900 XTX;1;0.474251389503479;Foutje van ondergetekende, de GTX 1080 Ti, RTX 2080 Ti en RX 5700 XT staan er nu allemaal ook bij
RX 7900 XTX;3;0.32567840814590454;Hebben jullie het allemaal gezien? in Modern warfare 2 is amd op de eerste plaats. daar is de Radeon 7900XTX sneller dan de RTX4090. Wel geen ray-tracing test te zien.
RX 7900 XTX;3;0.4311465919017792;in een handvol andere titels ook over de meeste games echter is de 4090 veel sneller momenteel.
RX 7900 XTX;2;0.5306742787361145;Ik vind het erg jammer om te zien dat in een game als Forza Horizon 5 de gpu's niet geweldig beter presteren dan een 6800XT in 1440p. Geldt niet alleen voor de nieuwe AMD's, maar ook een beetje voor de Nvidia varianten. Het lijkt wel of er een cpu bottleneck oid begint op te treden. Ik zou die game graag eens goed willen zien op 1440p 240hz.
RX 7900 XTX;3;0.5426297783851624;Prima videokaarten voor de prijs. Alleen had ik verwacht dat AMD ook met een DLSS 3.0 tegenhanger kwam of waren dat enkel geruchten?
RX 7900 XTX;3;0.6744314432144165;Ik had de 7900 XTX als concurrent van de 4090 verwacht in rasterisation performance, of in elk geval op de hielen van die kaart. Dat valt helaas een beetje tegen. Als je dan ray tracing ook gaat meenemen is het zelfs een verloren zaak tegen de 4080. Gelukkig is de prijs wel redelijk in vergelijking. Hopelijk snoept amd wat marktaandeel weg en krijgen we wat meer concurrentie.
RX 7900 XTX;4;0.573752760887146;Snelle kaart, nog beter verbruik. Maar ook nog wel een stuk langzamer dan nVidia
RX 7900 XTX;3;0.5124768614768982;Het verbruik tijdens gamen valt inderdaad nog wel mee, maar het idle verbruik met 2 schermen is wel heel erg hoog volgens de review van techpowerup.
RX 7900 XTX;2;0.37552395462989807;Niet als je appelen met appelen vergelijkt, wat AMD aanbied prijs/performance steekt boven nvidia uit momenteel.
RX 7900 XTX;3;0.4772239327430725;Jammer dat de meeste het Op deze manier verwoorden. Ze zijn sneller dan een 3090 Ti.
RX 7900 XTX;1;0.5518215298652649;En laat dat goedkoop er ook maar af!
RX 7900 XTX;2;0.4791956841945648;Spijtig dat Blender niet mee opgenomen is geweest in de test. Ik had vooral gehoopt dat daar het gat zou kunnen dichtgereden worden. Het is al geen grand canyon meer zoals voordien maar toch nog steeds teleurstellend.
RX 7900 XTX;2;0.3302714228630066;"Klein foutje in de video-review: Vanaf minuut 3:40 t/m 3:57 worden de gemiddelde framerates in fps aangegeven op 4K (tabel). De kop spreekt echter over ""index 1080P - 1920x1080 - ultra."""
RX 7900 XTX;3;0.44697314500808716;"In vergelijk met de topmodellen van de 6000 serie zijn er echt serieuze stappen gemaakt. Toch is dat vergelijk niet direct relevant. Het vergelijk met nvidia's 4000 serie loopt wat scheef omdat de 4090 serieus sneller is. Daar betaal je dan echter ook meer dan ruim voor. Precies op dat punt heeft amd een hele goeie slag geslagen imho. Niemand had ze er lelijk op aangekeken als de prijzen gelijk waren gebleven. Sterker nog ; dát zou al een enorme verassing zijn geweest. Dat men de kaarten nu een mooie stap lager heeft geprijsd t.o.v de vorige generatie is naar mijn mening een gouden move geweest. Benieuwd of nvidia toch nog wat aan de prijzen bij de topmodellen gaat doen !"
RX 7900 XTX;3;0.44335228204727173;Ik dacht dat de reviews pas morgen zouden komen? Desalniettemin mooie prestaties! Ik had alleen het stroomverbruik ietsjes lager verwacht tegen over de RTX 4080!
RX 7900 XTX;3;0.5129766464233398;beetje jammer dat tweakers nooit op productivity test zoals blender en encoding etc. bedoel t lijkt me wel duidelijk dat je met alles boven een 3080 zelden fps tekort komt in 99% van de gevallen
RX 7900 XTX;3;0.38539475202560425;Waarom doen reviewers bijna nooit de Virtual Reality prestaties testen met de Nvidia en AMD kaarten ? Dat vind ik namelijk wel interessant ivm mijn Oculus Quest en PC VR Gaming.
RX 7900 XTX;2;0.45696380734443665;Vraag me af hoeveel mensen deze generatie gaan kopen. Ik heb nog mazzel dat ik m’n 6800xt voor adviesprijs hebt kunnen kopen. Maar als je vorige jaar minder mazzel had en vergelijkbare presterende kaart hebt gekocht voor meer als 1000 euro en wat je er nu 2de hands voor krijg, dan is deze generatie kaarten niet de moeite waard…voor mij dan. Moet dan ruim 400 euro bijleggen voor 33% betere prestaties als ik naar de 7900xt kijken en even COD pak als game.
RX 7900 XTX;1;0.7729008197784424;Ik denk dat de prijzen sowieso totaal ontspoord zijn. Ik wil graag een 7900XTX maar dan voor max 700. Dat gaat dus nooit gebeuren. Bij Nvidia is het helemaal absurd.
RX 7900 XTX;3;0.5912179946899414;Wat dat betreft is AMD aardig consistent geweest: de top kaart voor 999 dollar. Alleen in 2020 was de optie eronder wat interessanter.
RX 7900 XTX;1;0.4078599810600281;@Trygve Waarom weeral enkel games getest? Content creation, video encoding mag men ook eens standaard beginnen meenemen.
RX 7900 XTX;2;0.44416722655296326;Prijs, af te wachten. Beschikbare kaarten MSRP vooral voor de USA heb ik gehoord. Persoonlijk gebruik ik graag AMD drivers maar deze kaart valt zo tegen dat ik geopteerd heb voor de tegenpartij vlaggenschip. Voor zover ik begrepen heb zijn de drivers van deze kaart helemaal niet goed. Zal waarschijnlijk wel rechtgetrokken worden maar uiteindelijk hoe je het ook draait of keert AMD heeft geen antwoord op de tegenpartij vlaggenschip. Pc gaming wordt een niche for the happy few met deze waanzinnige prijzen!
RX 7900 XTX;1;0.4765361547470093;Een overgroot merendeel speelt 1440P en daarmee vind ik het eerder een 4090 dan een 4080 concurrent. Voor veel minder geld!
RX 7900 XTX;1;0.2817544639110565;En Ebay stroomt al vol met scalpers Heb zowaar het geluk gehad om een XTX te kopen op de AMD shop direct. En nu is 2600$ leuk op Ebay, maar deze wordt een mooi kerst kado voor zoonlief. Voor Kerst '22, '23 EN '24 Roblox zal best lekker draaien hierop
RX 7900 XTX;1;0.4551886022090912;Hoi pap, ik ben het je verloren zoon, waar is mijn 7900 xtx?
RX 7900 XTX;1;0.36343660950660706;"Ik antwoord altijd op de vraag "" hoeveel kinderen heb je?"" > 1 voor zover mij bekend, de rest hebben nog niet aangebeld"
RX 7900 XTX;3;0.3550017774105072;Het wordt voor mij tijd om een nieuwe game pc aan te schaffen en die mag best high end worden, mijn huidige is 8 jaar oud. Ik speel eigen voornamelijk first person shooters zoals battlefield en CoD en zo nu en dan RTS games zoals COH, dat is ook gelijk het zwaarste wat ik doe. Is een 7900xtx dan eigenlijk te prefereren boven een 4080? Dit keer wil ik graag mooie beeldkwaliteit. Ik ben overigens niet van plan tussentijds te upgraden, dus hij zal 7-8 jaar meegaan.
RX 7900 XTX;3;0.8013306260108948;Oke AMD is misschien niet beter qua prestaties (zeker in RT), prijs/prestatie verhouding is wel een beetje beter dan Nvidia en het is wat te onzuinig. Maar ik heb ook dingen gehoord dat er veel bugs zijn in RDNA3, veel te lage clocks, bugs, crashes, BSODs, etc. Als ik zou moeten gokken gaat AMD's FineWine weer een ding zijn en gaan deze kaarten in de toekomst het veel beter doen.
RX 7900 XTX;5;0.27708664536476135;Elke launch zal zo zijn bugs hebben, maar ik vind het geniaal hoe jij dit soort dingen hoort op launch dag als alleen reviewers die kaarten hebben.
RX 7900 XTX;2;0.39849162101745605;Omdat ik het hoor van reviewers...? Of het zo erg is als wordt beschreven geen idee, maar het lijkt me niet heel verbazend gezien het hele chiplet gebeuren.
RX 7900 XTX;3;0.3247653841972351;Vraagje: Kan ik de 7900 XTX gebruiken op mijn X570 Aorus Elite moederbord? Ik gebruik nu een 5700XT. Het is een AM4 moederbord en dat slaat natuurlijk op processoren die ervoor gebruikt kunnen worden (de nieuwste generatie AMD processoren vereisen AM5 dacht ik). Maar misschien vergt het kunnen gebruiken van een GPU ook een dergelijke techniek?
RX 7900 XTX;5;0.30741870403289795;Dit is gewoon mogelijk. Let alleen even op je voeding zijn/haar vermogen.
RX 7900 XTX;4;0.3275790512561798;Bedankt voor jouw snelle reactie. Deze heb ik: Sharkoon SilentStorm Cool Zero 850 watt. Die heeft een gold label. Dat zal wel goed genoeg moeten zijn toch?
RX 7900 XTX;3;0.4210715591907501;Ja, dat moet prima zijn.
RX 7900 XTX;5;0.41527295112609863;Yep 850 is prima
RX 7900 XTX;2;0.5426390171051025;Gaat waarschijnlijk wel werken. Dat er 850wat op staat zegt helaas niets. Sharkoon staat bij mij igg niet bekend als hoge kwaliteit spul. Je zou voor de zekerheid even na kunnen vragen in het Voedingen advies topic: forumtopic: Voeding advies en informatie topic - Deel 34 Daar zitten een aantal mensen die dat waarschijnlijk met meer zekerheid kunnen vertellen. Het gaat er vooral om dat de voeding snel kan schakelen en de power spikes van de betreffende kaart aan kan. Vooral dat laatste gaat bij de mindere kwaliteit voedingen vaak mis. Ik heb geen idee hoe het met de powerspikes zit van de rx7000 serie.
RX 7900 XTX;5;0.4647580087184906;Benieuwd of ik met mijn 650Watt voeding nog genoeg is voor 1440p gaming met een 7900 xtx.
RX 7900 XTX;4;0.5942077040672302;Afhankelijk van de PSU kon je met een 6800XT soms al wat problemen hebben. Maar een hele goede PSU kan het weer prima aan. Officieel wordt er een hoger model aanbevolen. Maar als je bv een Seasonic Prime PSU hebt zou ik het persoonlijk wel aandurven. (mits je niet te veel andere zware hardware hebt. Stel je hebt ook een 13900K en ga je zowel de CPU als GPU overlocken wordt het wel een ander verhaal). Maar met een 5900X die stock power draait moet kunnen.
RX 7900 XTX;1;0.3637691140174866;Het betreft een BeQuiet! I.c.m. een 3700X e dan op een Super Ultra Wide resolution. Zal toch upgraden worden naar iets van 850 vrees ik.
RX 7900 XTX;2;0.5467736721038818;Je kan het proberen maar het is geen garantie. Ik ben niet zo goed op de hoogte van de BeQuiet lineup maar als het een topmodel is wat niet te oud is dan zou je het kunnen proberen aan de andere kant als je dit soort geld voor een Videokaart uitgeeft is het misschien beter om de rest ook netjes op orde te brengen,. Zelf draai ik alles op een 750 watt Prime Titanium en die gaat er de komende jaren nog niet uit. Maar dat was ook een hele dure PSU.
RX 7900 XTX;5;0.29456740617752075;"Ik draai met een Ryzen7 5700X & 6800XT op onderstaande 650w voeding. PW ; uitvoering: Cooler Master V650 Gold-V2 Werkt feilloos."
RX 7900 XTX;3;0.44118669629096985;Moet zeggen dat de reference kaarten van AMD qua design (mijn mening) nog altijd lelijker zijn dan de Founders Edition-kaarten van Nvidia.
RX 7900 XTX;2;0.4806078374385834;gelijk heb je. enkel niet voor de persoonlijke mening van vorm. gewoon de connector ook. het is gewoon beter
RX 7900 XTX;3;0.3934156596660614;Ziet er goed uit, wanneer komen de midrange kaarten???
RX 7900 XTX;3;0.3390100300312042;Weet iemand hoe het nu precies in de praktijk zit met DLSS? Je leest vaak opmerkingen dat als je profijt wil hebben van ray tracing je toch echt een groene kaart moet hebben omdat games daar geoptimaliseerd voor zijn, is dat al een beetje recht getrokken en kan team rood nu ook goed mee?
RX 7900 XTX;3;0.357744961977005;Nee amd loopt nog steeds flink achter met RT maar het is wel beter. Hopelijk volgende generatie. Als je RT op fatsoenlijke framerates prijs stelt is eigenlijk je enige optie nvidia.
RX 7900 XTX;1;0.5286704301834106;Als je RT absoluut wil is enkel nvidia daar, amd loopt nog steeds achter.
RX 7900 XTX;2;0.3636161983013153;hmm 899$ = 1050€? en 999$ = 1160€? heb kan aan mij liggen maar volgens mij klopt dit niet helemaal
RX 7900 XTX;2;0.46701887249946594;Inderdaad, volgens mij hebben ze een conversiefoutje gemaakt! Als dit zou kloppen dan zou deze hele release gelijk kunnen worden afgeschreven... Maarja ze hebben het bij de prijzen van de 4080/4090 ook over bedragen die veel hoger zijn dan MSRP, wat ook klopt met de realiteit (het minimum zelfs), dus als dit een voorbode is op retailers die de consument wederom gaan uitmelken dan wordt het er niet veel beter op.
RX 7900 XTX;1;0.5711550712585449;Dollarprijzen zijn sowieso altijd zonder BTW, de europrijzen met.
RX 7900 XTX;3;0.5202057361602783;Kan me vergissen, maar is het niet opvallend hoe goed de frametimes zijn van de XT en XTX?
RX 7900 XTX;1;0.36494752764701843;de 1% lows volgens de cijfers van Hardware Unboxed vielen mij ook al in die zin op.
RX 7900 XTX;5;0.5910195708274841;Deze kaart zou ik ook nog wel willen bestellen, als die uitkomt. Mooi beestje dit. Wat AMD nog steeds heel goed in is het energie gebruik voor de geleverde performance en uiteraard zijn de drivers subliem. Ik vond de oudere 7900 series ook al tof. Toentertijd de eerste kaart met 28nm GPU die en GDDR5 dacht ik 384 bit. Mooie tijden waren dat.
RX 7900 XTX;1;0.5888209939002991;Iemand enig idee welke websites de kaarten zullen verkopen bij de lancering om 15:00? Thanks alvast
RX 7900 XTX;1;0.3804599642753601;Is the reference model even going to be available in the netherlands?
RX 7900 XTX;1;0.7282659411430359;Waar koop je deze kaart? Via tweakers de pricewatch in de gaten houden en via Amd.com/shop? Nederland staat daar weer niet bij.
RX 7900 XTX;2;0.5348677039146423;De 7900XT is geen beste koop. 16% minder performance voor 90% van de prijs van een 7900XTX. De XT is dus zeker € 200 te duur. Normaliter heeft het topmodel de slechtste prijs/prestatieverhouding.
RX 7900 XTX;2;0.575791597366333;Weet niet wat de meeste zien maar het is in het algemeen teleurstellend. Puur raster prijs-prestatie is het beter dan the 4080, maar al de rest is duidelijk voor Nvidia. Meer stroom, loopt achter op RT, (aanname) slechtere media engine. Het is wel iets meer future proof door meer VRAM en DP2.1. Het vergelijk met the 4080 is dus niet slecht en je kan zelfs zeggen iets positiever, maar vergeet niet dat het verhaal van de 4080 ronduit slecht is. 7900xtx is dus iets beter dan slecht... Ook de conclusie dat AMD een beter winstmarge heeft, omdat board partners een hogere marge hebben, is ook een vreemde. Het is bekend dat NVidia(NGreedia) alle marge voor zichzelf houdt en slecht weinig marge overlaat aan de AIBs. Als je een grove schatting maakt met de die sizes, is de 4080 duidelijk goedkoper te produceren. The GCD is 300mm2 vs 379mm2, met dezelfde process family (N5 en N4). Dus slechts 25% meer en daarvan moet 6 MCD (>200mm2) en de base en de packaging van betaald worden. En ook nog eens 50% meer VRAM. Dus nee, Nvidia is nog steeds de marge koning, chiplets in deze generatie maken daar geen verandering in.
RX 7900 XTX;1;0.4389137029647827;Ik zit overal net even te checken maar nergens te koop lijkt het, vandaag is launchday toch?
RX 7900 XTX;3;0.40730828046798706;15u ja, maar in NL overal de XTX uitverkocht, alleen op sommigesites nog een 7900XT voor 1100+
RX 7900 XTX;3;0.24231606721878052;Lekker gaar dit weer. Zat om 15:00 te kijken maar he-le-maal niets te koop. Moet dat reference model hebben want die past in mijn case. Dan maar Nvidia. Gepruts dit.
RX 7900 XTX;2;0.4328920841217041;de 7900 XT reference kaart is nog wel bij amd te verkrijgen voor 1046 eur , maar ook ik had iets meer pagina's of iets verwacht. Grote retailers zoals Alternate heb ik al helemaal niets voorbij zien komen. Blijkbaar is deze release nog schaarser dan toen mining populair was.
RX 7900 XTX;3;0.33679720759391785;Mindfactory in Duitsland heeft de XTX´en wel nog liggen... ...toch niet meer
RX 7900 XTX;1;0.47025546431541443;amazon nl is selling it for 1172 eur
RX 7900 XTX;1;0.4756309390068054;7900 XTX gekocht van amazon voor €1172. Op Megekko waren ze meteen uitverkocht voor €1199.
RX 7900 XTX;3;0.26633724570274353;degene met 7+ dagen levertijd?
RX 7900 XTX;5;0.4054613709449768;Yes, komt vanuit Portugal dus best mogelijk 😅
RX 7900 XTX;1;0.5936696529388428;Stond in die wachtrij op AMD site zelf... XTX -> Out of stock Megekko ook meteen uitverkocht. Daar zag ik een Asrock en Powercolor staan met afbeelding van reference model. Kunnen deze merken reference verkopen onder hun eigen naam?
RX 7900 XTX;1;0.4711705446243286;Bij Megekko is de R7900 XTX meteen uitverkocht. De gewone XT is maar 100 euro goedkoper... Iemand nog tips waar deze te krijgen is?
RX 7900 XTX;1;0.43547603487968445;Prijzen inmiddels bekend op AMD Site: 7900XT > €1012,88 7900XTX €1125,55 Jammer genoeg XTX al uitverkocht
RX 7900 XTX;1;0.4495204985141754;Waarom zie ik die prijzen hoger? XTX: 1.163,38 € XT: 1.046,93 €
RX 7900 XTX;1;0.45033615827560425;Geen idee dat waren de prijzen oo de amd site omstreeks 15:30.
RX 7900 XTX;1;0.5754466652870178;Zijn er ergens nog Asus kaarten beschikbaar? Heb zowel op hun eigen site als op retailers niets kunnen vinden.
RX 7900 XTX;2;0.32190874218940735;Wat is de verwachting qua voorraad voor de 7900 xtx? Is daar iets over bekend? Bijvoorbeeld gemiddeld gezien X weken na launch weer beschikbaarheid of zoiets? Op AMD is de XT namelijk nog wel beschikbaar, maar twijfel of ik moet wachten of gewoon die dan maar moet bestellen.
RX 7900 XTX;1;0.43978989124298096;Had het wel gedacht dat deze kaarten niet voor beneden de 1000 hier in de winkels komen......
RX 7900 XTX;1;0.2927248477935791;mocht iemand nog een kaart willen... XFX Speedster MERC310 AMD Radeon™ RX 7900XT Black Gaming grafische kaart Game Clock Up To: 2220 MHz Boost clock Up to: 2560 MHz
RX 7900 XTX;3;0.38379523158073425;de xtx is er ook nog, alleen de prijs is een stuk duurder dan rond 15u. toen lag de prijs op 1172, nu op 1313
RX 7900 XTX;4;0.5929589867591858;Toch wel bijzonder hoe AMD zich weer terug heeft weten te plaatsen in de videokaarten industrie... niet zo heel lang geleden liepen ze gewoon consistent achter en waren ze (voor videokaarten dan) een beetje het budget merk... Nu zijn ze gewoon weer up and running en doen ze volwaardig mee. Mooi om te zien!
RX 7900 XTX;5;0.2948513925075531;Ik ben ontzettend blij dat AMD hier nu mee komt. Mijn PC kan moderne games bijna niet meer draaien, laat staan dat ik kan genieten van de graphics zoals de maker had bedoelt. Na bijna 7 jaar ben ik toe aan een nieuw beestje, maar van de NVIDIA prijzen schrik ik elke keer weer. Hopelijk zijn deze geweldige en veel betaalbaardere kaarten van AMD goed beschikbaar tegen de lente, zodat ik goed voorbereid ben als Diablo 4 uit komt.
RX 7900 XTX;3;0.384233683347702;Wat mij hier dan interessant lijkt, zou ook een toevoeging van een normale 2080 Ti zijn, en niet de FE die gewoon onbeschikbaar is. Idem een 1070 en 2070. Gewoon - omdat het kan. En omdat vooral de prestaties per Watt wellicht nog wel sterk uiteen kunnen lopen.
RX 7900 XTX;3;0.45251500606536865;"Men is redelijk negatief over de 7900 XT, omdat deze vergeleken met de 7900XTX meer performance inlevert dan dat het prijs inlevert. Maar een oprechte vraag; stel ik wil ca 1000 euro uitgeven aan een kaart die beschikbaar is. Dan is de 7900 XT toch de beste koop? Op AMD is deze nu beschikbaar voor ca €1050 en hij is qua performance vergelijkbaar met 3090. Ik snap niet zo goed waarom men negatief is, of mis ik wat? NB: voor 100 euro meer kan je de 7900XTX kopen, maar met een beetje pech is deze de komende tijd niet voor handen."
RX 7900 XTX;2;0.47292059659957886;Wat ik frappant vind is dat op het moment dat er AAA spellen uitkomen, zoals bijvoorbeeld CP2077 of Metro Exodus dat de videokaarten op het moment van uitgave nog lang niet snel genoeg zijn om dit soort spellen in al hun pracht en praal te kunnen spelen. Op het moment dat er videokaarten op de markt zijn die die wel kunnen is het spel (in mijn geval) allang uitgespeeld. En ik geen zin meer heb om het spel aan te slingeren (bin there done that) Ik zou graag zien als er een zwaar spel op moment van uitkomen zo geoptimaliseerd is dat het op de current gen high end kaarten in al hun pracht en praal te spelen is. En niet pas 2 generaties verder. Videokaart bakkers lopen eigenlijk mijlen ver achter op het aanbod van spellen, of game makers zijn gewoon lui...
RX 7900 XTX;3;0.402760773897171;Ik vind dat de absurde 4090 nog in de bences wordt meegenomen.
RX 7900 XTX;1;0.6229218244552612;Ik begrijp het niet. Ik kom helemaal niet in de buurt van de fps die jullie halen in cyberpunk en rdr2. Met een r7 3700x, ddr4 16gb ram aan 3600mhz, install op m2, met de rx 7900 xtx haal ik in cyberpunk 1440p ultra maar 70-80 fps en bij rdr2 rond de 90 fps. Het kan toch niet enkel die cpu en ddr5 van hun testbench zijn dat zo een groot verschil maakt? Zie ik hier iets over het hoofd?
RX 7900 XTX;4;0.42639443278312683;Ik denk dat dit wel mijn nieuwe videokaart wordt. Ik heb nu de 5700XT en is aan vervanging toe. de prijs wat aangeboden wordt is prima, en wordt waarschijnlijk nog goedkoper volgend jaar.
RX 7900 XTX;1;0.4769898056983948;Hmm de RTX 4090 blijft wel the go to te blijven. AMD is het gewoon net niet. Maarja ik heb dan ook 3 GSync monitoren en voor AMD gaan verlies ik dat. RTX 4080 doet het ook zo slecht nog niet.
RX 7900 XTX;1;0.5497578978538513;"Ik heb hier ook een Alienware 38"" Ultrawide met G-sync ultimate en een 6950Xt met Freesync en adaptive sync aangezet. Ik zie/merk 0.0 verschil met G-sync..."
RX 7900 XTX;2;0.538570761680603;Helaas kunnen 2 van die monitoren (AW3418DW) niet met Freesync om. Had dat al opgezocht. De derde en primaire (AW2721D) kan het wel maar ik merk wel dat het slechter werkt dan GSync.
RX 7900 XTX;3;0.3672522306442261;"Dat is inderdaad wel balen met die AW3418DW. Mijn 38"" biedt gelukkig wel ondersteuning voor Freesync, ondanks dat hier op die poot een stickertje hangt met Gsync Ultimate. Allemaal verwarrend voor de consument"
RX 7900 XT;3;0.43480736017227173;Aan die prijzen zijn er niet veel goeie redenen meer om voor NVidia te kiezen. Wordt ook interessant om te zien wat NVidia gaat doen met prijzen en nog te lanceren kaarten nu algemene vraag lager ligt en AMD een volwaardig alternatief is. Voor de 30 serie is AMD eigenlijk een beter alternatief omdat daar geen absurd lage geheugen configuraties aangeboden worden. NVidia staat met hun 10GB en minder kaarten met de broek op de enkels.
RX 7900 XT;5;0.30566951632499695;(linux) drivers, is en blijft (ook na jaren) voor mij -de- reden om voor groen te gaan. hieronder komen reacties van kamp rood, dat dit 'al lang niet meer is' blijf het [amd] proberen, maar nvidia (icm linux) blijf voor mij beste prestatie geven minste koppijn geven
RX 7900 XT;1;0.2868382930755615;"Vroeger was dit inderdaad de consensus, dat Nvidia veel betere Linux-drivers biedt. Maar dat is al enkele jaren niet meer zo. Open een willekeurige Reddit-thread over dit onderwerp en je zult zien dat vrijwel iedereen je AMD adviseert vanwege de ondersteuning die ingebakken is in de kernel en dus juist minder koppijn geeft (theoretisch) dan de proprietary drivers van Nvidia. Nu moet ik zeggen dat ik zelf ook al heel lang Nvidia combineer met Linux (Fedora momenteel) en dat ik in mijn specifieke situatie nauwelijks problemen ervaar, en dat ik vermoed dat men op fora een beetje is doorgeschoten in het bashen van Nvidia. Maar ik geloof gerust dat de ondersteuning van AMD flink is verbeterd sinds mijn laatste GPU-aankoop. Voor mijn volgende videokaart zou ik AMD beslist overwegen... Enfin, mijn punt is: zo zwart-wit is het volgens mij niet. Er zijn nog steeds goede redenen om voor een groene kaart te kiezen, NVENC om maar iets te noemen. Maar 'minder gedoe' is nou juist een argument dat in mijn ervaring meestal door Radeon-eigenaren wordt gebruikt. ;-)"
RX 7900 XT;3;0.3627561628818512;Op het gebied van NVENC zou deze generatie AMD kaarten ook een gigantische inhaalslag hebben moeten maken. Helaas dat reviewers dat zelden bekijken.
RX 7900 XT;2;0.5160173773765564;EposVox heeft al wat info, het ziet er helaas niet goed uit. Nog steeds slechter dan Nvidia en Intel. En bovendien neemt het veel te veel van de GPU in beslag, bij streaming niet ideaal.
RX 7900 XT;4;0.48043063282966614;Ziet er prima uit, met wel nog wat ruimte voor verbetering
RX 7900 XT;5;0.74887615442276;En laat dat nou net zeer interessant zijn voor mensen zoals ik, die hun computer vooral gebruiken voor videobewerking en niet zozeer voor gaming. In combinatie met Adobe Premiere Pro presteren Nvidia videokaarten traditioneel fors beter dan hun AMD equivalenten. Het prestatieverschil daarbij is groter dan bij de meeste games.
RX 7900 XT;1;0.43046262860298157;Wat is de naam van de tegenhanger van AMD ?
RX 7900 XT;2;0.4002768099308014;VCE. Wil je het weten voor hardware encoding van video of realtime encoding voor streaming? Voor encoding zou nu hardwarematig AV1 en HEVC t/m 8K 60fps supported moeten zijn. Voor streaming zie ik ook de term AMF. edit: Lijkt er op dat alleen AV1 nu 8K 60 doet, en HEVC beperkt is tot 8K 48. Beetje verrassend en ook misleidend in de AMD presentaties. Lijkt er op dat er een speciale chip voor AV1 is toegevoegd en dat daardoor dat nu extreem goed presteert, maar HEVC niet in dezelfde mate verbeterd is.
RX 7900 XT;3;0.33506327867507935;Ik streamde inderdaad vandaar de vraag, maar is vrij duidelijk zo!
RX 7900 XT;4;0.37089961767196655;In mijn ervaring zijn Linux gebruikers wel een beetje bevooroordeeld ten opzichte van AMD door die semi open source drivers. Nu is er zeker veel over te zeggen, ik heb AMD en het integreert gewoon beter, Wayland is ook erg fijn, ze hebben geen ongelijk. Maar je zag mensen al AMD aanraden toen fglrx nog maar pas dood was en de nieuwe drivers nog steeds allerlei mankementen hadden. Dat was in mijn optiek nog wel voorbarig. Maar tegenwoordig heeft AMD wel duidelijk mijn voorkeur vanwege de drivers. Nvidia heeft wel functies, zoals CUDA, DLSS, raytracing, daar loopt AMD echt achter, maar de basics zijn bin AMD tegenwoordig ook wel echt beter. Dat begon met de 5000 serie toch serieus te worden, en sinds de 6000 serie al helemaal.
RX 7900 XT;5;0.6397565603256226;Zelfde ervaring hier. Ben echt blij van Nvidia afgestapt te zijn, eerst een 5500xt gehad, nu een 6700XT en alles werkt out of the box, geen proprietary drivers. Ideaal als je een rolling release gebruiken bent of af en toe voor de lol zelf een kernel compileert. Een verademing.
RX 7900 XT;3;0.3830205500125885;En nog afgezien van rolling releases of kernel compilaties, het integreert gewoon fijner. Ik heb een 165Hz display met Freesync (eerder was dat 144Hz zonder Freesync), plus 2 60Hz schermen. Op Nvidia zouden de zijpanelen flink wat last hebben van tearing. En dan had ik Freesync kunnen vergeten. Hier werkt dat zoveel beter. Wat alleen niet fijn werkt is de combinatie 1080P en 1440P, waarbij je ook eigenlijk iets van desktop scaling zou willen gebruiken, maar ik heb ook een soortgelijk probleem op het werk waar ik wél Windows gebruik, en daar werkt dat nóg slechter.
RX 7900 XT;1;0.4858613610267639;Oh ja, multi-monitor... Ja voor dat soort dingen zijn proprietary drivers gewoon shit.
RX 7900 XT;1;0.627787172794342;Jawel, het is nog steeds zo dat AMD drivers slechter zijn. LINUX geeft dit ook duidelijk aan in zijn video, terwijl hij dik voor AMD gaat nu.
RX 7900 XT;1;0.4606952369213104;Deze thread gaat over Linux. Ik heb de video niet bekeken, maar aangezien het Linus is neem ik aan dat het over de Windows driver gaat, een totaal andere dus. Linus heeft afgelopen jaar ook een reeks gemaakt waarin hij zgn probeerde over te stappen naar Linux, laten we het erop houden dat dit geen succes was. Vandaar mijn aanname.
RX 7900 XT;3;0.3213234841823578;Volgens mij haal je wat namen door elkaar...
RX 7900 XT;5;0.3106626868247986;Ja….auto correctie iPhone haha Linus Je weet wat ik bedoel
RX 7900 XT;1;0.7002450227737427;WTF!? Linux drivers van Nvidia zijn een ramp... security bugs die al in jaren niet gepatcht zijn, gebonden aan bepaalde kernel versies, closed source etc... Je ziet zo geen dergelijke verhalen over AMD : AMd drivers op Linux zitten in elke distro bij en hebben geen extra werk nodig. Die dingen werken out-of-the-box, super stabiel en zijn open source. En dat is al jaren zo. Linux ondersteuning is waarom Nvidia er bij mij niet inkomt...
RX 7900 XT;3;0.42166152596473694;Redelijk recent kwam er wel een belofte op beterschap uit kamp groen: nieuws: Nvidia maakt zijn gpu-drivers voor Linux open source beschikbaar
RX 7900 XT;2;0.48206937313079834;"Je koopt resultaten geen beloftes als je slim bent. Ik draai zelf nu al jaren AMD (6800 & 6900XT naast een 2060 mobile en een 1080). De drivers zijn enorm veel beter geworden, ze waren waardeloos. Ze zijn nu meer dan prima. Ik prefereer ze inmiddels boven die van Nvidia. maar dat is persoonlijk. Dus ik ga niemand tegenspreken over de ""betere drivers"". Wat ik leuk en enorm vervelend tegelijk vindt is dat AMD-kaarten vaak een ja na release een stuk beter zijn dan op moment van release. Mijn oude 5700XT was toen ik hem kocht soms wel 25% zwakker dan een jaar later. Mijn 6800 en 6900XT zijn ook aanzienlijk krachtiger nu dan een jaar eerder als ik de benchmarks van 1,5 jaar geleden moet geloven. Mijn verwachting is dat AMD dat deze generatie ook weel zal hebben, maar dat is GEEN reden om deze kaarten nu te komen. Als ze je NU aanstaan, dan kan je de trekker overhalen. Want de belofte dat het beter gaat worden is niks waard."
RX 7900 XT;2;0.41489124298095703;Zijn er goede benchmarks te vinden waarbij de resultaten aantonen dat de 5700XT of een vergelijkbare kaart dergelijke sprongen heeft gemaakt, simpelweg door drivers? Ik geloof best dat in sommige games een 25% gelukt is, maar verwacht zelf dat dat dan ook komt doordat de drivers of de ondersteuning vanuit de game makers in het begin zo slecht waren.
RX 7900 XT;2;0.31732749938964844;Daar heeft het ook mee te maken. Maar ik heb zonder veel moeite een artikeltje weten te vinden waar de 5700XT een 10% verbetering ogenschijnlijk kreeg in 2019 (Bron: Daarnaast heeft AMD natuurlijk ook wat voordeel met het feit dat nagenoeg alle games met hun RDNA-architectuur in gedachte worden gemaakt (Xbox & PS). vervolgens wordt er op PC natuurlijk veel voor Nvidia geoptimaliseerd omdat een enorm deel van de markt op die kaarten draait, maar dan nog.
RX 7900 XT;1;0.38252729177474976;"de titel van dat artikel luidt zie het woord "" should "", dat betekent hier "" zou moeten "". het artikel bevat de volgende tekst. daar staat dus dat de 19. 7. 1 drivers 10 % langzamer zijn dan de drivers daarvoor, met een rx 590. dat betekent dus, dat amd met de 19. 7. 1 drivers, blijkbaar de eerste drivers die de rx 5700xt en 5700 ondersteunen, dingen heeft moeten doen waardoor de drivers in het algemeen 10 % langzamer worden voor ( sommige van ) de bestaande / oudere kaarten. als dit artikel betrouwbaar zou zijn ( ik heb sterk mijn twijfels maar daarover zo meer ), dan bewijst dit artikel exact het tegenovergestelde van wat je beweert : de bestaande kaarten ( in dit geval de rx 590 ) gaat dus 10 % in performance achteruit. nu wil je misschien zeggen, ja maar de schrijver beweert dat dit goed nieuws zou kunnen zijn, zie : ik zit niet goed genoeg in de logica / formal fallacies ( om gelijk te zeggen welke fallacies het hier betreft, maar minstens 1 zal hier flink van toepassing zijn. het klopt niet, het is niet logisch. en natuurlijk games waarbij de game / driver combinatie specifiek slecht was, waardoor je inderdaad die 15 % performance boost haalt. maar alle fabrikanten brengen nieuwe en over het algemeen betere drivers uit, waar wat winst mee valt te behalen. en het wordt telkens aangetoond wanneer er grondige reviews worden uitgevoerd, dat het uiteindelijk maar weinig aan de verhoudingen tussen verschillende videokaarten verandert in die zin, dat je de benchmarks die nu gedaan zijn over 2 jaar in grote lijnen en wat betreft de onderlinge verhoudingen ( voornamelijk over meerdere games heen ), nog uitstekend zullen voldoen, indien we de resultaten van amd en nvidia uit het verleden mogen gebruiken als voorspelling."
RX 7900 XT;4;0.48175686597824097;Prima!~Mijn ervaringen met mijn 5700XT waren positief over de 2 jaar dat ik het draaide en het werd in de game die in die tijd veel speelde (Destiny 2 en dat is een game die niet bijzonder goed met AMD werkt) werden de prestaties over die 2 jaar beter, toen ik begon was het 65-70 FPS en tegen het einde 80-85 FPS. Of dit optimalisaties van Bungo waren of AMD heb ik niet in detail bij gehouden. Het viel me wel op dat het tussen drivers kon verschillen. Maar goed, dit is N=1 en meer een anecdote dan waarheid. Dus... doe ermee wat je wilt. Ik vind het mooi en vervelend tegelijk dat AMD dit gedrag vertonen. Want het vertelt mij dat de drivers nooit af zijn, zoals ze dit bij Nvidia wel zijn. Want double digit prestatie verbeteringen krijgen is natuurlijk leuk, maar het laat achterstallig werk zien.
RX 7900 XT;2;0.4027363061904907;Huh? Ik game al sinds de GeForce 8800GTX exclusief op nVidia, maar heb de afgelopen 24 jaar altijd AMD kaarten voor mijn Linux systeem gekocht omdat elke nVidia kaart mij op Linux hoofdpijn bezorgde. nVidia was altijd wel flink sneller, maar wat koop ik daar voor als hun drivers crashes veroorzaken die een complete harde rest eisen waarbij de volgende boot een eeuwigheid duurt vanwege de filesystem check? AMD heeft in die 24 jaar minder crash problemen gehad en belangrijker, meestal geen reboot nodig maar de driver (automatisch) opnieuw opstarten en klaar.
RX 7900 XT;1;0.5205349922180176;Hoezo bezorgde AMD je geen hoofdpijn? Tegenwoordig zijn de drivers wel fijn, maar ik kan me nog wel fglrx herinneren. Had je echt helemaal niets aan. Dat was pas hoofdpijn, en was was de reden dat ik wel een decennium Nvidia heb gebruikt op Linux. Pas dit jaar gebruik om weer AMD.
RX 7900 XT;2;0.4255349934101105;Voor R300 en R400 gebruikte ik de OS drivers. 3D was heel traag, maar de drivers heel stabiel. Zoals ik al zei gamen deed ik er niet op, linux machines zijn voor (voornamelijk c++) software development. De AMD proprietary fglrx heb ik derhalve nooit gebruikt. Veel interessante games waren er indertijd overigens ook niet op Linux en Proton/Wine was ook niet echt bestaand/viable, dus ik heb nooit het idee gehad dat ik wat miste. Als je 3D modelling software gebruikte dan kan ik me voorstellen dat je het worstellen met de nVidia drivers voor lief nam, daar was AMD toen niet echt geschikt voor.
RX 7900 XT;1;0.5491917729377747;Sorry maar dit is toch wel echt lijnrecht het tegenovergestelde van mijn ervaring. bij het selecteren van een laptop voor linux is echt mijn voornaamste, leidende criterium dat er GEEN nvidia gpu in zit. Ik weiger OOIT nog een seconde van mijn tijd in Nvidia op Linux te steken Mijn huidige laptop heeft een quadro M1200 en er is maar 1 distributie die uberhaupt succesvol boot out of the box en dat is Pop_OS. Alle andere distributies die ik heb geprobeerd, en die lijst is LANG, hebben showstoppende problemen met nvidia. Echt om gek van te worden gewoon. 99% van de distributies haalt na een verse installatie werkelijk nieteens de desktop, de meesten nieteens de login manager Daarentegen zitten de AMD GPU drivers tegenwoordig gewoon in de kernel gebakken. Dus ik weet niet hoe lang het geleden is dat je amd hebt geprobeerd, maar minstens 5 jaar denk ik. Zelfs debiab stable heeft ondertussen een nieuw genoege kernel Vandaar ook de welbekende video van linus torvalds waarin hij nvidia vervloekt en zijn middelvinger rechtstreeks tegen de camera opsteekt
RX 7900 XT;4;0.5218971967697144;Geen problemen hier met Ubuntu 22.04 en 22.10 (beide geprobeerd) op een Dell XPS 9510 met een RTX 3050ti. Ook blijkt dat Nvidia samen Ubuntu werkt voor het server GPU geweld (kwam ik tegen bij het zoeken naar de driver . Als je voor de Nvidia driver hebt gekozen kunt je door nvidia-settings in de terminal te typen het settings grafische programma gwaarmee je grafisch zaken (power/anti-alias etc). kunt regelen en zie je op hoeveel MHz het geheugen en de GPU werkt. Voor mij goed genoeg!
RX 7900 XT;1;0.6718441843986511;ben je nu aan het trollen ? Nvidia is een ramp op Linux met de proprietary drivers. Daarnaast zoals andere al aangeven zitten de AMD drivers in de Kernel ingebakken plug en play. Voor mij nooit maar dan ook nooit Nvidia het werkt gewoon niet goed op Linux. Daarnaast nu steamdeck, Ps5 en Xbox gebruikt maakt van AMD architectuur is het maar een kwestie van tijd tot dat alle nieuwe games echt in het voordeel van team rood gaan vallen. Je kan tegen optimalisatie niet genoeg invoeren op hardware. Je ziet het al bij Modern Warfare.
RX 7900 XT;1;0.3857836127281189;Dat word al jaren geroepen sinds het begin PS4 dat games beter draaien op AMD. COD is daar niet echt een voorbeeld van want het is zelfs een Nvidia sponsored game geloof ik want de game heeft DLSS en Nvidia reflex en is maar 1 uitschieter. Je zou al jaren lang moeten zien dat games met AMD hardware sneller zou zijn dan vergelijkbare videokaarten van Nvidia. Ik vind dit tegenwoordig meer een mythe.
RX 7900 XT;3;0.4261760115623474;Game heeft ook gewoon FSR, Fidelity CAS en XESS.... Daarnaast ben ik zelf juist vaker van DLSS aan het stappen omdat het toch echt wel een lagere kwaliteit beeld is en uiteraard altijd met artifacts.
RX 7900 XT;3;0.5272451043128967;Je ervaring is valide, maar ik had eigenlijk andersom verwacht omdat de AMDGPU driver in de mainline kernel zit.
RX 7900 XT;1;0.26951220631599426;Linux en Nvidia is een regelrechte nachtmerrie. Weet je zeker dat je niet op Windows zit?
RX 7900 XT;1;0.34620392322540283;Toegegeven, ik had laatst wat problemen met de pro diver, maar dat is niets vergelijken met wat ik met Nvidia op Ubuntu heb meegemaakt. En om Phoronix the quoten:
RX 7900 XT;3;0.43955498933792114;Blij dat je goede ervaringen hebt met Linux en Nvidia. Echter, mijn eigen ervaring wijkt hier toch redelijk sterk vanaf. Integratie van de drivers in de kernel zoals bij de concurrentie (Intel iGPU drivers en AMD) was voor mij toch steeds volledig pijnloos op Manjaro en Fedora. Met Nvidia ook, toch op Manjaro, altijd last om in de TTY te proberen raken als de desktop env vastliep. Om nog maar te zwijgen van wat er gebeurt met je systeem eens Nvidia de support voor jouw kaart laat vallen (Desalniettemin kudos voor de mensen achter Nouveau). Maar ik kan natuurlijk ook enkel uit persoonlijke ervaring spreken.
RX 7900 XT;1;0.5322799682617188;De NVidia linux drivers zijn naar mijn ervaring allesbehalve vlekkeloos. Op zowel Arch als Ubuntu serieuze problemen met tearing (vooral bij scrollen) gehad totdat ik enkele opties heb aangepast in het NVidia control center. Ook werkte hibernation niet meer tussen april en begin deze maand op Arch (zwart scherm bij resume) terwijl dat op een ander system zonder NVidia gpu niet het geval was. Ik ga hoogstwaarschijnlijk een AMD GPU kopen waarbij het driver fiasco van NVidia een grote rol speelt.
RX 7900 XT;3;0.3490563631057739;Ik snap wel een beetje waar je vandaan komt want ik ben ook jarenlang steeds weer bij NVidia teruggekeerd. Toch ben ik afgelopen BF overgestapt naar rood omdat de drivers van AMD (meer) open source zijn. Het is nog wat te vroeg voor grote conclusies maar ik moet zeggen dat het dusver vlekkeloos gaat. Dan bedoel ik niet alleen dat games lekker lopen maar ook userland ervaring zoals het omgaan met mijn ingewikkelde en flexibel multi-monitor setup. Out-of-the-box een betere ervaring dan ik de afgelopen 10 jaar met nvidia heb gehad Daar moet ik wel bij zeggen dat ik die kaart pas kort heb en alleen nog maar een paar games heb gespeeld. Met kerst wil ik ik een HPC/AI-projectje doen. Mijn beleving is dat die wereld erg op NVidia is gericht. Momenteel heb ik echter het vertrouwen dat dit ook goed gaat werken. Dusver mijn anecdata.
RX 7900 XT;2;0.3595048189163208;Het is juist andersom, AMD heeft betere drivers voor Linux.
RX 7900 XT;3;0.4045525789260864;Nee hoor, je moet het zelf weten. Echter de opensource drivers van AMD zijn fantastisch en zeer stabiel onder Linux. De opensource versie van nVidiab is daarentegen lachwekkend als het niet zo triest zou zijn. Dat is voor mij de redenom AMD in mijn setup te hebben en houden.
RX 7900 XT;2;0.35392090678215027;""" vroeger "" was het toch zo dat je zo ongeveer intel + nvidia moest hebben ook maar een kans te maken om spellen met wine te draaien? ik ben pas sinds vorig jaar 100 % overgestapt op linux ( en dus ook gaan gamen met wine ), en ik heb eigenlijk nooit iets anders gehad dan intel + nvidia, maar er ook nooit problemen mee gehad. ik moet wel zeggen dat ik altijd een half jaar tot een jaar achter de feiten aan loop : ik ben zelden of nooit een early adopter van hardware. ook nu heeft mijn ( oude ) i7 - 6700k met gtx 1070 geen problemen onder debian 11. ( ik draai dan wel kde op xorg ; ik ga pas over op wayland als debian er de standaard van maakt. ) wel staat er een nieuwe computer op het programma, na de release van debian 12 bookworm. die computer komt er daarom waarschijnlijk ergens in juni - augustus. die gaat een ryzen 7950 in eco mode bevatten ( want ik wil per se 16 "" normale "" cores in dit systeem ), en mogeljk / waarschijnlijk een radeon 7000 - serie videokaart. het enige waar ik een beetje mee zit is het volgende : als ik de open source driver wil gebruiken, dan is daarvoor linux 6. 0 nodig ( vandaag is 6. 1 lts uitgekomen, dus ik ga ervan uit dat die in debian 12 terechtkomt ), en mesa 22. 2 ( die zit al in bookworm ; 22. 3 zit al in sid, dus ik ga ervan uit dat debian 12 mesa 22. 3 gaat bevatten ). voor mijn gevoel is dat een beetje te kort dag, omdat ik niet verwacht dat debian bugs gaat fixen of backporten, behalve security fixes ; als iets niet goed werkt in de rx 7000 serie kaarten en daarvoor een nieuwe kernel + nieuwe mesa nodig is, dan zal het niet werken voor 2 jaar lang, behalve als er ( heel snel ) een nieuwere kernel + mesa in backports terecht komt. hoe makkelijk / lastig is het tegenwoordig om de proprietary amd - driver te installeren? de nvidia driver is... eh... niet makkelijk, als je secure boot nodig hebt, maar op een nieuw systeem gaat dat mogeljik gewoon uit staan omdat het alleen linux gaat draaien. mijn desktopcomputer krijgt geen windows meer. nooit meer."
RX 7900 XT;3;0.5598829388618469;Ik begrijp niet zo goed waarom.je nieuwe hardware met Debian wilt combineren. Neem dan een distro die de kernels wat sneller volgt, en dat moet ook geen Arch zijn, zit nog wel wat tussen.
RX 7900 XT;5;0.543692409992218;Ik koop al 10 jaar lang enkel radeon en heb altijd op HD of nu 4K ultra kunnen spelen en nooit problemen gehad met drivers of spel/ stability issues. Kiezen voor het groene kamp is gewoon fanboi zijn en je geld over de balk smijten.
RX 7900 XT;3;0.27275365591049194;Je bedoelt de oude fireglx elende of de huidige amdgpu driver? Die oude was brak, die nieuwe is top.
RX 7900 XT;2;0.5318211317062378;Bij de RTX 4080: DLSS3.0, Raytracing performance, CUDA, power efficiency. vs een 20% hogere prijs. Het prijsverschil verdien je terug met 3-5,5 uur per dag (5 jaar lang) gamen in stroom verbruik... (Het is weird om Nvidia als energie 'zuinig' neer te zetten!) Persoonlijk zou voor mij het prijsverschil de genoemde features waard zijn om toch voor Nvidia te gaan. Maar ik sla deze generatie even over. Wellicht wanneer de energie prijzen weer zijn gedaald naar normale niveau's dat ik weer eens ga kijken naar een PC build met een losse kaart. Tot dan zal ik hier primair zeer energie zuinige AMD CPUs gebruiken met intergrated graphics + cloudgaming (naast de Steam Deck, welke ook AMD gebruikt).
RX 7900 XT;1;0.44583484530448914;"En dan hebben we het nog niet over idle verbruik, want schijnbaar lukt het AMD alweer niet om fatsoenlijke drivers te hebben bij multi-monitor setups: (iemand linkte deze in AMD topic op GoT). En zelfs als je zegt: ""Maakt mij niet uit, heb toch maar één monitor"", je gaat je dan wel afvragen wat er verder nog meer aan de hand is met hun drivers. Verder ook geen idee wat het probleem is van een kaart zonder 12GB+ geheugen. Volgens mij loopt mijn 3070 nog niet vol."
RX 7900 XT;2;0.47124814987182617;Schijnbaar lukt het Nvidia niet eens een fatsoenlijke driver te leveren bij een single monitor setup: Om dan meteen te gaan afvragen wat er verder allemaal mis is met de drivers gaat mij een beetje ver, maar ik begrijp waarom je nog een 30 series kaart hebt want de 40 series kan jij niet vertrouwen. Het geheugen is belangrijk voor hogere resoluties en nieuwere spellen, de 8 gb van de 3070 gaan makkelijk gevuld worden over een paar jaar. Erop bezuinigen bij een dure kaart is gewoon planned obsolescence. Met meerdere monitoren zou ik zeker ook even wachten of dit issue weggaat.
RX 7900 XT;1;0.4821544587612152;De spellen die over een paar jaar makkelijk de 8GB geheugen vullen, gaan toch niet op hoge settings draaien op mijn 3070. En op lagere settings past dat vast weer wel binnen de 8GB. Een specifieke bug bij bepaalde moederborden als geen drivers geinstalleerd zijn, is natuurlijk iets wat wat makkelijker door QA kan glippen dan 100W+ verbruiken gewoon standaard bij een multi-monitor setup (en iets soortgelijks bij een Youtube videotje). En bugs kunnen er altijd zijn, liefst niet al teveel, maar het zal helaas nooit nul zijn. Maar dit verbruik is schaar ik daar niet onder, het is niet zozeer een bug als gewoon heel slecht geoptimaliseerde drivers. En was dit nou een enkel dingetje, dan was ik het met je eens hoor dat de stap naar slechte drivers groot is. Maar laten we wel wezen, dit is het zoveelste deel in AMDs drivers die het slecht doen bij meerdere monitoren qua verbruik en/of bij monitoren die niet op 60Hz draaien. En algemeen is één van de grote problemen van AMD al een tijdje langer de drivers. (Tot mijn 3070 was het merendeel van mijn kaarten AMD. Na de 5700XT had ik daar voor voorlopig wel genoeg van).
RX 7900 XT;4;0.3929961025714874;Het is niet alleen multimonitor verbruik, maar kennelijk ook bij het afspelen van video's, zoals van Youtube. Genoeg VRAM kan een punt zijn als je graag exorbitante texture mods draait zonder verdere optimalisatie.
RX 7900 XT;2;0.39642494916915894;Definitie van multimonitor setup is heel breed en wordt nooit terdegen uitgezocht door review sites. Ik had dit probleem voorheen ook met mijn 6800XT, na het plaatsen van 2 *4K op 144hz refresh sync was het probleem weg, voorheen was het een FullHD+QHD met 60-144hz combo. je geeft aan alsof als er 1 probleem is het vol met problemen is en nvidia er geen enkele heeft helaas is soms zelf wat onderzoek doen moeilijk tegenwoordig en is het leuker om een issue gewoon te blijven linken.
RX 7900 XT;3;0.5052107572555542;I got three... Ouch!
RX 7900 XT;2;0.4094336926937103;5.5 uur per dag gamen gedurende 5 jaar. Nu niet echt een gebruikelijk scenario.
RX 7900 XT;2;0.5387763977050781;Inderdaad. En dat moeten dan ook games zijn die je videokaart zwaar aan het werk zetten. Als je dan tussendoor ook nog wel es wat minder vereisende spellen speelt klopt de berekening ook al niet meer.
RX 7900 XT;4;0.31502440571784973;Bij gelijkblijvende energieprijzen.
RX 7900 XT;1;0.5771655440330505;DLSS3 = stront tussen je frames net zoals tvs van 10 geleden dat deden. Nee dank je, ik hoef geen interpolatie. RT: zoals Jelle in de video zegt: leuk om even aan te zetten, maar uiteindelijk zet je het uit, want visueel is het verschil erg klein, en de impact op fps is enorm (en die impact wordt niet kleiner: bij de 4000 serie verlies je net zoals 2 generaties terug in de 2000 serie ongeveer 50% van je framerate). CUDA: wordt in geen enkele game gebruikt, dus voor de meeste consumenten. Uitermate irrelevant. Zoals ik op vorige artikelen al zei: het is erg vreemd hoe mensen deze punten telkens weer opnoemen terwijl de realiteit aantoont dat dat slechts niche toepassing zijn die de gemiddelde consument niet gebruikt. Uiteindelijk draait het oh altijd om raster performance, tegen de tijd dat RT er echt toe doet is je 4000 serie kaart echt niet meer relevant.
RX 7900 XT;3;0.29978641867637634;Raytracing, betere professional software support etc zijn toch zeer sterke redenen. Die 10GB is overigens helemaal geen probleem.
RX 7900 XT;3;0.40620195865631104;Nee dat is wat individueel relevant is. Dus voor de een, de enigste keuze en voor de ander heeft meer keuze.
RX 7900 XT;1;0.47619011998176575;Verkondig het dan niet als een feit.
RX 7900 XT;2;0.42068588733673096;Wellicht als games het enige zijn wat je doet op je PC maar wanneer je bijvoorbeeld bezig bent met 3d, renderen of AI dan is er eigenlijk geen enkele serieuze keuze buiten nVidia. Vrijwel alle software/tools zijn geoptimaliseerd voor nVidia en wanneer je bijvoorbeeld de CUDA cores in wil zetten voor andere taken/berekeningen dan biedt nVidia een heel scala aan gratis tools.
RX 7900 XT;2;0.4955081343650818;Helaas komen deze kaarten vanwege hun buitensporig idle verbruik in een multi-monitor setup bij voorbaat al niet meer in aanmerking: Het is te hopen dat AMD hier nog een driver update voor uitbrengt. Echter is, voor zover ik weet, de R390/R9 Fury generatie de laatste die niet een bovensporig verbruik hiervoor met zich meebracht. @Trygve Omdat zeker onder Tweakers vaak met meerdere beeldschermen wordt gewerkt, is het misschien een goed idee even snel een twee monitor aan het systeem te hangen om dit ook expliciet in de Tweakers reviews mee te nemen. Een idle verbruik van 80W wanneer je een extra beeldscherm aan je GPU hebt hangen, zal afgezien van de hogere stroomrekening ook niet meer passief te koelen zijn.
RX 7900 XT;3;0.41887789964675903;Bij laptops kan je switchen naar IGPU, jammer dan het bij desktops niet kan..
RX 7900 XT;2;0.35285645723342896;Laptops hebben hier dan ook een hardware onderdeel voor die het ingebouwde beeldscherm kan switchen tussen de igpu en dgpu. Die zogenaamd fysiek de connectie om. Meestal als ze een hdmi poort hebben voor externe monitoren zit die wel direct op de dgpu aangesloten zonder mogelijkheid om die via de igpu aan te sturen. Dat ben ik gaan onderzoeken toen ik me afvroeg waarom mijn hdmi poort het niet deed als ik in linux mijn dgpu op disabled had staan. Het is blijkbaar heel normaal
RX 7900 XT;2;0.4641452729701996;Als ik het destijds goed gelezen heb is dat wel een AM5 feature. Helaas hebben zowel Tweakers als Techpowerup op AM4 of Intel getest. Ik hoop in ieder geval dat het een oplossing kan bieden of dat AMD middels de drivers het verbruik de kop in kan drukken.
RX 7900 XT;5;0.4591715633869171;Je kunt zover ik weet met 13th gen ook gewoon je monitor op het moederbord aansluiten met een dgpu erin en dat ie die gewoon gebruikt. Vorige week ook een paar keer geprobeerd en dat ging prima.
RX 7900 XT;1;0.43957701325416565;Ja maar dan draait hij altijd op je igpu, en nooit op je dgpu zover ik weet. Dus niet dat je dan desktop werk op igpu doet en spellen op de dgpu.
RX 7900 XT;1;0.4754151403903961;Nee hoor, had in de games gewoon fps zoals met de videokaart zelf.
RX 7900 XT;1;0.6018384099006653;Dit doet mijn GTX 1080 default ook gewoon. Als je een tweede monitor aansluit weigert hij naar idle state te gaan en blijft hij 1200-1700 MHz draaien en warm worden. Ik heb ooit een of ander tooltje geinstalleerd die specifiek hiervoor is gemaakt waarin je kunt customizen bij welke 2D en 3D load de kaart uit idle mag komen Dus ik ben het zeker met je eens dat een multimonitor idle test zeer nuttig zou zijn, maar ik denk niet dat de resultaten per se zo zullen zijn als jij schetst Dat het niet passief te koelen is maakt mij niet uit. Ik zet die 0 fan mode per definitie uit. Gewoon altijd minimaal 5% dutycycle. Daar merk je in geluid helemaal niets van, maar het scheelt je sowieso meer dan 10 graden idle. Ik test het met iedere nieuwe gpu altijd even. in de avond als het stil is thuis zet ik even met het handje de duty cycle steeds 1% hoger totdat ik hem kan horen veranderen. Dan zet ik hem weer 1 of 2% terug en dat wordt het minimum percentage Om dezelfde redenatie hebben bequiet voedingen bijvoorbeeld ook geen semipassieve fan, maar draait hij gewoon heel langzaam bij lage load
RX 7900 XT;3;0.48012876510620117;Maar als je gpu idle 80W verbruikt zal een minimale snelheid mogelijk ook niet afdoende zijn. Afgezien daarvan dat zoiets een serieus verschil maakt op de jaarrekening als je je PC ook voor kantoorwerk gebruikt zo’n 1600 uur op jaarbasis. Dat scheelt gerust een kWh of 100 even snel gerekend. Mijn ervaringen met tools om verbruik van AMD kaarten te drukken zijn met m’n R9 niet erg positief. Die wordt bij de geringste aanpassing overigens al instabiel. Vandaar dat een officiële correctie voor mij wel wenselijk is. Maar dat terzijde. Het is wel een goed alternatief en goede tip
RX 7900 XT;2;0.44731035828590393;Misschien even kijken of anderen er al een oplossing voor hebben. Ik had er ook last van. Mijn GTX 1080 liep 50-60 watt idle. Nvidia inspector geinstalleerd, multi display power saver aangezet en hoppa, 13 watt. Stukken beter. 80 watt is voor de gemiddelde gpu koeler piece of cake. Een 6800-6800XT bijvoorbeeld trekt meer dan 250 watt onder load. Een schamele 80 watt kunnen ze prima geruisloos koelen. De meeste aardige kwaliteit AIB kaarten zijn al bijna onhoorbaar tijdens het gamen. Dan kunnen ze echt wel op minimaal toerental 80 watt aan.
RX 7900 XT;2;0.40611332654953003;staat wel niet bij met welke multimonitor setup dit getest is en in hoeverre deze monitors op elkaar afgesteld zijn. op mijn 6800XT had ik dat probleem ook tot ik 2 * monitor met zelfde resolutie (4k) en refresh rate (144hz) eropzet en het was opgelost... weet dus niet of het aan refresh rate ligt of aan resolutie. het is wel een pak hoger verbruik dan de 6000 serie idle met dit probleem...
RX 7900 XT;2;0.4777821898460388;Als dat de oplossing is, dan worden Nvidia GPU's ineens relatief een flink stuk goedkoper. Daarnaast is er, zeker voor niet-gaming doeleinden, vaak sprake van een asynchrone setup natuurlijk. Is het niet vanwege het gebruiken van een oudere monitor als secundair scherm, dan wel omdat de ene in landscape en de ander in portrait modus staat. Maar je hebt gelijk, die nuance dient wel gemaakt te worden, er zijn mensen waar dat natuurlijk de situatie is.
RX 7900 XT;3;0.4524793028831482;combinatie van oude monitor, kleuren afstelling en verschillende pixel size ga je je enkel toch maar irriteren op de verschillen en het windows ongemak....
RX 7900 XT;3;0.383937269449234;Dat is geen normale OCD maar OC’D OCD
RX 7900 XT;2;0.5072572231292725;Dit doet mijn 1070 ook anders. Al is daar een Quick fix voor. Maar loopt hij warm te draaien en hoge clocks terwijl hij niks doet. Dus denk AMD het wel fixt met een drivers.
RX 7900 XT;4;0.364163339138031;Interessant, want mijn 1070 doet dat niet. Ik zit nu in dual monitor set-up en hij verbruikt ongeveer 10 watt en de gpu clocksnelheid is 139mhz. Ik heb hier ook nooit problemen mee gehad
RX 7900 XT;3;0.46291813254356384;Met de huidige prijzen is dat imo zeker wel iets dat ze even aan de test of iig aan de conclusie zouden moeten toevoegen.
RX 7900 XT;5;0.3457639217376709;Komt snel een update voor, is al aangekondigd.
RX 7900 XT;1;0.24268148839473724;Dit is een driver issue en bevestigd door AMD. Hier wordt aan gewerkt en wordt door middel van een patch opgelost. Zie de review van LTT op YouTube.
RX 7900 XT;2;0.43691691756248474;Haha in mijn appartement gaat de kachel ook al amper aan. Stadsverwarming verbruik is bij on bijzonder laag. Dit jaar is het wel lang leven de vega 64, de gratis elektrische kachel!
RX 7900 XT;2;0.5656667947769165;Ondanks dat de prijs prestatie verhouding qua Raster Prima is heeft AMD wat mij betreft wel een beetje de plank mis geslagen. Ze laten mooie presentaties zien met 50-70% prestatie winst vervolgens is het eerder 35-40% en worden die hogere prestaties alleen enigszins met RT gehaald. In het verleden waren AMD presentaties nog te geloven en kreeg je ook echt wat je in die slides zag. Maar nu zijn ze dus gigantisch aan het cherry picken geweest en komt het resultaat niet in de van de beloofde waardes in de presentatie. Helaas gaan ze dus ook met dit soort dingen meer de kant van nVidia op. Erg jammer want je hebt hier helemaal niets aan. Consumenten komen er toch wel achter als ze maar een review lezen. Een beetje jammer want op deze manier verpest je het vooral voor jezelf. Voortaan dus ook bij AMD niets meer van de marketing slides geloven. Dan over het resultaat. Een erg magere winst tov de 6950XT gezien dit een hele nieuwe generatie is. Waar de 6900XT en 6950XT het nVidia in raster moeilijk konden maken lukt AMD dat nu nog helemaal niet. Sommige games doen het heel goed en dan zie je de potentie van de architectuur maar meestal komt dat niet naar boven. Of dat nu de Drivers zijn of toch wat nadelen van de eerste keer met chiplets werken zal de tijd leren. Er gaan ook geruchten over een eventuele N31+ / respin die mogelijk hoger kan clocken. Of dat waar is zal de tijd ook leren maar deze resultaten vallen toch wel tegen na alle leaks en eigenlijk ook als je naar de spec's en het productie process kijkt. Ik had verwacht dat die kaart toch wat hoger zou gaan clocken. Pas als N32 uit komt kunnen we zien of er inderdaad wat mis was met de N31 opzet waardoor die minder hoog clockt of dat RNDA3 toch minder potent is dan gehoopt. Al met al is het wel een upgrade maar qua prijs prestatie verhouding gaan we er niet heel veel op vooruit.
RX 7900 XT;2;0.3225991129875183;"AL hun presentaties waren dan ook ""tot"" en ze halen dat gewoon. Misschien wou je het gewoon geloven maar dit was niet anders dan elke keer ervoor. De 6950XT is een pak boven de 7900 xtx als je appelen met appelen vergelijk is qua prijs de 7900 xtx de 6900xt van vorige generatie. EN daar zie je 50-100% winst mee in 4k/ultra wat heel mooi is qua winst tegen een gelijke prijs bij release. vergelijken met de enorm veel duurdere 4090 is beetje absurd, de 7900 XTX scoort een pak beter dan de 4080 in raster en kost een pak minder. Als je dat niet goed genoeg vind ..."
RX 7900 XT;2;0.41527754068374634;klopt maar dit is wel marketing ten top natuurlijk tot 70 % average maar 31 - 38 % halen afhankelijk van de site die je leest. dat is wel heel wat anders dan het beeld van 50 - 70 % wat ze schetsen in de presentatie want daar laten ze geen lagere scores zien. de rdna2 gpu ' s konden waar maken wat ze lieten zien in de slides en deden dat consistent in het merendeel van de games. maar hier is het andersom. dan is het af en toe een uitschieter waar de prestaties goed zijn. de drivers of de chiplet strategie zijn duidelijk nog niet op orde. ik neig nu nog wat meer naar driver issues en hopelijk kunnen ze de prestaties wat opkrikken. maar voorheen schetste amd een realistisch beeld in presentaties van wat je gemiddeld kon verwachten. dat is nu overduidelijk voorbij. ik draag amd al jaren lang een warm hart toe maar dit soort dingen keur ik gewoon niet goed. ik snap niet waarom ze zich tot het niveau van nvidia moeten verlagen. vergeet niet dat in tegenstelling tot de cpu markt amd in de gpu markt al lang niet meer zo dichtbij is gekomen als met rdna2. ze hadden minimaal moeten zorgen dat het prestatie gat gelijk zou blijven. en liever wat inlopen / de 4090 verslaan want er komt nog een 4090 ti. vervolgens kunnen ze de 4080 maar net aan. dus de 4080 ti, 4090 en 4090 ti zullen allemaal sneller zijn dan de 7900xtx ( tenzij amd een wonder driver kan uitbrengen ). en dat is een duidelijke achteruitgang. ook qua performance per watt doen ze het momenteel niet goed ook daar val rdna3 tegen tov rdna2 ja het is een vooruitgang maar niet zo goed als je van een 5nm product mag verwachten. en mogelijk zijn de chiplets daar wel de oorzaak. maar je ziet dat nvidia hier echt voordeel heeft met hun monolithic 4nm design. ik ben in ieder geval klaar met amd free passes geven omdat ze de underdog zijn. ze staan er nu goed voor. hebben meer geld dan ooit. en vragen ook veel hogere prijzen voor hun producten. dan mag je ook wat kritischer zijn en er meer van verwachten.
RX 7900 XT;2;0.48594269156455994;"neen dat is niet correct : de 70 % was specifiek in 1 benchmark, de rest van de cherry picked benchmarks gaven tot 50 %. toen ik die zag dacht ik "" mooi dus kan je ergens tot zoveel verwahcten in bepaalde games "" als je dan denkt dat het in alle games 50 - 70 % is dan maak je jezelf gewoonweg iets wijs. enkel als je specifiek dezelfde settings overnam en dat was het in andere games in veel gevallen minder. hier hebben we geen settings en zelfs geen specifiek getal gewoon "" tot x % "" ( wat ze de vorige keer ook zo vermelde trouwens maar dan met fps ). neen bepaalde games, maar idd met rt kom je op nog hogere waardes tov de 6900 ' s, en wat ze kosten in de vs doet er weinig toe. hie kost een 6900xt nog steeds 850 - 900 euro je moet vergelijken met wat er nu te koop is. de 4080 is duurder en in wat voor de meeste mensen belangrijk is : raster, trager en duurder. dat je dan nog veel meer prestatie vraagt tegen een log lagere prijs is een beetje absurd. vergeleken met wat nvidia uitbracht is deze kaart gewoon beter geprijsd qua performance zelfs tegen de 4090. het geld ligt niet in het high end segment, dat ligt vooral daaronder. high end is vooral om aandacht te krijgen en de mensne dan te doen geloven dat de lagere kaarten even goed zijn. 4080 en 4090 zijn absurd hoog geprijsd, dat amd daar niet mee meedoet is goed ( en logisch ze zijn de underdog en moeten in princiepe wel ) dit een flop noemen omdat ze alle achterstand niet kunnen wegwerken gaat veel te ver en weeral gaat eerder over dat je jezelf onrealistische verwachtingen aangepraat hebt dan iets anders. gezien de huidige prijzen en gezien de prestaties vind ik het alvast een goede kaart van amd. de voornaamste concurent is hun eigen 6800 lijn, tegen dat die volleldig uitverkoicht is zal de opvolger in de 7x00 lijn daar ongetwijfeld zijn en bouwt die verder op deze prijs / prestaties."
RX 7900 XT;2;0.5078128576278687;als het nu 50 % was geweest was het ook prima. maar het is vaker veel minder tot zo laag als 15 - 20 %. gemiddeld is het slechts 31 - 38 % afhankelijk van de review dat is ver onder de 50 %. daar deden ze ook geen vergelijking met een andere kaart als ik me niet vergis. maar de cijfers die ze lieten zien kwamen overeen met de reviews. alleen was de geteste scene soms wel anders waardoor je andere waardes had maar dat lag veel meer in lijn met de werkelijkheid dan de 50 % die ze hier lieten zien en bijna nooit gehaald wordt. 50 % is al cherry picked met de huidige drivers. we weten nog niet wat ze hier gaan kosten. het zou mij niet verbazen als de aib versies richting de 1200 - 1300 euro gaan we zullen het straks zien. want wat je zelf ook aanhaalt de prijs in amerika is niet de prijs hier. dat ze daar een 7900 xtx voor 999 kunnen kopen is leuk maar wij moeten in euro ' s heel veel meer gaan betalen. en stroomverbruik is zeker belangrijk. het is niet slecht maar had gewoon veel beter kunnen zijn. qua value for money ga je er tov rdna2 niet op vooruit. en qua idle verbruik en geluid productie ga je er op achteruit. ik vind dat best dingen om over na te denken. misschien is dat voor sommige mensen niet belangrijk. maar voor mij persoonlijk is dat wel belangrijk. ik ga er overigens wel een testen ( mits hij rond de 1100 euro blijft ) omdat ik de afgelopen jaren ervaringen topic ' s op tweakers gemaakt heb met core clock / power scaling tests en dat wil ik deze generatie ook doen. ik weet alleen niet of ik de kaart uiteindelijk ga houden of dat ik weer terug ga naar een 6800xt of 6950xt. hopelijk kan amd de nodige zaken via de drivers verbeteren. en vooral de games die nu weinig winst laten zien wat verder opkrikken. anders is een 2e hands 6800xt voor 525 - 575 gewoon interessanter gezien de huidige nadelen bij in ieder geval de referentie kaarten. ( maar dat is mijn persoonlijke mening ).
RX 7900 XT;2;0.4287000894546509;"Omdat je andere games erbij haalt dan die AMD liet zien. Nogmaals dat komt omdat je droomede van 70% beter overal wat imho absurd is. Zo een sprongen maken generaties niet, niet amd en niet nvidia. Gezien wat nvidia deed is wat amd deed gelijkaardig, beter dan de 4080, minder dan de 4090 met als verschil dat bij beide ook de prijs naarboven ging en bij AMD niet. Jawel hoor en ook met de ""up to"". Hebben ze nu iets harder ge cherrypicked ? Waarschijnlijk wel, dan afronden en met de grafieken die je toonde en sommige beginnen te dromen dat AMD tegen de helft van de prijs een 4090 op de mark zet. Ik kan je zeggen dat dit nooit zal gebeuren . Ik vergelijk wat we momenteel kunnen vergelijken, en idd gezien een 4080 hier al 1400-1500 euro kost zal de 7900xtx ergens tussen de 1100-1300 vallen vermoed ik. Hoe dichter tegen de 1400 hoe onaantrekelijker de kaart natuurlijk word. Stroom verbruik is echt geen issue, 3080 vebruikt ook dubbel zoveel in idle als een 6800 geen haan die ernaar kraaide. Idle zonder meerdere monitors verbruikt de 7900 xtx trouwens veel minder tov de 4080/4090 : 11wat tegen 14/22 watt. Ziende dat 2-4% multimonitor is maakt dit weinig uit, en zoals al gezegd waarschijnlijk een bug die men wel oplost . Kost per frame is de 7900xtx 15% lager dan de 6950xt en is de 4080 12% duurder tov de 3080 idle verbuik is (die bug niet meegerekend) een paar watt hoger dan de vorige generatie en nog steeds lager dan wat nvidia heeft (daar ben je fout in) . Geluid is lager dan bij de 6950 xt en luider dan de 6900/6800 maar de temeprturen zijn beter, ongetwijfeld als de de fans wat terugdraait je op iets gelijkaardig uitkomt als de vorige generatie. En idd iets om bij na te denken als je deze koopt maar valt al bij al echt wel mee imho."
RX 7900 XT;2;0.4695018231868744;Ik heb het ook niet zo zeer over 70% maar vooral die 50% die wordt ook gewoon bijna niet gehaald. HWU heeft behoorlijk wat games getest. TPU ook en daar zie je hetzelfde zelfs die 50% is gewoon vaak niet haalbaar. Nee met de 50-70% terwijl het gemiddelde op de 35% ligt. Stop er dan ook gewoon een paar games van rond de 35% in om een realistisch beeld te krijgen. Nu hebben ze alleen de beste resultaten gepakt. En dat is cherry picking ja. Dat ze de resultaten van 20% niet laten zien kan ik mij nog iets bij voorstellen al was dat wel beter geweest dan ben je gewoon eerlijk over je product. En dan zou niemand teleurgesteld zijn bij de reviews. Je kan dan ook prima de 50-70% resultaten laten zien dan krijg je gewoon een beter totaal plaatje. Nu krijg je alleen het rooskleurige plaatje. 1165-1199 is de daadwerkelijke prijs geworden. Ik kraai wel naar het idle verbruik. Ik zal een van de weinige zijn. Al heb ik meer mensen gezien die het belangrijk vinden. Multi monitor @ 85 watt moet wel echt opgelost gaan worden tho. Zou kunnen de 6950XT kan stock erg luid zijn die cooler is identiek aan die van de 6900XT en die koelers kunnen prima met 300 watt omgaan maar bij meer zijn ze niet zo stil meer. Het verschilt wel een beetje van kaart tot kaart. Sommige hadden hogere hot spot temps en die fan's waren dan ook een stuk luider. Je kan die resultaten zien in het Navi 2X ervaringen topic ik heb echt veel 6800XT, 6900XT en een paar 6950XT kaarten getest. Des al niet te min is een luidere fan tov de 6900XT jammer. En gezien de temps hadden ze de fan curve ook iets aan kunnen passen maar dat geef je zelf ook al aan. Ik snap alleen niet waarom AMD dat dan zelf niet doet. Want ze weten na vele jaren hoe belangrijk mensen geluidproductie vinden.
RX 7900 XT;2;0.35148096084594727;CODMW2 : +45% slide : 50% Legion (vind enkel met raytacing terug) : +55% slide (zonder RT) : 50% Cyberpunk +59% slide: +70% Metro exodus : +55% slide : +50% ... Zonder twijfel kan je met de juiste settings die waardes van AMD verkrijgen. Zijn deze titels representatief? Tja, heel populaire recente games en natuurlijk pakt AMD er de beste uit, dat is toch echt maar logisch. Ze hadden zelfs kunnen beweren sneller te zijn dan de 4090 want dat gebeurt ook heel af en toe. Het is dan toch echt aan jou dat jij ervan maakt dat deze een gemidelde zijn terwijl AMD dat nooit gezegd heeft. Wat dus 250 euro minder is dan nvidia (of 4080 is 22% duurder), voor betere performance in rasterization kan je toch echt geen slecht resultaat noemen voor algemeen gamen met deze kaart. Als over idle : nogmaals dat is louter voor multi monitor, alle andere gevallen vebruikt die veel minder dan een 4080 laat staan 4090 wat volgens jou het verschil dus nog veel groter zal maken qua prijs. Zelfs voor geluid en waremte: als je die links hierboven ziet is het duidelijk dat AMD hun kaarten agressiever koelt om legere temperaturen te krijgen tegen meer lawaai. Brengt die temperatuur naar iets gelijkaardig die 6800/6900 heeft en qua lawaai zullen die niet verschillen. Logisch ze verstoken ongeveer evenveel en de koelers zijn heel gelijkaaardig. Dat de formfactor hetzelfde is en niet enorm absurd groot en je niet moet gaan klooien met andere stroomkabels zijn enkel voordelen voor AMD.
RX 7900 XT;2;0.36672329902648926;dit zijn echt de beste resultaten. zullen we er nog een paar bij gaan pakken? en dit zijn ook geen onpopulaire titels. ik ben het met mlid eens. het zou goed kunnen dat ze verwacht hadden in meer games die 50 - 70 % te kunnen halen maar toch tegen te veel issues aan gelopen zijn. want zoals gezegd is de range groot waar de 6950xt tov de nvidia kaarten heel consistent is qua prestaties gaat de nieuwe 7900 serie alle kanten op. dus iets zit duidelijk niet lekker. maar in het verleden deden ze dit : en ja dit was ook rooskleuriger want rage mode stond aan en sam ook waardoor de radeon kaarten wat sneller waren. nvidia had toen nog geen rebar. maar toen de reviews uit waren klopte de positionering wel : je moet wel even naar de avg fps kijken maar het werd waar gemaakt. de slides waren te geloven. ik ben het helemaal met mild eens hij denkt er hetzelfde over als ik : amd heeft een maand terug gewoon een te mooi plaatje laten zien. en om eerlijk te zijn zo gek was het niet om 50 - 70 % te verwachten. maar als je dan een standaard gebruiker hebt die normaal nvidia had en dan amd een keer probeert. die vervolgens herrie hoort, slechte prestaties ziet in sommige spellen die hij of zij speelt of crashes krijgt. zal dan denken nvidia was echt beter laat ik maar weer terug gaan. ik heb het al te vaak gelezen. vooral ten tijden van de 5700xt waar blackscreen een probleem waren. dit is natuurlijk minder erg. maar wil je mensen echt over halen. en in jou kamp houden zul je het echt heel goed moeten doen. en ik wil dat ook graag zien. ik wil graag dat de markt beter in balans gaat komen dat nvidia niet 80 % of hoger in handen heeft want dat is niet goed voor ons als consument. maar dan is het jammer als er van dit soort dingen mis gaan. en sommige van deze dingen zoals de afstelling van de fan ' s of coilwhine hadden voorkomen of verder beperkt kunnen worden tijdens het ontwikkel proces. dus ik zeg dit juist van uit passie niet om maar zonder goede onderbouwing amd zwart te maken.
RX 7900 XT;1;0.4244144558906555;"dit was als commentaar op "" ze halen zelfs niet de 1. 5 - 1. 7 "" dat doen ze dus wel. ja er zijn andere titels die minder winst laten zien dat veranderd niks dat we ze toonde wel klopt volgesnde reviews. zou misschien kunnen, kan je moeilijk nu rekening mee houden. is pure speculatie. bevat ook veel "" up to "" dat is iets wat amd en nvidia altijd gebruiken. en zoals aangetoond die 1. 5 en 1. 7 klopt gewoon. dat was het wel, gemiddeld 60 % winst zou bijna ongeevenaard zijn, en dan nog eens bakken goedkoper dan nvidia? sorry men zegt altijd als iets te mooi is om waar te zijn... de performance winst van de 7000 series is on par met winsten ervoor en zelfs een pak hoger in rt. je blijft praten over teleurstellingen, wat uitmaakt is wat mensen kopen en in dat opzicht is voor deze prijs de 7900 xtx gewoon de betere keuze. het is dat of ultra high end gaan met compleet van de pot gerukte prijzen. dat het gemiddelde maar tegen de 40 % is en niet 50 % is maakt darin weinig uit, in rt klopt dit wel trouwens daar was amd gewoon correct. is beetje absurd, amd weet zelf wel gerust hoe hun bedrijf te runnen. err is weinig reden voor amd om zichzelf te benadelen. tegen dat deze kaarten in mensen hun handen zitten zijn die handvol bugs er heus wel uit, reviewers zijn soms daarin veel te voortvarend en zoeken gewoon issues om toch maar aandacht te krijgen. het kabel debacle van nvidia is het beste voorbeeld, uiteindelijk was het gewoon user error nadat reviewers mensen bang lagen te maken dat hun huis ging afbranden. het is en blijft gewoon momenteel de beste kaart in dat prijs segment "" het had beter gekund "" tja het kan altijd beter, geen enkele kaart is ooit perfect. doen alsof dit zo eenvoudig is is gewoon verkeerd. soit het onderste segment is het belangrijkste we zullen zien wat amd en nvidia daarin brengen."
RX 7900 XT;2;0.450834721326828;We gaan het er duidelijk niet eens over worden. Onze meningen verschillen gewoon en dat mag. Ik vind het zelf een beetje onnodig om @ launch zo veel bugs in de kaart te hebben zitten. Zeker als de kaart dan ook nog eens 1000 euro kost. Dat geld ook voor nVidia. Die power connector problemen zouden ook niet mogen gebeuren die had ook beter ontworpen moeten worden.
RX 7900 XT;2;0.4823334813117981;"Idd die ""bugs"" zijn veelal iets persoonlijk voor jou en maken weinig uit voor de meeste of komen zelfs niet veelvuldig voor, performance en prijs is daar dat is het belangrijkste voor de meeste."
RX 7900 XT;5;0.3546141982078552;En nu worden de xtx aangeboden voor 1.499 bij Alternate 😂
RX 7900 XT;5;0.24069362878799438;Yeah winkels gaan nu aan het scalpen omdat ze populair zijn
RX 7900 XT;3;0.4360993504524231;Het is volgens hardware unboxed de beste price to performance op MSRP op dit moment. Bizar dat een flagship deze titel heeftil, ze verleiden mij bijna tot koop. Maar daar waren ze er ook al negatief over de kaart. Ik ben juist onder de indruk dat mn voorspelling daarover klopte. Alleen energyverbruik valt beetje tegen. Dat was ook echt beter voorspelt vs 4080. Ben benieuw hoe ze downscalen in lagere wattages.
RX 7900 XT;5;0.47788164019584656;Er zijn altijd negatieve zaken op te merken, er zijn weinig kaarten die perfect zijn. Het feit blijft dat high end is dit de beste kaart die er momenteel is voor de meeste gamers.
RX 7900 XT;1;0.37739866971969604;Waar heeft u het over??
RX 7900 XT;1;0.5282890200614929;Welk stuk bedoel je? en wat is er niet duidelijk?
RX 7900 XT;1;0.7657938003540039;Die prijzen beginnen ook gewoon echt absurd te worden, ik ga niet meer beginnen aan NVidea. Ik heb heel lang getwijfeld om mijn 1080 Ti te gaan upgraden naar een 3/4000-series of de 7900 XTX, uiteindelijk vanwege de kosten maar niet gedaan i.v.m. de hogere prijzen tegenwoordig. Uiteindelijk heb ik halverwege November een mooie 6900 XT kunnen bemachtigen bij Azerty voor slechts 731 euro. En ook de prijs van deze kaart fluctueert alweer rond de 1000 euro: uitvoering: ASUS TUF Gaming Radeon RX 6900 XT (Hoe snel kan het gaan met de prijzen).
RX 7900 XT;1;0.9131137728691101;Gewoon niets meer kopen. Dat is de enige manier om de prijzen weer normaal te krijgen. Zolang ik geen kaart meer kan kopen dat tussen de 200 en 300 euro kost waarmee je redelijk een nieuw spel kan spelen, game ik niet meer op de PC. Dan gaan we wel over op een Xbox of steamdeck
RX 7900 XT;3;0.3129662275314331;U vraagt, wij draaien. De RX 6600, daar kan je prima nieuwe games op spelen. Voor slechts €269 is die voor jou. En met een beetje geluk krijg je met de nieuwe generatie nog meer bang voor je buck. Desalniettemin ben ik het volledig met je eens: er zijn prima andere opties dan een nieuwe GPU kopen. Ik kan nog steeds alles spelen met mijn 1070 en die is tweedehands €175. Heck, met een goede APU kan je ook de tijd overbruggen.
RX 7900 XT;2;0.6080358624458313;Mijn 1070 recent overclocked en nu draait ie de meeste games op 1440p weer prima rond de 60fps. Zou het wat beter kunnen, sure, maar ik vind het de investering nog echt niet waard. Wellicht als de 7700 of 7600 uit komt en niet te veel kost...
RX 7900 XT;1;0.4137742221355438;En dan te bedenken dat het gaat om de adviesprijs hè. Leuk om te lezen dat jij ook eentje bent die nog net een 6900XT voor een zacht prijsje heeft kunnen bemachtigen: het is mij ook in november gelukt, namelijk 699€(!!!!) bij Megekko. Misschien was die prijs wel een fout van een algoritme want na mijn bestelling klom hij nagenoeg direct weer richting de 800€. Edit: betreft die van AsRock.
RX 7900 XT;2;0.35734567046165466;"Nvidia.. Ik zie veel comments hier over de hoge prijzen van de 4xxx Nvidia kaarten, maar let wel: je gaat met een dergelijk kaart waarschijnlijk wel 3 a 4 maal zo lang doen als je ooit met een videokaart hebt gedaan. let me explain; DLLS 3.0 heeft frame interpolatie. Oftewel; in 2030 zul je ( hoe zwaar de games ook worden ) nog steeds op 120+ frames kunnen gamen. 4K zal nog heel lang het maximale mainstream blijven. Al zakt de native framerate naar 24 frames, interpolatie lost dat gewoon op, en interpoleerd met gemak naar 120+ frames. TV's doen dat al jaren. Ik ben echt een enorme liefhebber van hoge framerates, en heb juist doormiddel van frame interpolatie vele games kunnen spelen die ik anders nooit zou hebben gespeeld op mijn PS4 en Switch. Mijn LG tv bijvoorbeeld interpoleerd naar 120 frames, en dat is echt brilliant. Enigste nadeel van mijn LG TV is dat het Lag geeft, en wat beeld anomalies, maar dat heb ik er prima voor over. ( games als RDR2, TLOU, Tomb Raider etc zijn prima te spelen met een klein beetje extra lag ) De DLLS 3.0 techniek van Nvidia gaat dat echt veel beter doen. ( neem ik aan ) Dat betekend dat ( als je met de eventuele nadelen van interpolatie kan leven ), je oneindig door kan gaan met een dergelijke kaart. Ik heb mijn keuze al gemaakt, en ga voor een 4080/4090. ( heb huidig een 3080 ). Het verbaasd mij dat deze techniek uberhaupt zo lang op zich heeft laten wachten. Ik heb het jaren geleden al voorspeld dat deze techniek naar GPU's zou komen. En nu, met de huidige AI mogelijkheden, ben ik er van overtuigd dat dit echt de oplossing voor langdurig gebruik van GPU's gaat brengen. Lag-free that is.."
RX 7900 XT;4;0.4070769250392914;Erg nette performance. Op RT na vallen beide kaarten me enorm mee. Tenzij Nvidia het licht ziet en de 4080 wat normaler gaat prijzen kies ik (en denk ik velen met mij) dit keer gewoon voor AMD.
RX 7900 XT;5;0.48087772727012634;Herkenbaar. Ik zit zelf met een 2080 TI en wil graag wat soepeler op 4k spelen maar een speciale ATX 3.0 voeding, een power connector die problemen kan geven het het idiote bedrag wat Nvidia durft te vragen skip ik die. Ik ben heel benieuwd naar de prijzen iig!
RX 7900 XT;3;0.43169453740119934;Je hebt geen ATX 3.0 voeding nodig voor de RTX 4x00. Wel een verloopkabeltje.
RX 7900 XT;3;0.5674500465393066;moeten niet nee maar gezien de hoeveelheid stroom dat ding trekt is het wel handig om je PSU te helpen met info over de attached hardware. Daarnaast wil je niet het risico lopen met een 2200 euro kostende videokaart.
RX 7900 XT;2;0.31394627690315247;Volgens mij blijkt nergens uit dat dit nodig zou zijn
RX 7900 XT;3;0.3642212152481079;Het is ook geen eis nee. dat heb ik nooit gezegd. Wel zijn er al tal van voorbeelden van afgefikte kabels dus voorzichtigheid is wel gepast.
RX 7900 XT;2;0.42811715602874756;Voorzichtig zijn kan zeker geen kwaad, maar voor zover ik zie zijn er ook met ATX 3.0-voedingen smeltproblemen. Ik wil maar zeggen: er lijkt me niks dag zo'n nieuwe voeding zelfs maar nuttig maakt
RX 7900 XT;5;0.3713255226612091;xtx vandaag bij alternate voor de vriendenprijs van 1499 😂
RX 7900 XT;3;0.41212770342826843;Ik sta op het punt een MSI 4080 af te rekenen voor 1500. Gezien de prijzen van een 6900XT etc. zal een 7900XT(X) toch niet veel goedkoper uitvallen lijkt me?
RX 7900 XT;1;0.5449605584144592;Neem dit met een korrel zout gezien de instabiele Euro, maar ga als maximale prijzen hier vanuit: De adviesprijs van de RX 7900 XT komt uit op 899 Amerikaanse dollar, de RX 7900 XTX krijgt een adviesprijs van 999 dollar. Omgerekend naar euro's is dat met de huidige wisselkoers inclusief btw momenteel 1110 euro voor de RX 7900 XT en 1233 euro voor de RX 7900 XTX.
RX 7900 XT;3;0.5300113558769226;Het gaat op dit moment wel weer wat beter met de euro, omgerekend is de 7900XT €1036 en de 7900XTX €1151. Nog steeds niet goedkoper dan een 6900XT, maar niet zoveel duurder.
RX 7900 XT;5;0.47839027643203735;1499 alternate vandaag 😂 ze mogen hem houden..
RX 7900 XT;1;0.5224242210388184;"Wat je noem je ""veel goedkoper""? Is 300 euro goedkoper veel? Als je een 6950XT voor 899 op voorraad kan krijgen, dan lijkt 1200 euro voor een 7900XT(X) prima haalbaar. Ik zou als ik jou was een paar dagen wachten tot je de echte prijzen gaat zien. En of Nvidia of de winkels nog iets aan de prijs van de 4080 gaat doen. Want die is echt te hoog voor wat ie levert en winkels komen er niet vanaf. Dus die prijs zal echt wel gaan zakken."
RX 7900 XT;5;0.2419336438179016;De 6900xt kan je voor 850 euro halen..
RX 7900 XT;1;0.5396453738212585;En van de week voor 50 euro meer de 6950XT.
RX 7900 XT;1;0.3017394542694092;Ook ver boven advies dan, toch?
RX 7900 XT;1;0.4839319586753845;Nee? Adviesprijs voor de verlaging was 1000 euro
RX 7900 XT;1;0.4869624078273773;Ik ben geen Google pagina. Nog los van dat je originele comment helemaal niet over adviesprijzen ging.
RX 7900 XT;3;0.33876490592956543;700 dollar. Voor het gemak: 700euro+belasting, dan zit je er ongeveer.
RX 7900 XT;1;0.6464839577674866;Er is voor NL zelfs nog geen officiële verlaging van de 6900XT of 6950XT, die kosten bij AMD nog altijd €1163 en €1280 respectievelijk ( ), wat als je dit terugrekent naar dollars excl btw, laat zien dat AMD voor NL nog altijd $999 als MSRP voor de 6900XT reference hanteert en $1099 voor de 6950XT, in plaats van de verlaagde MSRP's die je in de US ziet. Terwijl wanneer AMD de MSRP's officieel zou hebben aangepast voor NL, je die veranderding daar normaliter direct ziet.
RX 7900 XT;1;0.4990468919277191;@SHiNeye Wat probeer je te doen? Eerst ging het nog over de prijs vs de 4080, dat argument ben je kwijt, begin je maar over de adviesprijs, dat argument ben je kwijt, begin je maar over latere adviesprijzen, ook daar heb je geen gelijk? Bestel gewoon wat je wilt dat is je goed recht en dat is jou zaak, maar vervolgens je keuze hier proberen te verantwoorden met loze argumenten (en met welk doel?) heeft niemand wat aan.
RX 7900 XT;3;0.3665408194065094;Gaat me erom dat ik me niet kan voorstellen dat een 7900XT(X) voor veel goedkoper zal worden dan een 4080.
RX 7900 XT;5;0.2990112900733948;op AMD website is de 7900 XT weer op voorraad voor 1050
RX 7900 XT;1;0.3332327604293823;Ah dat is geen verkeerde prijs.
RX 7900 XT;3;0.41947466135025024;De 7900xtx gaat echt voor een stuk minder over de toonbank morgen
RX 7900 XT;1;0.4263724982738495;Ben benieuwd. De 4080 kan altijd nog retour. Hopelijk test iemand de 4080 vs 7900XT incl DLSS, want dat zou ik sowieso aanzetten.
RX 7900 XT;1;0.3710283041000366;Als FSR net zo goed kan upscalen als DLSS wil ik die vergelijking best zien ja. Heb tot op heden alleen DLSS gebruikt, en heb begrepen dat FSR nog niet op het niveau van DLSS zit. En hoezo spammen? Discussie voeren is in jouw ogen spammen?
RX 7900 XT;1;0.3398294746875763;In dat artikel wordt toch ook gezegd dat FSR nog niet op het niveau is van DLSS..
RX 7900 XT;3;0.323510080575943;Dat klopt maar het is duidelijk dat je op zoek bent naar een Nvidia kaart, dat doorziet @d3x prima en daar komt de ergernis dan ook vandaan dat iemand zogenaamd geïnteresseerd komt doen. Ik had hiervoor de 3080 en een 6900XT en gezien ik geen upscaling wil gebruiken heb ik juist de AMD gehouden. En voor de prijzen die ik verwacht ga ik vanmiddag hopelijk een 7900XTX op de kop kunnen tikken en later misschien nog een 4080 dan kan ik ze weer vergelijken en heb ik weer hetgeen het beste is voor mij
RX 7900 XT;4;0.2846148908138275;
RX 7900 XT;3;0.5513821244239807;Is nog afwachten. Voor mij is het ray tracing performance gat te groot dus vooralsnog blijf ik bij nvidia.
RX 7900 XT;2;0.30922824144363403;Hoe relevant is Ray Tracing nog, nu AMD samenwerkt met Epic Games voor de Unreal engine en daar de techniek Lumen bij ingebakken zit?
RX 7900 XT;1;0.7319632768630981;Ray tracing is überhaupt een niche binnen een niche. Dure videokaarten van meer dan pak 'em beet 600 euro is al een niche opzich, volgens mij minder dan 5% van de gamers op steam bijvoorbeeld heeft zo'n kaart. Vervolgens is van die groep die ook echt met raytracing aan speelt ook weer een kleine groep. De meeste die 800 euro voor een videokaart hebben betaald willen ook graag 100fps behalen in de spellen die ze spelen.
RX 7900 XT;3;0.43308258056640625;Lumen heeft een high-quality mode waar ze wel hardware Ray Tracing gebruiken. Dus ik zou juist zeggen het hangt af of je quality op max wil hebben of niet. Zelfde met de 'next-gen' Witcher 3 update, zie ook Als je de de software ray-tracing van Luman van plan bent te gebruiken, dan is wel compute belangrijk, iets wat veel reviewers nog niet goed testen.
RX 7900 XT;3;0.3924287259578705;Interessant, ik wist niet dat er ook een hardware RT modus was van Lumen, dank.
RX 7900 XT;3;0.4907322824001312;Dat hangt van de toepassing af. Lumen's voordeel, minder krachtige hardware nodig, begint nu wel een een beetje te vervagen met deze generatie kaarten. En als je bijvoorbeeld grafisch werk doet, is Ray Tracing toch echt beter. En de Unreal Engine is natuurlijk niet de enige engine. Het aantal spellen dat Ray Tracing ondersteunt is gewoonweg hoger.
RX 7900 XT;3;0.4010511040687561;Nou ja, dan moet je die kaarten wel kunnen of willen betalen natuurlijk, dus vervagen doet het voordeel m.i. niet, maar het klopt dat UE niet de enige engine is. Dan kun je afwegen of je de games op die andere engines erg belangrijk vindt of niet.
RX 7900 XT;1;0.6128146052360535;Ik word helemaal niet warm van RT.
RX 7900 XT;3;0.7218179702758789;Best wel relevant.. Ik heb o.a. Control en Cyberpunk 2077 met Raytracing aan gespeeld.. Ziet er echt wel een stuk mooier en realistischer uit.. Vooral de reflecties in glas zijn een zeer duidelijk verschil..
RX 7900 XT;2;0.5233884453773499;Het ziet er uiteraard veel mooier uit, maar die games zijn sowieso al niet in Unreal Engine gemaakt. Ik had het echter specifiek over UE's Lumen omdat die techniek m.i. goed genoeg in de buurt komt van RT zonder de enorme performance hit en bijkomstige stookkosten.
RX 7900 XT;5;0.5902824401855469;Dit is de realiteit! uitvoering: Sapphire AMD Radeon RX 7900 XTX 24GB
RX 7900 XT;5;0.702434778213501;Ziet er goed uit AMD. Top. Ben benieuwd wat Nvidia zijn reactie is. Zeker omdat de XTX soms echt op de hielen van 4090 zit, en soms zelfs sneller. en dat voor de voor de helft van het geld (RT even uitgesloten)
RX 7900 XT;3;0.2675885260105133;Ik denk dat je de 4080 bedoelt? De 4090 steekt overal met kop en schouders boven uit (prijskaartje is er ook naar). Dan is dat nog steeds een hele goede prestatie van AMD
RX 7900 XT;3;0.47070273756980896;Heb je wel naar alle benchmarks gekeken? genoeg benchmarks waar ze boven de 4090 uitsteken. Dus als de 7900 serie stuk beter betaalbaar is dan word het voor de gemiddelde tweaker een makkelijke keuze
RX 7900 XT;3;0.3544376492500305;Ik zie van de 10 games alleen COD hogere FPS halen dan de rest
RX 7900 XT;3;0.39917823672294617;Volgens mij pakt ie de 4090 op lagere resolutie wat vaker, maar er is denk ik niemand die een 4090 koopt om op 1440p te gaan gamen
RX 7900 XT;2;0.437995582818985;Tenzij je echt voor één spel gaat, kijk je toch naar gemiddelden? En als er genoeg benchmarks zijn waar hij boven de 4090 uitsteekt, en hij gemiddeld op ongeveer 4080 niveau uit komt, betekend het dus dat er ook genoeg benchmarks zijn waar de 4080 er ruim bovenuit steekt. (En dat lijkt één spel te zijn die getest is, maar overall is het gewoon in de buurt van een 4080 zonder ray tracing, meestal iets erboven).
RX 7900 XT;3;0.4810764491558075;En dan is het nog niets niet eens een full die. Zeker knap van Nvidia. Maar ik denk dat AMD het tog wat slimmer doet kwa prijs prestatie verhouding . Ik wil graag een 4090 maar mijn max is tog echt wel 1500.
RX 7900 XT;3;0.2992742359638214;Ik doelde op AMD die het goed doet - aangepast
RX 7900 XT;3;0.3833727240562439;Ik ook. Alleen we kunnen de ongelooflijke prestaties van de 4090 natuurlijk niet negeren. Prijs moet alleen normaal.
RX 7900 XT;2;0.511064350605011;Het zouden pas ongelooflijke prestaties zijn als er een normale prijs aan zou zitten Dat die prijs omlaag gaat kun je op je buik schrijven. Van de 4080 waarschijnlijk wel, omdat ie dezelfde prijs prestatie van de 4090 heeft, maar nu veel te zware concurentie van AMD krijgt. Maar de fans blijven toch wel 4090s kopen, omdat het verschil met AMD zodanig is dat ze het voor zichzelf kunnen rechtvaardigen toch de snelste kaart te kopen. Ook al is ie belachelijk duur. En omdat de prijs/prestatie gelijk is bij de 4080 is het makkelijker te rechtvaardigen de snelste kaart te kopen. In het verleden moest je voor 10% extra prestatie 50% extra prijs betalen. Dan dachten mensen wel twee keer na. Maar als 10% extra prestatie 10% extra prijs is, dan word dat sneller gedaan. Uiteindelijk is het meestal niet zo is, dat die 10 of 20% de doorslag geeft. Het is meer of iemand het waard vind om dat bedrag uit te geven. Niet of ze dat bedrag ook daadwerkelijk hebben.
RX 7900 XT;1;0.6261849403381348;Ik geloof er geen reet van dat die kaart goed verkoopt laat je vooral hypen door Nvidia. Ik ben ook een grote fanboy van Nvidia en ik koop hem dus echt niet voor 2k. En volgens mij is er al een pricedrop van Nvidia van 5% ofzo geweest. In deze economie heeft niet iedereen zoveel geld over voor een videokaart. AMD heeft al het licht gezien met hun nieuwe processors en een enorme pricedrop ingevoerd. En ik verwacht hetzelfde van Nvidia. En zoniet mijn 3090 op water draait lekker op 2000mhz stabiel🫡
RX 7900 XT;3;0.36565327644348145;Die 5% was volgens mij (meer) prijscorrectie dan prijsverlaging.
RX 7900 XT;1;0.5216016173362732;Ja en het exorbitante verbruik ook niet. 😉
RX 7900 XT;3;0.5458782315254211;Tja. Het is persoonlijk maar ik zie RT wel echt als een belangrijke feature voor next gen games. Voor rasterized only op hoge resoluties volstaat de vorige generatie vaak ook nog wel.
RX 7900 XT;4;0.4864300489425659;Ideale kaarten voor e-sports waarbij hoge refresh rates nodig zijn. Echter 4k ultra met raytracing dan zit je bij nvidia beter. En de efficiëntie van de 4090 is uitstekend. A plaque tale: requiem trekt 580 watt uit de muur stock 2800mhz@1100mv, maar zodra ik ga knijpen naar 2500mhz@850mv zakt dit naar 430 watt! Uiteraard lever ik 5-10fps in maar dit is nog steeds sneller dan de 7900XTX
RX 7900 XT;2;0.4475215971469879;Maar tegen welke prijs. De 4090 is gewoon te duur.
RX 7900 XT;3;0.2779829204082489;Iets is te duur als je een product koopt en er helemaal geen gebruik van maakt. Met de korte en koude dagen maak ik er nu enorm veel gebruik van, dus ik vind mijn geld goed besteed en heb er plezier van. Ik kan begrijpen dat het voor meerdere mensen buiten budget valt en te duur wordt bestempelt.
RX 7900 XT;3;0.5827513933181763;Iets duur vinden heeft niet standaard te maken met dat het buiten budget valt. Je kan ook iets duur vinden omdat je normaal 600-800 euro betaald voor een high end kaart en dat het nu 1500-2500 euro is. Mijn budget is prima, alleen zegt mijn hoofd zoiets van....tot hier en niet verder.
RX 7900 XT;3;0.4156314730644226;price/performance/verbruik vind ik een belangrijkere maatstaaf dan max fps. Dat is voor iedereen natuurlijk anders. Plus het feit dat Nvidia denk nog steeds pre crypocrash prijzen te kunnen vragen. Dan is het gewoon een principe kwestie
RX 7900 XT;4;0.5081532001495361;Goede business case voor een tandarts
RX 7900 XT;3;0.5368122458457947;Nette review. En inderdaad een fijne mooie nieuwe lancering. Voor de redactie echter wel de vraag . . . in kader van 1 is geen check ik natuurlijk ook andere benchmarks: Kunnen jullie bij de testen specifiek de instellingen zetten? Raytracing wel aan, niet aan, DLSS / FSR wel of niet aan, wel of geen SAM / RBAR? Vind het namelijk opvallen dat sommige benchmark cijfers een redelijke afwijking hebben met andere reviews.
RX 7900 XT;2;0.3024541735649109;Rbar staat bij ons altijd aan. Rtx of niet staat vrij duidelijk in de naam van de benchmarks. We hebben fsr en dlss uit staan tenzij expliciet aangegeven. Voor deze tests niet gebruikt. Verder is het ook belangrijk op te merken dat onze systemen draaien op 5500 all cores, altijd. De e-cores zijn uitgeschakelt. In sommige games is dat een voordeel, in sommige een nadeel. Het geheugen draait op 7200MT/s. Ook wat sneller dan de meeste publicaties volgens mij.
RX 7900 XT;3;0.4877794682979584;Hmmmm interessant. Dank voor de extra toelichting. Viel mij op dat in sommige gevallen de max fps zowel 30 negatief als positief uitvallen.
RX 7900 XT;3;0.5582082867622375;Tenzij je de exacte instellingen weet van de games en het systeem blijft het toch altijd lastig om te vergelijken.
RX 7900 XT;5;0.262753963470459;Goeie vraag, ik dacht hetzelfde.
RX 7900 XT;3;0.42452332377433777;Best jammer dat usb-c(video) maar niet wil doorslaan bij desktops en alleen van toepassing bij laptops.
RX 7900 XT;3;0.5180385112762451;Wat voor voordeel brengt het dan voor desktops? Hooguit zou je dan de monitor als USB hub kunnen gebruiken, maar dan gaat je controller wel over de videokaart heen (qua bandbreedte lijkt me dat ook niet ideaal). Als je het wil gebruiken om de monitor van voeding te voorzien, dan zouden de videokaarten met nog meer stroom aansluitingen moeten komen om dit aan te kunnen.
RX 7900 XT;3;0.434063196182251;Ik sluit er regelmatig een portable monitor op aan en baal er wel van, de Intel iGPU heeft het gelukkig wel, het is een kleine moeite om erop te bouwen, ze hebben ook een audiochip op gebouwd dan kan usb-c ook gezien dat de standaard gaat worden bij steeds meer monitoren.
RX 7900 XT;3;0.3443548381328583;De 7900XT (en ook de 7900 XTX) heeft wel een usb-c aansluiting (ik dacht dat je bedoelde enkel usb-c aansluitingen).
RX 7900 XT;3;0.5944229364395142;Een fijne review om te lezen en gezien te hebben. Echter snap ik echt niet waarom er niet voor een AMD systeem gekozen is i.v.m. Smart Access Memory (kan niet worden geactiveerd op Intel systeem). Als ik dit aanzet kan het in sommige spellen een enorme boost geven van 0-50%. Kan toch echt wel een verschil maken. Over Ray Tracing ben ik het persoonlijk met de reviewer eens. Ik vind de performance hit (nog) te groot voor de visuele uplift die het nu brengt. Neemt niet weg, dat het in de toekomst steeds beter en mooier zal worden en blijven doorontwikkelen wel gewenst is.
RX 7900 XT;1;0.3459101617336273;Waarom zou je op een Intel geen SAM kunnen aanzetten? Ik heb bij mijn Intel systeem met AMD 6900XT gewoon Re-Size BAR Support. Dat is volgens mij namelijk hetzelfde als SAM.
RX 7900 XT;2;0.3817698061466217;Je kan het inderdaad activeren in de bios van Intel. Echter van wat ik overal lees heb je smart acces memory alleen als je combinatie Radeon / Ryzen hebt (althans dat zegt AMD). Als je Alt + R drukt en naar prestaties en dan naar afstemmen gaat staat SAM onderaan dan rood en het vinkje aan?
RX 7900 XT;3;0.2455073744058609;Yup. Misschien een manier om klanten AMD CPU's te kopen.
RX 7900 XT;4;0.5093337297439575;Bedankt voor je aanvulling, want het wordt inderdaad anders gebracht online (zelfs bij LTT tijdens de review van de 7900 xtx). Fijn dat het ook kan worden geactiveerd op een Intel platform aangezien het soms flinke verbeteringen geeft.
RX 7900 XT;5;0.6031951308250427;Klopt, het is bij mij ook echt merkbaar. Zowel in games als in synthetische benchmarks zoals timepsy.
RX 7900 XT;5;0.5468804836273193;Inderdaad ik merkte na het activeren van RBAR in de bios en daarmee SAM, dat sommige spellen nog beter liepen. Echt een hele fijne kaart en alles draait super vloeiend. Ik wist niet dat het ook invloed had op synthetische benchmarks. Net de normale Timespy gedraaid en heb een graphics score van 22000. Mag ik vragen wat jij haalt?
RX 7900 XT;2;0.35279592871665955;SAM ofwel resize bar is geen AMD exclusieve feature maar hebben hooguit weer onder de aandacht gebrachr dat het bestaat. Het zit namelijk al vrij lang in de pcie spec.
RX 7900 XT;4;0.4247595965862274;Goed om te weten en bedankt voor de aanvulling. Als ik wat reviews bekijk lijken de AMD GPU's er gemiddeld meer baat bij te hebben dan Nvidia, maar dat maakt voor deze reviews niet meer uit. Er vanuit gaande dat dit nu standaard bij reviewers aan staat.
RX 7900 XT;5;0.35826802253723145;Jep. Bij ons staat het standaard aan. We controleren regelmatig of het niet per ongeluk uit is gegaan
RX 7900 XT;1;0.4128377437591553;Dan heb je een andere review gekeken dan ik. LTT zegt duidelijk dat AMD met hun marketing slides iedereen wil laten geloven alsof het alleen maar bij AMD cpus werkt, maar Linus geeft zelf ook aan dat het ook met Intel werkt.
RX 7900 XT;3;0.3132918179035187;Is er een energie zuinige kaart wat makkelijk games op ultra kan spelen op 1440 resolutie? Heb momenteel een 1080 TI
RX 7900 XT;1;0.3826448619365692;Je kan volgens mij gewoon een RTX 4090 downvolten.
RX 7900 XT;1;0.5989675521850586;Ja klopt maar waarom zou ik een hele dure kaart kopen, undervolten en eigenlijk een onderpresterende 4090 willen hebben? Die kaarten kosten meer dan 2000 euro..
RX 7900 XT;5;0.37760961055755615;"Als je energie zuinig wil zijn is dit denk ik de beste optie. Nu snap ik dat dit voor bijna niemand het geld waard is. Maar je kan het zelfde doen met alle andere kaarten. Ze zijn dan of zuiniger dan je RTX1080 TI of presteren beter. Zonder undervolten heb je denk ik gewoon pech ;-)"
RX 7900 XT;5;0.2762593626976013;Omdat een 4090 op 250 watt veel betere performance per watt en veel stiller is dan een goedkopere kaart.
RX 7900 XT;1;0.3706739544868469;Waarom zou je een 4090 willen hebben voor 1440P?
RX 7900 XT;4;0.31864896416664124;Raytracing op hoge framerates
RX 7900 XT;1;0.3717119097709656;Welke kaart zou ik dan nemen voor 1440p 165hz?
RX 7900 XT;5;0.24118585884571075;6800xt/6900xt
RX 7900 XT;1;0.3934994041919708;Ik lees op meerdere review sites dat ze veel last hadden van coil whine. Hoe zat dat bij jullie?
RX 7900 XT;3;0.714959442615509;Best heftig aanwezig in veel gevallen. Is wel een beetje fps afhankelijk. Boven de 100 fps was het vaak duidelijk aanwezig.
RX 7900 XT;1;0.6024584770202637;Ik ben benieuwd of dat op den duur wegtrekt. Mijn R9 Fury had het namelijk ook significant. Maar na een half jaartje ofzo was er niks meer van te horen. Nooit gedacht dat een GPU ingespeeld moet worden.
RX 7900 XT;1;0.37673813104629517;"Coilwhine is bij mij (Ik had toendertijd een Geforce GTX 970) ""verdwenen"" toen ik een andere PSU had aangeschaft. En eigenlijk ben ik nooit meer een kaart tegen gekomen dat Coilwhine had. Sindsdien heb ik een GTX 1070, RX 580, Vega 56, RTX 2070, RX 5700X, RX 6700XT en RX 6800XT gehad."
RX 7900 XT;3;0.5563618540763855;Het is niet altijd het geval. Mijn RNDA2 reference kaarten hebben het altijd gehouden. Wat wel hielp was undervolten want dan werd de toon veel minder hoog en scherp en hij werd ook minder luid. En daar naast FPS cappen want hoe hoger de FPS hoe luider en aangezien mijn monitor toch maar 144Hz is hoeft hij ook niet hoger te komen. Op die manier had ik er weinig last van. Maar het verschil een beetje van kaart tot kaart. De een heeft het erger dan de ander ookal zijn het bv alle drie reference 6800 XT's.
RX 7900 XT;3;0.421945184469223;Mijn 6900XT Red Devil doet na iets meer dan een jaar ook nog steeds coilwhine. Hangt erg van het spel af, ook van de FPS, maar in mijn ervaring ook niet altijd. Ik game namelijk op 60fps4k. En daar heb ik ook nog bij best veel spellen coilwhine. Ik heb mijn GPU bios well op silent gezet. Maar volgens mij maakt dat niet super veel uit. Ik speel de laatste tijd wel wat Civilization VI, daar met max settings 4k60fps heeft de kaart ook nog best wat coilwhine, pitch veranderd naarmate de zoom en waar je naar kijkt. Is irritant, zeker zonder koptelefoon op. Maar goed wat doe je eraan. Volgens mij doet bijna elke moderne kaart het wel tot een behaalde hoogte.
RX 7900 XT;1;0.6741968393325806;Voor prijzen die ook ruim boven de €1000,- zullen liggen gaan deze videokaarten qua verkopen net zo hard floppen als de RTX 4080 nu al doet.. Weinig mensen zullen een AMD videokaart kopen voor zoveel geld.. En gelukkig maar, want dit moeten we als consumenten gewoon niet meer pikken na ruim 2 jaar ellende met videokaartprijzen.. De tijd van enorme vraag is nu de Covid19 crisis en de crypto mining hype voorbij zijn toch echt voorbij.. Dat moet alleen nog even doordringen bij Nvidia en ook AMD die nog steeds denken met dit soort veel hogere prijzen weg te komen.. Deze AMD kaarten hadden maximaal 700-800 euro moeten kosten, waar de RTX 4080 rond de 800-900 had moeten zitten.. Ze zijn nu allemaal rond de 400-500 euro te duur..
RX 7900 XT;3;0.4394235908985138;4080 is een pak duurder voor een pak minder performance. Denk dat deze beter gaan verkopen dan 4080 hoor, eerder in lijn met de 4090 omdat die qua prijs/performance overeen komen.
RX 7900 XT;2;0.45368126034736633;3-5% meer performance noem ik geen pak meer. Reken je RT mee dan is de RTX 4080 gemiddeld wel een pak meer sneller en wat zuiniger. AMD is deze keer echt niet nagekomen wt ze hadden beloofd. 55-70 % halen ze niet dat ligt rond de 30-40 %
RX 7900 XT;3;0.6081491112709045;Ik verwacht nog wel wat hogere fps bij optimalisatie. Paar games vallen echt beetje buiten de boot.
RX 7900 XT;3;0.5115710496902466;Ik verwacht ook nog wel optimalisaties. Al .oet ik zeggen dat Nvidia ook nog zit met driver overhead. De RTX 4090 word nooit helemaal benut, ik weet niet of dat ook voor de RTX 4080 geldt. De RX 7000 serie presteerd ook zeker niet slecht en heeft een betere prijs prestatie verhouding.
RX 7900 XT;1;0.6935046911239624;Het gaat toch echt nooit meer 100% worden wat het was hoor. We zitten al 2 jaar met een inflatie van +10%.
RX 7900 XT;1;0.6313797831535339;1. De inflatie is pas sinds juli 2022 boven de 10%, en dan vooral alleen maar vanwege gestegen energieprijzen.. 2. Dat is geen excuses voor een nog veel hogere stijging van de adviesprijzen van videokaarten.. Vooral Nvidia maakt het erg bont met hun RTX 4080.. Met een adviesprijs van €1469,-.. De RTX 3080 had een adviesprijs van €719,-.. Dat is dus meer dan een verdubbeling, meer dan 100% zogenaamde inflatie.. De RTX 4080 had hooguit rond de €900 moeten kosten, en die AMD kaarten nog daaronder..
RX 7900 XT;1;0.48093628883361816;Hoe berekent AMD de FLOPS? Ik snap niet helemaal hoe ze aan die 61,6 TFLOPS komen. Het is immers streamprocessors x kloksnelheid x 2 / 1.000.000. Dan kom ik bij de 7900 XTX uit op 28,26 TFLOPS. Nu heeft RDNA3 net als Nvidia tegenwoordig per core een extra set integer/floatingpoint processors, maar dan nog kom ik niet verder dan 56,52 TFLOPS. Bij de 7900 XT kom ik uit op 21,5/43 TFLOPS, en niet op 52 TFLOPS.
RX 7900 XT;1;0.37061503529548645;"Tweakers heeft de ""gameclock"" opgelijst. Da's zover ik begrijp de gegarandeerde basissnelheid. Daarnaast is er nog het opportunistisch klokken naar een ""boostclock"". Die is 2500MHz voor de XTX en 2400MHz voor de XT. Komt het dan wel uit?"
RX 7900 XT;3;0.4573669731616974;Ze worden aardig aantrekkelijk maar ik ben benieuwd wat de RTX 4070 TI gaat presteren en kosten....
RX 7900 XT;1;0.4402170181274414;3090 voor 800/900. oftewel teveel geld voor een xx70 klasse kaart.
RX 7900 XT;1;0.6065467596054077;Ik ben van plan morgen een 7900xtx (reference model / Founders edition?) te halen. Maar waar komen deze beschikbaar? Shop van AMD zelf? Notebooksbilliger? Begreep dat hij om 3u smiddags wordt gelaunched? Ik lees hier dat ie voor €1160,- verkocht gaat worden. Waar komt die informatie vandaan? Nog tips om er een te kunnen scoren?
RX 7900 XT;2;0.3489764928817749;Zou je dat echt doen? Veel klachten over coil whine, zou je denk it beter n boardpartnerkaartje kunnen kopen
RX 7900 XT;4;0.400851845741272;Performance ziet er goed uit, zeker $ per frame. Zijn er tips of is er al informatie waar deze kaarten morgen beschikbaar zullen zijn in Nederland. Daarnaast al iets bekend over de prijzen in de EU (ook niet onbelangrijk) ?
RX 7900 XT;2;0.35574862360954285;Zal zeker wel rond de 1200 euro zitten, megekko, azerty etc. Zitten allemaal nog rond de 900 euro met de 6900xt kaarten
RX 7900 XT;1;0.41275614500045776;Toch had een ieder hier op 25-11 jl. een 6800XT kunnen kopen voor €599
RX 7900 XT;1;0.5374112725257874;zal dan wel bij een onbetrouwbare winkel zijn geweest , ik heb dat niet gezien
RX 7900 XT;1;0.2689436078071594;Gewoon via Megekko de PowerColor Red Dragon....zie PW dip op 25-11-'22
RX 7900 XT;2;0.34402430057525635;dan was zeker een kaart met 3 pci-e power connecters , ik zocht alleen naar kaart met max 2
RX 7900 XT;1;0.47667816281318665;Nee gewoon met 2x 8-pin voeding...kan je ook zien als je dit type even door Google had getrokken...🤪
RX 7900 XT;2;0.3527849614620209;die heb ik gemist helaas, had ook 2 zag ik zojuist
RX 7900 XT;1;0.3844676911830902;Waarom kan Radeon wel hun kaarten dun houden en Nvidia niet? Gemakzucht?
RX 7900 XT;2;0.3879130780696869;De geruchten zijn dat nvidia veel hoger wou klokken maar daarop teruggekomen is (mogelijks omdat ze nu al enorm veel stroom vreten). Maar de koelers ed waren al hierop ontworpen.
RX 7900 XT;3;0.37933874130249023;En daarnaast is het in zijn algemeenheid een afweging tussen geluidsproductie en afmetingen. Zolang het in mijn kast past heb ik liever een grotere kaart die stiller is, dan extra loze ruimte in mijn kast en meer lawaai. Tot je dat andere PCI slot natuurlijk wil gebruiken
RX 7900 XT;1;0.6268401741981506;ZO een enorme koeler en PCB kosten geld hoor, iets wat nvidia liever in hun zak steekt. :-)
RX 7900 XT;1;0.5314859747886658;Idd in mij tweede slot zit een M2 ssd adapter. Die is bij zo'n dikke kaart niet meer bruikbaar.
RX 7900 XT;1;0.424643874168396;Iemand al enig idee welke webshops de kaarten komen?
RX 7900 XT;3;0.3199693262577057;ik gok Megekko en azerty die vanaf vanmiddag wel wat hebben liggen.
RX 7900 XT;3;0.2546863257884979;Heeft iemand ook enig idee hoe laat?
RX 7900 XT;1;0.29215148091316223;15:00 volgens megekko zelf
RX 7900 XT;5;0.7225822806358337;Zeer mooie ontwikkeling van AMD. Als mijn huidige RTX 3060 aan vervanging toe is, kijk ik zeker serieus naar AMD, maar ook naar Intel.
RX 7900 XT;2;0.3939821124076843;EUR 1.160,- voor de XTX is lager dan ik verwacht had. Ik ging eigenlijk uit van EUR 1.250,-. Hopen dat de Board partners geen flinke marges erover heen gooien, dan koop ik er wellicht morgen één, indien verkrijgbaar uiteraard haha
RX 7900 XT;1;0.7488188147544861;Realiteit!!!! 1.758,76 € Absoluut benieuwd wat de board partners gaan doen? Ik niet meer, heb gewacht voor deze ondermaatse kaart en toch maar voor de concurrent gekozen.
RX 7900 XT;1;0.5547013282775879;Als ik jouw link volg krijg ik hem voor €1.163,38, dus waar je die 1758,76 vandaan haalt?
RX 7900 XT;4;0.38357430696487427;Gezien de nieuwe kaarten beter presteren op 4K in verhouding to 1080p, verwacht ik dat we nog prestatieverbeteringen gaan zien met driver updates in de toekomst. In elk geval kunnen beide kaarten de concurrentie met de 4080 aan, en dit met een significant lagere prijs en een kleiner verbruik. Zeker dus geen misser van AMD, ondanks dat de verwachtingen wel hoger waren. Zeker gezien Nvidia met de 4090 op prestatievlak dan, een betere concurrent op de markt gebracht dan verwacht.
RX 7900 XT;2;0.446706086397171;Ik hoop vooral dat de prijs/performance verhouding doortrekt naar de mid-end RX7xxxx series en dat deze kaarten goed beschikbaar zijn. Anders is het een paper launch met adviesprijzen die weer totaal niet realistisch zijn. Het zal vast nog even duren voordat ze iets dergelijks aankondigen. Is maar is die overstock aan 68xx / 69xx kwijtraken.
RX 7900 XT;2;0.5078996419906616;Een beetje wat ik verwacht had qua prijs/performance. Altijd zijn er weer mensen die optimistisch hopen dat AMD compleet Nvidia wegvaagt. Mijn gok was altijd geweest dat AMD qua ruwe prijs/performance een 10-15% beter dan Nvidia gaat zitten. Immers ze zijn de underdog, dus ze moeten een reden geven voor mensen om AMD te kopen, tegelijk als ze teveel goedkoper gaan zitten, dan zal Nvidia hun prijzen bijstellen, en kost dat ze beide winstmarge. Los van claims over dat AMD betere marges zou hebben (lijkt mij heel lastig te bepalen zonder toegang te hebben tot hele gevoelige informatie voor beide partijen), gaat AMD echt geen prijzenoorlog tegen Nvidia beginnen. Daar hebben ze het geld niet voor. En als we zeggen dat de 4080 vergelijkbaar is met de 7900XTX (iets mindere rasterperformance, veel betere raytracing), dan is op adviesprijzen gebaseerd de 7900XTX dus 17% goedkoper voor dezelfde performance. Net buiten mijn 10-15% gok . Uiteraard had ik meer gehoopt, maar niet meer verwacht. Ook ondanks de klachten van het rode kamp over het energieverbruik van Nvidia, het is niks beter bij AMD, zelfs wat slechter. (En ook hier, de klachten over Nvidia waren terecht wat mij betreft, maar je hebt dezelfde over AMD dus).
RX 7900 XT;3;0.2788821756839752;Ik zit mij te verbazen over de energieconsumptie en de effectieve prijs-per-uur dat je zit te gamen. Dat begint inmiddels serieuze vormen aan te nemen met de huidige energieprijzen. Met een fannatieke dagelijkse game-hobby kan het richting de 30-60 euro per maand aan stroom gaan met de hoge huidige prijzen. Doe je dat over een periode van drie jaar bijvoorbeeld dan is dat in potentie meer dan 2.000,- alleen aan stroomkosten. Het loont dus om niet alleen de meest effeciente kaart te kiezen, maar ook te gaan werken met FPS-limitatie of andere undervolt opties om relatief zo veel mogelijk uit een systeem te krijgen zonder dat alles op de 100% prestatie draait. We zien immers dat je voor een beetje prestatieverlies al veel energie kunt besparen. En in de winter vol gas, zodat je gameroom geen verwarming nodig heeft - dat dan weer wel
RX 7900 XT;3;0.4072152078151703;Prijs/prestatie is hij gelijk aan de 6800XT: Ik vind eigenlijk dat een nieuwe generatie meer bang voor buck moet geven. Maar ja.
RX 7900 XT;4;0.2487512081861496;"@Trygve Is het een idee om bij GPU reviews ook een mooie perf/prijs scatter te maken zoals jullie bij de 12400 review deden? Dat zouden jullie met PW prijzen (mediane prijs per chip) zelfs dynamisch kunnen maken. Zo'n prijs/perf scatter maakt gelijk duidelijk welke kaarten een ""goede deal"" zijn. review: Intel Core i5 12400 - Goedkope Alder Lake zet 5600X buitenspel"
RX 7900 XT;3;0.5653418302536011;Binnenkort komt er weer een gpu-vergelijking met prijs/performance, en voor reviews wil ik kijken of dat ook kan terugkomen. Dynamische prijzen opnemen in die grafieken is momenteel helaas wat lastiger om te doen, wie weet kan dat in de toekomst wel gebeuren.
RX 7900 XT;2;0.4958743453025818;Goed idee. Maar in plaats van een mediane prijs per GPU, stem ik voor de laagste prijs per GPU. De goedkoopste GPU zal de snelheid halen waarmee de GPU in de benchmarks staat. Een duurdere GPU is sneller en dat komt niet overeen met de benchmarks.
RX 7900 XT;4;0.4196022152900696;Precies. Zo was het tot een aantal jaren geleden ook, maar nu lijkt het wel alsof je simpelweg tevreden moet zijn met performance/prijs. Idiote trend!
RX 7900 XT;1;0.4985025227069855;Ik denk dat Nvidia nog in crypto prijzen denkt en AMD prijst gewoon naar Nvidia's prijzen. Ik verwacht dat net zoals de 4080, dat deze AMD kaarten slecht zullen verkopen. Eenzelfde prijs/performance als de vorige generatie is geen upgrade. Het is wachten op een flinke prijsdaling of de volgende generatie over 2 jaar. Helaas.
RX 7900 XT;3;0.5355994701385498;Goed om te zien dat AMD qua raw power goed mee komt. Voor contentproductie blijft het helaas treurig wat betreft encoder/decoder support. Apple, Nvidia en Intel lopen daar zowel op GPU als CPU vlak flink op voor. AV1 zit er nu in ieder geval wel alvast in, maar ik heb nog geen goeie vergelijkingen gezien en het is nog even afwachten hoe AV1 opgepakt gaat worden als vervanger van H.264/H.265.
RX 7900 XT;2;0.47037991881370544;Ray power meekomen? voor raytracing schiet het gewoon te kort. Voor rasterized volstaat een 3000 serie nvidia of 6000 serie radeon ook nog wel.
RX 7900 XT;3;0.3644125163555145;Raytracing is natuurlijk zeer specialistisch proces. Niet zo interessant vanuit productiviteit perspectief en beperkt vanuit gaming. In productiviteit testresultaten die de raw compute power van deze modellen weergeven geven een indrukwekkend beeld. Wanneer OpenCL ondersteund wordt gaan ze de 4090 in aantal toepassingen voorbij.
RX 7900 XT;3;0.480033814907074;Hoe bedoel je beperkt vanuit gaming? Vrijwel alle nieuwe grote titels ondersteunen het. Het is echt wel een step up qua immersie.
RX 7900 XT;3;0.4759807586669922;Hangt er vanaf welke ervaring je gevoelig voor bent. Ik ben veel gevoeliger voor hoge texturen en FPS dan iets realistischer licht.
RX 7900 XT;3;0.6005641222000122;Ik ook, maar voor mij is het en/en. De 4090 biedt eindelijk 120hz plus met raytracing op 1440p . Voor competitive gaming is dat qua responsetijd niet voldoende, maar voor immersive singleplayer games wel
RX 7900 XT;5;0.7804800271987915;Nou, top. Puur genieten. Andere mensen kunnen 1000,- besparen en hun ideale ervaring nu krijgen.
RX 7900 XT;3;0.44782477617263794;Comp settings is vrijwel alles op low mits mid/high voor latency zorgen
RX 7900 XT;2;0.42322883009910583;Ik was ontzettend fan van Ray Tracing. Echter moet ik bekennen dat ik het wow effect nooit gehad heb. Ja het is leuk qua weerspiegelingen maar eerlijk gezegd is het zo minimaal om nu te spreken over game changers.
RX 7900 XT;2;0.4291899800300598;Waar ik erg benieuwd naar ben is hoe de koeling van deze kaarten zich houdt (ook de aftermarket versies) in een kast met een 90 graden geroteerd moederbord. Ik heb nog een oude Silverstone FT02 staan. Een dijk een van een kast, maar videokaarten uit de vorige generatie (zowel AMD als Nvidia) bleken niet overweg te kunnen met de hangende GPU-configuratie. Ook bij nieuwere cases zoals de Alta F1 bleek dat probleem te spelen. Als dat ook met de huidige generatie weer zo is, zie ik me helaas genoodzaakt om een nieuwe kast te kopen...
RX 7900 XT;2;0.33381402492523193;Waarom zou dat uitmaken hoe ze hangen? Tenzij de fans alleen horizontaal fatsoenlijk kunnen lopen, maar dat lijkt mij apart.
RX 7900 XT;3;0.4134785830974579;Het is jammer dat AMD niet ver genoeg mee kan komen. Geruchten gaan dat ze snel met een refresh komen omdat er 2 zaken vanuit AMD niet als belangrijk werden bepaald om zoveel mogelijk winst te halen. De chiplets clocken veel lager dan origineel de bedoeling was, want het ontwerp zou 3 ghz+ zijn en nu komen ze tot 2.5 ghz. Een 2e is dat ze meer chiplets kunnen toevoegen om meer cores te maken welke ze nog niet gedaan hebben. De 7900 XTX heeft nu 96 compute units t.o.v 80 van de vorige gen. Geruchten zeggen dat er een 120 C.U variant uit gaat komen die ergens rond de 3ghz zou halen. Wel is het zo dat Nvidia nu een zeer solide GPU heeft en deze goed weet uit te melken. Als de prijs echt rond de 1159€ zou zijn en de goedkoopste 4080 nu 1399 dan is er nog een verschil van 240€ welke de 7900 XTX toch interessant kan maken. In gewoon rester is de 7900XTX iets sneller, de 4080 is raytracing een stuk beter. AMD heeft de helft meer geheugen, maar Nvidia de betere drivers voor Windows. Ik moet zeggen hoewel AMD zeker een goede videokaart heeft gebouwd valt die toch wel achter op Nvidia. De 4090 heeft geen tegenstander en ze kunnen ook nog de 4090 TI uitbrengen. Hopelijk komt AMD snel met een geüpgrade versie die meer kan meekomen. Nu heeft Nvidia in het topsegment vrijspel. En mensen die zeggen dat het maar weinig kaarten zijn die zo verkocht worden, er zijn al ruim 100K 4090 verkocht wereldwijd zelfs na een paar dagen. AMD moet bij blijven. Wel zal de huidige 3000 serie en 6000 serie kaarten een stuk lager in prijs moeten worden om interessant te blijven.
RX 7900 XT;1;0.40665706992149353;"In de testverantwoording staan de GTX 1080, RTX 2080 en RX 5700 XT. Ik dacht: ""Mooi, kan iedereen die de afgelopen jaren de scalper- en mining-waanzin heeft overgeslagen deze nieuwe kaarten vergelijken met wat men nog heeft draaien, top!"". Om vervolgens die kaarten nergens meer tegen te komen? Klopt de testverantwoording niet, of zijn de tabellen met resultaten te klein? Afgekapt op 10 resultaten?"
RX 7900 XT;5;0.4902341365814209;Hier ben ik ook erg benieuwd naar, heb hier nu de RX 5700 XT en ben voorzichtig aan het kijken naar een upgrade. Zou het verschil in de 6800 (XT) en deze nieuwe generatie best eens in zo'n mooi vergelijk willen zien.
RX 7900 XT;5;0.5230346918106079;Staat erbij nu.
RX 7900 XT;5;0.6603482961654663;Gracias!
RX 7900 XT;1;0.640900731086731;Ik heb de redacteur een berichtje gestuurd. De 5700 XT is sowieso getest.
RX 7900 XT;1;0.474251389503479;Foutje van ondergetekende, de GTX 1080 Ti, RTX 2080 Ti en RX 5700 XT staan er nu allemaal ook bij
RX 7900 XT;3;0.32567840814590454;Hebben jullie het allemaal gezien? in Modern warfare 2 is amd op de eerste plaats. daar is de Radeon 7900XTX sneller dan de RTX4090. Wel geen ray-tracing test te zien.
RX 7900 XT;3;0.4311465919017792;in een handvol andere titels ook over de meeste games echter is de 4090 veel sneller momenteel.
RX 7900 XT;2;0.5306742787361145;Ik vind het erg jammer om te zien dat in een game als Forza Horizon 5 de gpu's niet geweldig beter presteren dan een 6800XT in 1440p. Geldt niet alleen voor de nieuwe AMD's, maar ook een beetje voor de Nvidia varianten. Het lijkt wel of er een cpu bottleneck oid begint op te treden. Ik zou die game graag eens goed willen zien op 1440p 240hz.
RX 7900 XT;3;0.5426297783851624;Prima videokaarten voor de prijs. Alleen had ik verwacht dat AMD ook met een DLSS 3.0 tegenhanger kwam of waren dat enkel geruchten?
RX 7900 XT;3;0.6744314432144165;Ik had de 7900 XTX als concurrent van de 4090 verwacht in rasterisation performance, of in elk geval op de hielen van die kaart. Dat valt helaas een beetje tegen. Als je dan ray tracing ook gaat meenemen is het zelfs een verloren zaak tegen de 4080. Gelukkig is de prijs wel redelijk in vergelijking. Hopelijk snoept amd wat marktaandeel weg en krijgen we wat meer concurrentie.
RX 7900 XT;4;0.573752760887146;Snelle kaart, nog beter verbruik. Maar ook nog wel een stuk langzamer dan nVidia
RX 7900 XT;3;0.5124768614768982;Het verbruik tijdens gamen valt inderdaad nog wel mee, maar het idle verbruik met 2 schermen is wel heel erg hoog volgens de review van techpowerup.
RX 7900 XT;2;0.37552395462989807;Niet als je appelen met appelen vergelijkt, wat AMD aanbied prijs/performance steekt boven nvidia uit momenteel.
RX 7900 XT;3;0.4772239327430725;Jammer dat de meeste het Op deze manier verwoorden. Ze zijn sneller dan een 3090 Ti.
RX 7900 XT;1;0.5518215298652649;En laat dat goedkoop er ook maar af!
RX 7900 XT;2;0.4791956841945648;Spijtig dat Blender niet mee opgenomen is geweest in de test. Ik had vooral gehoopt dat daar het gat zou kunnen dichtgereden worden. Het is al geen grand canyon meer zoals voordien maar toch nog steeds teleurstellend.
RX 7900 XT;2;0.3302714228630066;"Klein foutje in de video-review: Vanaf minuut 3:40 t/m 3:57 worden de gemiddelde framerates in fps aangegeven op 4K (tabel). De kop spreekt echter over ""index 1080P - 1920x1080 - ultra."""
RX 7900 XT;3;0.44697314500808716;"In vergelijk met de topmodellen van de 6000 serie zijn er echt serieuze stappen gemaakt. Toch is dat vergelijk niet direct relevant. Het vergelijk met nvidia's 4000 serie loopt wat scheef omdat de 4090 serieus sneller is. Daar betaal je dan echter ook meer dan ruim voor. Precies op dat punt heeft amd een hele goeie slag geslagen imho. Niemand had ze er lelijk op aangekeken als de prijzen gelijk waren gebleven. Sterker nog ; dát zou al een enorme verassing zijn geweest. Dat men de kaarten nu een mooie stap lager heeft geprijsd t.o.v de vorige generatie is naar mijn mening een gouden move geweest. Benieuwd of nvidia toch nog wat aan de prijzen bij de topmodellen gaat doen !"
RX 7900 XT;3;0.44335228204727173;Ik dacht dat de reviews pas morgen zouden komen? Desalniettemin mooie prestaties! Ik had alleen het stroomverbruik ietsjes lager verwacht tegen over de RTX 4080!
RX 7900 XT;3;0.5129766464233398;beetje jammer dat tweakers nooit op productivity test zoals blender en encoding etc. bedoel t lijkt me wel duidelijk dat je met alles boven een 3080 zelden fps tekort komt in 99% van de gevallen
RX 7900 XT;3;0.38539475202560425;Waarom doen reviewers bijna nooit de Virtual Reality prestaties testen met de Nvidia en AMD kaarten ? Dat vind ik namelijk wel interessant ivm mijn Oculus Quest en PC VR Gaming.
RX 7900 XT;2;0.45696380734443665;Vraag me af hoeveel mensen deze generatie gaan kopen. Ik heb nog mazzel dat ik m’n 6800xt voor adviesprijs hebt kunnen kopen. Maar als je vorige jaar minder mazzel had en vergelijkbare presterende kaart hebt gekocht voor meer als 1000 euro en wat je er nu 2de hands voor krijg, dan is deze generatie kaarten niet de moeite waard…voor mij dan. Moet dan ruim 400 euro bijleggen voor 33% betere prestaties als ik naar de 7900xt kijken en even COD pak als game.
RX 7900 XT;1;0.7729008197784424;Ik denk dat de prijzen sowieso totaal ontspoord zijn. Ik wil graag een 7900XTX maar dan voor max 700. Dat gaat dus nooit gebeuren. Bij Nvidia is het helemaal absurd.
RX 7900 XT;3;0.5912179946899414;Wat dat betreft is AMD aardig consistent geweest: de top kaart voor 999 dollar. Alleen in 2020 was de optie eronder wat interessanter.
RX 7900 XT;1;0.4078599810600281;@Trygve Waarom weeral enkel games getest? Content creation, video encoding mag men ook eens standaard beginnen meenemen.
RX 7900 XT;2;0.44416722655296326;Prijs, af te wachten. Beschikbare kaarten MSRP vooral voor de USA heb ik gehoord. Persoonlijk gebruik ik graag AMD drivers maar deze kaart valt zo tegen dat ik geopteerd heb voor de tegenpartij vlaggenschip. Voor zover ik begrepen heb zijn de drivers van deze kaart helemaal niet goed. Zal waarschijnlijk wel rechtgetrokken worden maar uiteindelijk hoe je het ook draait of keert AMD heeft geen antwoord op de tegenpartij vlaggenschip. Pc gaming wordt een niche for the happy few met deze waanzinnige prijzen!
RX 7900 XT;1;0.4765361547470093;Een overgroot merendeel speelt 1440P en daarmee vind ik het eerder een 4090 dan een 4080 concurrent. Voor veel minder geld!
RX 7900 XT;1;0.2817544639110565;En Ebay stroomt al vol met scalpers Heb zowaar het geluk gehad om een XTX te kopen op de AMD shop direct. En nu is 2600$ leuk op Ebay, maar deze wordt een mooi kerst kado voor zoonlief. Voor Kerst '22, '23 EN '24 Roblox zal best lekker draaien hierop
RX 7900 XT;1;0.4551886022090912;Hoi pap, ik ben het je verloren zoon, waar is mijn 7900 xtx?
RX 7900 XT;1;0.36343660950660706;"Ik antwoord altijd op de vraag "" hoeveel kinderen heb je?"" > 1 voor zover mij bekend, de rest hebben nog niet aangebeld"
RX 7900 XT;3;0.3550017774105072;Het wordt voor mij tijd om een nieuwe game pc aan te schaffen en die mag best high end worden, mijn huidige is 8 jaar oud. Ik speel eigen voornamelijk first person shooters zoals battlefield en CoD en zo nu en dan RTS games zoals COH, dat is ook gelijk het zwaarste wat ik doe. Is een 7900xtx dan eigenlijk te prefereren boven een 4080? Dit keer wil ik graag mooie beeldkwaliteit. Ik ben overigens niet van plan tussentijds te upgraden, dus hij zal 7-8 jaar meegaan.
RX 7900 XT;3;0.8013306260108948;Oke AMD is misschien niet beter qua prestaties (zeker in RT), prijs/prestatie verhouding is wel een beetje beter dan Nvidia en het is wat te onzuinig. Maar ik heb ook dingen gehoord dat er veel bugs zijn in RDNA3, veel te lage clocks, bugs, crashes, BSODs, etc. Als ik zou moeten gokken gaat AMD's FineWine weer een ding zijn en gaan deze kaarten in de toekomst het veel beter doen.
RX 7900 XT;5;0.27708664536476135;Elke launch zal zo zijn bugs hebben, maar ik vind het geniaal hoe jij dit soort dingen hoort op launch dag als alleen reviewers die kaarten hebben.
RX 7900 XT;2;0.39849162101745605;Omdat ik het hoor van reviewers...? Of het zo erg is als wordt beschreven geen idee, maar het lijkt me niet heel verbazend gezien het hele chiplet gebeuren.
RX 7900 XT;3;0.3247653841972351;Vraagje: Kan ik de 7900 XTX gebruiken op mijn X570 Aorus Elite moederbord? Ik gebruik nu een 5700XT. Het is een AM4 moederbord en dat slaat natuurlijk op processoren die ervoor gebruikt kunnen worden (de nieuwste generatie AMD processoren vereisen AM5 dacht ik). Maar misschien vergt het kunnen gebruiken van een GPU ook een dergelijke techniek?
RX 7900 XT;5;0.30741870403289795;Dit is gewoon mogelijk. Let alleen even op je voeding zijn/haar vermogen.
RX 7900 XT;4;0.3275790512561798;Bedankt voor jouw snelle reactie. Deze heb ik: Sharkoon SilentStorm Cool Zero 850 watt. Die heeft een gold label. Dat zal wel goed genoeg moeten zijn toch?
RX 7900 XT;3;0.4210715591907501;Ja, dat moet prima zijn.
RX 7900 XT;5;0.41527295112609863;Yep 850 is prima
RX 7900 XT;2;0.5426390171051025;Gaat waarschijnlijk wel werken. Dat er 850wat op staat zegt helaas niets. Sharkoon staat bij mij igg niet bekend als hoge kwaliteit spul. Je zou voor de zekerheid even na kunnen vragen in het Voedingen advies topic: forumtopic: Voeding advies en informatie topic - Deel 34 Daar zitten een aantal mensen die dat waarschijnlijk met meer zekerheid kunnen vertellen. Het gaat er vooral om dat de voeding snel kan schakelen en de power spikes van de betreffende kaart aan kan. Vooral dat laatste gaat bij de mindere kwaliteit voedingen vaak mis. Ik heb geen idee hoe het met de powerspikes zit van de rx7000 serie.
RX 7900 XT;5;0.4647580087184906;Benieuwd of ik met mijn 650Watt voeding nog genoeg is voor 1440p gaming met een 7900 xtx.
RX 7900 XT;4;0.5942077040672302;Afhankelijk van de PSU kon je met een 6800XT soms al wat problemen hebben. Maar een hele goede PSU kan het weer prima aan. Officieel wordt er een hoger model aanbevolen. Maar als je bv een Seasonic Prime PSU hebt zou ik het persoonlijk wel aandurven. (mits je niet te veel andere zware hardware hebt. Stel je hebt ook een 13900K en ga je zowel de CPU als GPU overlocken wordt het wel een ander verhaal). Maar met een 5900X die stock power draait moet kunnen.
RX 7900 XT;1;0.3637691140174866;Het betreft een BeQuiet! I.c.m. een 3700X e dan op een Super Ultra Wide resolution. Zal toch upgraden worden naar iets van 850 vrees ik.
RX 7900 XT;2;0.5467736721038818;Je kan het proberen maar het is geen garantie. Ik ben niet zo goed op de hoogte van de BeQuiet lineup maar als het een topmodel is wat niet te oud is dan zou je het kunnen proberen aan de andere kant als je dit soort geld voor een Videokaart uitgeeft is het misschien beter om de rest ook netjes op orde te brengen,. Zelf draai ik alles op een 750 watt Prime Titanium en die gaat er de komende jaren nog niet uit. Maar dat was ook een hele dure PSU.
RX 7900 XT;5;0.29456740617752075;"Ik draai met een Ryzen7 5700X & 6800XT op onderstaande 650w voeding. PW ; uitvoering: Cooler Master V650 Gold-V2 Werkt feilloos."
RX 7900 XT;3;0.44118669629096985;Moet zeggen dat de reference kaarten van AMD qua design (mijn mening) nog altijd lelijker zijn dan de Founders Edition-kaarten van Nvidia.
RX 7900 XT;2;0.4806078374385834;gelijk heb je. enkel niet voor de persoonlijke mening van vorm. gewoon de connector ook. het is gewoon beter
RX 7900 XT;3;0.3934156596660614;Ziet er goed uit, wanneer komen de midrange kaarten???
RX 7900 XT;3;0.3390100300312042;Weet iemand hoe het nu precies in de praktijk zit met DLSS? Je leest vaak opmerkingen dat als je profijt wil hebben van ray tracing je toch echt een groene kaart moet hebben omdat games daar geoptimaliseerd voor zijn, is dat al een beetje recht getrokken en kan team rood nu ook goed mee?
RX 7900 XT;3;0.357744961977005;Nee amd loopt nog steeds flink achter met RT maar het is wel beter. Hopelijk volgende generatie. Als je RT op fatsoenlijke framerates prijs stelt is eigenlijk je enige optie nvidia.
RX 7900 XT;1;0.5286704301834106;Als je RT absoluut wil is enkel nvidia daar, amd loopt nog steeds achter.
RX 7900 XT;2;0.3636161983013153;hmm 899$ = 1050€? en 999$ = 1160€? heb kan aan mij liggen maar volgens mij klopt dit niet helemaal
RX 7900 XT;2;0.46701887249946594;Inderdaad, volgens mij hebben ze een conversiefoutje gemaakt! Als dit zou kloppen dan zou deze hele release gelijk kunnen worden afgeschreven... Maarja ze hebben het bij de prijzen van de 4080/4090 ook over bedragen die veel hoger zijn dan MSRP, wat ook klopt met de realiteit (het minimum zelfs), dus als dit een voorbode is op retailers die de consument wederom gaan uitmelken dan wordt het er niet veel beter op.
RX 7900 XT;1;0.5711550712585449;Dollarprijzen zijn sowieso altijd zonder BTW, de europrijzen met.
RX 7900 XT;3;0.5202057361602783;Kan me vergissen, maar is het niet opvallend hoe goed de frametimes zijn van de XT en XTX?
RX 7900 XT;1;0.36494752764701843;de 1% lows volgens de cijfers van Hardware Unboxed vielen mij ook al in die zin op.
RX 7900 XT;5;0.5910195708274841;Deze kaart zou ik ook nog wel willen bestellen, als die uitkomt. Mooi beestje dit. Wat AMD nog steeds heel goed in is het energie gebruik voor de geleverde performance en uiteraard zijn de drivers subliem. Ik vond de oudere 7900 series ook al tof. Toentertijd de eerste kaart met 28nm GPU die en GDDR5 dacht ik 384 bit. Mooie tijden waren dat.
RX 7900 XT;1;0.5888209939002991;Iemand enig idee welke websites de kaarten zullen verkopen bij de lancering om 15:00? Thanks alvast
RX 7900 XT;1;0.3804599642753601;Is the reference model even going to be available in the netherlands?
RX 7900 XT;1;0.7282659411430359;Waar koop je deze kaart? Via tweakers de pricewatch in de gaten houden en via Amd.com/shop? Nederland staat daar weer niet bij.
RX 7900 XT;2;0.5348677039146423;De 7900XT is geen beste koop. 16% minder performance voor 90% van de prijs van een 7900XTX. De XT is dus zeker € 200 te duur. Normaliter heeft het topmodel de slechtste prijs/prestatieverhouding.
RX 7900 XT;2;0.575791597366333;Weet niet wat de meeste zien maar het is in het algemeen teleurstellend. Puur raster prijs-prestatie is het beter dan the 4080, maar al de rest is duidelijk voor Nvidia. Meer stroom, loopt achter op RT, (aanname) slechtere media engine. Het is wel iets meer future proof door meer VRAM en DP2.1. Het vergelijk met the 4080 is dus niet slecht en je kan zelfs zeggen iets positiever, maar vergeet niet dat het verhaal van de 4080 ronduit slecht is. 7900xtx is dus iets beter dan slecht... Ook de conclusie dat AMD een beter winstmarge heeft, omdat board partners een hogere marge hebben, is ook een vreemde. Het is bekend dat NVidia(NGreedia) alle marge voor zichzelf houdt en slecht weinig marge overlaat aan de AIBs. Als je een grove schatting maakt met de die sizes, is de 4080 duidelijk goedkoper te produceren. The GCD is 300mm2 vs 379mm2, met dezelfde process family (N5 en N4). Dus slechts 25% meer en daarvan moet 6 MCD (>200mm2) en de base en de packaging van betaald worden. En ook nog eens 50% meer VRAM. Dus nee, Nvidia is nog steeds de marge koning, chiplets in deze generatie maken daar geen verandering in.
RX 7900 XT;1;0.4389137029647827;Ik zit overal net even te checken maar nergens te koop lijkt het, vandaag is launchday toch?
RX 7900 XT;3;0.40730828046798706;15u ja, maar in NL overal de XTX uitverkocht, alleen op sommigesites nog een 7900XT voor 1100+
RX 7900 XT;3;0.24231606721878052;Lekker gaar dit weer. Zat om 15:00 te kijken maar he-le-maal niets te koop. Moet dat reference model hebben want die past in mijn case. Dan maar Nvidia. Gepruts dit.
RX 7900 XT;2;0.4328920841217041;de 7900 XT reference kaart is nog wel bij amd te verkrijgen voor 1046 eur , maar ook ik had iets meer pagina's of iets verwacht. Grote retailers zoals Alternate heb ik al helemaal niets voorbij zien komen. Blijkbaar is deze release nog schaarser dan toen mining populair was.
RX 7900 XT;3;0.33679720759391785;Mindfactory in Duitsland heeft de XTX´en wel nog liggen... ...toch niet meer
RX 7900 XT;1;0.47025546431541443;amazon nl is selling it for 1172 eur
RX 7900 XT;1;0.4756309390068054;7900 XTX gekocht van amazon voor €1172. Op Megekko waren ze meteen uitverkocht voor €1199.
RX 7900 XT;3;0.26633724570274353;degene met 7+ dagen levertijd?
RX 7900 XT;5;0.4054613709449768;Yes, komt vanuit Portugal dus best mogelijk 😅
RX 7900 XT;1;0.5936696529388428;Stond in die wachtrij op AMD site zelf... XTX -> Out of stock Megekko ook meteen uitverkocht. Daar zag ik een Asrock en Powercolor staan met afbeelding van reference model. Kunnen deze merken reference verkopen onder hun eigen naam?
RX 7900 XT;1;0.4711705446243286;Bij Megekko is de R7900 XTX meteen uitverkocht. De gewone XT is maar 100 euro goedkoper... Iemand nog tips waar deze te krijgen is?
RX 7900 XT;1;0.43547603487968445;Prijzen inmiddels bekend op AMD Site: 7900XT > €1012,88 7900XTX €1125,55 Jammer genoeg XTX al uitverkocht
RX 7900 XT;1;0.4495204985141754;Waarom zie ik die prijzen hoger? XTX: 1.163,38 € XT: 1.046,93 €
RX 7900 XT;1;0.45033615827560425;Geen idee dat waren de prijzen oo de amd site omstreeks 15:30.
RX 7900 XT;1;0.5754466652870178;Zijn er ergens nog Asus kaarten beschikbaar? Heb zowel op hun eigen site als op retailers niets kunnen vinden.
RX 7900 XT;2;0.32190874218940735;Wat is de verwachting qua voorraad voor de 7900 xtx? Is daar iets over bekend? Bijvoorbeeld gemiddeld gezien X weken na launch weer beschikbaarheid of zoiets? Op AMD is de XT namelijk nog wel beschikbaar, maar twijfel of ik moet wachten of gewoon die dan maar moet bestellen.
RX 7900 XT;1;0.43978989124298096;Had het wel gedacht dat deze kaarten niet voor beneden de 1000 hier in de winkels komen......
RX 7900 XT;1;0.2927248477935791;mocht iemand nog een kaart willen... XFX Speedster MERC310 AMD Radeon™ RX 7900XT Black Gaming grafische kaart Game Clock Up To: 2220 MHz Boost clock Up to: 2560 MHz
RX 7900 XT;3;0.38379523158073425;de xtx is er ook nog, alleen de prijs is een stuk duurder dan rond 15u. toen lag de prijs op 1172, nu op 1313
RX 7900 XT;4;0.5929589867591858;Toch wel bijzonder hoe AMD zich weer terug heeft weten te plaatsen in de videokaarten industrie... niet zo heel lang geleden liepen ze gewoon consistent achter en waren ze (voor videokaarten dan) een beetje het budget merk... Nu zijn ze gewoon weer up and running en doen ze volwaardig mee. Mooi om te zien!
RX 7900 XT;5;0.2948513925075531;Ik ben ontzettend blij dat AMD hier nu mee komt. Mijn PC kan moderne games bijna niet meer draaien, laat staan dat ik kan genieten van de graphics zoals de maker had bedoelt. Na bijna 7 jaar ben ik toe aan een nieuw beestje, maar van de NVIDIA prijzen schrik ik elke keer weer. Hopelijk zijn deze geweldige en veel betaalbaardere kaarten van AMD goed beschikbaar tegen de lente, zodat ik goed voorbereid ben als Diablo 4 uit komt.
RX 7900 XT;3;0.384233683347702;Wat mij hier dan interessant lijkt, zou ook een toevoeging van een normale 2080 Ti zijn, en niet de FE die gewoon onbeschikbaar is. Idem een 1070 en 2070. Gewoon - omdat het kan. En omdat vooral de prestaties per Watt wellicht nog wel sterk uiteen kunnen lopen.
RX 7900 XT;3;0.45251500606536865;"Men is redelijk negatief over de 7900 XT, omdat deze vergeleken met de 7900XTX meer performance inlevert dan dat het prijs inlevert. Maar een oprechte vraag; stel ik wil ca 1000 euro uitgeven aan een kaart die beschikbaar is. Dan is de 7900 XT toch de beste koop? Op AMD is deze nu beschikbaar voor ca €1050 en hij is qua performance vergelijkbaar met 3090. Ik snap niet zo goed waarom men negatief is, of mis ik wat? NB: voor 100 euro meer kan je de 7900XTX kopen, maar met een beetje pech is deze de komende tijd niet voor handen."
RX 7900 XT;2;0.47292059659957886;Wat ik frappant vind is dat op het moment dat er AAA spellen uitkomen, zoals bijvoorbeeld CP2077 of Metro Exodus dat de videokaarten op het moment van uitgave nog lang niet snel genoeg zijn om dit soort spellen in al hun pracht en praal te kunnen spelen. Op het moment dat er videokaarten op de markt zijn die die wel kunnen is het spel (in mijn geval) allang uitgespeeld. En ik geen zin meer heb om het spel aan te slingeren (bin there done that) Ik zou graag zien als er een zwaar spel op moment van uitkomen zo geoptimaliseerd is dat het op de current gen high end kaarten in al hun pracht en praal te spelen is. En niet pas 2 generaties verder. Videokaart bakkers lopen eigenlijk mijlen ver achter op het aanbod van spellen, of game makers zijn gewoon lui...
RX 7900 XT;3;0.402760773897171;Ik vind dat de absurde 4090 nog in de bences wordt meegenomen.
RX 7900 XT;1;0.6229218244552612;Ik begrijp het niet. Ik kom helemaal niet in de buurt van de fps die jullie halen in cyberpunk en rdr2. Met een r7 3700x, ddr4 16gb ram aan 3600mhz, install op m2, met de rx 7900 xtx haal ik in cyberpunk 1440p ultra maar 70-80 fps en bij rdr2 rond de 90 fps. Het kan toch niet enkel die cpu en ddr5 van hun testbench zijn dat zo een groot verschil maakt? Zie ik hier iets over het hoofd?
RX 7900 XT;4;0.42639443278312683;Ik denk dat dit wel mijn nieuwe videokaart wordt. Ik heb nu de 5700XT en is aan vervanging toe. de prijs wat aangeboden wordt is prima, en wordt waarschijnlijk nog goedkoper volgend jaar.
RX 7900 XT;1;0.4769898056983948;Hmm de RTX 4090 blijft wel the go to te blijven. AMD is het gewoon net niet. Maarja ik heb dan ook 3 GSync monitoren en voor AMD gaan verlies ik dat. RTX 4080 doet het ook zo slecht nog niet.
RX 7900 XT;1;0.5497578978538513;"Ik heb hier ook een Alienware 38"" Ultrawide met G-sync ultimate en een 6950Xt met Freesync en adaptive sync aangezet. Ik zie/merk 0.0 verschil met G-sync..."
RX 7900 XT;2;0.538570761680603;Helaas kunnen 2 van die monitoren (AW3418DW) niet met Freesync om. Had dat al opgezocht. De derde en primaire (AW2721D) kan het wel maar ik merk wel dat het slechter werkt dan GSync.
RX 7900 XT;3;0.3672522306442261;"Dat is inderdaad wel balen met die AW3418DW. Mijn 38"" biedt gelukkig wel ondersteuning voor Freesync, ondanks dat hier op die poot een stickertje hangt met Gsync Ultimate. Allemaal verwarrend voor de consument"
RX 6900 XT;3;0.5157635807991028;Tja, net als bij de RTX 3090 is ook deze kaart compleet buitenproportioneel qua prijs. Dat was natuurlijk te verwachten, de 6800XT heeft 90% van de Compute Units van de 6900XT, terwijl de prijs van de 6800XT op 67% zit van de 6900XT. Het is dus een GPU voor de dikste portemonnee en degene die het beste van het beste wil hebben (een segment waarbij de eindgebruiker sowieso niet echt maalt om performance/watt ratios). Het probleem is alleen met deze kaart, het is niet de beste kaart op de markt. De 3090 doet het overall net even wat beter en is met tensor cores en raytracing cores ook nog wat geavanceerder. Ik vind het jammer dat AMD het niet aangedurfd heeft om met een grotere (en duurdere) chip aan te komen zetten. Dan had Nvidia nog steeds het voordeel wat betreft raytracing en tensor cores, echter zou de AMD kaart de ‘king of floating point performance’ zijn geweest, en viel er écht wat te kiezen. Nu voelt het meer als aansluiting voor een betere prijs, in een categorie waar de prijs er voor de eindgebruiker veel minder toe doet, die wil gewoon het beste van het beste.
RX 6900 XT;1;0.5657441020011902;Of ze hadden gewoon de kloksnelheid van deze kaart moeten opschroeven. Als je andere reviews bekijkt, dan is een OC van 14% op de core mogelijk. En dan komt de 6900XT akelig dichtbij of voorbij de 3090, terwijl het gevraagde verbruik nog steeds binnen de perken blijft.
RX 6900 XT;3;0.3037865161895752;Ik vraag me ook af of in deze review een moederbord zit dat amd's gedeelde memory aan kan, dat maakt deze kaart ook nog een stukje sneller.
RX 6900 XT;4;0.3641721308231354;Toch wel grappig hoe dat “AMD’s gedeelde memory” heet tegenwoordig. Dit terwijl Reizable BAR gewoon onderdeel is van PCIe (zie ook: nieuws: ASUS brengt bios-bètaversie met Resizable BAR-functie uit voor Z490-m...) Het enige verschil is dat het nooit op Intel moederborden geactiveerd is geweest. Tot nu ... ASUS heeft het geïmplementeerd op hun Z490 moederborden, zodat Intel en NVIDIA het ook kunnen gaan gebruiken.
RX 6900 XT;3;0.31861793994903564;Tja, het is toch ook nog steeds vaak RTX, terwijl dat gewoon Nvidia's marketingsaus over DXR is.
RX 6900 XT;3;0.3129579424858093;ASUS heeft het ook al beschikbaar op x570 bord. He gisterenavond BIOS geüpdate en beetje mee lopen spelen. Doet op dit moment niet veel btw
RX 6900 XT;5;0.31922483444213867;Het zelfde wat AMD heeft gedaan met Freesync? en Nvidia is ook niet onschuldig hier aan met Ray tracing.
RX 6900 XT;5;0.5105931162834167;Heb hier een B460 bord die dit sinds kort ook kan.
RX 6900 XT;2;0.487155944108963;Dat lijkt in deze review niet te zijn meegenomen. Jammer, want het systeem kan het wél aan.
RX 6900 XT;1;0.47711628675460815;Dat maakt de review in mijn ogen onvolledig. Zware steek die men liet vallen.
RX 6900 XT;1;0.5106226801872253;Alleen laten ze die steek helemaal niet vallen, staat gewoon in de test verantwoording: @bamboe en @Jeroenneman ook even getagged voor volledigheid.
RX 6900 XT;4;0.35234975814819336;Dat is helder, ik had eroverheen gelezen, en vooral naar de tabel gekeken voor de specs. Niettemin zetten de meeste andere publicaties het ook nog even in de grafieken.
RX 6900 XT;1;0.3136807978153229;Dat krijg je dan als je 'feiten' van anderen vertrouwd en niet zelf checked... I stand corrected.
RX 6900 XT;1;0.4749094247817993;Bedankt, ik had dit stukje niet goed gezien vandaar dat ik me dat afvroeg.
RX 6900 XT;3;0.4750690162181854;Maar dan moet je dat natuurlijk wel bij beide kaarten doen, uiteraard test je alle kaarten op stock, ga je overclocken moet je natuurlijk ook alle kaarten overklokken anders ga je appels met peren vergelijken.
RX 6900 XT;2;0.35037219524383545;Ik bedoelde dat AMD deze kaart uit de fabriek met hogere kloksnelheid had moeten releasen
RX 6900 XT;5;0.4631699025630951;"Ah op die manier, dat is altijd een ""lastig"" compromis omdat je zoveel mogelijk verkoopbare chips wil hebben, dus kies je een specificatie die een aanzienlijk deel van de chips zullen behalen en laat je de eindgebruiker al dan niet overklokken. Daarnaast grote kans dan AMD binnenkort ook met zakelijke varianten komt waar dan de beste die's naar toe gaan."
RX 6900 XT;1;0.5072249174118042;Dan schieten ze de AIB Partners in de voeten, die mogen het OC gebeuren verkopen (voor een meerwaarde uiteraard).
RX 6900 XT;3;0.4189144968986511;Er zal vast wel een AIB partner komen met een OC editie die precies dat doet.
RX 6900 XT;2;0.3849047124385834;Nu is het dus afwachten met wat voor mooie overclocks de AIB's komen. Reactie nog niet gezien waarin hetzelfde al werd gezegd. Nevermind.
RX 6900 XT;3;0.3860189616680145;Persoonlijk vind ik het knap dat AMD terug aansluiting vindt, na enkele jaren afwezigheid in het top-segment. Hier is hard werk voor verricht. Hopelijk kunnen ze zo verder gaan.
RX 6900 XT;2;0.36488452553749084;Vooral knap omdat de kaarten van AMD minder verbruiken. In plaats van andersom..
RX 6900 XT;2;0.4441896080970764;Je kan het van alle kanten bekijken en zeker nu met leveringsproblemen. Wat is het minst erg, een AMD flagship aan +/-1000€, of een Nvidia flagship aan +/-1600€. Het moeilijkste is inschatten hoe raytracing gaat evolueren in games vind ik. Ik heb nu een 2080ti , en behalve eens proberen hoe dat eruit ziet in Control, game ik voor de rest gewoon nooit met raytracing. Dan zou ik terug naar AMD kunnen denk ik, tenzij raytracing echt wel iets leuks wordt bij nieuwe games. Dan val ik met RDNA2 toch in hetzelfde gat als mijn 2080ti. Langs de andere kant, wil ik NOG meer stroomverbruik en warmte in mijn kast dan nu reeds het geval is... Moeilijke upgrade tijden vind ik het.
RX 6900 XT;2;0.37448152899742126;DLSS is wat dat betreft een veel betere troefkaart als RT...
RX 6900 XT;1;0.298211008310318;DLSS is alleen een GPU-vendor specifieke techniek die een per-game implementatie nodig heeft. Dat soort dingen zijn zelden een succes op de lange termijn. Ik hoop dat AMD kan waarmaken wat ze zeiden en met een alternatief kunnen komen gebaseerd op open/industrie standaard die werkt op elke GPU.
RX 6900 XT;2;0.3263627290725708;Zo lang Nvidia het makkelijk maakt om het in te schakelen, bijvoorbeeld voor alle nieuwe GameWorks games, kan het erg hard gaan. Zeker nu Nvidia zelf hun DGX-1 servers gebruiken om het neurale netwerk te trainen (sinds DLSS 2.0).
RX 6900 XT;1;0.4703703224658966;Er is geen per-game training nodig voor DLSS 2.0. Onderandere door de TAA techniek is dat overbodig geworden. En aangezien de console hardware het niet ondersteund denk ik niet dat developers heel happig zullen zijn om tijd en geld in een GPU specifieke features te stoppen die alleen voor sommige PC users werk. Zeker niet omdat AMD gezegd heeft te werken aan een niet-GPU-specifiek alternatief, die daarom gaat werken op zowel consoles als PC's.
RX 6900 XT;1;0.28400784730911255;dLSS gebruik eje eigenlijk alleen in games waar je per see RT aan wilt hebben toch?
RX 6900 XT;5;0.49524953961372375;Je hebt een 2080Ti, waarom zou je willen upgraden? Die 2080Ti voldoet toch prima? Hou het geld lekker in je zak en kijk over een jaar of 1-2 weer eens rond. Dan is raytracing weer een stap verder en heb je een waardige upgrade.
RX 6900 XT;2;0.5076291561126709;Er is specifiek spel waar hij op 2560x1080 niet loopt zoals ik wil voor een shooter, en dat is Tarkov. Laat het nu net dat zijn waar ik ( als ik eens kan gamen ) een betere performance wil. Momenteel is dat tussen de 60-90 fps, en op sommige momenten loopt het gewoon niet 100%. Ligt aan het spel en aan het CPU gebruik ( Ryzen 3700x ). Speel ik zonder players, dan heb ik continu 120+ fps, speel ik met players op Internet of bots, dan zakt het naar 60-90. Dus jah, ik heb het niet echt nodig momenteel, is echt wel een snelle kaart, behalve dan dat ene spel...
RX 6900 XT;1;0.4943835437297821;Is daar de gou of de cpu de limiet of kunnen beiden er niks aan doen en is het de techniek van het spel die de framedrop veroorzaakt? Een upgrade heeft alleen zin als je zeker weet dat de gpu de bottleneck is. Ik vermoed dat dat in jouw geval niet zo is.
RX 6900 XT;5;0.5262534618377686;Het word winter dus perfecte tijd om te upgraden. Dan zit je er (nu zonder familie) tijdens kerst toch nog lekker warmpjes bij. Wel even de website store 10000x refreshen zodat je er ook 1 kan bestellen en dan duimen dat ie ook daadwerkelijk geleverd word voor kerst, tussen oud en nieuw of zelfs begin 2021. Wat leven we toch in een heerlijk tijd
RX 6900 XT;3;0.35778704285621643;Ik hap toch even: Zolang de leverbaarheid van een videokaart één van de grotere problemen is, doen we het nog niet zo gek niet.
RX 6900 XT;4;0.4427240192890167;Ja je hebt ook gelijk en het doet me goed om te lezen dat je er positief instaat
RX 6900 XT;1;0.4651396870613098;Ja, ik heb net de oude Nintendo van zolder gehaald, en mijn dochtertje enthousiast gemaakt. Als je die graphics ziet, en toch lol kan hebben realiseerde ik me dat we het nog zo slecht niet hebben! Probeer er wat van te maken.
RX 6900 XT;2;0.5617765188217163;Het is niet zo heel ingewikkeld. Beide kampen hebben nu een goed aanbod, kies gewoon degene die het best leverbaar is en goed geprijsd en geen bagger AIB implementatie heeft. Wat dat betreft is Nvidia onder de 3090 wel iets beter bedeeld qua prijspunten. AMD verkoopt Navi ERG duur als je het mij vraagt. Dat zal nog wel gaan bewegen en misschien is dat al ingecalculeerd, maar dat icm de slechte leverbaarheid maakt het nu een uitermate slecht moment om een GPU aan te schaffen. Je weet gewoon dat je veel te veel betaalt nu. Stroomverbruik en hitte in je kast is netto gelijkwaardig, zou ik niet om wakker liggen. Ampere zonder OC of Navi met... lood om oud ijzer. Technisch heeft AMD duidelijk een veel beter product dat stiller en zuiniger kán presteren dan Ampere. Qua featureset heeft Nvidia een minimale voorsprong met DLSS icm RT. Maar die is zeer selectief, en de vraag is ook hoeveel games je nu op release moet spelen. Is vaak geen pretje met alle day one patch en driver bullshit plus het spoonfeeden van content updates. Ik stel eigenlijk alles met een jaar uit en pak dan gewoon de GOTY uit de budgetbak. En in dat geval zit je bij AMD prima, want RT werkt nog steeds en tzt is het wel geoptimaliseerd.
RX 6900 XT;2;0.4679131805896759;Zelfs aan de huidige prijzen kunnen ze het moeilijk verantwoorden om voldoende van hun reserveringen bij TSMC voor consument GPU te voorzien, prijzen zullen in de eerste jaren niet zakken.
RX 6900 XT;3;0.3031861484050751;Ga ergens anders trollen/flamebaiten.
RX 6900 XT;2;0.44727030396461487;Daarvoor moet het wel grappig zijn, napraten wat de rest van het internet al roept is dat echt niet.
RX 6900 XT;5;0.45585954189300537;AMD had gewoon geen zin om een 400 Watt GPU te bouwen, en terecht. Performance per watt is AMD nog steeds de beste, en wat let je om de RX 6900 XT te overclocken? Er is nog 60Watt! ruimte over t.o.v. de RTX 3090. Overclocken bij de RTX 3090 zit je sneller tegen limieten aan als ik statistisch bekijk. - 7nm vs 8nm - 300W vs 360W
RX 6900 XT;4;0.42658039927482605;"op gpu gebied is nvidia juist al jaren de beste qua performance/watt...de ""nog steeds"" is n beetje misplaatst denk ik. maar goed om te zien wat voor grote efficientie sprong amd heeft gemaakt om juist daarin de koppositie te pakken."
RX 6900 XT;2;0.49788224697113037;Ja alleen de allerbeste chips halen de status van flagship GPU. Die zitten meestal in het midden van de plaat, hoe verder uit het midden hoe lager de yields. Daar zijn er ook meteen meer van. De slecht funtionerende delen zetten ze uit, wordt 1 of 2 lagere tiers. En dus stuk goedkoper.
RX 6900 XT;3;0.5904008150100708;Bij veel reviews zie je dat de 6900XT beter presteert op 1080P en 1440P maar achterhaakt op 4K. Het is dus niet zo dat de 3090 OVERAL beter is. Echter in mijn ogen koop je zo een kaart voor 4K en dan is de 3090 of vaak zelfs de 3080 gewoon een betere keuze.
RX 6900 XT;2;0.38892942667007446;Op zich helemaal mee eens maar sommige mensen spelen competitief op lage resolutie en hoge framerate en dan is een 6900XT net beter. En over 5 jaar kan je video kaart de nieuwste spellen sowieso niet op 4k meer en zou een 6900XT wellicht ook beter zijn. Wie een 6900XT koopt om em 4 jaar te houden is natuurlijk bijna gek, beter 2x een midrange kopen dan heb je gemiddeld betere prestaties maar wellicht zal een 6900XT zijn waarde 2e hands wat beter behouden... Maar goed het blijft toch allemaal theoretisch want ze zijn allebei niet verkrijgbaar!
RX 6900 XT;2;0.35116803646087646;Volgens mij is de 6900XT meer value for money dus gaat AMD hier meer aan verdienen.
RX 6900 XT;2;0.44022491574287415;Dat de prijs bovenproportioneel is, is maar hoe je het bekijkt. De adviesprijs van de 3080 ligt op €720. De OC versies presteren nauwelijks beter en toch zijn er massa's mensen die daar €800 tot €1.000 voor neer willen leggen. Dan zou ik toch liever €999 neerleggen voor een 6900XT met (in ieder geval bij sommige games) wel noemenswaardig betere prestaties.
RX 6900 XT;3;0.5334032773971558;Duidelijke review, maar ligt het aan mij of wordt er geen wordt gerept over het feit dat de introductieprijs van de GTX3090 meer dan €600,- hoger ligt dan die van AMD. Nvidia mag dan nog steeds de kroon dragen van snelste videokaart, maar voor welke prijs. Als toevoeging zou het mooi zijn om niet alleen de prestaties per watt te zien, maar ook de prestatie per €, want dan is het snel beslist. Toch zou ik voor mijn nieuwe kaart eerder voor Nvidia kiezen gezien de Raytracing kwaliteit en prestaties. Puur omdat ik graag in 3d werk en de functies voor Raytracing wel interessant vindt. Voor puur gamen zal het me nog niet zo veel uitmaken, al zie ik wel graag de tegenhanger van AMD voor DLSS 2.0. Dit is dan wat mij betreft een van de beste functies van de nieuwe Nvidia kaarten, dit zorgt voor goede prestatiewinsten zonder al te veel in te leveren op kwaliteit. Ach uiteindelijk is het allemaal niet interessant gezien de meeste kaarten toch niet of erg slecht leverbaar zijn. Ik ga geen honderden euro's meer betalen voor een nieuwe grafische kaart om deze eerder te bemachtigen. Laat al die mensen maar lekker hun geld besteden, ik koop de kaart wel als er gewoon voorraad is en ze voor 'normale' prijzen te koop zijn. Een RTX3070 voor meer dan €700,- Ik vermaak me nog wel even met mijn GTX970 van €360,-
RX 6900 XT;3;0.3752959668636322;Staat toch vrij duidelijk in de conclusie?
RX 6900 XT;1;0.3591797649860382;Dan heb ik er overheen gelezen of het stond er nog niet.
RX 6900 XT;3;0.7190508246421814;Beetje flauw om de prijs te vergelijken met een kaart die beter presteert. De RX 6900 zit meer op een lijn met een 3080, dus dan zou je ook die prijs moeten nemen.
RX 6900 XT;2;0.31812039017677307;Hoe kom je daarbij? De 6900XT praten we hier over en die steekt toch over het algemeen uit boven de 3080, behalve op gebied van Raytracing.
RX 6900 XT;2;0.5090643167495728;Jammer dat tweakers de undervolting performance niet test. Het lijkt er stelselmatig op namelijk dat de rtx30XX serie gewoon relatief te hoge voltages als default heeft. Undervolts zonder snelheids verlies bieden al gauw 80watt winst.
RX 6900 XT;2;0.5790771245956421;Ondervolt en overclock mis ik ook. Vooral als bij prestaties per watt wordt benoemd dat de 6900XT goed scoort, én als je ziet dat de RTX3090 360W verstookt, versus <300W voor de 6900XT. Wil wel eens zien wat er over blijft van de voorsprong als de 6900XT ook 360W kan verstoken.
RX 6900 XT;3;0.36471226811408997;Dat is geen slecht idee, maar voor een launch review is daar simpelweg geen tijd voor, want zoiets goed doen, kost veel teveel tijd.
RX 6900 XT;3;0.35462644696235657;Hoezo? Andere sites krijgen het wel voor elkaar. Dat zal inderdaad geen super getunede OC zijn, maar een simpele hoever kunnen we zonder te crashen. Maar iets is beter dan niets.
RX 6900 XT;5;0.44961512088775635;We testen tegenwoordig alle kaarten die we gebruiken in de review opnieuw zodat alle scores vers en vergelijkbaar blijven en kijk zelf even naar de lijst met kaarten die getest is. Veel andere sites hergebruiken hun data van maanden geleden, zo niet langer. Ik denk dat we voor deze review al meer dan genoeg extra hebben gedaan.
RX 6900 XT;1;0.49010398983955383;Tja, dat werk halen jullie natuurlijk jezelf op de hals. Een kaart van een halfjaar geleden hoef je van mij echt niet opnieuw te testen, als er vanuit de game of driver geen grote prestatie verschillen verwacht worden. Ik lees dan liever een review mét overclock resultaten.
RX 6900 XT;3;0.45818448066711426;Dat klopt, echter is er ook genoeg volk dat liever nieuwe resultaten ziet. Hierin moeten we dan een keuze maken en dingen zoals overklokken of undervolten kunnen we dan beter uitgebreid in een apart artikel doen.
RX 6900 XT;1;0.29331326484680176;Eens! Gewoon deze deadline halen met de standaard info en dan later als de kaarten beschikbaar worden nog eens op terugkomen met de wat meer diepgaande materie.
RX 6900 XT;2;0.48880940675735474;Of je bijvoorbeeld een 3080 van 320watt undervolt naar 240watt. Verbruik verbeterd niet de prestaties linear. Een milde undervolt laat een rtx al hoger boosten. Verder undervolten zorgt voor gelijke prestaties maar veel minder verbruik. Bij de 6800 series is dit nog onbekend.
RX 6900 XT;2;0.48634928464889526;Probleem met undervolting is dat het erg afhangt van de individuele chip. Als je dat goed wil testen moet je tientallen/honderden chips hebben.
RX 6900 XT;1;0.3163599967956543;Alleen is er tot op heden geen enkele case op techsites, fora, reddit etc te vinden waar dit niet bij lijkt te lukken.
RX 6900 XT;2;0.5543610453605652;Je kunt inderdaad elke kaart undervolten. Maar hoe veel is volledig afhankelijk van de individuele chip. Bij de slechtste chips zal dat heel minimaal kunnen, bij de betere chips meer, en bij de beste chips heel veel meer. Dat maakt undervolting (en overclocking) lastig om goed in een review te verwerken. In een review alleen vermelden dat undervolten altijd kan maar ymmv, is niet heel nuttig. En uitwerken hoe goed het review model toevallig werkt is ook niet heel nuttig. Als je zeker weet dat een bepaalde kaart altijd een van de beste chips krijgt, zoals voor Nvidia bij een EVGA Kingping, kun je dat beter testen. Maar dat zijn wel dan de overdreven dure kaarten.
RX 6900 XT;2;0.25257423520088196;Tot nu toe zijn er toch echt signalen dat zelf de minder binned chips uitstekend undervolten.
RX 6900 XT;2;0.43043243885040283;"De 6800 reeks GPUs ondervolt ook erg stabiel blijkt uit de eerste testen, dus ik vermoed dat beide kaarten een stuk zuiniger kunnen zijn of hoger geklokt kunnen worden. De doorsnee koper gaat zich daar echter niet aan riskeren. Ik game vooral samen met ex-studiegenoten en collega's, allemaal techneuten. En toch ben ik de enige in mijn groep die zich ""waagt"" aan het undervolten/overklokken/BIOS modden van mijn hardware. Ik vermoed dat veel mensen zich hier niet mee willen bezighouden, of schrik hebben van mogelijke risico's. Ik vermoed dat beide makers de kaarten zo'n hoog voltage geven om zeker te zijn dat hun kaarten stabiel draaien. Beide fabrikanten hebben de voorbije generaties meegemaakt dat hun beperkte engineering samples well stabiel waren op een lager voltage of een hogere klok, waarna er allerlei stabiliteitsproblemen aan het licht kwamen bij massaproductie, met alle negatieve berichtgeving errond tot gevolg. Ik gok dat beide makers ook wat marge overhouden voor wanneer ze een refresh moeten uitbrengen, mogelijks als reactie op het aanbod van elkaar. Ik kan mij wel voorstellen dat Nvidia eind volgend jaar met een ""Super"" reeks komt die deze marge benut, of AMD een refresh brengt onder de RX 7000 noemer oid."
RX 6900 XT;5;0.5665983557701111;"""Tweakers"" en dan alles stock draaien en gewoon lijstje afwerken. Ik wil Zienn tot wat Die kaarten echt waard zijn en wat ze toe in staat zijn. Voorlopige Overfloris en undervolts zijn al goed genoeg. Hoeft geen extreme tune te zijn."
RX 6900 XT;3;0.3359774649143219;Als ik het zo zie, kan je beter deze halen voor 1000 op dit moment dan de scalperprijs van 1100 voor een RTX3080. De RTX3090 valt wel te bestellen maar veel duurder. Helaas blijft het bij mij Nvidia door o.a. GeForceNow en de experience die ze daar mee hebben gemaakt (profielen, microfoondemping, webcam aanpassingen). Ik stream geen games. Maar zou ik dat willen doen dan kom je een heel eind met de stock software van Nvidia. Mooie tijd, laat ze elkaar maar ophitsen. Eindelijk weer battles die gevochten kunnen worden. Go team Red, go team Green!
RX 6900 XT;1;0.7101919054985046;Groot verschil is dat je deze voor 1000 euro nooit gaat krijgen. De launch die zojuist plaats heeft gevonden was een groot drama. Ik vraag me af of er uberhaupt kaarten in NL beschikbaar zijn gekomen. Het is nu wachten op AIB kaarten maar die zullen ook wel 1300+ euro zijn (en nauwelijks leverbaar). Als je bij een scalper koopt (wat ik niemand wil aanraden) heb je wel direct je kaart, dat is ook de reden van het grote prijsverschil.
RX 6900 XT;1;0.6194682121276855;Zelfde geld wel voor Nvidia, je kan nergens een moderne grafische kaart krijgen voor MSRP momenteel. Zelfs 2dehands kaarten van 2 tot 5 jaar oud gaan wisselen belachelijke prijzen van eigenaar. Ik heb vorige week mijn grafische kaart, een Vega 56, verkocht voor 50 euro minder dan ik er voor betaald heb destijds. Als ik kijk naar kaarten als de GTX 1050Ti/1060/1070/1080, worden die nu voor veel meer geld verkocht dan eind vorige jaar of zelfs het jaar daarvoor. De logica is compleet zoek.
RX 6900 XT;2;0.44079703092575073;Ik zou het niet te snel onder één kam scheren. Vorige week kon je een 3060 Ti vanaf 450 euro scoren (ventus 2x) tot aan 499 (gaming trio). Bij de 6800(XT) lancering lukte dit niet en ondertussen zijn ze nog steeds niet te pre-orderen bij alernate (itt de 3000 serie).
RX 6900 XT;1;0.527164876461029;Logisch, dit is wat er gebeurt bij schaarste. Het zijn echt niet alleen de grote bedrijven die elke cent eruit melken. De meeste mensen doen dat, zit in ons DNA. Niks nieuws onder de zon.
RX 6900 XT;3;0.39559462666511536;3080 Valt ook te bestellen mits je niet super kieskeurig bent en het in de gaten houd. Zo had ik binnen 2 weken mijn 3080 Gigabyte gaming OC. 850,- Tips: Hou de topics op het forum nauw in de gaten.
RX 6900 XT;5;0.36556217074394226;Nvidia Broadcast met ruisonderdrukking is fantastisch Ik heb dat voor invoer en uitvoer aanstaan en laatst klopte iemand in discord expres hard op tafel en iedereen behalve ik hoorde dat
RX 6900 XT;2;0.311161071062088;Uit de conclusie: En dan met name Nvidia videokaarten. Het is apart om te zien dat de strijd nu niet gevoerd wordt op wie de beste hardwareprestaties heeft, maar wie het beste hardware kan leveren.
RX 6900 XT;1;0.4036458730697632;Met name Nvidia videokaarten? De launch van AMD is absoluut niet beter geweest, eerder een nog grotere paper launch.
RX 6900 XT;1;0.7259218692779541;Ik wilde net zeggen. De launch van de AMD stock kaarten was hilarisch. Daarna iedereen heeft nog een paar dagen geroepen dat de AIB VEEEEEL meer kaarten zouden hebben. Om er vervolgens achter te komen dat dit ook niet waar was (no way..). En als het klopt wat NVIDIA heeft gezegd (het tkort zit hem onder andere is GDDR6 en substraten voor de kaarten) dan is het ook niet meer dan logisch dat AMD ze ook niet kan aanslepen... en waarom Ryzen zo slecht verkrijgbaar is.
RX 6900 XT;2;0.42687296867370605;Vergeet ook niet dat een heleboel hardware per passagiervliegtuig wordt vervoerd, op dit moment vliegen er lang niet zoveel vluchten als normaal waardoor er dus ook minder ruimte in de laadruimen over zijn om deze hardware te vervoeren. Als je al deze tekorten bij elkaar optelt en kijkt naar eerdere launches is het wel redelijk te verantwoorden.
RX 6900 XT;1;0.44791847467422485;Nee ben ik niet mee eens, ze wisten van deze probleem al ruim 6 maanden, dus hadden ze de kaarten later moeten uit brengen en overal al naar toe moeten sturen tot ze een veel grotere opslag hadden, en dan pas overal te koop aanbieden, nu heb je de zelfde probleem weer als wat Nvidia heeft met al haar GeForce RTX 3000 series kaarten, en AMD hun Radeon RX 6800 XT en hun Ryzen 5000X CPU's, en nu dus weer met hun Radeon RX 6900 XT kaarten, nee sorry dit licht meer aan AMD, en ook Nvidia dan aan de rest. Wat ik wel vreemd vind is dat de Radeon RX 6900 XT het overal (reviewers) slechter doet in 4k DX12: Shadow Of The Tomb Raider dan de GeForce RTX 3080, behalve bij Tweakers? En met VULKAN: Red Dead Redemption II is ook de Radeon RX 6900 XT langzamer in 4k dan de GeForce RTX 3080, maar dat scheelt op Tweakers heel weinig met die van Guru3D. En dat er toch wat dingen niet in orde zijn met de Radeon RX 6900 XT, dat de wattage op 250w blijft hangen en niet de 300w wat de kaart hoort te wezen bij JayzTwoCents, en de vreemde dingen in sommige spellen waar de Radeon RX 6900 XT gewoon langzamer of een stuk langzamer is dan de GeForce RTX 3080. Hoop dat het gefixts allemaal woord en dat wanneer ze te koop zijn het allemaal opgelost is met drivers. Edit foutjes
RX 6900 XT;2;0.3759111762046814;"Ik heb geen probleem met paper lunches. aLs je ander wil zou nV en AMD in februari moeten release om redelijke stock te krijgen voor beetje beter leverbaarheid. Maar houd ook in dat NDA ook veel later komen en je gigantische lekken en mis interpretaties naast de fake nieuw en nog grotere hype krijgt. Ik heb nog een Vega 56 en sta niet te springen om nu perse een G-kaart te moeten hebben. Nu weet ik redelijk hoe nextgn presteert en ga begin next year iets halen of niet. Daarnaast 3090 ""custom"" Reference vs 6900XT reference is nV de king maar dat kan tussen AIB top OC varianten van Custom producten mogelijk meer voordeel van AMD vallen. En ja daar zal je dus sowieso wat langer op moeten wachten"
RX 6900 XT;2;0.49755027890205383;"Ik heb wat commentaar en verwarring op de video van JayzTwoCents; Als eerste wat ik voornamelijk kwalijk vindt gaf hij aan dat er een fout was met zijn Radeon driver/software en concludeert direct dat het zo hoort in plaats voor alle zekerheid even navraag te doen aan AMD of dit wel zo hoort. Misschien heeft hij een defecte prototype gekregen? Ik heb van GamerNexus/HW Unboxed hier geen commentaar over gehoord van een power limit. Ook JayzTwoCents keuze in games; geen ene titel kwam van dit jaar, 4 synthetische benchmarks en nog een paar opmerkingen die niet correct waren of aan de vage kant, maar die laat ik even achterwege. Vindt het nogal erg slordig van iemand met ruim 3 miljoen subscribers om zo onnauwkeurig er mee om te gaan..."
RX 6900 XT;2;0.4090866148471832;Klopt, maar ik zie de problemen wat hij had met zijn Radeon RX 6900 XT ook terug bij Guru3D. Zo als met VULKAN: Red Dead Redemption II waar de Radeon RX 6900 XT langzamer is dan de GeForce RTX 3080 op 4k, of met DX12: Shadow Of The Tomb Raider waar de Radeon RX 6900 XT langzamer is dan de GeForce RTX 3080 op 4k, of DX11: Microsoft Flight Simulator waar de Radeon RX 6900 XT een aardig stuk langzamer is dan de GeForce RTX 3080 op 4k en ongeveer gelijk staan aan de GeForce RTX 2080 Ti, of DX11: The Witcher III Wild Hunt waar ook de Radeon RX 6900 XT een aardig stuk langzamer is dan de GeForce RTX 3080 op 4k, en dan reken ik Ray tracing niet mee. Ook al is het maar een paar fps, is het nog steeds erg vreemd, en hoop ik dat het opgelost woord met nieuwe drivers .
RX 6900 XT;3;0.31324759125709534;Ik geef toe, AMD drivers zijn funest. Ik heb met de 5700XT een RMA moeten aanvragen gezien de drivers alleen de issue waren ivm BSOD/black screens. En uiteindelijk voor de groene kamp heb gekozen met de 2070 super zonder de driver issues op dat moment speelde. Dus ja je komt weer neer op de drivers die hopenlijk opgelost worden met dat aspect in tijd, maar fundamenteel ga je je dan afvragen wat de 6900XT wat dat well zich kan bieden als het de optimalisatie in tijd zich bied?
RX 6900 XT;1;0.4255237281322479;Ja dat is altijd de vraag, ik hoop gewoon dat alles opgelost is wanneer ze te koop weer zijn in januari/februari 2021, we zullen zien.
RX 6900 XT;1;0.7331575751304626;Daar kun je nog niets over zeggen bij AMD. We zitten nu 3 weken na launch. Een volledige productierun voor de chips zelf duurt normaal gesproken 6-8 weken, waarna ze naar de AIB's gaan. Een run die op de release datum is gestart zul je pas 8-10 weken later in de winkel zien. Het is echter aannemelijk dat er vóór release al een run is gestart, dus zó lang zal het niet duren. De Zen 3 CPU's zijn nu een dikke maand uit en daar zijn vorige week forse aantallen van geleverd als je naar de verschillende retailers luistert. Bij Nvidia is het een ander verhaal. We zitten daar nu 11 weken (bijna 3 maanden!) na release en er is nog steeds niets beschikbaar. Alle retailers zeggen ook dat er geen grote leveringen zijn geweest of komen, het druppelt allemaal maar extreem beperkt binnen.
RX 6900 XT;1;0.5987735390663147;Nee, dat kun je echt wel zo stellen. AMD had nul voorraad bij de launch en ook nu is alles nog steeds uitverkocht. Ryzen voorraden zijn trouwens ook laag. Productie problemen bij Sony en MS. En mensen blijven volhouden dat het specifiek een NVIDIA probleem is. Ik geloof er helemaal niets van. AMD zit gewoon met hetzelfde probleem als NVIDIA, die hebben al aangegeven dat het niet eens een GPU productie probleem is. GDDR6 en substraten zouden het probleem zijn. En dan kun je GPU's produceren tot je eens ons weegt, maar als je geen PCB's hebt om ze op te plakken kom je niet heel ver.
RX 6900 XT;1;0.467960000038147;Nee, je kunt nog niet stellen dat het bij AMD net zo slecht is gesteld als bij Nvidia, want ze zitten bij andere fabs. Daarnaast heeft AMD een veel groter repertoire aan producten te produceren: Zen 3, RDNA 2 en de chips voor de PS5 en XSX. De volumes voor die laatste twee liggen vele malen hoger dan Zen 3 en RDNA 2. AMD heeft voor Q4 zo'n 120k wafers ingekocht bij TSMC, enkel voor productie van de console chips. Dat is maar liefst een derde van TSMC's volledige 7nm capaciteit. De overige twee derde moet verdeeld worden tussen AMD's Zen 3 en RDNA2, maar ook Apple's A13 en meerdere Qualcomm chips. En dat zijn enkel de grote namen. Als die consoles eenmaal de deur uit zijn kan AMD schakelen. Op launch dag was het bij allebei ruk, echter is 3 (of bijna 5 voor Zen 3) weken niet genoeg om te zeggen dat AMD in eenzelfde situatie als Nvidia zit. Daar duurt de malaise al 3 maanden en is er nog geen eind in zicht. Zelfs ten tijde van Fermi (GTX 400) waren ze niet zo slecht af. Ook AMD's Polaris (RX 480) was na 2 maanden gewoon goed verkrijgbaar, terwijl die bij launch en daarna ook nergens te bekennen was.
RX 6900 XT;1;0.5895127058029175;De vraag is of 'slecht gesteld' wel de juiste bewoording is. Zowel AMD (55.52%) als Nvidia (56.8%) zagen hun omzet enorm groeien afgelopen kwartaal. Nvidia is Q4 19, Q1 20, Q20 gedaald met hun inventory en is daarna gaan opbouwen (+16, +46). Bij AMD is er sprake van systematische toename van +16, +11, +30, +24. Het verhaal dat die vraag momenteel extra groot is moet dus waar zijn. In geval van AMD is er echter nog iets anders aan de hand. Daar zie je de R5 3600 van 159 naar 217 euro gestegen. Hetgeen overigens ook geldt voor de 3600X, 3700X, enz. Dat zijn allemaal cpu's die al meer dan een jaar oud zijn. De meest logische verklaring is dat AMD deze vraag niet had verwacht en al minstens een jaar geleden had afgesproken de productie capaciteit van Q4 2020 primair in te zetten voor de console launches. Als dat waar is dan heeft AMD voor de cpu en gpu tak een groot probleem want er was al sprake van een mismatch tussen vraag en aanbod. Het aanbod kan niet omhoog en de voorraden raken uitgeput. Dus dan krijg je een opstapeling van problemen.
RX 6900 XT;2;0.4218076765537262;Nee, ze zeiden dat het niet alleen een GPU productie probleem is. maar een (klein) deel is substrates en componenten dus.
RX 6900 XT;2;0.4358443319797516;"Daar kun je natuurlijk prima wat overzeggen, lange termijn is natuurlijk nog een grote onbekende, maar ik heb niet het idee dat Patrick het heeft over lange termijn, maar juist de launches aan het vergelijken is. Echter wanneer je kijkt naar de day 1 launches dan lijkt Nvidia het zowel bij de RTX 3080, RTX 3090 en RTX 3070 beter voor elkaar te hebben gehad dan AMD met de 6800XT, 6800 en 6900XT vandaag. Voor zover ik kan nagaan had Nvidia in alle gevallen meer voorraad (al is meer nog steeds dramatisch), zowel bij reference als bij AIB kaarten. bij AMD lijkt er alleen een kleine reference voorraad geweest te zijn op launchday, zo klein dat de meeste webwinkels zelfs statements uitbrachten dat ze geen kaarten gekregen hadden of dat de voorraad zo klein was dat verkoop geen zin had, bij de AIB kaarten hetzelfde, wederom winkels die aangaven geen idee te hebben of er kaarten zouden komen, wanneer ze zouden komen of wat ze zouden gaan kosten. Zelfs reviewers hadden geen idee van de prijzen omdat de AIB partners deze niet wilden noemen tot na de embargo lift van AMD. Zet je daar het bespottelijke gedrag van het AMD marketing team nog bij, voornamelijk Frank Azor trouwens me zijn lacherig doen over Nvidia's launch en zijn uitspraken als ""Ik neem je $10 graag in ontvangst"" en ""Ik had een kaart met slechts enkele keren refreshen"". En dat terwijl juist doordat AMD, Nvidia's launches gezien heeft ze de kans hadden om de verwachtingen veel beter te managen en juist te leren van Nvidia's launches. Maar nee, wat doen ze, ze maken er nog een groter clownfiesta van. Voor mij is toch echt duidelijk, en dat zeg ik als iemand met een belang in AMD, dat AMD de day 1 launches toch echt duidelijk het slechts voor elkaar had. Ik hoop natuurlijk dat AMD het op langere termijn beter doet dan Nvidia qua beschikbaarheid."
RX 6900 XT;2;0.40084949135780334;Op launch dag zeker, dat ontken ik ook niet. Het is echter ook al jaren zo dat op de dag zelf voor veel producten vrijwel alles uitverkocht is, dus je moet de post-launch situatie ook bekijken. En daar is het bij AMD nog te vroeg voor. Als ze in Januari nog steeds geen reet kunnen leveren, dan hebben zij ook problemen. Gezien ze echter (naar het schijnt ) zo'n absurd aantal wafers hebben ingekocht bij TSMC, lijkt het er niet op dat ze kampen met een tekort aan grondstoffen. Zelfs Turing had een tweede productierun nodig, ondanks de fors lagere vraag vanwege het matige product (vergeleken met Pascal). Toen die eenmaal had plaatsgevonden was het geen probleem meer. Hetzelfde geldt voor Polaris. En Pascal. En Maxwell. En Tahiti. En Cypress. Die hadden stuk voor stuk een tweede run nodig, maar vanaf de tweede run waren ze goed verkrijgbaar. Die tweede run is er inmiddels al geweest voor Ampère, voor RDNA 2 nog niet. En dat riekt naar Fermi perikelen, die ook maandenlang niet te krijgen was. De enige twee die ik me kan herinneren van de afgelopen 10 jaar waarbij er weinig leveringsproblemen waren zijn Cayman (een refresh) en Kepler (GK104, kleine chip op een procedé met genoeg capaciteit om yields te compenseren). En bij in elk geval die laatste heb ik toen de levertijden niet echt op de voet gevolgd dus wellicht dat ik dat gemist heb
RX 6900 XT;2;0.41786283254623413;Post-launch moet je ook zeker naar kijken, maar wanneer een C-level marketeer de concurrentie zo'n beetje uitlacht en daarnaast ook zijn klanten in het gezicht slaat met onzinnege tweets terwijl wanneer puntje bij paaltje komt de eigen launch nog slechter lijkt te zijn dan die van de concurrent die je uitlacht, dan moet je er niet vreemd van opkijken wanneer daar terechte kritiek op komt van de consument. Als je verwachtingen schept moet je ze ook waarmaken. dat alles uit zou verkopen was natuurlijk al ruim voor de launch duidelijk, daar zal de meeste kritiek denk ik ook ik niet op zijn. Als AMD de verwachtingen correct gemanaged zou hebben en niet gehint zouden hebben naar een betere launch dan die van Nvidia was er denk ik een stuk minder kritiek geweest. Uiteraard heb je altijd teleurgestelde mensen, maar dat zullen er minder zijn geweest als AMD vooraf gewoon eerlijk was geweest en Frank Azor zich niet als clown had gedragen. Ik hoop dat het uitkomt met die wafers, en ik vraag me af of het genoeg is. Ik verwacht namelijk niet dat de vraag naar consoles snel gaan afnemen zolang we nog last hebben van corona, daarnaast gaan de geruchten dat AMD dit kwartaal 120k wafers heeft voor de consoles en dat ze 80% van hun wafer allocatie zouden gebruiken voor de consoles was zou duiden op een Q4 allocatie van 150k wafers. Kijk je dan naar het feit dat AMD in 2020 voor de console ramp volgens de geruchten al op 30k wafers per maand zou zitten alleen voor eigen producten zou dat betekenen dat ze in Q4 eigenlijk helemaal niet zoveel wafers hebben aangezien ze er dan 60k 'te kort' zouden komen voor hun eigen normale productie. Zelfs als je dan in Q1 50% minder wafers naar de consoles zit je dan in Q1 op de capaciteit die ze ook voor de console launch hadden voor zichzelf. Op basis daarvan vraag ik me echt af of we, zeker vroeg in Q1, wel gaan zien dat ze weer kunnen voldoen aan de vraag.
RX 6900 XT;1;0.574020266532898;nividia is maanden al uit he. vergeet dat niet.
RX 6900 XT;2;0.39645126461982727;Dat is wanneer je de day 1 launches vergelijkt natuurlijk niet van belang Misschien even mijn beide posts lezen
RX 6900 XT;1;0.5024189949035645;"Niet alleen het feit dat het een ook een grote paper launch is maar dat de AiB kaarten VER boven adviesprijs worden verkocht. Volgens HardwareUnboxed zeggen anonieme bronnen bij de AiB's dat ze anders gewoonweg geen winst kunnen maken, wat betekend dat AMD erg hoge prijzen vraagt aan haar partners voor de RDNA2 chips. Kijk maar eens bij Megekko naar de Asus TUF 6800XT OC, gewoon een dikke 900 EUR terwijl de Asus TUF RTX3080 850 EUR kost. Big deal zou je zeggen ware het niet dat de 6800XT een adviesprijs van 650 EUR heeft en de RTX3080 720 EUR. Dan is het toch wel erg schokkend te zien dat een TUF versie 6800XT 250 EUR boven het adviesprijs zit terwijl de TUF RTX3080 ""maar"" 130 EUR erboven zit. Hetzelfde geld bij de andere partners, MSI Gaming Trio X, Gigabyte Gaming Force, etc. Bij ze allemaal is het verschil met het adviesprijs bij de AMD varianten groter dan de Nvidia tegenhangers."
RX 6900 XT;2;0.3314566910266876;Gamers Nexus gaf aan dat hetzelfde geldt voor Nvidia kaarten. Waarom het verschil bij AMD groter is, is de vraag. Winkels die nu wat extra plussen? Fabrikanten die nog wat meer winst willen voor het einde van het jaar ?
RX 6900 XT;1;0.47368323802948;Nee GamersNexus gaf aan dat dit geld voor het lagere segment, vanaf de 3060Ti ging. De TUF3080 was bijvoorbeeld de eerste week van release gewoon voor 740 EUR te bestellen preorderen. De 3060Ti heeft echter een zeer hoge adviesprijs die maar 100 EUR onder de 3070 ligt, GamersNexus gaf al aan dat AiBs het hierdoor wel erg moeilijk gingen krijgen omdat de marges voor de 3070 al magertjes waren.
RX 6900 XT;1;0.42742303013801575;De 3060Ti was nog niet een bekend toen hij dit aangaf. Wellicht hebben we een andere video gezien. Weet haast wel zeker dat het hier over gehad heeft tijdens een HW recap of tijdens z'n 3090 overclock show oid. Maakt ook niet uit, het is een bende. Ik ga het niet opzoeken iig, zijn lange shows.
RX 6900 XT;2;0.4145890772342682;AMD is nog slechter verkrijgbaar dan nvidia momenteel, toch een beetje jammer na de grootspraak van AMD dat ze beter leverbaar zouden zijn dan nvidia, ze hadden beter niks kunnen zeggen.
RX 6900 XT;2;0.39208510518074036;Dat is echter niet wat er gezegd is. Alleen dat het geen paper launch zou zijn, en dat is het ook niet. dat is zelfs later nog nader uitgelegd maar dat lijken sommige mensen (moedwillen?) te vergeten. Een webwinkel had zijn webverkeer volume laten zien voor verschillende launches. webverkeer voor de 6000 series was 5 hoger als bij de nvidia 30 serie, en zelfs 30 keer zo hoog als bij de launch van de Ryzen 3000 + rx5700 series een jaar geleden. AMD had dus zeg 3 keer hun normale launch voorraad GPU's kunnen hebben en het had er voor consumenten nog net zo uitgezien als nu.
RX 6900 XT;2;0.39947810769081116;"Wel of geen paper launch is natuurlijk vooral een semantische discussie, zeker omdat er geen officiële definitie van een paper lauch lijkt te bestaan. Voor de een is het alleen paper launch waneer er geen product is, tja, wil je die uitleg aanhouden dan zijn de launches van AMD en Nvidia inderdaad beide geen paper launches, neemt niet weg dat het beide wel bedroevend slechte launches zijn vanuit de consument gezien. In de volksmond is een paper launch echter een stuk breder en wordt de term ook gebruikt wanneer aanbod en vraag zover uit elkaar liggen dat slechts in een klein deel van de vraag voorzien kan worden. Dat is hier duidelijk het geval en dus is ook logisch dat er over paper launches gesproken wordt, ook al hebben beide fabrikanten wel degelijk een aantal producten geleverd. Dat Frank Azor dat niet gezegd zou hebben vind ik ook wat makkelijk, die ""I look forward to your $10"" of iets in die strekking impliceert gewoon, zeker kijkende naar de tweet waarop hij reageerd, dat AMD het wel even veel beter zou gaan doen als Nvidia. Uiteindelijk pakt dat niet zo uit, en doen ze het op het oog zelfs stukken slechten, tja dan kan je commentaar verwachten. Dat was gewoon een oerdomme actie van Azor, net als die tweets dat ""hij met een paar keer refreshen een kaart had"", dat is gewoon de klanten in het gezicht slaan."
RX 6900 XT;2;0.26919472217559814;Duizenden keren refreshen vanmiddag en voor de meesten nog niets, maar het was al bekend dat Azor waarschijnlijk een pre-order code gebruikt heeft (loopt via een site die wel te benaderen was). Een paar hadden vandaag geluk voor een 6900X maar dat was het. (zie het leveringstopic op GoT voor alle verhalen, wat ik las is het aantal op 1 hand te tellen).
RX 6900 XT;1;0.5596648454666138;"Helemaal eens met de stelling dat een ""paper launch"" ook opgaat als en product nagenoeg niet te verkrijgen is. Als ik terug denk aan een technische paper launch dan is het wel de Titan CEO die nvidia zogenaamd heeft uitgebracht, alleen al het feit dat er nooit of te nimmer er een benchmark is van uitgekomen bewijst na zo een lange tijd dat het ding echt niet bestaat. Ook nu vind ik het weer een paper launch want ook nu weer zijn er amper kaarten te verkrijgen, maar jawel de zogenaamde reviewers hebben die dingen wel allemaal gekregen. Jij als klant bent blijkbaar niet belangrijk genoeg voor beide kampen, dat blijkt al uit de extreme kadootjes die de reviewers ontvangen van de fabrikanten. Een ware speciale koelkast bij het van M$ hun spel console en ga zo maar, kosten of moeite worden bespaard. In plaats van te zorgen dat een klant na het zogenaamde uitbrengen werkelijk iets kan kopen heeft blijkbaar geen enkele interesse bij deze figuren. Eerlijk gezegd wist ik het al meteen de laatste jaren zijn gewoon een herhaling van de waanzin met de 20 serie van het groene merk. Voorlopig zie ik helemaal af om teveel te moeten betalen voor een product wat in mijn ogen weer veel te duur is geworden. Ik blijf lekker door draaien op mijn stok oude 1070 tot het kreng het begeeft, dan ga ik nog een kijken of ik met de intel gpu verder kan."
RX 6900 XT;2;0.3523295223712921;"Feit is dat Frank Azor, ""Chief Gaming Architect"", in rechtstreeks antwoord op de bewering dat de 3090 een paper launch was en dat datzelfde ook wel zou gelden voor de AMD kaarten, er wel $10 om durfde te verwedden dat dat niet zo was. Dan heb je inderdaad letterlijk alleen gezegd dat het geen paper launch zou worden en heb je technisch gelijk. Maar als ""Chief Gaming Architect"" mag je ook wel nadenken over hoe je opmerkingen in de praktijk over gaan komen als de beschikbaarheid net zo slecht, zo niet slechter is als bij Nvidia (zeker in het begin). ""De vraag was gewoon heel hoog, het was geen paper launch"" was ook Nvidia's verweer, tenslotte. Correct is gewoon je mond houden en niet alvast gaan pochen over dat je het wel even beter gaat doen dan de concurrent. Laat dat maar rustig in de praktijk blijken en dan mag je gaan pochen. Radeon en marketing, dat blijft toch gewoon een onhandige combinatie, zelfs als ze een keer een goed product hebben. Die $10 mag hij gewoon betalen aan ieder die erom vraagt, wat mij betreft."
RX 6900 XT;2;0.4552573263645172;Alleen hadden ze niet 3x hun normale launch voorraad. In Nederland had geen enkele retailer voorraad. In US kwamen diverse berichten dat er retailers waren die niks kregen en retailers waren die een hand vol kaarten geleverd kregen. Maar dat was het dus wel. In Nederland (mits geen onderdeel van team red) kon je geen kaart bemachtigen. De frustratie is bij de mensen extra groot juist omdat AMD vooraf aangaf het beter dan Nvidia te gaan doen. AMD heeft het niet beter gedaan, er was ook inde US minder supply dan tijdens de 3080 launch. Bij Nvidia waren in Nederland in ieder geval kaarten te koop (naar het schijnt iets meer dan 100 FE's bij de 3080). Ook bij de meest recente 3060 Ti launch was het mogelijk om kaarten te bemachtigen bij Nederlandse webshops. Zelf heb ik een Gaming Trio en een Eagle OC mogen ontvangen. Op het moment dat Nederland geen enkele kaart krijgt dan is dat geen verrassing, dan weet je dat van te voren. AMD had niet moeten impliceren dat ze het beter zouden doen maar transparant moeten zijn dat ze zeker niet genoeg voorraad zouden hebben.
RX 6900 XT;2;0.33004674315452576;Waar en wanneer heeft AMD gezegd dat ze beter leverbaar zouden zijn van Nvidia? kan zijn dat ik dit gemist heb, maar valt me op dat veel mensen uitspraken doen die zogenaamd een fabrikant gezegd heeft, maar uiteindelijk alleen een vaag gerucht was dat een reviewer heeft gehoord
RX 6900 XT;2;0.33272385597229004;Iets met een (redelijk hooggeplaatste) AMD werknemer en 10 dollar.
RX 6900 XT;1;0.7020973563194275;Je hebt best wat gemist ja. Feit blijft dat je nergens een 6800xt kan of kon kopen, op die 7 mensen na die mazzel hadden. het is en blijft gewoon kansloos. Al ga je met 1000 euro langs alle winkels, je komt gewoon weer met je 1000 euro thuis.
RX 6900 XT;1;0.2723933458328247;Dit heeft wel degelijk een medewerker van AMD gezegd op twitter, Frank Azor
RX 6900 XT;3;0.29557037353515625;heb je daar een tweet vna? zodat we het kunnen bevestigen?
RX 6900 XT;4;0.2846148908138275;
RX 6900 XT;2;0.43481969833374023;Het issue blijft prijs, er is van beide leveranciers wel iets leverbaar, maar de winkels/groothandels hebben er enorme prijsverhogingen aan gehangen.
RX 6900 XT;2;0.3734443783760071;Dat is natuurlijk een kwestie van vraag en aanbod. Als het aanbod meer in lijn was geweest met de vraag, dan had je niet dergelijke prijsverhogingen gezien.
RX 6900 XT;5;0.39652159810066223;Ik ben zelf voor de 3090 gegaan omdat ik op 4K speel en graag alles uit mijn games wil halen (ray-tracing +DLLS) maar ik ben onder de indruk van de 6900XT. Mooi kaart van AMD en goed dat er eindelijk weer concurrentie en daarmee het marktaandeel van AMD op gebied van GPU's weer toeneemt. Daar heeft de consument alleen maar voordeel van op de lange termijn.
RX 6900 XT;1;0.3091049790382385;Nu hopen dat ergens Q1 deze beschikbaar wordt gezien ik het geen kans geef dat de kaart in 2020 beschikbaar zal zijn. Ook wil ik graag afwachten hoe de branded modellen gaan functioneren. Al met al wat een bakbeest van een kaart zeg! Overgins mooie review guys!
RX 6900 XT;2;0.5114298462867737;Toch jammer dat nu AMD eindelijk een goede concurrentie bied tegen Nvidia er zoveel problemen zijn met de levering van alle kaarten. Waar ik had gehoopt dat op dit moment de prijzen zouden dalen door de concurrentie stijgen ze juist door gebrek aan levering.
RX 6900 XT;3;0.3060198128223419;"Ik weet niet hoe het zit met andere 3D-programma's zoals Blender. Maar de Cuda/Optyx optimalisatie heeft een onzettende grote voorsprong op zijn AMD broertje. Natuurlijk is er zoiets als ProRender, maar dat is eigenlijk best wel huilen omdat je daat veel cycles-nodes mist en dus zit te kloten met nodes die speciaal van ProRender zijn. Cuda/Optyx zijn volledig ondersteund en dus ook alle ""vanilla""-nodes. Daarbij zijn er ook zat tools uit, die over het algemeen veel vaker Cuda ondesteunen. Kan er zo geen noemen, maar doe een blindfold handgreep in de Two-Minute Papers-papers, en je zult zien dat echt het meerendeel alleen en uitsluitend voor Cuda is gebouwd. Tijd zal het leren volgens mij, maar alsnog heeft Nvidia hier een dikke streep voor."
RX 6900 XT;2;0.34974557161331177;"Ik denk dat Linux gebruikers als ik heel blij zijn met deze inhaalslag van AMD. Ze zijn er nog niet, maar ik denk dat een hoop Linux gebruikers de kleine perfomance hit graag nemen voor fatsoenlijke drivers op Linux! inb4: ja maar gamen op Linux is kak: Valt mee voor veel games, zeker icm Proton, maar een hoop mensen, inclusief ik, dual-booten hun PC met Windows voor de games. Echter de meeste games speel ik ""gewoon"" op Linux."
RX 6900 XT;3;0.4276605248451233;Maar is er voor de benchmarks nu DirectX 11 of Vulkan gebruikt ?
RX 6900 XT;1;0.3895864188671112;Breakpoint testen we met DirectX 11. Vulkan gebruiken we alleen voor RDR2 en Doom.
RX 6900 XT;1;0.4916467070579529;Op de pagina Testverantwoording is in een tabel uiteengezet welke games we op welke API draaien. In de tekst op de pagina van Breakpoint stond het er inderdaad niet duidelijk bij, heb het toegevoegd.
RX 6900 XT;4;0.586438775062561;Dank voor de toevoeging ! Ik had niet meer aan de tabel gedacht. Korte termijn geheugen is ook niet meer wat het geweest is. Wel een klein beetje jammer dat we bij geen enkele game een Direct X 12 vs Vulkan vergelijking hebben, maar verder een top artikel om te lezen !
RX 6900 XT;2;0.4386560916900635;Iets wat hier (enigzins logisch) niet wordt meegenomen is DLSS 2.0 . Dit is echter exact de reden dat ik mijn 3080 niet cancel: als ik de benches van Tom mag geloven geeft dit vaak een bump van maar liefst 20 (!) fps in Cyberpunk. Als in 30 naar 50. Ik weet niet wat dit betekent voor beeldkwaliteit maar dat is toch wel reusachtig.
RX 6900 XT;1;0.39588460326194763;Je vraagt je bijna af waarom we dit nog lezen. Tegen de tijd dat je er 1 thuis hebt staan de reviews van de opvolger online.
RX 6900 XT;1;0.6783561110496521;Dit zijn schrodinger kaarten, ze bestaan, maar ook niet. Tegen de tijd dat deze dingen ergens tegen de normale retailprijs 'redelijk' te koop zijn, heeft Nvidia z'n Super al uitgebracht, AMD heeft de xx50 en xx70's, nog beter, nog sneller, nog vetter. Leuk om te lezen hoe rap het is gegaan met grafische kracht (en CPU kracht), maar dit is iets leuks voor volgend jaar misschien, als je dan het geluk hebt. (buiten dat zover ik begreep de 3060 alweer populair is onder de minders, dus we krijgen buiten scalpers ook nog eens die jongens erbovenop)
RX 6900 XT;3;0.5452579855918884;In Assassin's Creed Valhalla presteert de 6800 een stuk beter dan de 3090... bijna 30% sneller. De gemiddelde score zou aardig opgekrikt worden in het voordeel van AMD denk ik, als er wat meer recente games ook getest worden.
RX 6900 XT;4;0.2770328223705292;Bedankt voor de info en link! Als het me lukt om een 6800 vast te krijgen is dat fijn meegenomen. Want het was juist voor ac Valhalla dat ik een nieuwe pc aan het samenstellen ben gegaan.
RX 6900 XT;1;0.45113927125930786;Succes daarmee! Ik zit zelf alleen naar een nieuwe GPU te kijken en die is al nergens te krijgen...
RX 6900 XT;1;0.48979389667510986;Waarom staat in de grafieken van Geluidsproductie en temperatuur de RTX 3090 er niet bij ? Heeft die dezelfde geluidsproductie en temperaturen als de RTX 3080 mss ?
RX 6900 XT;2;0.4361591935157776;Die staat er niet bij omdat we van de RTX 3090 geen Founders Edition ter beschikking hadden. In dit type review (3D-chips) vergelijken we voor o.a. geluidsproductie en temperatuur enkel referentiekaarten en FE's onderling, terwijl we custom kaarten ook weer apart behandelen.
RX 6900 XT;1;0.5659321546554565;De 6900XT lijkt me nogal een kansloze kaart, want wie gaat dit kopen?!? Een 6800 non-XT blijft de beste keuze qua prijs/prestatie aan de AMD kant, en voor mensen die perse het beste van het beste willen ongeacht wat het kost is dat nog altijd de RTX 3090.. Dus wie blijft er dan over? Alleen die-hard AMD fans met geld zat en die bereid zijn meer dan 1000 euro aan een videokaart uit te geven en vanuit fanboyisme weigeren een Nvidia RTX 3080 of 3090 te kopen.. Deze GPU is trouwens mede een reden waarom Nvidia straks alweer met een RTX 3080Ti komt denk ik, die waarschijnlijk hetzelfde zal gaan kosten als deze 6900XT qua adviesprijs (999 euro).. In de praktijk dus meer richting 1100-1200 euro.. En dan zitten we weer op RTX 2080Ti prijzen in feite.. Niks veranderd, behalve dat we in het segment van 500-800 euro daaronder nu wel veel betere keuzes hebben, zowel bij Nvidia als AMD nu..
RX 6900 XT;2;0.38489362597465515;90% van de performance voor 66% van de prijs, dat lijkt me reden genoeg.
RX 6900 XT;1;0.672547459602356;Meer dan 1000 euro voor een videokaart die amper beter presteert dan de 6800XT of RTX 3080 kaarten daaronder blijft nog steeds gewoon een slechte deal voor het geld.. Inderdaad wel minder slecht dan de RTX 3090 die een nog slechtere deal is.. Maar zoals ik al zei, mensen die dit soort kaarten kopen boeit prijs/prestatie verhouding sowieso weinig tot niks denk ik, die willen gewoon het beste van het beste ongeacht wat het kost.. Of je dan nou meer dan 1000 of 1500 euro betaalt maakt dan ook niet veel meer uit, en heb je wel beste van het beste deze generatie en niet de op 1 na beste.. Iedereen die highend wil met iets van prijsbewustzijn koopt een RTX 3070/3080 of AMD 6800(XT)..
RX 6900 XT;2;0.35679253935813904;Dan koop je de 6800XT en niet de 3080. Want die is en blijft beter dan de 3080. Daarnaast betaal je 600,- meer voor de 3090 voor 10fps meer. Denk dat de keuze dan snel gemaakt is. Halo producten zijn het sowieso. Maar voor 600 minder weet je dat je een betere deal hebt met de 6900XT kaart. Daarnaast zeg je zelf. Nvidia ziet concurrentie opkomen en komen nu met een reeks nieuwe kaarten aan om het gat te dichten wat AMD in hun voordeel heeft. Kortom Nvidia is net als Intel geweest, jarenlang niks doen. Denkende er komt toch niks. En dan nu is AMD er opeens moeten er meer kaarten komen
RX 6900 XT;1;0.607456386089325;Dat Nvidia net als Intel jaren niks gedaan heeft is echt onzin.. Nvidia innoveert heel veel, alleen met de 1e RTX Turing serie hebben ze te weinig progressie gemaakt qua prestaties.. De laatste stap met Ampere qua prestaties was gewoon weer goed en normaal.. Intel doet al 4-5 generaties te weinig met hun CPU's.. Nvidia Ti modellen zijn ook niks nieuws, die brengen ze al uit sinds de 700 serie.. En dat is niet alleen om beter te kunnen concurreren maar ook vooral om meer chips te kunnen verkopen ongeacht de yields.. Voor de rest had ik gehoopt dat meer mensen zouden kiezen voor een AMD 6800XT i.p.v. RTX 3080 zodra die uitkwam, zodat de wachtrijen voor RTX 3080 kaarten kleiner zouden worden.. Maar dat is dus totaal niet gebeurt wat ook wel weer genoeg zegt over hoe de mensen die dit soort highend videokaarten werkelijk kopen erover denken..
RX 6900 XT;1;0.724105179309845;Wederom dramatische drivers dus nee...
RX 6900 XT;1;0.7727597951889038;Niet 1 multiplayer game getest, fps van statische single player gamebenchmarks zeggen me niks..
RX 6900 XT;2;0.3921298384666443;Hoewel we deze info graag zouden willen geven, is betrouwbaar benchmarken in multiplayer games haast onbegonnen werk. In deze reviews kijken we puur naar de gpu-prestaties, bij multiplayer games begint de cpu en de latency van het netwerk ineens ook een andere rol te spelen.
RX 6900 XT;3;0.37971246242523193;Tja, juist daarom is het interessant, als je in nederland zit moet latency geen probleem zijn,met de meeste servers heb je <20 ms.
RX 6900 XT;4;0.4292392432689667;Dat het interessant kan zijn ben ik met je eens maar het moet ook consistent getest kunnen worden om betrouwbare resultaten op te leveren, vervolgens ook in een redelijke set games om een gedegen uitspraak te kunnen doen over de gemiddelde prestaties.
RX 6900 XT;1;0.5944892168045044;Grafische performance tests met MP game geven veel te inconsistente resultaten en kunnen voor de consument erg misleidend cq onbetrouwbaar cq verwarrend zijn.
RX 6900 XT;2;0.3862558901309967;Zo moeilijk is het nou ook weer niet, spring een volle 64 man server binnen op dezelfde map en je krijgt behoorlijk consistente resultaten. Overdrijven is ook een vak.
RX 6900 XT;5;0.5299628376960754;En.. het is uitverkocht!
RX 6900 XT;1;0.3290317952632904;Was het tekoop dan?
RX 6900 XT;3;0.4943973422050476;Toch gek dat er bij sommige games zo'n significant verschil zit met de 3090 zoals Total war. Dat soort verschillen trekt de score flink naar beneden. Nu ook afwachten of dit weer een fine-wine product is/wordt
RX 6900 XT;1;0.5094820857048035;Nou, dit ook inderdaad. Hier me van de week nog zitten verbazen dat Star Wars Squadrons op mijn Fury (non-X) gewoon bijna strak op 60 FPS blijft. Sommige games is de uitdaging wat groter, zeker, maar ik klaag écht niet over hoe die kaart 5 jaar na lancering draait, helemaal nu met NVMe-schijf erachter. En die had ik voor 320 euro in huis, toen.
RX 6900 XT;4;0.2992527186870575;Nu ik blijf toch bij de keuze voor een RX 6800 XT als vervanging voor mijn R9 290 (Zeker nu ik op 1440p zit.), als ze ooit beschikbaar komen en dan ook voor een bedrag dat lager ligt dan de advies prijs van deze RX 6900.
RX 6900 XT;4;0.36272791028022766;Pfoe, dan ben je inderdaad wel aan vernieuwing toe en het geeft tegelijkertijd aan dat die kaarten gewoon rock-solid zijn. Ik heb jaren gedraaid met 2x R9 290 in Crossfire, de gameprestaties waren goed in het handjevol games dat goed schaalde, maar wat ik me met name kan herinneren waren de enorme herrie en het feit dat we in de winter geen kachel nodig hadden op de werkkamer :-) Succes met de opvolger!
RX 6900 XT;3;0.49603620171546936;Ha ja herrie maakt ie zeker (pas sinds een jaar veel meer dan voorheen.) en verwarming ook zeker niet nodig, maar hij kwam nog prima mee op 1920x1200 high/ultra alleen sommige UT4 spellen gingen wat zwaarder. Nu op 1440 trekt een oud spel als WoW het niet eens meer fatsoenlijk met alle toeters en bellen aan. Black dessert daarintegen weer wel.
RX 6900 XT;3;0.4985242187976837;knappe prestatie maar ik heb liever een nvidia.
RX 6900 XT;3;0.30888262391090393;Is er een reden waarom er geen RTX3090 in de temps/geluidsdruk tabel staat?
RX 6900 XT;3;0.4740886390209198;Wij hebben een aftermarket 3090, geen Founders Edition, vandaar dat de vergelijking daar een beetje mank zou gaan.
RX 6900 XT;3;0.28977981209754944;Dat had misschien wel in de testverantwoording moeten staan. En als aanvulling daarop, de resultaten van de aftermarket 3090 toch ergens in het verhalende gedeelte noemen. Ik was bijna de vierde die een top-level reactie hierover zou plaatsen.
RX 6900 XT;4;0.4257468581199646;Uit dit artikel kan je wel concluderen dat je tegenwoordig prima betaalbaar op 1440p ultra 144Hz kan gamen. Eindelijk komen die monitoren met high refresh rate ook tot hun recht bij gamers die ook van mooie beelden houden
RX 6900 XT;5;0.6482827663421631;Wat een kaart de RX 6900XT! Als ik nu een nieuwe videokaart wilde kopen was dit mijn keuze de RX 6900XT geweest net zo snel als RTX 3090 voor rond de €1000. AMD is terug in de race en hun processor. De RX 6900XT zou een mooie combinatie zijn met mijn nieuwe Ryzen 7 5800X build
RX 6900 XT;1;0.5709814429283142;Dat zou ook mijn keuze zijn omwille van 2 dingen. Verbruikt 60 watt minder dan de Nvidia en 600€ verschil in prijs 600€ meer uitleggen voor 10 FPS is een smak geld. Laat de Nvidia fanboys nu maar beginnen huilen dat die AMD zuiniger is. Dit was meermaals het argument om aan te halen dat AMD je kamer verwarmde hehehe. Nu ik ben geen Nvidia of AMD fanboy ! Ik wil gewoon een goed product voor de juiste prijs En vind dat Nvidia toch wel de consument zijn geld de laatste jaren sterk misbruikt.
RX 6900 XT;3;0.35136333107948303;Als je gewoon een goed product wilt voor de juiste prijs, dan ben ik toch benieuwd waarom je zou kiezen voor een 6900xt of en 3090. Dan zou ik juist verwachten dat je kiest voor een 6800 (met of zonder xt) of de 3080.
RX 6900 XT;1;0.5596975088119507;Als ik moet kiezen tussen een 3090 voor minimum 1600€ Of de 6900XT voor 999€ , dan word het alvast de 6900XT. Reden hiervoor omdat die 600€ die Nvidia meer vraagt , die kleine performance niet rechtvaardigt. Zelf zou ik deze kaarten nooit kiezen omwille van de hoge prijzen. Koop me dan liever om de 2 jaar juist onder de topkaart Vraag me eigenlijk wel af wat zo een kaart werkelijk kost aan productieprijs ? Ik weet nog dat APPLE een Iphone maakte voor 90€ , en die verkocht dik over de 1000€.(= prijs x 11) Deze werd dan ook van een Amerikaans bedrijf , volledig in China gemaakt. Wat rechtvaardigt die prijzen is de vraag die bij me steeds terug komt. Alles word tegenwoordig gemaakt met machines en er komt nog weinig handarbeid aan te pas ! Het word langs om meer met minder mensen ,maar wel hogere prijzen! De lonen stijgen bijna niet ,en langs om meer minder mensen en veel minder service.
RX 6900 XT;5;0.25662195682525635;Check even de review van Jayztwocents
RX 6900 XT;3;0.3398359417915344;Ik ben wel benieuwd of de verdubbeltruc van Nvidia echt zin heeft, 5120 cores voor AMD en 10496 voor Nvidia. Al zou AMD hetzelfde doen zou AMD er winst mee hebben en een stuk sneller zijn, het lijkt me niet.
RX 6900 XT;5;0.7167198657989502;Heb de Gulden Middenweg keuze gemaakt .... RX 6800 (16GB) op 2 monitoren 3440x1440 (144hz) en 2560x1440 (165hz) Draait als een speer, zeer tevreden!
RX 6900 XT;5;0.23405340313911438;Welke monitoren als ik vragen mag?
RX 6900 XT;1;0.47630637884140015;Tuurlijk mag je dat vragen .... Het gaat om een Iiyama G-Master GB3461WQSU Red Eagle en een Dell S2721DGF.
RX 6900 XT;5;0.3155282735824585;Het was lastig om op de AMD site de bestelling af te ronden als je er al doorheen kwam maar.......... Na de ontvangst deze week van mijn AMD 5950X Processor (Megekko) ben ik waarschijnlijk een van de gelukkigen die op de site van AMD zelf een 6900 XT GPU heb kunnen bestellen. Kosten inclusief verscheping per FEDEX (vanuit Ierland met 2-3 weken levertijd) 1003,75 Euro. Mooie mogelijkheid om mijn AZZA Pyramid high end build af te ronden
RX 6900 XT;1;0.2662511169910431;Waarom geen Frostbite benchmark? (BF V, Star Wars: Squadrons) edit: ah! daarom
RX 6900 XT;3;0.4150206744670868;Battlefield V heeft eerder deel uitgemaakt van onze testsuite van games. Zoals ook op de pagina Testverantwoording staat: De engine is dus zeker een van de aspecten die meespeelt in de selectie van onze testsuite, en gezien de populariteit van Frostbite-gebaseerde games hadden we ook graag een dergelijk spel er in meegenomen. Het obstakel voor ons is dat deze games vanaf een bepaalde update niet meer consistent genoeg presteerden om betrouwbare benchmarks mee te draaien, de prestaties verschillen van run tot run. Onze testprocedures schaven we regelmatig bij, in de toekomst kan dit dus veranderen.
RX 6900 XT;3;0.3844413161277771;Sure, de prestatie per watt score gaat vooruit in deze generatie, maar als bezitter van een 1080 vraag ik mij toch af of de upgrade boeiend is qua stroomverbruik? Mijn monitor / tv combi ondersteunt 60hz. Stel dat ik een game cap op 60 fps (indien de game dat toelaat), zit ik dan niet alsnog op een veel hoger stroomverbruik met de nieuwe generatie van nvidia dan wel AMD dan nu op mijn 1080? Ik heb nu geen raytracing, maar ik zit in de meeste games in 4k al rond de 45-60 fps (met naar mijn smaak onzinnige features als depth of field en motion blur uit).
RX 6900 XT;1;0.6146251559257507;Dit is ook meer voor 120/144hz 4k TV's, uberhaupt geen zin om te kopen als je op 60hz zit.
RX 6900 XT;3;0.5567367672920227;Hartstikke leuk en aardig al die reviews overal maar waar kan ik in godsnaam zo'n ding kopen?
RX 6900 XT;2;0.5740097761154175;1000 euro is te veel voor deze kaart, voor dat geld wil je gewoon alle gave nieuwe effecten hebben... Raytracing performance valt tegen, iets wat nog enigszins acceptabel is bij een 6800 maar niet bij een 1000 euro kostende 6900 naar mijn mening.
RX 6900 XT;1;0.31732985377311707;Amd en drivers nee bedankt.
RX 6900 XT;2;0.538950502872467;AMD mist de finesse van Nvidia, zowel qua featureset als driver stabiliteit. Heb het aan de lijve ondervonden met de 5700 , en nu na anderhalf jaar hebben ze het nog steeds niet onder controle . (features die van driver-tot-driver omvallen of spontaan weer starten met functioneren) Ik kan er nu mee leven, maar er komt geen AMD videokaart meer in hier
RX 6900 XT;2;0.3406562805175781;Als je een nieuwe driver installed en die geeft je problemen , dan ga je toch gewoon terug naar de vorige versie die wel werkte. Voor die finesse van Nvidia betaal je wel een pak geld Ik zou eens de freesync van AMD willen aanhalen , tegen de forse meerprijs van Nvidia voor G-sync. Bij de aankoop van een monitor met G-sync moest je ook nog eens een meerprijs neertellen ! Zo hebben ze beiden hun voor en nadelen. Er zijn ook tal van 5700 kaarten die geen probleem hebben.
RX 6900 XT;2;0.37020906805992126;Drivers die als stable en WHQL gemarkeerd zijn, zouden gewoon moeten werken. In mijn geval ging het om VSR. Daar had iedereen last van. En je hebt weinig keus wel- of niet te upgraden als er los van de features ook major performance issues zijn, die wel opgelost zijn in die specifieke driver update Enerzijds werd het probleem niet erkend door de support desk (ik werd aan het lijntje gehouden), na de installatie van 5 drivers had ik de hoop opgegeven..... Met mij meerdere mensen. Totdat het probleem 'spontaan' opgelost was in een nieuwe driver. In de daarop volgende drivers werkte de functionaliteit af en aan. Tot op heden.... Ze hebben het gewoon niet op orde. Opzich is het g-sync argument natuurlijk valide. Ik snap tot op de dag van vandaag niet waarom je G-Sync (met aparte dure FPGA chip) nodig had als er al een standaard was. Realiseer je dat AMD nu hetzelfde truucje toepast met SAM ? Daar is al een standaard voor, BAR . Waarvan AMD nu claimt dat BAR implementeren veel werk kost, maar eerst zien dan geloven
RX 6900 XT;1;0.44146934151649475;Zo je betaald dus 600 meer puur voor 10fps meer? Naah keuze is dan snel gemaakt. En het wordt de 6800XT.
RX 6900 XT;3;0.3290652334690094;Ik zie vaak rare resultaten bij gpu tests van Tweakers. Eigenlijk altijd wel, nu bijvoorbeeld bij Far Cry New Dawn, 4k is de 6900xt iets sneller, normaal zou je zeggen, 1440p is hij trager dan de 6800xt? Hoe verklaren jullie dit soort verschillen?
RX 6900 XT;3;0.31459277868270874;Super kaart zo te zien maar niet iedereen heeft dat toch nodig zo'n hoge fps? Vanaf hoeveel FPS kan je zeggen dat je al een fijne game ervaring begint te ervaren? Is dit hetzelfde voor als je hoger gaat dan 1080p naar bijvoorbeeld 4K? Heb je op 4K meer FPS nodig om het allemaal net zo prettig er allemaal uit te laten zien als op 1080p?
RX 6900 XT;3;0.48428452014923096;Als gamer wil je liever niet lager gaan dan 60FPS. Alles erboven is natuurlijk mooi meegenomen Resolutie heeft er verder niet zoveel mee te maken, die twee staan los van elkaar. 60FPS op Full HD zal er net zo vloeiend uitzien als 60 FPS op 4K, het enige verschil is dat de details scherper zijn door de hogere resolutie.
RX 6900 XT;3;0.3288618326187134;Iemand die 1440p high settings benchmarks heeft gevonden? Zelf speel ik vaker op high setting (met evt ultra textures)
RX 6900 XT;1;0.2370290607213974;AMD rekt dus
RX 6900 XT;3;0.2698724865913391;Jij hebt zeker een andere review gelezen
RX 6900 XT;2;0.30853790044784546;Logisch dat Nvidia's kaarten RTX ondersteunen en die van AMD niet. Het is namelijk de term die Nvidia bedacht heeft voor hun implementatie voor real time raytracing. Het gaat om de ondersteuning van DXR, ondersteuning die AMD weldegelijk heeft. AMD geeft hier het naampje Ray Accelerator aan. Het verschil tussen de twee implementaties (RTX vs Ray Accelerator) is dat Nvidia specifieke cores heeft voor ray tracing, en AMD heeft in elke core een stukje power reserveert voor ray tracing (eenvoudig gezegd). M.a.w. een verschillende benadering met dezelfde uitkomst: ray tracing.
RX 6800 XT;1;0.8284943699836731;Proberen ze deze uberhaupt te verkopen? Nergens te koop.
RX 6800 XT;1;0.5578858852386475;Wellicht kan je met deze info wat meer? Staat per land gespecificeerd welke vendors het gaan verkopen.
RX 6800 XT;1;0.4164920151233673;Ben ik de enige die geen West-Europa ziet staan ?
RX 6800 XT;5;0.364202618598938;Bijzonder, deze stond er eerder wel gewoon Zie hier
RX 6800 XT;1;0.7548179626464844;Ik dacht even dat ik scheel was. Echt heel deze launch is te absurd voor woorden.
RX 6800 XT;1;0.2799822688102722;Had ik ook, dacht dat ik scheel was ofzo.. om gek van te worden
RX 6800 XT;3;0.4924485683441162;Ondertussen staan ze er wel weer tussen.
RX 6800 XT;3;0.28614237904548645;Voor de genen die niet geloven dat Megekko CDROMLand is, hier kun je het ook zien dat Megekko niet als amd partner staat.
RX 6800 XT;4;0.4399060010910034;Werkzaam bij Megekko. Magazijn is grotendeels Megekko, klein deel CD-ROM. Werknemers doen werkzaamheden voor beide zaken.
RX 6800 XT;5;0.3355713188648224;CD-ROM-LAND en Megekko opereren allebei onder de koepel van Shopping pour vous waarbij CD-ROM-LAND naast de webshop i.t.t. Megekko ook nog een fysieke winkel heeft:
RX 6800 XT;1;0.5244147181510925;Bedankt, wist geeneens dat CD-ROM-LAND en Megekko bij de zelfde horen, en is toch wat dat ze nergens al meer te koop zijn de AMD Radeon RX 6800 (XT), en als ze wel te koop zijn, ze ontzettend duur zijn. Erg mooi om te zien dat AMD met hun nieuwe Radeon RX 6800 XT, Nvidia's GeForce RTX 3080 bij kan houden, en dat ze bij meerdere spellen zelf iets of net zo snel zijn, en dat de Radeon RX 6800 sneller is gemiddeld dan de GeForce RTX 3070, het woord voor mij deze keer de Radeon RX 6800 XT, als ze weer normaal te koop zijn en de prijzen weer rond de €650,- zijn. Ben echt blij dat AMD weer is een erg snelle Grafische kaart uit brengt, zo dat er eindelijk weer concurrentie is, en er normaal voor gaat zorgen dat de prijzen omlaag gaan. Edit foutjes
RX 6800 XT;1;0.49876636266708374;Ja ik heb het zelf ook niet geweten dat CD-ROM-LAND en Megekko onder zelfde dak zaten. Ik kwam pas erachter nadat ik zeer slechte ervaring had met after-sale service bij Megekko en mijzelf beloofde nooit meer daar iets te kopen. Later iets gekocht bij CD-ROM-LAND. Heel toevallig ook daar ik te doen gehad met zeer slechte after-sale service die zeer veel leek op Megekko. Aan de telefoon vroeg ik rechtuit of ze familie waren van elkaar. Hoezo vroeg hij? Welnu, goedkope prijzen, makkelijk te kopen maar exact zelfde after-sale service praktijken. En toen was het stil aan de overkant.
RX 6800 XT;1;0.7777012586593628;Ja erg triest, en ik vraag me af hoe veel andere bedrijven bij elkaar (zelfde eigenaar) horen net als CD-ROM-LAND en Megekko, want dan betekent het dat we helemaal niet veel concurrentie hebben, op de tech gebied in Nederland als Tweakers ook al niet, Hardware.Info is laatst ook al over genomen door DPG Media, en is dus geen concurrent meer van Tweakers die ook bij DPG Media hoort. Ik vraag me echt af van alle winkels in Pricewatch echt nog van een eigenaar is en welke allemaal van de zelfde zijn maar onder een andere naam.
RX 6800 XT;5;0.24219846725463867;Je kan via de KVK achterhalen welke bedrijven allemaal bij elkaar horen. Hoeft niet eens iets te kosten.
RX 6800 XT;2;0.32529211044311523;Dat bedrijven dezelfde eigenaar hebben hoeft niet te betekenen dat ze hetzelfde zijn of dat het nadelig voor de klant is. Zo kunnen ze zich bijvoorbeeld op andere delen van de markt richten of onderscheiden met extra diensten die je bij het zusterbedrijf niet krijgt.
RX 6800 XT;2;0.3522353768348694;Dat zal, maar het is dan nog steeds geen concurrent, dus het aantal concurrerende bedrijven is 1 minder dan als ze een verschillende eigenaar hadden.
RX 6800 XT;1;0.7292623519897461;Het is 'gewoon' het huidige marktmodel van machtsbeluste grijpgrage investering maatschappijen, concerns en andere spelers die macht van en op de markt willen. We gaan helaas naar de tijd dat de prijzen door slechts nog een paar grote bedrijven worden bepaald. Zowel de inkoop als verkoopprijzen. Neem de supermarkten, internet, mobiele providers.. allemaal zijn deze sectoren in handen van hooguit 3 grote spelers, met daaronder wellicht wat sub merken/bedrijven. En de overheid doet daar niets tegen... en staat dit soort, ik noem het maar maffia en kartel praktijken gewoon toe.
RX 6800 XT;2;0.43749529123306274;Ja precies, het is heel triest, en je doet er jammer genoeg niks tegen, ja wij met zijn alle maar dat gebeurd jammer genoeg niet.
RX 6800 XT;3;0.3001909852027893;Dit staat ook op hun website.... ze zijn zo goedkoop omdat de service minimaal is... dat is dus het risico als je daar koopt. We doen niet alles. Maar wat we doen, doen we graag en goed. We kaderen graag af wat we doen. Zo kun je bij ons niet terecht voor advies of telefonische ondersteuning. Dat wil niet zeggen dat je bij ons geen service krijgt.
RX 6800 XT;4;0.3314206898212433;Idd erg mooi als je denkt dat hij bijna het rtx geweld kan temmen. Het zijn maar minieme vergelijkingen dat de nvidia sneller is, met sommige meetingen is nvidia zelfs langzamer.Amd heeft het goed voor elkaar en is nu eindelijk ook in het hogere top segment te vinden. Ik kan alleen maar zeggen tegen amd ga zo door, gooi er nog een paar duizend cores ertegen aan en dan zijn ze misschien even snel als nvidia.
RX 6800 XT;3;0.43618491291999817;Denk met een paar driver optimalisaties dat ze de performance wel een procent of 5 hoger krijgen of misschien wel meer. AMD heeft lange tijd gewoon de betere architectuur gehad, alleen was het niet of haast niet ondersteund worden in windows e.d. het probleem. De hardware was vaak de tijd vooruit. GCN was indien goed geïmplementeerd veel beter dan nvidia. Denk dat als het zoals in het verleden het is gegaan, AMD op den duur met deze serie na wat driver updates net wat sneller zal zijn. Ik zie dat mijn RX580 en 5700XT sinds release echt beter zijn gaan draaien, en de 2070 kaarten en 1060 kaarten juist minder of niets zijn verbeterd qua performance.
RX 6800 XT;3;0.4592926800251007;Het is onderdeel van CDROMLand en gebruiken voor zover ik weet het zelfde magazijn, maar ze hebben niet exact het zelfde asortiment.
RX 6800 XT;3;0.3349469304084778;Allebei onderdeel van Shopping pour vous BV, zie: voor meer info over de merken die zij onder zich hebben. Enkele merken van Shopping pour vous BV:CD-ROM-LAND - - -
RX 6800 XT;3;0.3391985595226288;Opzich niets mis mee zolang ze deze bundeling maar gebruiken om de prijzen laag te houden
RX 6800 XT;3;0.36862894892692566;Uiteraard. Had dit alleen voor de informatie gepost.
RX 6800 XT;3;0.42664241790771484;Ik snapte em Overigens heb ik zelf nooit geen issues gehad met Megekko.. Aardig wat besteld en altijd goede service gehad wanneer nodig.
RX 6800 XT;5;0.3962583541870117;Ja, ik ook. Ze zijn top. Daarom is het ook positieve reclame. Het bedrijf daarachter doet toch iets goed
RX 6800 XT;2;0.32446831464767456;Megekko staat er toch gewoon tussen in het rijtje? Onderin?
RX 6800 XT;1;0.3958825469017029;"Ik zie bij Alternate 2 6800XT's staan.. vanaf 999,--.. En de ""gewone"" 6800 vanaf 899,--.. Hahaha.. Ik zou nog ff wachten.."
RX 6800 XT;1;0.522767186164856;Nee hoor, gewoon meteen bij Coolblue reserveren, voor... €9.999 voor de XT, of dezelfde prijs voor de gewone Coolblue weet heel goed hoe je scalpers tegen moet gaan, gewoon veel hogere prijzen vragen dan welke scalper dan ook zou doen.
RX 6800 XT;1;0.5649676322937012;Reserveren is niet mogelijk bij coolblue, ook niet voor 9999. Je kunt alleen een melding krijgen als het product op voorraad is.
RX 6800 XT;1;0.4249734580516815;Ja beetje rare invoer van prijs. Houd dan msrp aan + 50 euro ofzo. Dit schrikt alleen maar mensen af.
RX 6800 XT;1;0.6685559153556824;"Ik denk dat ze daarmee bedoelen ""reserveer voor dat geld en krijg daaena het verscil terug."
RX 6800 XT;1;0.7471894025802612;Shame on you Alternate!!! Adviesprijs van 669 usd, aan de huidige koers is dat 570 euro. Als ze dan nog 1 op 1, 669 euro zouden vragen dan is die 999 euro voor een 6800XT nog steeds volledig onaanvaardbaar. AMD komt met goede producten aan een goede prijs om de markt naar een hoger niveau te brengen voor iedereen en dan heb je van die winstjagers als alternate die de winst dan simpelweg bij hun verhogen zodat de consument weer de dupe is...
RX 6800 XT;1;0.4774305820465088;USD met NL Euro prijzen 1 op 1 vergelijken gaat helemaal niet op want de USD prijzen zijn altijd ex-VAT belasting waar de NL prijzen altijd incl. 21% BTW zijn.. 669 USD = 570 euro + 21% is 689 euro.. Dat is de 1 op 1 prijs, alleen kan daar in de VS dus nog tot zo'n 8% VAT bijkomen.. MSRP (adviesprijs) in USD is altijd ex belasting omdat dat per staat en soms zelfs per stad verschilt daar.. De VAT loopt daar van 0% tot +/- 8%.. Sowieso veel minder dan de standaard 21% hier..
RX 6800 XT;1;0.45206746459007263;Dat was ik helemaal vergeten, maar dat klopt als een bus. Zelfs als je daar in de winkel loopt (VS) dan zie je vaak prijzen zonder belasting. Daar had ik me goed in vergist, daar stond ik bij de kassa met exact het bedrag op het product. Mocht ik ineens meer betalen. Na uitleg van de medewerker werd het me duidelijk.
RX 6800 XT;1;0.26565229892730713;Had ik idd de eerste keer in Panama ook, behoorlijk lopen bitchen dat ze gewoon de juiste prijs moeten vermelden, kreeg het mee met de belasting als korting 😂
RX 6800 XT;1;0.38009530305862427;Kun je in zo'n geval de BTW niet gewoon terugkrijgen op het vliegveld?
RX 6800 XT;1;0.6083464026451111;Geen idee, niet geprobeerd
RX 6800 XT;3;0.5016815662384033;Vorig jaar in Singapore wat gekocht, en daar kregen we een speciale bon mee om belasting terug te vorderen op het vliegveld bij vertrek. EU en USA hanteren volgens mij ook dergelijke regelingen, maar je moet er wel ff wat extra voor doen. Meestal is een bon waaruit blijkt wat je aan VAT/BTW betaald hebt voldoende, maar je moet altijd ff uitzoeken hoe het in zijn werk gaat, want dat kan per land nogal verschillen.
RX 6800 XT;1;0.25607314705848694;@trapper Nog even wat oude artikelen aan het doorspitten? Maar inderdaad, zelfde ervaring bij mij met spullen uit Japan.
RX 6800 XT;1;0.4451516568660736;Ja, ik had van de 6900 aankondiging doorgeklikt naar dit artikel over de 6800. Even niet meer bij stil gestaan dat dit al een oud artikel was...
RX 6800 XT;1;0.9028043746948242;Haha gekkenhuis. Is voor niets.. Doe er maar 2.. Gewoon heerlijk verschrikkelijk slecht dat ze de prijzen dan zo omhoog gooien. Ik vind het gewoon crimineel
RX 6800 XT;3;0.34490934014320374;Zo, je kan wel zien dat Duitsland een belangrijke markt is.
RX 6800 XT;2;0.3580911159515381;West europa staat er al niet eens meer tussen.
RX 6800 XT;1;0.43719807267189026;"Uit het artikel; Nergens voorraad. Ik had verwacht dat AMD niet beter verkrijgbaar zou zijn dan Nvidia, maar dit is wel heel extreem. Dit is een échte paper launch. Overigens geeft Alternate aan nog in 2020 te kunnen leveren (wel voor 899). Wie persé een kaart wil kan die deal maar beter accepteren. Zo lang Nvidia leverproblemen heeft zal er uberhaupt extra vraag ontstaan. Positief punt uit de TPU review: Eindelijk de multi monitor bug opgelost ."
RX 6800 XT;1;0.37229081988334656;De multimonitor bug was al gefixt in de AMD vega familie. Dankzij HBM2.
RX 6800 XT;2;0.324444442987442;Navi heeft er last van, zo gaat een 5700XT idle van 8W naar 36W wanneer je er meer dan één scherm op aansluit. Zie
RX 6800 XT;2;0.33201393485069275;"Klopt, maar Vega 64 had geen last van dit probleem: Multi-monitor power consumption seems solved, finally. With 17 W, the power increase over single-monitor idle is just 3 W, which is a huge improvement over the 15 W or so that were previously added in that state (on Polaris); the same goes for media playback. Dus onderstaande geldt niet: I didn't think I would see the day where AMD fixes its multi-monitor power consumption. Finally, the memory no longer runs at full speed in that scenario, which greatly reduces power, down to 7 W, which is now much better than NVIDIA."
RX 6800 XT;1;0.261769562959671;Alsof iemand een Vega 64 heeft of gaat kopen. 5700XT is een populaire kaart en dus de referentie.
RX 6800 XT;1;0.7493893504142761;Op diverse sites waren mensen er van overtuigd dat amd de launch beter voor elkaar zou hebben etc. Maar bij die mensen was het sowieso allemaal slecht wat Nvidia heeft gedaan. Had zelf nooit het idee dat deze launch beter zou zijn dan die van Nvidia. Zo is het nu eenmaal bij zo ongeveer elke release van high end zooi. Heb 1* meegemaakt dat er bij een high-end launch veel beschikbaar was. Jaren geleden bij een nvidia release, er waren geen leaks etc, opeens de aankondiging en dag erna hadden alle shops kaarten.
RX 6800 XT;1;0.6532946228981018;Dit komt ook door de slechte communicatie vanuit AMD + het feit dat AMD later lanceerde. Tweets zoals deze werken in ieder geval niet mee.
RX 6800 XT;2;0.4171851575374603;Klopt door die tweet dacht ik ook dat er voldoende voorraad was - nu ook wel teleurgesteld.
RX 6800 XT;1;0.6713902354240417;En dan gaat ie gewoon nog even door met een tweet waarin hij pronkt met een screenshot waarop zijn eigen order van een Radeon kaart op de site van AMD is geslaagd. Was waarschijnlijk de enige kaart die ze beschikbaar hadden Die $10 dollar is ie wel kwijt. Dit is nog een slechtere launch dan die van Nvidia, waar ze bij AMD zo'n grapjes over aan het maken waren. En niet alleen de GFX kaarten zijn paper launches, maar ook die van de 5950x, die in geen velden of wegen te bekennen is en die ik natuurlijk weer net wil hebben
RX 6800 XT;1;0.47344595193862915;De communicatie geeft het idee dat de beschikbaarheid goed zou zijn, maar dat is niet perse wat hij zegt. AMD verscheept op het moment een krankzinnige hoeveelheid chips, de vraag is daarentegen ook enorm. Vergeet niet dat AMD ook de hardware van de PS5 en Xbox levert. De consoles gebruiken ook RDNA2 en Zen3. Daar is gewoon niet tegenop te produceren. Is het een paper launch? Nee zeker niet. Zijn er enorme tekorten door enorme vraag? Ja. Niet zo gek, met de volledige consolemarkt naast een goed deel van de desktopmarkt.
RX 6800 XT;1;0.4243924915790558;Bij die tweet dacht ik al wat men dan verstaat onder een paper launch. Aangezien er wel kaarten te koop waren. Maar het waren er zo weinig dat het dan toch weer dicht bij een paper launch komt.
RX 6800 XT;5;0.3892468810081482;"Ach, op deze manier houden we allemaal een heleboel geld over aan het einde van het jaar! ;-)"
RX 6800 XT;4;0.3964531123638153;Goed advies om een deal te adviseren die 250 euro duurder is Dan de normale prijs..
RX 6800 XT;1;0.6455442905426025;Let even op het woordje persé . Het zit er dik in dat de situatie de komende maanden niet verbeterd. Er is nu nog geen zicht op verbetering puur omdat we niks horen. Als je bereid bent maanden te wachten dan wel een alternatief van Nvidia te overwegen (niet dat die nu leverbaar zijn, maar wellicht februari/maart) dan zou ik zeker die pre order niet plaatsen. Maar dan wil je hem dus ook niet persé hebben.
RX 6800 XT;1;0.39951664209365845;Heb m Al binnen voor de normale prijs van 719,- als je hem persé wou hebben Dan je gepre-ordered toen de prijzen nog niet inflated waren geraakt.
RX 6800 XT;1;0.38067105412483215;"Als we dan toch op onze woordjes letten, het is ""per se"", met spatie en zonder accent aigu."
RX 6800 XT;1;0.47616586089134216;"Op de site van Megekko: ""AMD Radeon 6000 videokaarten: Wat is geleverd en wat wordt verwacht Officieel is vandaag, 18-11-2020 de launch-date van de nieuwe AMD Radeon 6000 videokaarten. Echter, het betreft alleen de reference designs van AMD. Deze kaarten zijn helaas nog nergens verkrijgbaar en het is ook niet aannemelijk dat deze op korte termijn beschikbaar komen. Zodra wij meer informatie hebben, updaten we jullie via deze pagina."""
RX 6800 XT;3;0.3517521917819977;AIB's mochten kiezen om geleverde chips in de reference design te zetten of te gebruiken voor hun eigen kaarten. Het overgrote deel van de chips is natuurlijk in AIB kaarten gezet. Die zijn er vanaf volgende week, dus dat is wanneer we mogelijk wat in winkels zien (in Europa iig)
RX 6800 XT;1;0.45370009541511536;Dit lijkt meer op een nieuw distributiemodel. De fabrikant (AMD/nVidia) verkopen de eerste batch direct aan consumenten en doordat ze een groot deel van de oude keten overslaan, levert dat veel op. Dit is een geleidelijke stap om de keten te verkorten.
RX 6800 XT;2;0.4467008709907532;Klopt, maar er spelen een aantal zaken. Nog niet zo heel lang geleden kwam het geregeld voor dat AIB's met kaarten kwamen die niet voldeden aan de specs, te laat met nieuw kaarten kwamen of samenstellingen maakten die de bestaande line-up en marketing van de fabrikant beschadigden. Het reference model is precies dat. Een baseline voor testers, die voor iedereen hetzelfde is. AIB kaarten die slechter presteren dan de baseline worden genadeloos door de pers afgebrand. Betere prestaties kunnen worden afgezet tegenover de baseline etc. AMD heeft hier een moeilijke strategische keus gemaakt. Door AIB's meteen hun eigen kaarten te laten maken lijkt de initiële voorraad erg slecht. Aan de andere kant heeft AMD die AIB's extreem hard nodig. Anders dan NVIDIA kan AMD de AIB's niet een middelvinger opsteken en controleren. De klanten van AMD zijn nu eenmaal de AIB's, niet consumenten. Hoewel, zoals je aangeeft, ze dat misschien anders zouden willen zien.
RX 6800 XT;3;0.254252552986145;Pre order op Alternate voor €999
RX 6800 XT;2;0.4775889217853546;Zwaar teleurstellend dit, beschikbaarheid is een ding kun je niet veel aan doen, maar eventjes snel een extra 40% winst maken is echt diep triest.
RX 6800 XT;3;0.3370038866996765;Ik snap je reactie, maar dit is marktwerking. Vraag en aanbod bepalen de prijs. Als verkoper kan je er maar een paar verkopen en door het gebrek aan aanbod geraken die dingen toch verkocht. Kan je als verkoper lekker cashen. Adam Smith - The invisible hand
RX 6800 XT;1;0.5475336909294128;Daar ben ik het niet met eens. Je kan eenmalig incasseren. Maar wat betekent die 300 euro nu voor hun? Zelfs als hebben ze 10 kaarten. 3000 euro klinkt fijn voor jou en mij, maar als filiaal en al helemaal als keten is dat echt gewoon een druppel. Ik heb als student in het weekend gewerkt voor office center in België. Het devies daar was. Een klant krijgen is niet moeilijk, ons doel is om een terugkerende klant te hebben. Een winkel als alternate, die naar mijn weten nooit vecht op prijs (en als ze het doen zijn ze er belabberd in want ze zijn meestal duurder), vecht dus op basis van service en advies (die naar mijn ervaring beide wel goed zitten). Als daar je kracht zit voor terugkerende klanten en loyaliteit, is zoiets flikken niet iets dat het sentiment van je klanten positief bevordert. Nou ben ik geen adviseur voor een winkel zijn marketingstrategie en ieder kiest wat zij willen. Maar gewoon zeggen dat vraag & aanbod dit reguleren is niet van toepassing hier. Je kan genoeg artikels vinden die zeggen dat vraag & aanbod stuff stuurt, maar dit kan je niet op alles zomaar toepassen. Edit: gezien de reacties die er al komen op die prijs is dit inderdaad iets dat eerder negatief dan positief werkt. Je wilt geen toekomstige klanten een wrange smaak geven over je keten.
RX 6800 XT;1;0.31020545959472656;Helemaal waar. Gewoon een onwetende opmerking van sommigen. Ik zou liever tientallen kaarten verkopen met wat minder winst dan een paar met veel winst. Bovendien, mensen kopen waarschijnlijk er ook nog wat extra's bij, dus meer klanten is beter dan enkelen.
RX 6800 XT;2;0.41012895107269287;Alleen gaat je logica niet op op het moment dat het aanbod beperkt is en je te weinig product geleverd krijgt. Het is dan kiezen tussen een paar klanten iets verkopen voor een lage marge en een paar klanten iets verkopen met een hoge marge. Leuk voor de consument? Nee.
RX 6800 XT;3;0.6964059472084045;Heb je ook gelijk in, maar de stelling dat het lekker cashen is zoals sommigen beweren vind ik wat kortzichtig.
RX 6800 XT;3;0.3717458248138428;Nog steeds eigenlijk hoor. Je gaat misschien die paar kaarten dat je had verkocht hebben met een grotere marge. Maar waarom? Zoals ik aangaf zijn die bedragen niets voor retailers. Maar je hebt wel genoeg klanten die je prijs zien en denken wat een klootzakken. Dan kan je beter de adviesprijs aanhouden en er iets leutig rond doen naar mijn mening.
RX 6800 XT;1;0.3493594229221344;Hoe verklaar je dan die hoge prijzen als je het niet met mij eens bent?
RX 6800 XT;2;0.4209035336971283;Daarvoor zou ik informatie moeten hebben van de winkels zelf. Waarom sommige beslissen om ze wel duurder te verkopen weet ik niet. Het kan goed zijn dat inkoop voor hun gewoon duurder is. Dat heeft Coolblue een tijd geleden aan het begin van de pandemie gedeeld. Als ze hun marge gelijk houden gaan ze dan ook mee omhoog. Hetgeen ik het niet volledig mee eens was, is dat de retailers zelf dit doen om meer marge te pakken.
RX 6800 XT;2;0.3529716730117798;"Om een economische stelling van meer dan 200 jaar oud te ontkrachten heb je blijkbaar genoeg aan een paar voorbeelden. Om de prijsstijging te verklaren heb je ""extra informatie"" nodig. Het is natuurlijk niet zwart-wit. Ik ben akkoord dat er winkels zijn die dit niet doen wegens reputatieschade. Maar dat de economische wetten niet gelden voor de verkoop van videokaarten lijkt me sterk."
RX 6800 XT;3;0.36553457379341125;Dan zijn we het grotendeels eens uiteindelijk zeker :-). Uw wet houdt stand, maar niet alle winkels doen het vanwege mogelijke reputatieschade die dit gedrag met zich meebrengt.
RX 6800 XT;1;0.616051435470581;Winkels moeten naar een totaalplaatje kijken, een paar euro's verdienen aan een paar goed verkochte graphics cards weegt wellicht niet op tegen klanten die weglopen en anders wellicht een laptop / desktop / andere onderdelen mee zouden bestellen. Ik ben zelf geen gamer, en deze kaart zou ik niet op dit moment al kopen, maar ik koop wel regelmatig andere computer onderdelen. Als ik echter veel negatief sentiment zie over een webwinkel die het onderste uit de kan probeert te halen op deze manier, ga ik wellicht wel met mijn klandizie naar een andere winkel. In die zin kan dat dus averechts werken voor een webshop.
RX 6800 XT;1;0.43345245718955994;Ook daarboven op dat het niet is alsof ze een groot aantal hebben ingekocht en ze niet verkopen en dus de prijs omhoog moet om het verschil op te maken (of flink omlaag om verkoop te bervorderen) ze hebben een klein aantal ingekocht/binnen gekregen en als ze die voor dezelfde prijs als anders verkopen blijft de winst in verhouding hetzelfde
RX 6800 XT;1;0.39630362391471863;Alsof mensen dat niet snappen. Ik snap het en vind het nog steeds een dick move omdat ik geen gevoelloze econoom ben maar een nerd ben die een videokaart voor een normale prijs wil.
RX 6800 XT;2;0.3906172513961792;Dan moet je even wachten, of een oudere kaart kopen. De marktprijs is (helaas) de normale prijs.
RX 6800 XT;1;0.7902848124504089;Ook net gezien. Echt schandalig in mijn ogen. En dat voor een adviesprijs van pakweg 700 euro.
RX 6800 XT;2;0.3474767208099365;Ik vind 786 euro toch wel substantieel hoger als 700 euro. Vergeet niet dat de adviesprijzen van AMD in USD zijn ZONDER BTW. Dat is dan voor hun eigen kaart. Tel daar nog 100-150 euro winstmarge bij voor een aftermarketkaart (msi, gigabyte,...) en dit zijn de prijzen...
RX 6800 XT;1;0.5134483575820923;Staat in het artikel, geen enkele Nederlandse retailer heeft iets ontvangen.. Dit is dus met recht een lancering op papier..
RX 6800 XT;1;0.6129636168479919;Ze werden alleen via de Shop op AMD.com verkocht De 6800XT was, zoals gezegd, binnen 2 minuten weg. De 6800 geen idee, want het checkout systeem ligt nu plat edit: nu de hele shop lijkt het
RX 6800 XT;1;0.4076458215713501;Je kan er wel allerlei landen eraan plakken Sold Out.
RX 6800 XT;1;0.41663646697998047;Had het artikel niet kunnen lezen, vanwege het proberen te kopen, ga dat nu pas doen.
RX 6800 XT;1;0.5804703235626221;Op de Site van AMD. De 6800XT was binnen 2 minuten uitverkocht (en staat er nu daarom ook niet meer bij)
RX 6800 XT;1;0.7266731858253479;Ja hett slaat inderdaad helemaal nergens op al die paper releases!
RX 6800 XT;1;0.5745462775230408;Die foto toont het enige beschikbare exemplaar?
RX 6800 XT;5;0.26298704743385315;"Er is een oud spreekwoord. '"" Veel beloven weinig geven doet de gek in vreugde leven""' (=veel mensen zijn al blij met een belofte en geloven alles) leven de Influencers"
RX 6800 XT;3;0.5978636741638184;Mooi overzicht, maar jammer dat er bij het testen zo weinig aandacht is voor productiviteitsbenchmarks. Hoe presteren de kaarten voor Photoshop, Premiere Pro, DaVinci Resolve, Blender, .... om maar enkele belangrijke te noemen? De klemtoon ligt vaak te eenzijdig op gaming. Leuk en interessant, maar sommigen gebruiken hun pc ook voor andere zaken dan gaming. Hopelijk wordt er bij de reviews van partner-kaarten wat meer aandacht gegeven aan niet-gaming benchmarks.
RX 6800 XT;5;0.6831507682800293;Helemaal mee eens. Ik ben ook erg benieuwd naar performance in met name videobewerking. Zeker aangezien nVidia in het verleden altijd een streepje voor heeft in productiviteit dankzij hun CUDA cores.
RX 6800 XT;3;0.2805962562561035;Linus heeft hierop getest. Zijn oordeel was dat als je videobewerking doet je veel meer krijgt bij nvidia. Reden waarom ik ook na deze test toch nog een 3070 heb gekocht. Encoding, Decoding en 3D renderen is vooralsnog bij Nvidia beter.
RX 6800 XT;3;0.38718289136886597;Inderdaad, dit wou ik op tweakers ook bevestigd zien. Volgens Linus heeft Nvidia voordeel in grafisch werk (Blender, Maya, Solidworks,...) , maar heeft AMD voorsprong in medische tests, dankzij infinity cache. Daarbij mist AMD ook nog enkele features (DLSS onder andere).
RX 6800 XT;3;0.3636438250541687;"Voor de penguins onder ons: Zou toch cool zijn als Tweakers ook kan focussen op andere platformen in de toekomst. Edit. Even een TL;DR voor de niet-Linux gebruikers. De resultaten matchen ongeveer de Windows benchmarks. Upsampling en Raytracing zijn niet AMD's sterke kant. Daartegenin zie je dat bij performance-per-dollar dat AMD wint in combinatie met de open-source drivers. Dit laatste is natuurlijk een beetje vreemd om te horen als Windows gebruiker, maar AMD heeft op Linux twee drivers. AMDGPU, en Mesa. AMDGPU is van henzelf, en die wordt officieel aanbevolen. Mesa is een samenwerking van AMD, Valve en vele anderen, en deze is standaard voor alle Linux gebruikers. Met enkele uitzonderingen daar gelaten, wint de Mesa driver het van de 'officiële' driver dus dat scheelt Linux gebruikers weer wat extra software om te installeren. Tot slot heeft AMD volledige Wayland ondersteuning. Dit is een vervanging voor het ouderwetse X11, en NVidia blokkeert opzettelijk ondersteuning daarvoor. Vandaar ook dat er een bescheiden anti-NVidia sentiment is onder Linux desktop gebruikers. Voor Linux gebruikers is dit dus goed nieuws. Dat de kaarten niet daadwerkelijk verkrijgbaar zijn... dat probleem delen we"
RX 6800 XT;4;0.3163672089576721;Wat ik vooral geweldig vind is dat linux gamers (dual boot of niet) enifelijk een krachtige kaart heeft die ook wayland support heeft
RX 6800 XT;3;0.3132380545139313;Wayland inderdaad maar belangrijker is toch altijd ondersteuning bij nieuwe kernels en er zijn enkele voorbeelden waarbij nvidia bij nieuwe games niet goed presteerde en je volledig bent overgeleverd aan een driver update van nvidia. Doom Eternal bijvoorbeeld. Schijnt dat de RX 5000 serie wel weer wat issues had tot enkele kernels terug. Panics en problemen na slaapstand. Heb dat niet uit eerste hand vernomen, mijn RX 480 werkt nagenoeg perfect vanaf de launch en de situatie voor de RX 6000 schijnt beter te zijn. Goede ondersteuning vanuit Valve wat betreft mesa is ook een pré.
RX 6800 XT;3;0.2950790524482727;De mesa drivers vegen de vloer aan met de AMD drivers. Kan me goed voorstellen dat AMD die drivers ook steeds minder aandacht gaat geven aangezien het gros van de Linux gebruikers nu standaard de mesa drivers gebruikt.
RX 6800 XT;4;0.36842605471611023;Het is niet altijd mesa die wint. Wat sowieso erg fijn is is dat je, in ieder geval bij de vulkan, kunt kiezen per game welke driver je gebruikt. Zo heb ik voor vulkan zowel de drivers mesa radv, de amd opensource amdvlk als de amd proprietaire amdgpu-pro-vulkan geïnstalleerd en kan er gewisseld worden met een simpele environment variable in steam. Gamers kunnen hun hartje ophalen en zoeken naar dat laatste extra stukje fps, zonder dat dit andere games in de weg hoeft te zitten. Of bij een launch om driverbugs heen werken zoals bij Doom Eternal.
RX 6800 XT;5;0.3244014084339142;zoals de meme gaat: --my-next-gpu-wont-be-nvidia
RX 6800 XT;3;0.2712405025959015;Launch option van Sway, een i3 gebasseerde tiling window manager dat draait op Wayland. Nvidia's comptabiliteit met Wayland is uiterst matig, vandaar die launch option. Vrije software heeft humor.
RX 6800 XT;1;0.8642997145652771;Humor? Dit is dodelijk serieus! Denk aan al die kinderen die zichzelf niet warm kunnen houden in de sneeuw omdat hun NVidia Graphics Card alleen maar op base-clock draait. Deze kinderen moeten nu kiezen tussen de proprietary Nvidia drivers of de straffe winter-wind! Niets grappigs aan.
RX 6800 XT;3;0.3071717619895935;Met AMD ben ik heel blij dat ze goede concurrentie leveren, maar als je gebruik maakt van streaming/encoding dan kan je nog steeds beter bij Nvidia zijn. Als voor mij streaming (tussen eigen apparaten, dus hoge bandbreedte) geen probleem was dan zou ik voor mijn volgende upgrade zeker AMD meenemen in de overweging, en waarschijnlijk verkiezen boven Nvidia.
RX 6800 XT;3;0.3241477310657501;Bedoel je hiermee ook via je pc naar een tv streamen, en daar dan een game op spelen, leek mij wel leuk als ik op groot scherm rdr2 bv kan spelen.
RX 6800 XT;5;0.41298606991767883;Naar TV's, Android TV apparaten (Shield TV), notebooks, smartphones, etc. Nvidia kaarten hebben met NVENC minder moeite (lagere vertraging) en hogere bitrates dan AMD kaarten.
RX 6800 XT;2;0.41812264919281006;Is er een specifieke reden waarom Nvidia zoveel beter lijkt te presteren onder Linux ten opzichte van Windows? Waar bijv. de RX6800 op Windows gemiddeld genomen rondjes rent om de 3070 (die +- net zo goed presteert als een 2080Ti) is op linux ineens in vrijwel alle games de 2080ti de bovenliggende kaart van deze 2. Net als dat op Windows de 6800XT en de RTX3080 stuivertje wisselen, zie je ineens dat in de Phoronix gaming benchmarks de RTX3080 gemiddeld genomen ruim beter presteert AMD lijkt relatief gezien op Linux flink slechter te presteren in gaming dan in Windows het geval is. Verder weet ik niet of we naar dezelfde data gekeken hebben, maar Nvidia wint 5 van 9 Perf per $ graphs, waar AMD Mesa er 4 wint, al met al lijkt de perf per $ gemiddeld genomen over alle tests vrijwel gelijk te zijn tussen de RX6000 kaarten en de Nvidia RTX3080. Jammer trouwens dat Phoronix niet minimaal ook de RTX3070 meeneemt (deze presteert ongeveer gelijk als de RTX2080ti, maar met een $499 prijspunt kan deze wel een stuk interessanter zijn bij de prijs per $ metingen. Het zou mij niet verbazen als van de RX6800, RX6800XT, de RTX3070 en RTX3080 de 3070 misschien wel de beste prijs per $ kan hebben in de hier geteste games onder Linux.
RX 6800 XT;3;0.2796073853969574;Windows vs Linux met NVidia... Daar is veel over te speculeren. Misschien is het omdat NVidia meer tijd steekt in de Linux drivers. Alle serverparken die nu uit de grond worden gestampt met Tensorcores of CUDA kaarten, draaien Linux. De perf-per-$ laat inderdaad zien dat NVidia het goed doet. Ik was vooral blij om te zien dat de mesa drivers het beter doen dan de AMD drivers, want dat is voor mij belangrijker.
RX 6800 XT;2;0.5080851912498474;"Misschien dat ik het verkeerd verwoorde, maar ik bedoelde niet dat Nvidia in Linux beter is dan in Windows, maar dat in Windows AMD en Nvidia relatief gezien stuivertje wisselen en dat in Linux de AMD performance een flinke klap krijgt terwijl de Nvidia op niveau lijkt te blijken ten opzichte van de performance in Windows. AMD lijkt het dus in Linux relatief gezien een stuk slechter te doen op Linux dan op Windows, zeker met de eigen driver, maar ook met de opensource driver lijken ze achter te blijven. Moet wel zeggen dat ik niet direct die ""anti Nvidia"" houding begrijp, het komt op mij eigenlijk wat kinderachtig over als ik die memes zie. Ik beheer redelijk wat Linux machines (voornamelijk servers inderdaad) en er is eigenlijk nooit 'gedoe' met Nvidia hardware. Wanneer dat gebruikt wordt in een klantproject dan werkt het 'gewoon'. Ik ben wat Linux betreft wel een gebruiker / beheerder en geen evangelist, misschien dat daar het verschil zit."
RX 6800 XT;2;0.37579894065856934;Denk dat het een kwestie van budget is. Leuk dat AMD en Valve samenwerken aan een Linux driver, maar dat kan natuurlijk niet op tegen het budget van NVidia. Dat sentiment kan ik heel goed begrijpen als ontwikkelaar. Linux heeft een heel goed driver systeem waaraan veel fabrikanten mee doen. Als je Linux installeert dan zitten alle SATA, Wireless en IO drivers ingebouwd. Dit is efficiënt en zorgt voor veel standaardisatie en comptabiliteit. Naast AMD zorgt ook Intel dat al zijn drivers meteen in de Kernel zitten. Ideaal. De enige die hier niet aan mee wilt doen is NVidia. Vanuit NVidia's oogpunt logisch: Zij hebben drivers die niet alles toestaan, en ze kunnen enterprises en hosters meer geld vragen voor hardware welke wel de juiste encryptie-sleutels heeft. Qauadro kaarten zijn NVidias goudmijn, maar dit is in strijd met de Linux licentievoorwaarden. NVidia wilt dus graag Linux ondersteunen voor servers en dergelijke, maar ze willen niet samenwerken en op veel punten geeft dit dus gezeik. Zoals ik hierboven al aan haalde, mobiele oplossingen zijn kut (Fuck you, NVidia! names Linus Torvolds) en ook ondersteuning voor Wayland en dergelijke ontbreekt. Lang verhaal kort. Als je Linux alleen op servers gebruikt, en je kunt de Quadro meerprijs door berekenen, dan is NVidia prima. Anders, zijn ze een doorn in het oog.
RX 6800 XT;5;0.4457750916481018;Precies!
RX 6800 XT;3;0.2835671007633209;Bekijk ook zeker de video van Wendell (L1Techs/L1Linux). Ik denk niet dat ik hem ooit zo blij heb gezien met een GPU release
RX 6800 XT;4;0.3661244511604309;"Dit mag ook wel eens gezegd: Dankjewel voor een mooi en duidelijk review! Waar ik meest van onder de indruk ben is de transparantie en helderheid in de review. Setup configuratie is niet nieuw maar de manier waarop games zijn geselecteerd. Niet zozeer de aantallen of genres zelf maar op de gebruikte game-engines! Hoe vaak zie je wel niet bij andere reviews, bijvoorbeeld van de 10 games, 5 daarvan Unreal Engine games zijn waardoor er ongewenst scheve balans op kan treden. De optie ""Medium"" game settings diagram maakt het alleen maar meer transparant. Hierdoor heb je meerdere meetpunten om de balans op te maken. Hoe vaak komt het wel niet voor dat een game ""geintjes"" uithaalt op alleen hoge/ultra settings om bepaalde kaarten of bepaalde generatie architectuur slechter/beter uit te laten komen. Wat een verademing om weer goede onderbouwde GPU/CPU hardware review hier te zien. Ik geef toe dat ik lang niet meer hier voor PC hardware reviews kwam kijken vanwege de super belabberde en nog meer belabberde reacties van de vorige PC hardware reviewer toendertijd onder Polaris/Pascal tijdperk. Chapeau!"
RX 6800 XT;5;0.5613728761672974;Dankjewel voor je compliment!
RX 6800 XT;1;0.44952574372291565;2020, het jaar met de grote stappen van de titanen, waar AMD de strijd aangaat en durft te winnen, waar consoles het voortouw van snelheid nemen, maar hetzelfde 2020 waar werkelijk niets van dit ook daadwerkelijk te koop is
RX 6800 XT;2;0.3799719214439392;Nvidia legde de lat erg laag, maar AMD lijkt er zo op het eerste oog alsnog over te struikelen Edit: ik bedoelde dat nvidia de lat erg laag legde qua voorraad. Qua specificaties en prijs /kwaliteit is het een goude tijd.
RX 6800 XT;2;0.35576778650283813;Mwah. AMD leek tot 3 jaar geleden zowel op CPU als GPU vlak een beetje hopeloos te worden. Nu hebben ze op beide vlakken hun tegenstander ingehaald dan wel bijgebeend. De RX6800XT evenaart de RTX3080 vrij duidelijk, elk heeft een aantal wins afhankelijk van de specifieke test. Maar de Radeon doet het slimmer, aan lager verbruik en goedkoper. Da's verre van struikelen. Daarnaast zijn ze ook erg overtuigd dat de komende generaties nog meer aanzienlijke vooruitgang zullen kennen.
RX 6800 XT;3;0.4247872233390808;Ik doelde op het ontbreken van een voorraad. De prestaties zijn indrukwekkend. En, als er voorraad was geweest, had ik graag een 6800 gekocht. Maar dit lijkt (vooralsnog) niet mogelijk te zijn. Wellicht dat het de komende dagen/weken verbeterd.
RX 6800 XT;1;0.4141902029514313;Ah.. tja. Niet te weerleggen inderdaad. Vraag me af of bepaalde landen voorrang hebben gekregen.
RX 6800 XT;2;0.4764891564846039;Dat zal me niet verbazen inderdaad, dat b.v. de meeste voorraad eerst naar landen als de VS is gegaan ofzo.. Grotere markten worden wel vaker voorgetrokken in dat opzicht..
RX 6800 XT;1;0.6518594026565552;Ik vind dit eigenlijk niet. Ik vind het een heel slechte review van Tweakers omdat ze de kaarten getest hebben met SAM ingeschakeld. Alleen videokaarten en 5000 serie cpu's van AMD kunnen hier gebruik van maken en dus niet de videokaarten van Nvidia en cpu 's van Intel. Ik vind dit een oneerlijke vergelijking. Schakel SAM uit en de 6800XT presteert in meer games opeens slechter dan de 3080 daar waar hij met SAM aan nog beter presteert. Geen correcte review dus.
RX 6800 XT;3;0.4216329753398895;In de helft van de gevallen maakt SAM niet eens iets uit. In sommige andere wel. Het is een feature die óók naar Nvidia komt, heeft Nvidia al aangegeven. Verder wordt er gewoon getest met het snelste beschikbare platform van het moment, want dat wil je toch weten als je GPUs van ~700-900 euro test/koopt? Én er staan testresulaten met en zonder zodat je de vergelijking nog enigzins kan maken. Ik neem aan dat je dezelfde opmerking maakt als DLSS getest wordt?
RX 6800 XT;2;0.3645923435688019;"Idd SAM komt ook naar Nvidia maw de 3080 presteert in de meerderheid vd games beter dan de 6800XT en niet zoals Tweakers zegt dat ze gelijk presteren. Dit klopt dus niet. En met RT is Nvidia sowieso stukken sneller dan AMD. Techpowerup bijv. heeft een veel betere review; daar testen ze met SAM uit zoals het hoort. En daar presteert de 3080 6% beter dan de 6800XT in games. Minpunt van de 3080 is wel het stroomverbruik. Dan nog moet je met SAM uit testen want Intel cpu's hebben dit niet. Maar ja, het is algemeen geweten dat Tweakers AMD altijd bevoordeelt."
RX 6800 XT;2;0.30239030718803406;"Dus je vindt het scenario dat iemand z'n 5 jaar oude Intel CPU wil inwisselen voor een Ryzen 5000, samen met een nieuwe GPU, onwaarschijnlijk? Dan zou een review met SAM aan toch wel ""zoals het hoort"" zijn? Tweakers heeft een pagina toegewijd aan het verschil met-zonder. TPU heeft zo te zien een apart artikel met die verschillen, met hier de samenvattende performance en hier de conclusie. Interessant aan die conclusie: Als ik naar de specifieke 6800XT review ga zie ik inderdaad dat de RX6800(XT) minder presteert bij de Intel CPU. TPU hun cijfers: 1440p Intel + RTX3080: 155fps Intel + RX6800XT: 150fps AMD + RTX3080: 164fps AMD + RX6800XT, geen SAM: 165fps AMD + RX6800XT, wel SAM: 168fps 4K Intel + RTX3080: 97fps Intel + RX6800XT: 93fps AMD + RTX3080: 110fps AMD + RX6800XT, geen SAM: 100fps AMD + RX6800XT, wel SAM: 101fps Exact dezelfde uitkomst als Tweakers: ongeacht SAM is de Intel CPU een bottleneck voor zowel Nvidia als AMD. Op 1440p is AMD ietsje sneller, op 4K dan weer Nvidia. SAM is gemiddeld genomen een procentje verschil. Dus als je tóch het maximale uit eender welke kaart wil halen moet je een Ryzen 5000 in huis halen, want dat scheelt al snel 5-10%. En dan kan je SAM evengoed aanzetten."
RX 6800 XT;2;0.31321823596954346;Niet iedereen upgrade zijn cpu en gpu tegelijk, dus nee je vlieger gaat niet op. Met een cpu doe je meestal langer dan een videokaart. Zit je dus nog met een Intel cpu en wil je upgraden naar 6800XT dan moet je met SAM uit testen want de Intel cpu kan er geen gebruik van maken. In veel games is het verschil ts SAM aan en uit een win voor AMD of niet. Het is dus wel degelijk belangrijk dat je het uitschakelt bij de review.
RX 6800 XT;2;0.4409332573413849;Nvidia legde de lat erg laag? De meeste mensen welke ik spreek over de 3000 serie zijn het er over eens dat de lat hoog word gelegd, zeker kijkend naar de overgang van de 1000 naar 2000 serie. Daar lag de lat immens laag. En de prijzen immens hoog. AMD zet hier een paar mooie kaarten neer, vooral kijkend naar de performance per watt - de vaste grap van het groene kamp is altijd geweest dat je een ei kon bakken op AMD kaarten, dat is voor de gein een keertje omgedraaid. Voor het gros is het vooral de vraag hoe beide kampen het in de middenklasse gaan doen, het overgrote deel van de Steam gebruikers heeft een GPU tussen de €150 en €350, een prijsgebied waar geen van beide een kaart heeft in de huidige generatie. (en ook nog geen kaarten heeft aangekondigt)
RX 6800 XT;2;0.43159666657447815;Beter lezen, hij heeft het over de beschikbaarheid c.q. leveringen, niet over de prestaties..
RX 6800 XT;1;0.379374623298645;Dat stond er niet op het moment dat ik reageerde, gebaseerd op het artikel kan je bij de eerste zin enkel aannemen dat het om performance gaat. (Kijk even naar de tijd dat ik reageerde en de tijd van zijn edit. )
RX 6800 XT;1;0.48740246891975403;Vergeet dit pareltje niet.
RX 6800 XT;4;0.5320733785629272;Precies, bijna dubbel zo snel als een 2080 vind ik zo gek nog niet. Vind dan ook niet gek dat Nvidia wat problemen had met launch als ze alles uit de kaarten weten te halen. Met die uitspraak zal ik nog voorzichtig zijn. Gezien je een 3080 flink kan undervolten dropt de wattage aanzienlijk en de kaarten kunnen dan ook iets hoger boosten. (1.025 stock 0.900 undervolt) 2000mhz steady. Maar natuurlijk is het nog afwachten of je de 6000 serie kan undervolten.
RX 6800 XT;5;0.2889651954174042;En Apple die op technisch vlak de strijd aangaat met AMD, Nvidia én Intel, maar tegelijkertijd hun chips niet los aan consumenten verkoopt. Als Apple relatief tov inkomsten evenveel R&D uitgeeft als de anderen, hebben ze ook méér R&D capaciteit dan die drie anderen opgeteld. Gek genoeg levert het ook hard op.
RX 6800 XT;4;0.2629912793636322;Kaarten kopen die alleen in theorie bestaan is de toekomst.
RX 6800 XT;1;0.607357919216156;Linus z'n conclusie is totaal anders : die breekt de Radeons af: -raytracing niet op niveau -streamen tijden gamen niet ok -geen DLSS ...
RX 6800 XT;2;0.3543289005756378;Hij breekt de radeons niet af, en zegt voor traditionele rasterized games de Radeons op gelijk niveau zijn als hun Nvidia tegenhangers. Alleen voor extra's is nVidia beter. DLSS 2 is zeker een fantastische toevoeging, alleen brede adoptie voor deze techniek is er nog niet. RTX is al helemaal een gimmick en zelfs bij Watchdogs legions niet game bepalend, hoewel het daar voor het eerst ook iets toevoegt (en maybe Control). Voor streaming raad ik zowel nvenc (Nvidia) als hevc (AMD) af, beter is de cpu encode x264 preset in obs met medium encode op een 3900x of 5900x. De kwaliteit is dan veel beter! (game performance wordt nauwelijks beïnvloedt door de hoge thread count van deze processors)
RX 6800 XT;2;0.37977176904678345;"Ik vond Linus' review ook een negatieve toon hebben (in vergelijking met andere reviews). Een paar weken geleden stond iedereen nog te springen om de 3080, 2x de 2080 voor 700$!! Maar als AMD het doet (na jaren achterstand zelfs), met meer efficientie en voor minder geld ook nog, dan is het opeens ""verwacht"". Ja, DLSS is indrukwekkend, maar had iedereen soms verwacht dat AMD in één keer NVIDIA voorbij zou gaan op álle vlakken? Ik ben allang blij dat er weer concurrentie is!"
RX 6800 XT;3;0.3079870641231537;"Naar mijn weten is ""RTX"" toch een onderdeel van DXR (Windows DirectX 12 Ultimate) en dat AMD ook Ray-tracing heeft? Goed, dankzij dat ""RTX"" marketing, wist ik dat eerder niet van AMD kaarten en NVidia dat alleen had. (ja ""DLSS"" stretching vond AMD (nog?)niet nodig)"
RX 6800 XT;3;0.3465639054775238;"DXR is samen door Nvidia en Microsoft ontwikkeld. RTX zelf is niets anders een platform voor graphics en moet niet geïnterpreteerd worden als Ray Tracing X o.i.d. De rtx kaarten die we kennen maken gebruik van speciale RT cores voor Ray Tracing. AMD kaarten hebben niet zulke specialistische cores. AMD lost het Ray Tracing ""probleem"" gewoon anders op, zei het blijkbaar iets minder effectief."
RX 6800 XT;2;0.4293655455112457;Vind het meer constructief kritiek. Zo Van AMD kom nou eens met deze features. Anders betalen we Nvidia wel 50 dollar meer.
RX 6800 XT;5;0.3725798428058624;De 3080 is gewoon beter en je hebt nog een 3090 voor de mensen die echt het beste willen. Voor de prijs / kwaliteit lijkt een 3070 perfect te zijn, als deze nu eens voorraad hadden
RX 6800 XT;3;0.32461002469062805;Ik had toch stiekem op een benchmark gehoopt in Microsoft Flight Simulator. reviews: Microsoft Flight Simulator - Op reis met Ryzen 5000 en GeForce 3000
RX 6800 XT;1;0.6022338271141052;Hetzelfde als de andere grafische kaarten, medium settings heeft alles vanaf een 2070/5700xt rond de 55 fps ongeacht de resolutie en 30-35 fps op ultra settings ongeacht de resolutie. Compleet nutteloos spel om benchmarks mee te doen.
RX 6800 XT;3;0.43781203031539917;Aan de hand van die review lijkt het mij duidelijk dat een 6800(XT) GPU niet veel verschil gaat maken, aangezien msfs liever een snelle cpu heeft met hoge single core speed.
RX 6800 XT;5;0.5021694898605347;Linustechtips answered your prayers. 3min 20
RX 6800 XT;3;0.2391403466463089;Thnx! Hier was ik naar op zoek. Op 1080p en 1440p scoorde de RX 5700XT nog wel goed, maar op 4k was dit ongeveer de helft van de RTX 2070 Super en ik was benieuwd hoe de RX 6000 serie ermee om zou gaan.
RX 6800 XT;3;0.38549646735191345;Op Guru3d is FS2020 wel meegenomen:
RX 6800 XT;5;0.5865010023117065;Dankje!
RX 6800 XT;2;0.42370840907096863;Ze hebben hier laatst een uitgebreide benchmark van gedaan op Tweakers. Conclusie is dat het volledig CPU bound is dus geen goede benchmark voor GPUs.
RX 6800 XT;3;0.5121813416481018;Op hogere resoluties is er zeker een verschil te zien. Zo presteerde de 5700XT op 4k ultra zo'n 15fps tegen 36fps met de RTX 3080. Een RX 6000 videokaart zou hier zeker beter moeten scoren (zie ook de linkjes hierboven). De performance op lagere resoluties zijn idd vrijwel gelijk.
RX 6800 XT;2;0.4937407672405243;"Klopt maar tussen de kaarten in het segment van de RX 6800 (XT) waren de verschillen verwaarloosbaar, ook op 4K ultra. Ik snap daarom volledig dat ze MSFS niet hebben gebruikt als benchmark hier. Reviewers zijn vooral beperkt door de tijd die het kost de benchmarks te draaien en de data te organiseren; dan wil je uiteraard benchmarks draaien waar geen sprake is van een CPU bottleneck."
RX 6800 XT;2;0.5228988528251648;Helaas verrast door het complete gebrek aan voorraad.
RX 6800 XT;1;0.48417338728904724;Er zijn niet eens productpaginas....
RX 6800 XT;1;0.49097636342048645;In de review staat ook dat geen enkele Nederlandse webshop die gepartnered is met AMD geen enkele kaart hebben ontvangen. Complete paper launch dit, zowaar nog erger dan Nvidia.
RX 6800 XT;1;0.5683102011680603;Haha, en iedereen maar zeggen dat AMD gigantisch ging verkopen want ze hadden wel voorraad
RX 6800 XT;1;0.6914685368537903;Helaas is de beschikbaarheid van deze ‘standaard’ videokaarten zeer slecht en verwachten we hier de komende weken ook geen verandering in te zien. Om teleurstellingen over de lange levertijden te voorkomen nemen we op dit moment dan ook GÉÉN BESTELLINGEN aan voor de AMD Radeon RX 6800 en de AMD Radeon RX 6800 XT. Azerty verkoopt ze iig niet.
RX 6800 XT;3;0.3962593078613281;Aan het begin van het stuk begon ik bijna te balen dat ik €650,- voor een RTX3070 had neergelegd. Maar aan het eind: Toch maar blij dat ik 't wel heb gedaan. Een videokaart is beter dan geen videokaart.
RX 6800 XT;5;0.47094419598579407;En geduld is een schone zaak
RX 6800 XT;3;0.45592793822288513;Zeker waar. Alleen heb ik nu vakantie, dus wou ik nu gamen. Dan betaal je daar een premium voor, helaas
RX 6800 XT;5;0.5583157539367676;3070 is een harstikke mooie kaart, kan je lang mee vooruit!
RX 6800 XT;2;0.40544965863227844;Ik onderschrijf de conclusie dat RTX 3080 / RX 6800 XT lood om oud ijzer is. Omdat ik nu (ook) een NVidia in een AMD systeem heb zitten maakt het me niet echt uit. All AMD is leuk, maar geen must. Welke kaart leverbaar gaat zijn voor welke prijs gaat de doorslag geven.
RX 6800 XT;3;0.519791305065155;als ik dit zo zie is de 6800XT toch echt wel de betere keus voor 1440p, vaak sneller als de RTX3090 Voor 4K is de RTX3xxx series echter vaak wel de betere keus
RX 6800 XT;2;0.4818016588687897;Tenzij je b.v. Cyberpunk 2077 gaat spelen, dan ben je blij dat je DLSS kan gebruiken.. Het hangt echt van de games af wat beter is.. Uiteindelijk zit je met beide wel goed en zal vooral beschikbaarheid en prijs gaan uitmaken welke mensen kopen.. Op dit moment is het om moedeloos van te worden dat er werkelijk niks op voorraad is, wachttijden enorm lang en dat de prijzen daardoor ook kunstmatig te hoog zijn.. AMD moet flink gaan produceren en leveren voor dat Nvidia weer met een soort refresh komt in de vorm van een 3070Ti en 3080Ti begin 2021.. Anders pakken ze qua marktaandeel maar weinig terug..
RX 6800 XT;1;0.6650425791740417;De AIB kaarten Komen volgende week, Dan zal amd vast een heleboel verkopen
RX 6800 XT;2;0.38911840319633484;Prijzen zijn toch niet kunstmatig hoog? Als amd of Nvidia kon leveren, zouden ze dat echt wel doen.
RX 6800 XT;3;0.3947795033454895;Ik denk dat mijn volgende kaart voor het eerst Nvidia wordt (ooit). Maar dan wel de 3080ti, zie de bui al hangen ja. DLSS tegenhanger is nog volop in ontwikkeling, en de AI netwerken zijn bij Nvidia uiteraard van top niveau, verwacht dan ook een minder sterk functionerend geheel bij AMD. En als je dan toch raytracing wilt, wat in sommige games (net zoals HDR) fantastisch is, wel zo handig dat je niet of nauwelijks inlevert op resolutie/scherpte.
RX 6800 XT;2;0.46514710783958435;Het is een beetje zoals altijd het geval, afhankelijk van je use case. Linus tech tips heeft een goede review up staan. Maar productivity is amd achteruit gegaan en cuda maakt o.a. solidworks sneller. Als raytracing niks boeit is amd superieur nu en uitstekende bang for Buck. Maar als raytracing wel boeid en dlss dan is en blijft NV beter. Dlss gemaakt gebruik van tensor cores en daar heeft amd hardware matig niks voor. Ik wilde een 6800 xt halen omdat de 3080 niet beschikbaar was, maar zoals het nu gegaan is bestel ik wel een 3080. Sta ik tenminste in een lijst.
RX 6800 XT;3;0.3596895933151245;DLSS is voor mij de belangrijkste reden waarom ik waarschijnlijk voorlopig bij Nvidia blijf.. Ik wil straks o.a. Cyberpunk 2077 gaan spelen, en die ondersteunt dat gelukkig.. Gelukkig ondersteunt mijn huidige RTX 2080 dat ook, dus het is geen hele grote ramp als ik geen 3080 voor die tijd te pakken krijg.. Ander ding is mijn CPU, dat is nog een oude i7 6700K.. Kan ook nog wel heel even mee, maar het liefst vervang ik die ook voor wat nieuws zodra ik een RTX 3080 heb want ga anders toch wel fps laten liggen ben ik bang.. Zal dan een R5 5600X worden.. Maar ook die is ook gewoon nog niet te krijgen voorlopig.. Als ook dat te lang gaat duren trek ik het door naar 2021 en ga wachten op Intel waarmee die komen als antwoord op AMD Zen3.. Mocht dat tegenvallen kan ik altijd nog rond die tijd een AMD R5 5600X (of misschien wel een non-X) kopen voor minder geld dan nu, hopelijk gewoon uit voorraad ergens.. Die moet dan wel weer +/- 5 jaar meekunnen voor gaming net als mijn i7 6700K nu..
RX 6800 XT;1;0.41463175415992737;Ik heb nu de verschillende reviews gezien, maar als je kijkt naar Minecraft rtx, die is gewoon onspeelbaar op 6800xt (4k) de 3080 haalt net 30fps maar met dlss is het bijna 80fps. Dat is gewoon een alledaagse toevoeging die er echt toe doet en waar amd hardware matig nog geen antwoord op heeft (softwarematig zeker niet). Ik gebruik hem ook voor bedrijfsmatige doeleinden en daar regeert cuda simpelweg. Geen plugin nodig en het werkt gewoon (en sneller). Mijn 2080ti is kapot
RX 6800 XT;2;0.47820228338241577;"Op de site van Azerty: ""Over de lancering van de AMD Radeon RX 6800 en AMD Ryzen 6800 XT videokaarten Op 18 november zijn de referentiemodellen van de nieuwe AMD Radeon RX 6800 en de AMD Radeon RX 6800 XT officieel gelanceerd. Dit gaat nu nog om de videokaarten met het standaard AMD-ontwerp qua printplaat en koeler. Binnenkort introduceren de diverse merken hun kaarten met een eigen ontwerp. Helaas is de beschikbaarheid van deze ‘standaard’ videokaarten zeer slecht en verwachten we hier de komende weken ook geen verandering in te zien. Om teleurstellingen over de lange levertijden te voorkomen nemen we op dit moment dan ook GÉÉN BESTELLINGEN aan voor de AMD Radeon RX 6800 en de AMD Radeon RX 6800 XT. Wij vinden het niet netjes naar onze klanten toe om bestellingen aan te nemen als het nog niet duidelijk is of deze kaarten uiteindelijk -waar dan ook ter wereld- leverbaar gaan zijn. In plaats daarvan richten we ons op de ‘eigen ontwerp’ kaarten die binnenkort door de verschillende fabrikanten zullen worden aangekondigd. De verwachte beschikbaarheid van die modellen zal echter ook matig zijn. Wil je toch zeker weten dat je één van de eersten bent met een van deze nieuwe videokaarten? Kijk dan eens bij onze nieuwe [Azerty Gaming Platinum AMD] en [Azerty Gaming Esports AMD] gaming pc’s. Op dit moment is nog niet bekend wanneer we ze kunnen gaan leveren, maar doordat we snel kunnen schakelen zodra de beschikbaarheid toeneemt zul je met deze pc’s zo snel mogelijk aan de slag kunnen!"" De voorraad van AMD lijkt dus hekaas nog minder te zijn dan die van Nvidia."
RX 6800 XT;2;0.3867151439189911;Aan de ene kant netjes van Azerty, maar grote kans dat ze daardoor potentiele klanten verliezen door mensen die geen geduld hebben en daardoor bij een andere partij gaan pre-orderen.
RX 6800 XT;1;0.6583449840545654;Zoveel winst maken ze niet op die kaarten, en ze zijn wel veel tijd kwijt met klantenservice voor mensen die bestellingen annuleren of veranderen en en mensen die bellen met de vraag of er al voorraad binnen is, en wanneer ze voorraad verwachten.
RX 6800 XT;3;0.3651266396045685;Ja dat is ook weer zo. Marges zijn klein, want er is veel concurrentie. Ze zouden natuurlijk een pre-order met aanbetaling kunnen doen, maar dat schept ook weer verwachtingen
RX 6800 XT;1;0.47130098938941956;Die pre-built PC's is dus ook iets waar veel videokaarten in verdwijnen.. Azerty zegt dat hier ook gewoon indirect, dat hun pre-built gaming PC's voorrang krijgen t.o.v. losse verkoop.. Ze maken meer winst op die complete systemen denk ik.. Ik blijf het jammer vinden dat je geen videokaarten rechtstreeks bij de fabrikanten kan kopen, los van AMD of Nvidia zelf.. Ik geloof dat EVGA dat als enige doet en dan alleen in de VS zover ik weet.. Dan betaal je gewoon altijd netjes de adviesprijs en geen hogere prijs omdat de tussenhandel er ook aan moet verdienen..
RX 6800 XT;1;0.5233232975006104;"Ik lees niets in deze review over de tegenhanger van DLSS; super resolution (VSR). Is dit nog niet beschikbaar?"
RX 6800 XT;1;0.24442729353904724;Yes, op pagina 2
RX 6800 XT;3;0.26259365677833557;Ah thanks, ik zat specifiek de tekst te scannen op dlss en VSR/super resolution, dus had dit over het hoofd gezien.
RX 6800 XT;5;0.3318859934806824;Ik heb een bestelling voor de 6800 kunnen plaatsen
RX 6800 XT;1;0.6559069156646729;Heb je m via AMD gekocht? Ik ben er ook doorheen gekomen na een half uur problemen.
RX 6800 XT;2;0.3714434802532196;"Inderdaad ook direct bij AMD, blijkt dat als je die knopjes gewoon heel vaak en snel drukt dat ze wel reageren. Ik had zelfs een 6800xt in m'n cart maar ik kon niet naar de checkout voordat die uitverkocht waren, dus dat was wel een domper. Maar hé toch nog beter dan binnen een halve seconde van ""buy now"" naar ""out of stock"""
RX 6800 XT;3;0.431972473859787;Had precies hetzelfde met de XT, ook in de winkelwagen en vervolgens niet naar de checkout Nu maar hopen dat er geen teleurstellend bericht van AMD komt dat er toch iets teveel besteld zijn tov de voorraad.
RX 6800 XT;5;0.5439104437828064;We zullen het merken!
RX 6800 XT;1;0.5541725754737854;Ik had bijna hetzelfde. Ik kwam bij de checkout pagina maar kon het niet kopen. Ik ging snel door met paypal onetouch, maar het mocht niet baten. Er waren op een gegeven moment zelfs wat bugs waardoor de RX 6800 €492 was opeens 😂
RX 6800 XT;2;0.3925759792327881;Dit laat me erg twijfelen... ik heb een 3440x1440p 120hz GSYNC only scherm.. laat nou net AMD sneller zijn in deze resolutie, maar alleen Nvidia kan me de VRR geven. Daarnaast 10GB vs 16GB. Yikes!
RX 6800 XT;1;0.5261433720588684;2560*1440P is ongelijk aan 3440*1440P!
RX 6800 XT;3;0.335767537355423;Ja dat klopt, zit tussen 1440p en 4k 16:9 in, maar nog steeds dichter bij 1440p. 3,6M vs 4,9M vs 8,3M pixels
RX 6800 XT;2;0.42579591274261475;Ik heb hetzelfde. Zit hier met een Odessey G9 (5120x1440). Dan zou de 3080 beter scoren, omdat het veel meer bij 4k ligt. Aan de andere kant zijn ze allebei moeilijk te krijgen dus ik wacht wel tot ik er een krijg, en vergelijk ik het daarna als ik de andere binnen krijg
RX 6800 XT;1;0.4658649265766144;Heb de 3080 strix oc besteld. *5 maanden later*
RX 6800 XT;1;0.7286154627799988;Ik heb de Asus 3080 tuf niet oc besteld, begon op plek 56 en sta nu op 37, schiet niet op...
RX 6800 XT;3;0.29421308636665344;Bij Alternate?
RX 6800 XT;5;0.2728898823261261;Jupp
RX 6800 XT;3;0.28245943784713745;Hoe zie je je positie?
RX 6800 XT;1;0.25840625166893005;onderaan deze pagina. Dan je ordernummer en postcode invoeren.
RX 6800 XT;1;0.5237327814102173;RIP 367
RX 6800 XT;5;0.5564271807670593;Succes! Dan had je niet besteld op launch day zeker?
RX 6800 XT;5;0.5634902119636536;Jawel, op launch day van AMD!
RX 6800 XT;3;0.4221261739730835;Los van dat dik de prestaties indrukwekkend vind, zeker in combinatie met geluidsproductie en stroomverbruik en het feit dat er nu een geschikte test cpu is gebruikt (5950X) moet ik wel even een kleine kanttekening plaatsen. De meeste games waarbij dit mogelijk is wil je mét DLSS ingeschakeld spelen omdat de game er dan beter uit ziet dan met TAA. Bijkomend effect is dat je dan een flinke performance verbetering krijgt. Ik merkte in f1 2020 met mijn RTX 2070 een stijging van 88 naar 113 FPS. Dat scheelt 28%. Nu snap ik wel dat je als reviewer het eerlijk wil houden en dan zijn technieken als DLSS een lastig verhaal. Als consument ben ik wel benieuwd hoe de verhoudingen liggen mét DLSS, dat is immers toch de setting die ik zou gebruiken puur omdat dit de game mooier maakt. AMD heeft tegenwoordig FidilityFX dus dan krijg je ook de vraag of je die dan moet gebruiken, om het makkeliker te maken . SAM wordt overigens terecht apart getest want als ik een 6800XT koop heb ik nog geen SAM, dan moet ik eerst ook een Ryzen aanschaffen. DLSS krijg je natuurlijk bij elke RTX kaart. edit: Gamernexus doet control met DLSS on en off: Redelijk extreme verschillen: 3080FE DLSS on: 64.8 @4k / 136.9 @fullhd 3080FE DLSS off: 36.2 / 105.7 6800XT 23.0 / 70.2 DLSS is overigens prima zonder ray tracing te gebruiken, in die zin nog steeds niet precies wat ik bedoelde. Ik ben benieuwd naar de performance zonder ray racing mét DLSS.
RX 6800 XT;3;0.43557488918304443;Ja maar geen DLSS bij de meeste games dus ...
RX 6800 XT;3;0.33577218651771545;In deze review beschikken Control en F1 2020 en Shadow of the Tombraider over DLSS support. Mondjesmaat worden het er meer: AMD schijnt aan een tegenhanger te werken.
RX 6800 XT;3;0.43146005272865295;"Je hebt een punt en dat Nvidia DLSS aanbiedt zou vermeld moeten worden in een review. Aangezien AMD heeft aangekondigd ook met een geavanceerdere vorm van Super Sampling te komen, en DLSS in slechts een beperkt aantal games beschikbaar is, lijkt het mij wel eerlijker om dat in de benchmarks nog buiten beschouwing te laten of hoogstens apart te testen (zoals SAM + Rage Mode; wat overigens in elke game werkt zolang je de hardware hebt, dus inclusie daarvan vind ik logischer/belangrijker)."
RX 6800 XT;3;0.3528475761413574;Opvallend dat wanneer je van 'ultra' naar 'medium' gaat, dat je dan bij AMD relatief gezien veel meer fps terug krijgt dan bij Nvidia. Datzelfde had ik deze week al eerder in de geruchtenmolen gehoord. Als je van 'ultra' naar 'high' gaat, dan krijg je daar bij AMD meer prestaties voor terug dan Nvidia. In deze review zie je een nog groter verschil maar dat is dan ook van 'ultra' naar 'medium'. Als je dus genoegen neemt met wat mindere instellingen dan kun je behoorlijk wat extra fps uit je systeem krijgen met deze nieuwe AMD kaarten.
RX 6800 XT;4;0.2943039536476135;Komt omdat de Ampere architectuur het beste tot zijn recht komt op hogere resoluties en zwaardere belasting. Andere render pipelines. U
RX 6800 XT;5;0.3650332987308502;Maar dat wil niemand. Iedereen wil de pracht en praal van Ultra.
RX 6800 XT;2;0.4378558099269867;"Ik denk dat dat zou kunnen komen doordat er op 4K meer cache-misses zijn; er passen uiteraard meer items in de cache bij 1440p dan bij 4K. Cache-misses zijn kostbaarder voor AMD want ze hebben trager VRAM. Wellicht dat toekomstige optimalisaties (bv. m.b.v. SAM) hier nog wat verandering in kunnen brengen. AMD heeft aangegeven dat de meerwaarde van SAM groter wordt als developers hiervoor optimaliseren. Daarnaast beweert AMD dat SAM 'open source' is en in de toekomst ook naar Intel en Nvidia kan komen als die producenten dat wensen. Erg mooi dat ze deze technologie werderom niet voor zichzelf houden."
RX 6800 XT;3;0.42565420269966125;Ook al maakt AMD nu grote stappen voorwaarts, ik mis nog één dingetje ten opzichte van Nvidia: Streaming van games zoals met een shield of moonlight-embedded. Daar is naar mijn idee nog niet echt een equivalent van voor AMD. Correct me if I'm wrong .
RX 6800 XT;3;0.5291494131088257;Je kunt op bv de Nvidia shield de AMD link app installeren en dan heb je ook streaming met AMD kaarten. Persoonlijk vind ik de nVidia implementatie beter en stabieler, maar AMD link werkt wel.
RX 6800 XT;3;0.34762370586395264;"Hierboven ergens een comment van een medetweaker die aanraad de CPU x264 encoder in obs te gebruiken met medium profiel. Gaat blijkbaar ook snel als je een CPU met genoeg cores hebt en kwaliteit zou veel beter moeten zijn. Dat de kwaliteit beter zou zijn is iets wat ik wel vaker lees; daarom gebruik ik zelf altijd CPU encoders, maar zelf doe ik dat niet voor streaming maar gewoon offline encode."
RX 6800 XT;1;0.44811829924583435;En AMD beweren dat het geen paper launch zou worden
RX 6800 XT;1;0.3161469101905823;Geef ze een paar dagen ...
RX 6800 XT;3;0.3458879888057709;Het kan nog wel. Op launch day inderdaad een paper launch maar stel dat het de komende weken wel goed verkrijgbaar is dan is het geen paper launch. Wait & See. Niet dat ik er veel vertrouwen in heb dat ik een 6800XT kan scoren voor een goede prijs binnenkort!
RX 6800 XT;3;0.47709643840789795;Ja je hebt gelijk, het is een beetje voorbarig om te roepen dat het een paper launch is. We zullen zien Zelf was ik er bij de 3080 launch bij, maar heb toen niets kunnen bemachtigen. Ik denk dat ik iets meer naar de 3080 neig, maar pas in januari/februari weer wat ga overwegen
RX 6800 XT;3;0.5225750803947449;Ik vind het wel jammer dat de RTX 3090 niet is meegenomen, zo kan je het beste aanbod van Nvidia vergelijken met het beste aanbod van AMD. Verder wel erg uitgebreid. Verbaast mij ook dat SAM niet veel lijkt te doen, terwijl dit eerder wel veel leek te doen (tientallen fps verschil). Oja, potverdrie. Zou bijna vergeten dat de 6900XT er ook nog is. My bad.
RX 6800 XT;2;0.4222736656665802;de 3090 is niet te vergelijken met de getoonde kaarten. De 6900XT wordt de tegenhanger en dan pas kan je een goed beeld schetsen. 6800XT op niveau van 3080 en de 6800 op niveau van 3070. Ik ben sowieso voor Team rood op CPU als GPU .. en komen als het goed is beide volgend jaar begin volgend jaar in mijn kast.
RX 6800 XT;2;0.4301064610481262;Naar mijn weten zijn er ook gewoon prima meetresultaten te behalen voor de 3090, en vergelijken blijkt dan ook prima te kunnen (zie o.a. reviews: Nvidia GeForce RTX 3090 - De overtreffende trap van Ampere). Ik zie niet in waarom de vergelijkingen daar niet ook gemaakt zouden kunnen worden met ook de metingen van de RX 6800 (en XT). Ik vind het echt jammer dat er geen vergelijkende meetresultaten zijn meegenomen van de GeForce RTX 3090 in de grafiek. Waarom wel de resultaten van de RX Vega 64 - een kaart van 2 generaties / 3 jaar geleden? Het lijkt me namelijk dat de resultaten van een 3090 meer relevant zijn dan die van een kaart die zelfs bij uitkomen al bijna helemaal afgeschreven was. In de Testverantwoording kan ik het antwoord daarop (helaas) niet vinden.
RX 6800 XT;4;0.42270493507385254;De 64 was en is nog steeds een prima kaart.
RX 6800 XT;1;0.36661651730537415;Die wordt in een aparte review vergeleken met de 6900XT die op 8 december uitkomt.
RX 6800 XT;1;0.5326821804046631;Heb ik de 6900XT gemist? Daarnaast vindt ik ook niet dat je kan spreken van aanbod als niks verkrijgbaar is
RX 6800 XT;1;0.4444356858730316;Het staat letterlijk in de tekst, ''terwijl de strijd tussen de echte vlaggenschepen van AMD en Nvidia pas op 8 december zal plaatsvinden, met de RX 6900 XT tegenover de RTX 3090.''
RX 6800 XT;1;0.3902289569377899;Niet iedereen heeft de tijd om 22 pagina's te gaan lezen, en zag ze niet in de benchmarks staan, dusja.
RX 6800 XT;5;0.3897077143192291;Je kan ook gewoon de conclusie lezen .....
RX 6800 XT;1;0.24096858501434326;Laatste alinea was ik nog niet met lezen.
RX 6800 XT;5;0.30664291977882385;Via AMD.com zelf toch echt even de 6800 XT in het winkelmandje gehad... Twijfelde aangezien de processoren bij alle Nederlandse bedrijven direct in backorder gingen en zag het niet zitten om bij AMD zelf op de backorder lijst te staan. Die twijfel duurde een minuut en dat was zelfs al te lang... Inmiddels staan ze op de site van AMD zelf ook op out of stock. Mooie paper launch weer dus! Op naar 2021 en hopen dat kamp Rood en kamp Groen elkaar op prijs wat concurrentie gaan bieden.
RX 6800 XT;4;0.4471052289009094;Sowieso gewoon lekker wachten. Ik heb gelukkig op tijd een mooie MSI RTX2070 Super Trio goedkoop kunnen overnemen van een andere tweaker, ik zing het nog lekker uit met deze kaart en later haal ik wel een nieuwe vid als ze beschikbaar zijn.
RX 6800 XT;5;0.4620463252067566;Wat heb je betaald als ik vragen man? Ik en benieuwd wat een eerlijke normale prijs is voor een 2070 super. Ik heb zelf een GTX970 dus ik zoek een upgrade. Ik heb net een 1440p scher aangeschaft en wil fatnlijk gamen met cyberpunk in mijn achterhoofd. Masr moet ook op mijn budget letten en de gedachte iets langer doorsparen voor een goede kaart die heel goed kan meekomen op 1440p upcomming games.
RX 6800 XT;5;0.3469277620315552;Ik heb 400 eurie betaald. Die ik ook over genomen had van andere tweaker was praktisch nieuw (april dit jaar) dus vond ik een top prijs toen, en nog steeds overigens.
RX 6800 XT;4;0.3891051709651947;Mooie prestatie van AMD. Helaas weer grote voorraadtekorten, net als met bijna alle nieuwe hardware van dit jaar. Ik ben benieuwd hoe de implementatie van SAM op Nvidia kaarten gaat werken!
RX 6800 XT;1;0.41482681035995483;Laten we maar hopen dat AMD alle chips naar the AIB partners heeft gestuurd Dat is de enige goede reden die ik nog kan bedenken waarom AMD nu al de kaarten lanceert terwijl ze zelf geen voorraad hebben
RX 6800 XT;3;0.4309241771697998;Prijs/prestatie helaas niet de NVIDIA killers waar we op hoopten, maar in ieder geval is er nu een echte keuze te maken.
RX 6800 XT;2;0.27108901739120483;nvidia heeft zelf zijn vorige generatie al gekilled als het om prijs/prestatie gaat.
RX 6800 XT;5;0.37704354524612427;Pfoe. Blauw en rood zitten elkaar op de staart. Ii ben zeer benieuwd of er nog een software/driver foefje wordt uitgehaald om bepaalde limieten te unlocken (overclockende Youtubers zoals Jayz2Cents, PaulsHardware en Nexusgaming lieten zien dat er bij de RTX kaarten op gebied van voltage nog veel te halen viel). Mooie ontwikkelingen. Zodra de voorraden normaliseren dan heb je tegenwoordig voor 1500 euro een prachtige 4k gaming pc. Dat had ik een half jaar terug, ondanks geruchten, niet verwacht. Laten we we zijn, de RTX 3000 serie voldeed boven verwachting en heeft dan voor AMD de lat ook erg hoog gelegd. Op een volledig red platform verwacht ik met deze AMD kaarten nog een prestatiewinst dankzij snel geheugen, de cpu indeling en een gen4 m2. Laat maar komen die battles, dan kan mijn 1080ti op V&A.
RX 6800 XT;1;0.4562635123729706;Amd weet Nvidia ditmaal dus wel te evenaren en bij te houden maar zonder voorraad. Spijtig. Ik greep naast de rtx 3080, twijfelde over een rtx 3070 maar nu ik de review gelezen heb: doe mij maar een aftermarket 6800 XT voor de base prise van een FE rtx 3080 dus zeg €719. Maar nul voorraad dus. Tja, beter beetje bij beetje voorraad zoals Nvidia doet (al voelde dat ook als een papieren lancering) maar zo win je de strijd niet op gpu gebied. Corona of niet: als je dermate producten weet te releasen: zorg voor genoeg voorraad. Nu ben ik in afwachting van AM5/DDR5, morgen heb ik de PS5 in huis. Dus in die zin genoeg om te gamen. Maar ik had graag een AMD cpu/gpu combo upgrade willen doen voor mijn pc uit 2014.
RX 6800 XT;1;0.6004435420036316;Was er vanaf de eerste tik bij en er verschenen, in mijn subjectieve ervaring, direct advertenties voor de kaart met 'uitverkocht' erbij. Dus of de bots hebben prima werk geleverd of er waren nooit kaarten te koop. Afgaande op het bericht op BH denk ik het laatste. Deze hebben precies om klokslag 15:00 (9:00 DC) een bericht geplaatst met de mededeling dat ze geen vooruitbestellingen willen accepteren aangezien er toch niet geleverd kan worden. Chapeau aan BH.
RX 6800 XT;3;0.5197727084159851;Naast het gebrek aan voorraad vind ik het toch jammer dat ze nog niet volledig in de concurrentie mee kunnen. Er is geen kaart tegenover de RTX3090, en de RTX3080 doet niet onder voor de 6800xt en doet het veel beter op het vlak van raytracing. Echte voordeel van AMD is het ruimere geheugen zo te zien. Wel blij dat er meer concurrentie is, nu nog voorraad en concurrentie op elk vlak. Correctie, er komt nog een 6900xt aan, die kan wellicht het open vlak aan concurrentie op zich nemen. Ben benieuwd.
RX 6800 XT;1;0.3584873378276825;Er komt inderdaad nog een 6900XT aan die waarschijnlijk tegen de RTX 3090 aan zal zitten, maar dan voor 500 euro minder.. Nvidia komt daarna echter weer als antwoord met een RTX 3080Ti (in januari al volgens de geruchten) voor ook rond de 1000 euro, en zo staan we weer nagenoeg gelijk allemaal, op features als Raytracing, DLSS en SAM na..
RX 6800 XT;3;0.35873788595199585;Het zit dus allemaal heel dicht bij elkaar en het ligt eraan welke games je speelt welke kaart het snelste is.. Nvidia heeft nog wel mijn voorkeur vanwege Raytracing en vooral DLSS.. Maar mooi dat er eindelijk weer meer keuze is dan alleen Nvidia als je het beste wil.. Alleen erg jammer dat geen enkele videokaart op dit moment ergens normaal te koop is voorlopig en dat deze AMD launch dus nog erger lijkt dan die van Nvidia met 0 voorraad.. Beschikbaarheid en prijs kunnen wel eens gaan bepalen wie het beste verkoopt deze generatie.. Nu is trouwens ook duidelijk waarom Nvidia met een 3070Ti en 3080Ti wil komen volgens geruchten.. Daarmee pakken ze waarschijnlijk de prestatiekroon weer iets duidelijker terug.. Maar nogmaals, beschikbaarheid en prijs gaan waarschijnlijk belangrijker zijn dan weer een paar % sneller..
RX 6800 XT;2;0.3302828073501587;Het is mij gelukt om er eentje te bestellen met paypal, Maar ik zie nergens een plek op de AMD website waar je kan checken wat de status is van je order (oftewel of ik m ook echt heb). weet iemand waar ik dit kan vinden?
RX 6800 XT;3;0.44180047512054443;Overall zie het beeld dat AMD vooral heel veel waarde levert op het 1440p performancepunt met de 6800, en kan meekomen qua Raytracing met de vorige generatie topper van Nvidia de 2080Ti. Ik vind dat een slimme keuze van AMD om minder silicon aan Raytracing te besteden, waar ongetwijfeld de volgende generatie (RDNA3) dit geheel anders gaat zijn, bij de stap naar 5nm van TSMC waardoor de transistor density flink zal toenemen tov 7nm van circa 98 mln per mm2 naar 183 mln per mm2. En Raytracing een veel goedkopere functie zal worden. Daarnaast is het mooi om te zien dat geluid en thermals vanuit een reference design op en top zijn, they mean business :-). Ben erg benieuwd naar een tear down door tweakers.net, als we kijken naar het gebruik van de componenten in de launch versie van de 6800(XT) vs de Nvidia Grafische plankjes.
RX 6800 XT;1;0.46477654576301575;2020 is het jaar van de paper releases. Next gen consoles, processors, en videokaarten. Er is zo weinig beschikbaar dat het gewoon paper release galore is
RX 6800 XT;5;0.5760679244995117;zo de 6800XT insAllah wordt mijn nieuwe koop in 2021 samen met de 5900X CPU goede combo. En dan houdt ik het ook daarbij.
RX 6800 XT;3;0.4183867871761322;Iets gekend over stroompieken? Hebben we voor deze kaarten ook een veel hogere voeding nodig
RX 6800 XT;5;0.8136740922927856;Prachtig om te zien dat ook nVidia het lastig gaat krijgen met AMD. Deze concurrentie is geweldig voor de consument. Het feit dat deze kaarten vergelijkbaar presteren bij een veel lager vermogen, is zeer prettig. Ik denk dat een combi AMD Ryzen 5000 + AMD Zen 6000 voor velen de meeste aantrekkelijke aanbieding is van het moment. Als AMD dit momentum vasthoudt, dan belooft dat ook mogelijkheden voor VR naar mainstream te brengen. Nu alleen nog ergens te koop zien te vinden.
RX 6800 XT;1;0.4894939661026001;Ongelofelijk dit. Ik had toch gedacht dat het bij AMD anders zou gaan dan bij Nvidia mbt bevoorrading en al helemaal na zo'n geweldige presentatie. Bij Alternate heb ik beide versies kunnen vinden waarbij voor de 6800 wel 900 euro gerekend wordt en voor de 6800 XT 1000 euro. Nu begrijp ik dat bij schaarste de prijzen omhoog schieten maar come on... Edit: merkwaardig dat bij sommige partners niks te vinden is over de nieuwe kaart. Link AMD partners toegevoegd.
RX 6800 XT;3;0.36768051981925964;Mag je ook concluderen dat de 6800XT een hogere prestatiescore heeft tov de 3080 in 21:9, 3440x1440?
RX 6800 XT;1;0.2779906392097473;Volgens mij waren ze al uitverkocht voor ze online waren
RX 6800 XT;2;0.49451154470443726;Waarschijnlijk heeft AMD net als de hele (tech)wereld ook last van alle logistieke problemen op/rond China door de pandemie. Het is zelfs nu nog steeds erg lastig om betrouwbare leveringen uit China te krijgen, vooral per containerschip of trein via die zijderoute, en zelfs per luchtvracht. In dat laatste geval heb je te maken met een verdubbeling of zelfs verdrievoudiging van de kiloprijs, voor het beetje capaciteit wat er nog te krijgen is.
RX 6800 XT;1;0.588718593120575;Ook op het gebied van levering heeft AMD deze keer nVidia weten te verslaan. Nog slechter leverbaar.
RX 6800 XT;5;0.4933815896511078;Ineens ben ik heel erg blij dat ik de RTX 3090 heb terug gestuurd. Ben benieuwd wat de RX 6900 XT gaat presteren
RX 6800 XT;5;0.3758291006088257;Het is een nek aan nek race dus die als eerste over de meeste kaarten online kan zetten zal winnen. Niet te vergeten dat je de laatste amd cpu en moederbord moet hebben als je het volledige potentiaal met de laatste amd kaarten wil hebben.
RX 6800 XT;3;0.2966534495353699;Dan maar een pre-built
RX 6800 XT;1;0.7402333617210388;AMD had de kans om Nvidia een mokerslag uit te delen, maar ze hebben de bal laten vallen... zonde.
RX 6800 XT;5;0.7254611849784851;Via AMD.com de 6800 gekocht voor €599. Ben heel benieuwd!
RX 6800 XT;1;0.3557998239994049;Heb je daar een link van? Heb daar namelijk nog nooit zoiets dergelijks kunnen vinden
RX 6800 XT;2;0.3575117290019989;Maar is er ondertussen al afgehaald.
RX 6800 XT;2;0.3710940480232239;Goed om op te slaan dan denk ik. Weet niet hoe je daar bent gekomen, maar het wordt i.i.g. niet echt heel uitgebreid geadverteerd. Tnx for the link! edit: haha, toen ik net klikte stonden er nog 2 cpu's listed en nu niks meer....
RX 6800 XT;1;0.25395503640174866;Geluidsdruk 100% fan mist de data voor de kaarten waar de review over gaat?
RX 6800 XT;1;0.45930540561676025;Klopt, helaas werkte de handmatige fan control voor deze kaarten niet, dus konden we de fans niet op 100% instellen om dit te meten. Overigens zegt de 'ingame' meting natuurlijk sowieso het meest, gezien de fans nooit op 100% zullen draaien tenzij je die zelf zo instelt. De grafiek is in ieder geval uit de review gehaald
RX 6800 XT;4;0.3785412907600403;Ik ben ook wel erg benieuwd als ze gaan testen met games waar de CPU niet de bottleneck is wat het verschil gaat zijn met de nieuwste versie processors van AMD en die van Intel (dan doel ik dus vooral op het direct toegang hebben tot het werkgeheugen, wat doet dat qua performance).
RX 6800 XT;3;0.30052459239959717;Het zou fijn zijn als er ook videokaarten van 2 generaties terug even getest worden zoals een GTX1080, waarvan er heel veel verkocht zijn en waarvan de kans heel groot is dat de eigenaren overstappen.
RX 6800 XT;3;0.3815702497959137;Vraag me af of het niet anders kan met krachtige GPU's, als ik de grootte en hoeveelheid aan fans zie op zowel de nvida als amd kaarten is het niet anders dan bespottelijk te noemen. Het moet toch mogelijk zijn grafische wizardry op een energiezuiniger manier te realiseren?
RX 6800 XT;4;0.5348798632621765;Hele nette prestaties. Petje af voor AMD zonder raytracing is de RX 6800 XT net zo snel als een RTX 3080 FE die wel lagere boost haalt en vasthoud dan veel AIB varianten. Maar ik neem aan dat er ook nog OC modellen van de RX 6800 XT komen. Dat word een moeilijke keuze gezien ik raytracing ook wel belangrijk vind.
RX 6800 XT;1;0.2900966703891754;Als je raytracing belangrijk vind is er maar 1 juiste keuze. Nvidia. Als je zonder kan moet je bij amd zijn.
RX 6800 XT;5;0.7791532278060913;AMD is terug in de race en de concurrentie is terug! Ik durf weer om voor het eerst na jaren weer een AMD videokaart te kopen. Mijn laatste AMD Radeon videokaart was de R9 290 kaart met toen Mantle. De RX 6800XT (of de RX 6900XT) zou hier mijn keuze zijn als ik nu op het punt stond een nieuwe videokaart te gaan kopen. Mooie combinatie met mijn Ryzen 7 5800X build. Ik vind de RX 6800XT erg goed geprijsd in vergelijking met RTX 3080. Ik kijk uit naar de reviews met de RX 6900XT! AMD is echt goed bezig! Ze hebben mij laatst echt weten te overtuigen, met de snelste processors met de Ryzen 5000 generatie en nu snelste videokaart. Nu nog zien wat de RX 6900XT kan
RX 6800 XT;5;0.5623347759246826;Weer een mooie week voor de scalpers! Grootste aanbod vind je op marktplaats. Leuk dat de webshops al €900+ vragen voor 650/720 MSRP...
RX 6800 XT;1;0.38808000087738037;Er moeten toch enkele geweldig knappe koppen zitten bij AMD tegenwoordig, met een veel kleiner budget zowel Intel als Nvidia zowat evenaren tot ronduit aftroeven, je moet het maar doen. Had ik nu toch maar mijn instinct gevolgd en die amd aandelen gekocht in juni..
RX 6800 XT;3;0.3215027451515198;Yup eigenlijk zijn we dit jaar verwend met een bedrijf als AMD. Laten we hopen dat ze snel een level up doen met hun volgende kaart en NVidia overtreffen ipv te evenaren. Maar voor nu denk ik dat de champagne wel klaar mag staan voor deze jaren inzet en hard opboksen tegen de groten. Het bewijs dat talent, hard samenwerken als een team, je heel mooie resultaten kan opleveren.
RX 6800 XT;1;0.41798830032348633;Ik was ook al van plan amd aandelen te kopen maar vanwege de pandemie en de fratsen van die gek in het witte huis verwachte ik een enorme beurscrash dus toch maar niet gedaan. Nu heb ik er echt wel spijt van want de boel veert weer op en AMD gaat goed boeren nu ze weer op alle fronten mee kunnen met de marktleiders.
RX 6800 XT;1;0.8566039204597473;Dit worden de goedkoopste feestdagen ever.......niks te koop
RX 6800 XT;3;0.399370014667511;Deze nieuwe AMD kaarten zijn erg snel en geven goede concurrentie op de high end. Echter wacht ik rustig op een 6700(XT) om een 1440p upgrade te doen. Iedereen mag vechten om kaarten van 700+ euro, de grote markt zit in de <400 euro en een 6700(XT) zal hopelijk even snel of sneller gaan zijn dan een 2080S...
RX 6800 XT;2;0.47913551330566406;Geen 1 van de games die ik speel ondersteund Raytracing of DLSS (AC Valhalla & Apex Legends). De in alle benchmarks is de 6800 en 6800XT sneller dan de 3070 of 3080. Daarbij hebben de amd kaarten meer vram aan boord, wat in de toekomst nooit nadelig kan zijn. Er zijn genoeg mensen waar dit voor geld denk ik?
RX 6800 XT;5;0.47776052355766296;Hear Hear Hier wacht ik ook op !
RX 6800 XT;3;0.32183128595352173;“op 4k is het lood om oud ijzer” geen idee wat hier mee wordt bedoelt? Beeldspraak kan alleen wanneer je zeker bent dat iedereen het begrijpt imo, wel weer een leuke uitspraak om aan mijn vocabulaire toe te voegen!
RX 6800 XT;1;0.32495084404945374;Ha, is een uitspraak welke eigenlijk zo normaal is in het dagelijkse spreken dat het me verbaast dat iemand het niet kent. Het is volgens mij dat jeugdigen de normale Nederlandse gezegden en spreekwoorden missen in hun vocabulaire. Ik zie dat ook bij mijn eigen kinderen, of wordt ik oud :-)
RX 6800 XT;3;0.4424707293510437;Alleen jammer dat je wordT schrijft in de ik-vorm
RX 6800 XT;2;0.5110422968864441;Tja dat is dan inderdaad jammer, ik word hier dan ook niet goed van. :-O
RX 6800 XT;5;0.43724653124809265;Als 26 jarige hier ben ik het nog nooit tegen gekomen!
RX 6800 XT;5;0.4514344036579132;Het betekend dat ze beide even goed zijn.
RX 6800 XT;3;0.5663332939147949;Maar iets erbij leren is leuk... Dus wat extra beeldspraak kan graag.
RX 6800 XT;1;0.5458003282546997;Uhm...Tweakers... Ik weet niet wat jullie hebben gedaan met die testbench, maar jullie getallen liggen echt all over the place. In jullie officiele 3080 review krijgen jullie voor shadow of the tomb raider 153,6 fps op 1080p ultra. In deze review is dat plotseling 192,5 fps. Wtf?! Voor een heleboel games zijn er gigantische verschillen. Hoe moet ik deze benchmarks nou serieus nemen...
RX 6800 XT;1;0.4397638440132141;Ze gebruiken nu Ryzen 5000. 1080p is waarschijnlijk CPU bottlenecked.
RX 6800 XT;2;0.41389477252960205;Heb je ook geprobeerd de verschillen te zoeken tussen beide tests? In de 3080 review werd er gebruik gemaakt van een 3900XT en nu van een 5950X op 4.65GHz, wat een enorm verschil in prestatie is.
RX 6800 XT;1;0.4148244261741638;Er zit een super groot verschil tussen de 3900xt en ryzen5000 beetje inlezen de volgende keer
RX 6800 XT;1;0.2250044345855713;Ben ik de enige wie altijd nog NVIDIA boven AMD verkiest puur omdat ik echt niet kan wennen aan de Software van AMD? AMD is natuurlijk al heer en meester in de processoren en dit zal bij de videokaarten niet heel veel anders worden. Ik wil ook dolgraag een AMD build hebben om geld te besparen omdat het toch goedkoper is dan NVIDIA maar de Software blijft een dingetje. Noem me ouderwets.
RX 6800 XT;5;0.31104224920272827;Als er iets niet te doorgronden is, dan is het wel met NVidia, of je moet Geforce Experience installeren, maar dan moet je weer verplicht een account hebben. En dat is bij mij de reden geweest om van NVidia af te stappen. En nu ik zie dat de 6800XT in alle games die ik speel de snelste is (Sneller dan de 3080FE met meer dan een paar %) dan wacht ik nog even met geduld op de normale prijzen met AIB uitvoeringen
RX 6800 XT;2;0.4381304681301117;6800XT is zeker niet de snelste, hangt zeer veel samen met wel of niet gebruiken van raytracing.
RX 6800 XT;2;0.42945683002471924;In de spelen die ik speel is de 6800XT sneller, veel sneller zelfs. Ray Tracing is geen optie in die games omdat je dan zelfs met een SLI 3090 geen 144 fps haalt. Dus RT is geen argument, maar die 10+ procent aan extra FPS wel. Wanneer ik andere games zou spelen zou het misschien anders om zijn. Maar ik speel maar 2 spelen veel en daarbij verslaat de 6900XT in mijn resoultie zelfs de 3090 in 1 van de gevallen
RX 6800 XT;2;0.46679195761680603;Ik schakel bij mij zelfs de nvidia software uit. Die gpu werkt prima zonder die control panel draaiend op de achtergrond. Helaas weet ik niet hoe dit bij amd werkt. Amd lijkt ook voor zijn cpu veel meer software nodig te hebben voor optimale prestaties. Waar amd echt te kort lijkt te schieten is features als rtx voice, reflex, dlss, nvenc. Dat is mij meer waard dan software.
RX 6800 XT;3;0.4319787323474884;Ok, wat was je laatste ervaring precies? Zelf zit ik bijna nooit in de settings. In het begin paar kleine dingen zoals anti-lag aanzetten en de beeldverscherping aanpassen en dan heb ik het wel gehad. Nvidia ziet er juist nog erg ouderwets uit met hun interface, kan zo 15 jaar terug en zie weinig verschil?
RX 6800 XT;5;0.273563951253891;Waar kan je niet aan wennen dan? Het is toch veel duidelijker dan die jaren 90 interface driver van Nvidia? Je hebt die software niet nodig ook, MSI afterburner kan alles beter dan de Nvidia en AMD drivers bij elkaar.
RX 6800 XT;2;0.4386560022830963;Nee dat heb ik ook. 11 jaar lang AMD gehad en had best wat problemen, maar ik dacht, prima, hoort er vast bij, totdat ik er een Nvidia kaart in knalde omdat de Vega uitgesteld was een een mooie deal kreeg op mijn 2 Fury X kaarten. Toen zag ik pas wat goed drivers konde doen. Zichtbaar minder problemen en veel sneller problemen gefixt. De stap om nu terug te gaan naar AMD is wel een ding.
RX 6800 XT;1;0.3851674199104309;Misschien lees ik er overheen... Maar de gemeten vermogen (watt verbruik). Is dat alleen de kaart of het hele systeem? #durftevragen.
RX 6800 XT;3;0.2706943452358246;Alleen de kaart, zie onderaan de pagina 'Testverantwoording'
RX 6800 XT;1;0.5821132063865662;check, heel erg bedankt!
RX 6800 XT;1;0.6787142157554626;Het review NDA op hetzelfde tijdstip zetten als de verkoop can de kaarten is wel een hele grote misser van AMD. Je moet(als je dit jaar nog wilt gamen) de kaart blind gaan kopen en alleen op basis van AMD's benchmarks?
RX 6800 XT;1;0.5907418727874756;Dan koop je toch niet? Waarom doet iedereen net alsof je verplicht bent een kaart te kopen? AMD heeft 2 data aangekondigd: - vandaag voor de reference design, waarvan er altijd een gelimiteerde voorraad is geweest in welke voorgaande launch dan ook - Volgende week voor de AIB kaarten, waar normaal 99,99% van de verkopen vandaan komen. De launch van de 5700xt was exact hetzelfde. Alleen zaten er toen geen duizenden mensen hijgend op f5 te rammen en kon men gewoon een weekje wachten totdat de echte kaarten er waren.
RX 6800 XT;1;0.8042684197425842;Je hebt nog maanden de tijd hoor, maak je geen zorgen niks is leverbaar Ze hadden me helemaal leeg kunnen zuigen (geld-wise), heb de 3080 besteld, impuls/hype was er af heb order geannuleerd. Daarna 5900x besteld, impuls/hype begint er af te raken dus twijfel om te annuleren. Wilde een 6800 XT bestellen, maar je moet kiezen of je reviews wil lezen of dat ding wil kopen, want tegelijk gaat al niet want dan ben je te laat om te bestellen, als je al weet waar je moet zijn want waarom zou iemand daar ooit van te voren helder over communiceren? Hoezo klantvriendelijk? Het is gelukkig geen straf met een 3900x en een 2080ti te gamen, dus ik heb in dat opzicht niks te klagen maar dit geeft wel een nare smaak, en waar ik AMD 3 weken terug nog het beste wenste is dat nu omgeslagen naar een negatief gevoel.
RX 6800 XT;1;0.5574066042900085;Mijn 3080 pre-order is vorige donderdag gewoon geleverd voor 719,- hopen dat die prijzen weer terugkomen voor je..
RX 6800 XT;1;0.6462961435317993;Ik vind eigenlijk dat deze reviews gewoon niet geplaatst zouden moeten worden. Pas bij echte leverbaarheid online zetten. Wat hebben we hier nou aan?
RX 6800 XT;1;0.7159917950630188;"Precies wat @Loggedinasroot dus zegt: ""Het review NDA op hetzelfde tijdstip zetten als de verkoop can de kaarten is wel een hele grote misser van AMD. Je moet(als je dit jaar nog wilt gamen) de kaart blind gaan kopen en alleen op basis van AMD's benchmarks?"" Reviews pas toestaan als het product lanceert zou illegaal moeten zijn."
RX 6800 XT;1;0.8124937415122986;Gebaseerd op wat? Je hebt geen verplichting om te kopen? Als je het niet vertrouwd koop je het gewoon even niet. Dat allerlei mensen zich helemaal gek laten maken door hype is jammer, maar dit soort onzin is ook niet nuttig. Fabrikanten hebben geen enkele verplichting om een review toe te staan voor verkoop begint. Dat ze dat doen is een service voor consumenten. niets meer, niets minder.
RX 6800 XT;1;0.4816906452178955;"Hoewel ik het op papier helemaal met je eens ben werkt onze maatschappij niet zo, een zeer klein deel van de bevolking ""verziekt"" (tussen aanhalings tekens want dit is uiteraard slechts mijn mening en geen feit) het voor de rest door alles weg te kopen op de minuut dat iets beschikbaar komt. Mocht jij als individu binnen 10 minuten (op basis van reviews) kunnen besluiten dat iets het waard is om te kopen dan ben je al te laat omdat het uitverkocht is, en dan word je verplicht om maanden te wachten op voorraden. Dus hoewel wachten in eerste instantie een keus is, is het dat 5 minuten later niet meer. Je word dus in essentie verplicht iets (gehyped/populairs) te kopen voordat je een weloverwogen keuze kan maken omdat je anders sowieso moet wachten."
RX 6800 XT;2;0.4407472610473633;Niet echt, van de 3080 waren reviews de dag ervoor Al beschikbaar...
RX 6800 XT;2;0.5364786982536316;Wel heel slecht met de beschikbaarheid. De grote vraag is hoe is het volgende week met de kaarten van de AIB merken. Hoewel verwacht is het natuurlijk heel Kwalitatief Uitermate Teleurstellend.
RX 6800 XT;2;0.417584091424942;Is het in deze niet meer Kwantitatief uitermate teleurstellend?
RX 6800 XT;1;0.5255882143974304;Volgens mij stond er bij Azerty dat de custom kaarten kwamen, maar dat de vooruitzichten erg slecht waren. En dat ook die amper beschikbaar zou zijn. Kan het fout hebben hoor.
RX 6800 XT;3;0.35686635971069336;Ik weet dat Asus al aangaf dat ze weinig kaarten zouden hebben. Maar ik heb verder nog niets gehoord over andere merken. Ik hoop dat er over de komende weken meer kaarten komen maar het lijkt de RTX 30 series te volgen.
RX 6800 XT;3;0.42398595809936523;Hele goede score van AMD, ben ik onder de indruk...nee.... Het is best jammer dat het prijsverschil niet zo groot is. Want bij het kiezen van een nieuwe GPU neem je ook de drivers mee en die zijn van Nvidia een stuk stabieler, beter. Daarnaast is de beschikbaarheid weer enorm jammer. Net als bij Nvidia. Veel mensen vielen daar over, nou...tadaaaaa, het is bij AMD niet veel beter, ook niet qua prijs. Erg benieuwd naar de 6900 XT vs 3090. Dat gaat toch wel een optie worden tenzijn Nvidia wat moois in de pijplijn heeft...de 3080 ti. Wat ik wel BS vind, dat Warzone niet wordt meegenomen in de benchmarks...is maar een game met 60 miljoen spelers...
RX 6800 XT;1;0.5511499643325806;Altijd dat gezeik -sorry voor woordkeuze- over AMD drivers. Het lijkt wel een soort van mantra. Ik zou wel eens een onderzoekje willen zien onder Nvidia en AMD gebruikers hoe ze zelf hun drivers waarderen en wat ze van het andere merk denken op dat gebied. Het zal me niet verbazen dat het groene kamp massaal zegt dat AMD slechte drivers heeft maar dat de gemiddelde beleving weinig uitmaakt.
RX 6800 XT;5;0.39809805154800415;Ja drivers spelen eenmaal een grote rol en daar is bij AMD nog wel wat winst te behalen. Dit zeggen de AMD gebruikers zelf ook. Ook ik heb jaren lang AMD gehad.
RX 6800 XT;5;0.5801447629928589;De drivers zijn al een paar maanden zeer goed.
RX 6800 XT;2;0.3372941315174103;en waar merk je dat aan ? Ik merk om me heen dat er nog steeds veel klachten zijn namelijk, Dat weerhoud mij toch wel om een AMD gpu te kopen ?
RX 6800 XT;5;0.5935123562812805;Omdat ik zelf een amd gpu heb en een vriend van mij een 5700xt en de drivers werken sinds een aantal maanden geweldig, dus daar hoef je je geen zorgen om te maken.
RX 6800 XT;4;0.34015563130378723;"Het is juist die 5700 XT waar ik veel problemen over lees. Maar goed, laatste update was van Aug dus wellicht is dat opgelost, Thanx ;-)"
RX 6800 XT;5;0.4808065593242645;De power/ performance ratio is erg indrukwekkend moet ik zeggen. Als AMD een alternatief had op Nvidia's NVENC had ik er zonder twijfel één gehaald.
RX 6800 XT;4;0.3851410150527954;Gebruik alleen NVENC als je een trage processor hebt, op een 3900x of 5900x (10900K) gebruik X264 medium preset. Zeker bij streaming geeft dit een veel beter resultaat. NVENC h265 is wel top maar CPU encoding ook. Processor encoding bij zoveel threads levert geen verlies in fps op.
RX 6800 XT;3;0.48148059844970703;Zeker met je eens, echter ben ik ook van mening dat Nvidia NVENC (H264) en x264 maar weinig verschillen op hogere bitrate. Als je het analyseert zie je inderdaad verschil maar in een live setting valt het mij persoonlijk niet op als ik op Twitch streams kijk. Daarbij is Shadowplay toch wel een heerlijke tool imo.
RX 6800 XT;3;0.41631683707237244;Op hoge bitrate zijn ook de avc en hevc van AMD prima, onder de 10.000 gaan echt de verschillen ontstaan. Daarom is bij streaming de verschillen het grootst... 6.000 bitrate of lager typisch. nvenc is inderdaad hier veel beter dan wat AMD kan, maar zoals al eerder aangegeven: als je 12/16 core processor hebt dan kun je veel beter over gaan naar cpu encoding en ben je dus verlost van welke videokaart beter is. Dan kun je voor een twitch streaming pc ook voor de 6800 XT gaan daar je streaming kwaliteit niet meer afhangt van de gpu. En wil je met toeters en bellen streamen dan kun je niet om programma's als OBS heen. Shadowplay of AMD live zijn dan toch te beperkt, zeker als je met je kanaal ook inkomsten belangrijk vindt.
RX 6800 XT;1;0.604151725769043;Wil ik eindelijk weer eens een AMD CPU + GPU proberen zijn ze beiden niet verkrijgbaar
RX 6800 XT;1;0.8555706739425659;Het is een ramp, ik wacht al sinds 7 november 2020 op mijn Ryzen 7 5800X en GIGABYTE X570 AORUS PRO moederbord besteld en betaald ik stond maandag 6e plek
RX 6800 XT;1;0.8603259921073914;Zoveel nieuwe hardware producten en erg veel bijna of niet verkrijgbaar, mensen zijn koopziek geworden door corona lijkt het.
RX 6800 XT;2;0.5758254528045654;Niet echt koopziek, meer het thuis zitten en vrije tijd te veel hebben denk ik. Ook kan je geld niet op tig manieren uitgeven,dus hebben we extra over waarschijnlijk.
RX 6800 XT;1;0.7052493095397949;nergenst niks te zien?
RX 6800 XT;1;0.49962612986564636;Alternate, die heeft een gigabyte te koop voor € 1000,- terwijl deze € 700,- moet kosten...de boeven...
RX 6800 XT;1;0.5492105484008789;Geen verkoop zo te zien..
RX 6800 XT;2;0.37185949087142944;Had eigenlijk meer verwacht van de Rage mode en SAM. Verder is AMD weer helemaal terug op GPU vlak.
RX 6800 XT;3;0.5247280597686768;Denk dat met de tijd en driver/software updates het wel iets beter gaat worden dan dit. Want anders heeft het echt bar weinig nut.
RX 6800 XT;1;0.4900057315826416;Alternate 1000 euro voor 6800xt, dan laat ik hem gaan.
RX 6800 XT;1;0.5691193342208862;Alternate: site niet beschikbaar hahah
RX 6800 XT;1;0.5815824270248413;De non XT is 900 euro, die lui zijn gek..
RX 6800 XT;1;0.4236740171909332;Zou verboden moeten zijn
RX 6800 XT;3;0.40019750595092773;Dat was het ook in de Sovjet Unie. Maar ja, dat systeem had weer andere nadelen.
RX 6800 XT;1;0.5989280939102173;"Alternate wil gewoon geld verdienen. Als distributeur kopen ze relatief grotere partijen op en trachten het te verkopen met winst. Stem met je wallet; koop daar gewoon niet meer in het vervolg. Laat ze in het moeras stikken eerlijk gezegd."
RX 6800 XT;5;0.3482189178466797;De boeven !
RX 6800 XT;1;0.4814448952674866;Dit is wel het jaar van de tekorten nietwaar. Van pleepapier tot videokaarten. Ik ga me zo langzamerhand afvragen of het echt aan de levering ligt of dat het allemaal komt door onszelf (ik denk vooral het laatste). Wij consumeren ons zo gek dat er voor bepaalde groepen niets meer overblijft. Snel terugdringen die wereldbevolking.
RX 6800 XT;2;0.34029924869537354;en geen woord over gebruik van deze kaart binnen creative suite of ander zins dan gaming. 95% van de tijd gebruik ik hem voor iets anders dan gaming. kijk wel weer bij puget voor een test die wel zinvol is.
RX 6800 XT;2;0.5469540953636169;tja dit zijn dan ook niet echt de kaarten daarvoor. De RTX kaarten bijvoorbeeld ook niet. De Quadro's zijn de creative, render kaarten die je moet hebben. Gaming kaarten performen sowieso slechter.
RX 6800 XT;4;0.3534289598464966;In de 3D industrie gebruiken ze ook wel de gaming kaarten voor gpu rendering hoor. Dus een benchmark daarvan was best leuk geweest.
RX 6800 XT;3;0.5182378888130188;is wel apart dan. Heb jaren in een architectenbureau gewerkt en dat was altijd Quadro kaarten wat gebruikt werd in alle Workstations. Gaming kaarten kwamen nooit aan te pas. Daar wordt AutoCad, Revit en noem maar op gebruikt.
RX 6800 XT;1;0.46457716822624207;dat is lang geleden of niet Duke ? RtX kaarten worden gewoon in systemen gebruikt, ook de dure Puget systemen gebruiken bij Threadripper of 5950x systemen gewoon een 3080 of 3090 kaart. of is threadripper/3090 consumenten rommel en niet pro genoeg ? voor adobe is dat meer dan logisch vanwege de Cuda's. op content creation en adobe suite gaat AMD helemaal zoek. een onverkrijgbare kaart op een onverkrijgbare cpu testen in een best case scenario, really,? waarde van dit artikel is matig at best.
RX 6800 XT;1;0.6474102735519409;Ben benieuwd... Dear Rob xxxxxx, Thank you for ordering from the AMD.com Online Store Your order has been submitted and is currently being processed. You will receive another email with the details of your order within 48 hours. ORDER DETAILS Order Number: 6578xxxxxxx Order Date: 18 November 2020 Please note: This email message was sent from a notification-only address that cannot accept incoming email. Please do not reply to this message. Sincerely, AMD.com Online Store Customer Service
RX 6800 XT;1;0.5512273907661438;ik kan die niet bij AMD.com bestellen, kan alleen bij een webwinkel die ze doorverwijzen
RX 6800 XT;1;0.4491047263145447;Digital River? Heb ik toen mijn 3900X ook besteld, die nergens te krijgen was.
RX 6800 XT;5;0.4556830823421478;Coolblue is ook goed bezig haha RX 6800 XT - 9999,-
RX 6800 XT;2;0.5219735503196716;De echte verliezer lijkt toch Intel te zijn. Die roepen al een paar jaar mee te willen doen in de videokaartmarkt. Maar ze lijken het gat voorlopig niet te kunnen dichten. Het Duopolie blijft voorlopig nog even.
RX 6800 XT;2;0.32300642132759094;Intel heeft ander huiswerk te doen, op CPU vlak.
RX 6800 XT;3;0.263288676738739;Is er al iets bekend over of er budget varianten gaan komen op deze kaarten? Daar vind je nog altijd enkel de 4.5 jaar oude Polaris. Ook de groene echte low end kaarten (1050) zijn toch al weer 4+ jaar op de markt trouwens.
RX 6800 XT;1;0.33262309432029724;€ 9,999,- bij CoolBlue
RX 6800 XT;3;0.29000240564346313;Denk toch dat ik voor kamp groen ga voor mijn nieuwe pc. Puur om DLSS en raytracing.
RX 6800 XT;3;0.33662623167037964;Vraag me alleen af, of mijn voeding nog wel meekomt...
RX 6800 XT;3;0.4622996747493744;Ik heb de juiste beslissing genomen met de RTX3080. In RDR2 is de RX6800XT minder, en dat was voor mij van belang.
RX 6800 XT;3;0.3909386694431305;Kunnen jullie in de prestatiescore games aub. ook de 99 en 99.9p gemiddelde scores erbij zetten? Die lijken namelijk zich wat anders te gedragen dan de gemiddelde en max waardes en dit is ook wel om even goed naast elkaar te kunnen zien.
RX 6800 XT;5;0.43087708950042725;Wat een beest die 6800xt! De keuze is voor mij nu wel heel makkelijk!
RX 6800 XT;5;0.4314239025115967;Wauw 🥲
RX 6800 XT;4;0.2984035015106201;Ben beniewud wat de bandwidth voordelen van Nvidia welke tot uiting komt op 4K gaat betekent voor games in de toekomst op lagere resolutie. Ik ga ervanuit dat bandwidth vereisten ook daarin groter gaan worden.
RX 6800 XT;2;0.2711176872253418;Wauw, wie had een jaar geleden gedacht dat AMD zowel de gamingprestatiekroon voor CPU's als GPU's in handen zou hebben(nou ja, wat betreft CPU's was Ryzen 3xxx al dichtbij). Niet dat je ze ergens echt kan kopen nu, maar toch...
RX 6800 XT;1;0.38945648074150085;1) Open 2021_Ryzen_5000_Upgrade.txt. 2) Backspace... MSI RTX 3070 Gaming X Trio. 3) Typing... MSI RX6800 Gaming X Trio. 4) File.. Save... Close.
RX 6800 XT;1;0.3531099557876587;Waarom wordt dlss niet meegenomen in de grafieken? Dus 3080 FE (dlss on) en 3080 FE (dlss off). In de praktijk zal een gebruiker deze functie gewoon aanzetten wanneer dit bij een game beschikbaar is. Weinig games ondersteunen dit natuurlijk, maar om het nou weg te laten in de games die het wel hebben haalt de prijs per fps uit verhouding. Je moet immers betalen voor die functie.
RX 6800 XT;2;0.40968796610832214;Ik had nog gehoopt op een undervolt & overclock analyse gehoopt - volgens de geruchten doen sommige kaarten 2,5 GHz met een undervolt en minder energieverbruik. Zou tweakers dit nog toe kunnen voegen? Ik hoop het
RX 6800 XT;2;0.4864822030067444;Frequentie is irrelevant want met hogere clocks kunnen je prestaties omlaag gaan. Testen testen testen niet je blind staren op een nutteloos iets als frequentie.
RX 6800 XT;3;0.27906885743141174;Ik vraag toch ook om een test?
RX 6800 XT;5;0.3833359479904175;2.65 GHz op lucht van wat ik lees en dan is hij sneller dan een RTX 3090 op stikstof.
RX 6800 XT;2;0.44206687808036804;Ik mis hier het kopje connectiviteit. HDMI 2.1 is een belangrijk punt voor mensen die in hun vrije tijd liever niet achter het bureau zitten, maar toch de voorkeur geven aan gamen met de PC. Ondersteunen deze kaarten het überhaupt? Is de boel compatible met gangbare modellen of zijn er indicaties dat dit niet het geval is? Qua audio heeft AMD in het verleden ook vaak het nodige laten zien met hun GPU's, is dat nu ook weer het geval?
RX 6800 XT;1;0.5144819617271423;Leuk al die grafiekjes maar uiteindelijk vind ik zowel de Ampére als de RDNA2 launch de meest teleurstellende releases in een hele lange tijd. Waarom? Omdat de kaarten simpelweg niet verkrijgbaar zijn, of tegen belachelijk hoge prijzen. Ik was van plan om voor het einde van het jaar een fikse computer upgrade uit te voeren maar met dit soort prijzen ga ik dat echt niet doen.
RX 6800 XT;1;0.5656141638755798;2x zoveel prestaties voor je geld als 6 mnd geleden vind je teleurstellend? Het is lang geleden dat er zulke grote stappen werden gemaakt. En daarbij, als je high end wil komt dat met high end prijzen...
RX 6800 XT;3;0.5250876545906067;Misschien moet je verder lezen dan de eerste zin.
RX 6800 XT;1;0.4332844913005829;Tsja had ik gedaan. 2x zo snel en goed verkrijgbaar is gewoon nog nooit gebeurd in de geschiedenis van de PC. Wacht gewoon een paar weekjes en bekijk dan opnieuw de beschikbaarheid, aantallen zullen toenemen en de prijzen normaliseren. Het gaat vast lukken in 2020.
RX 6800 XT;1;0.6770460605621338;ik zet wederom mijn vraagtekend bij deze gehele review vooral omdat bijvoorbeld gamers nexus met veel meer test ervaring heel andere resultaten laat zien in dezelfde tests. Bijna alle resultaten hier zijn slechter voor alle kaarten. Terug naar de tekentafel dus en kijken wat er in vredesnaam anders is of verkeerd gaat.
RX 6800 XT;5;0.4447180926799774;Niet alleen GN maar ook Linus...
RX 6800 XT;2;0.46246814727783203;Ja ik heb nog veel meer sites bekeken en bij sommige is er gewoon een enorm verschil te vinden. Die uitersten gooi ik er dus ook uit en negeer ik.
RX 6800 XT;3;0.3803391456604004;Zou je dit concreter kunnen maken? Feedback is altijd welkom, met specifieke punten schaven we onze testprocedure regelmatig bij. Dat andere publicaties andere resultaten laten zien zal primair komen door verschillen in testsystemen, games en benchmarks. Dat alle testresultaten bij ons 'slechter' zijn betekent niet automatisch dat de verhoudingen onderling anders liggen, alleen dat onze benchmarks zwaar zijn voor de GPU's, wat ook exact de bedoeling is bij het testen van 3D-chips.
RX 6800 XT;3;0.28185102343559265;kijk filmpje maar eens
RX 6800 XT;2;0.4058406352996826;Als de RTX3080Ti uitkomt gaat dat de kaart worden denk ik. De RTX3080 met z'n 10GB blijkt nu het blok aan het been te zijn tegenover de RX 6800 XT Vroeg me al van in het begin af, waarom 10GB ?? En geen 12GB of ook 16GB net als AMD Prijs/kwaliteit is de RX 6800 XT de meest interessantste.. maar voorlopig blijft m'n RTX2080Ti OC de hoofdrolspeler, m'n tweede computerbuild moet dan maar tot volgend jaar wachten in de hoop dat er dan een deftige GPU uitkomt dat het verschil gaat maken in 4K.. want eerlijk gezegd is de prestatiewinst niet wat ik verwacht had..
RX 6800 XT;1;0.4508025646209717;Als ik alle info zo lees kan ik alleen maar concluderen dat er nog teveel variabelen zijn.Beiden hebben een aanlever probleem.Je zit met third-party builds die ook zich nog moeten bewijzen.Drivers staan nog niet op punt en de bugs zullen er nog uit moeten, kwestie van tijdDe benchmarks zijn nog te beperkt en zijn meer dan ooit afhankelijk van de cpu en andere componenten.ray tracing...Dit zijn voor mij toch nog grote vraagtekens bij eender welke kaart deze periode op de markt komt. Conclusie: ik wacht tot ten vroegste maart om genoeg informatie te hebben over deze dure aankoop.Pas dan weet je wat je koopt.
RX 6800 XT;2;0.36560750007629395;AMD doet alles goed totdat je de prijs hoort. Zaten ze vroeger nog vaak onder de prijs van Nvidia, zitten ze er nu zelfs boven! Koekoek.. De prijs zal toch echt een klap lager moeten zitten dan het equivalent van Nvidia, zoals ze wel doen met de RX 6900 XT tov RTX 3090, 1000 euro vs 1600 euro. Helaas denkt AMD dat dit niet nodig is voor de andere kaarten.
RX 6800 XT;5;0.26932036876678467;AMD zit nu hoger in de prijs dan vergelijkbare Nvidia kaarten.
RX 6800 XT;1;0.3976393938064575;Hoe zouden de resultaten er uit zien met een Ryzen 3600X ipv 5950, dus zonder Infinity Cache of SAM? Ik wil nml enkel m'n videokaart vernieuwen, niet m'n hele systeem.
RX 6800 XT;5;0.41557157039642334;Nvidia draait nog steeds sneller, zie Linus Tech op youtube.
RX 6800 XT;2;0.517743706703186;Er zijn te veel maren om die conclusie juist te noemen, hij doet dat ook niet. b.v. op 1440P kun je geen snellere kaart kopen dan de AMD
RX 6800 XT;3;0.5460795164108276;Mijn eigen voorkeur blijft toch Nvidia, heeft meer voordelen. Ik vond de conclusie van Tweakers nogal kort door de bocht om voor alle toepassingen AMD te kiezen.
RX 6800 XT;5;0.4955567419528961;Je kunt natuurlijk altijd je eigen voorkeur hebben, ieder zijn smaak toch.
RX 6800 XT;3;0.310680627822876;Rare conclusie want hier wordt toch een andere conclusie getrokken.
RX 6800 XT;3;0.4227117896080017;Ik vind het niet zo gek dat de AMD GPUs niet te krijgen zijn. AMD bouwt de CPU en GPU voor de Xbox, voor de PS5 en voor de desktop. Dat zijn héél veel chips.
RX 6800 XT;3;0.40433937311172485;Zijn er manual OC resultaten?
RX 6800 XT;2;0.29456403851509094;Wat misschien een goede racegame is, is Assetto Corsa Competizione, deze vraagt veel van een GPU. En de simwereld springt om benchmarks qua GPU's om dat de 6800(XT) en de 3xxx serie te zien. Misschien afgezet naar een 1080Ti, 2080Ti n 5700xt zodat men ook ziet hoe ze performen tov oudere generaties. Dit is bericht 372 oid, en staat op p3, geen hond die dit leest.
RX 6800 XT;5;0.2561784088611603;Jawel hoor
RX 6800 XT;4;0.23950062692165375;En ik ook!... Want dit is precies waarom ik geïnteresseerd ben in de 6800. Draai op mijn PC vooral (niet enkel, maar wel 90%) racegames, en tegenwoordig liefst ook in VR met de Quest 2, naast mijn ultrawide op 3440x1440. Draaide dat voorheen op een GTX 1660 Super, en dat draaide ok-ish, maar veel getweak nodig met de instellingen. Nu recent van een andere Tweaker een GTX 1080ti overgenomen, die het met grote stappen beter doet. Gezien het prijsniveau kijk ik nu ondertussen naar de 6800 (en heb ook een soort van bestelling ervoor, maar weet niet of t gaat lukken) en de RTX 3070. Er is wel 1-en-ander op YouTube te vinden, maar ik probeer nu adhv de standaard benchmarks (1080 / 1440 / 4K) te bepalen wat het ongeveer voor mij zou betekenen op die ultrawide en in VR. Er is een vuistregelen hier ooit geplubliceerd: 19% aftrekken van 1440p resultaten, of 40% optellen bij 4K resultaten voor de 3440x1440 indicaties. Daar houd ik me maar even aan vast.
RX 6800 XT;1;0.849811315536499;allemaal irrelevant als zowel nvidia als amd niets leverbaar heeft
RX 6800 XT;5;0.24562475085258484;Ik zie inmiddels wat prijzen in de webshop van Alternate verschijnen: ASRock Radeon RX 6800 16GB EUR 899 incl BTW (preorder) GIGABYTE Radeon RX 6800 XT 16G EUR 1049 incl BTW (uitverkocht) ASRock Radeon RX 6800 XT 16GB EUR 999 incl BTW (preorder)
RX 6800 XT;1;0.37410274147987366;"Haha, ""Rage mode"", back to 2001? Maar zijn de AMD drivers nog even ruk als tien jaar terug? Wat een ellende was dat zeg."
RX 6800 XT;1;0.7710349559783936;nVIDIA biedt zoveel meer prestatie winst met raytracing, cuda, alle andere opties die nVIDIA biedt. AMD heeft een kaart die kaal meekomt maar verder totaal niets anders biedt. Sterker, de media encoders e.d. zijn zo slecht van AMD dat het eindresultaat onbruikbaar is. LinusTechtips heeft hier een aardige video over gemaakt. AMD vraagt een stuk meer voor zijn kaarten maar levert een karig product af. Wat er in de toekomst door AMD nog toegevoegd wordt is maar af te wachten.
RX 6800 XT;5;0.2905336320400238;Eindelijk concurrentie!! Maar langs de andere kant weet intussen iedereen dat de 3070/3080 standaard een te hoge voltage mee krijgen. De meeste Tweakers onder ons undervolten deze kaarten waardoor ze makkelijk 100W minder verbruiken en beter presteren zonder de powerlimit aan te tikken. Dus in feite minder stroom gebruiken dan de AMD tegenhangers en dan weer meer fps genereren. Overigens stel ik me ook vragen bij de SAM benchmarks, ik zou de benchmark overdoen op een Intel systeem waar SAM momenteel niet mogelijk is dan zie je toch andere resultaten dan op Tweakers. Het is wel nice van AMD dat SAM opensource is waardoor dat SAM of een variant er van naar Ampère komt waardoor de cijfers nogmaals gaan veranderen. Ik persoonlijk ben voor een Intel + 3080 gegaan want ik heb geen zin om nog allerlei software te moeten installeren en gebruiken om AMD's CPU's deftig te laten werken, waar je met Intel meteen klaar mee bent. Ook hun Radeon software geraak ik niet wijs van, ik ben mijn Nvidia control panel zo gewoon, het ziet er misschien outdated uit maar alles is zeer duidelijk. Maar dit is gewoon persoonlijk. Again: top dat er concurrentie is, eens de kaarten van Nvidia en AMD goed verkrijgbaar zijn, kunnen ze aan de prijzen oorlog beginnen!
RX 6800 XT;2;0.3794441819190979;Ga er van uit dat als ik dus een raytracing kaart wil voornamelijk voor dxr/rtx aangedreven productivity (voornamelijk lighting baken in unreal en unity) nog steeds beter af ben bij het groene team vanwege een volwassenere raytracing chip?
RX 6800 XT;3;0.38513192534446716;De afgelopen week veel benchmarks voorbij zien komen ook op alle YT kanalen en overall komt AMD met de 6800XT wel als winnaar uit de bus ten opzichte van de Rtx3080 als het gaat om de FPS score. Zelfs indien men geen gebruik maakte van een Zen3 CPU. Tussen de 6800 en de 3070 ligt dit toch wel dichterbij elkaar en is het op benchmark niveau toch wel de 6800 welke triomfeert icm Zen3. Dit uit zich niet altijd even uitdrukkelijk in de fps score want indien er geen Zen3 systeem word gebruikt zijn er ook games waarbij de 3070 het aanzienlijk beter doet. Hierin blijft staan dat Nvidea überhaupt beter presteert op het gebiedt van Ray Tracing. Hier tegenover staat dan weer het verschil in Vram, 8GB bij de 3070 vs 16GB bij de 6800. In theorie zou je verwachten dat de 6800 hierdoor in de toekomst een groter gat gaat slaan ten opzichte van de goedkopere 3070.
RX 6800 XT;5;0.5368270874023438;Inmiddels komen de RTX3070's goed op gang qua voorraad. Ik heb er iig 1 kunnen bemachtigen, en het is een beest op 1440p ultra.
RX 6800 XT;5;0.30858051776885986;Welke heb je?
RX 6800 XT;2;0.3737355172634125;Corona, dus dichte fabrieken en zieke medewerkers En AMD moet ook nog eens leveren voor zowel de PS5 als de XBOX Series S/X En ik denk helaas dat de consoles in ieder geval tot aan kerst echt de prioriteit hebben Die worden namelijk gewoon veel en veel meer verkocht
RX 6800 XT;2;0.31842154264450073;En hebben veeeeeeel minder marge, maar het zullen vooral afspraken zijn waar ze zich aan moeten houden, want je verkoopt liever 10 apparaten met 100 euro winst dan 1000 chips met 50 cent winst
RX 6800 XT;2;0.375349223613739;Ja inderdaad De marges zijn veel lager, maar de contracten zullen ze waarschijnlijk dwingen om aan Sony en Microsoft te leveren Overigens is dat voor AMD ook wel weer chill, want dat betekent dat een groot deel van de games dadelijk al geoptimaliseerd is voor Big Navi, en de games dus op PC waarschijnlijk ook beter presteren op de nieuwe AMD kaarten
RX 6800 XT;3;0.4477013647556305;Dat wordt vaker geroepen maar dat was met de PS4 en Xbox ook al zo die waren ook RDNA (1) gebaseerd. Dus persoonlijk geloof ik er niet echt in dat dat verschil maakt, maar ik hoop dat je gelijk hebt
RX 6800 XT;2;0.31190136075019836;RDNA is pas in 2019 uitgekomen De PS4 en XBOX waren hier dus niet op gebaseerd En bovendien was dat ook nog eens in een tijd waarin AMD niet kon concurreren met Nvidia in de high-end Nu kunnen ze dat wel, en als de prestaties normaal gesproken redelijk gelijk zijn krijg je dan misschien net het voordeel van die optimalisatie
RX 6800 XT;2;0.33241236209869385;Ter nuance: men gaat er al nu direct vanuit dat er weinig voorraad is waardoor 90% van de kopers direct bij launch zit te F5-en. Verkopen die normaal over de eerste weken uitgesmeerd worden komen nu allemaal direct de eerste minuut binnen. Feit blijft inderdaad dat de vraag groter is dan het aanbod. Ik denk persoonlijk dat dit te maken heeft met het feit dat er dit jaar eindelijk weer significante stappen gemaakt worden qua performance waardoor iedereen wil upgraden. Dat in combinatie met de coronaciris waarin men sowieso al veelal thuiszit zorgt voor een flinke toename in vraag.
RX 6800 XT;2;0.37829142808914185;Vlak ook de problemen in de distributie niet uit. Ik hoor op veel plekken over voorraad problemen. Niet alleen in de IT, maar electronica in het algemeen. En niet alleen over eindproducten, maar ook over reserve onderdelen en halffabrikaten.
RX 6800 XT;2;0.4090813100337982;Ja als je net een 3080 hebt gekocht lijkt me het raar om dan gelijk over te stappen naar een andere kaart, zou wel heel snel wezen.
RX 6800 XT;2;0.3443606197834015;Was ook niet van plan om over te stappen maar zoveel mensen roepen omdat ze geeen 3080 krijgen.. Ik wacht wel op die van AMD!!! en nu is die nog schaarser en minder performance dan de 3080. te grappig
RX 6800 XT;2;0.45207884907722473;Ik weet niet welke benchmarks jij hebt gekeken maar de 6800xt is sneller dan de 3080 en de 6800 is sneller dan de 3070, zo grappig is het niet.
RX 6800 XT;4;0.3630521297454834;"Dit jaar zeker; veel mensen die normaal gesproken op vakantie gaan die dit nu niet doen en hierdoor vakantiegeld/kerstbonus aan iets anders uit kunnen geven. Daarnaast zijn veel mensen bereid om grof geld neer te tellen voor hun favoriete hobby, maar dit is natuurlijk per persoon verschillend."
RX 6800 XT;3;0.49849167466163635;Ja, blijkbaar...zal wel te oud zijn om het te begrijpen
RX 6800;1;0.8284943699836731;Proberen ze deze uberhaupt te verkopen? Nergens te koop.
RX 6800;1;0.5578858852386475;Wellicht kan je met deze info wat meer? Staat per land gespecificeerd welke vendors het gaan verkopen.
RX 6800;1;0.4164920151233673;Ben ik de enige die geen West-Europa ziet staan ?
RX 6800;5;0.364202618598938;Bijzonder, deze stond er eerder wel gewoon Zie hier
RX 6800;1;0.7548179626464844;Ik dacht even dat ik scheel was. Echt heel deze launch is te absurd voor woorden.
RX 6800;1;0.2799822688102722;Had ik ook, dacht dat ik scheel was ofzo.. om gek van te worden
RX 6800;3;0.4924485683441162;Ondertussen staan ze er wel weer tussen.
RX 6800;3;0.28614237904548645;Voor de genen die niet geloven dat Megekko CDROMLand is, hier kun je het ook zien dat Megekko niet als amd partner staat.
RX 6800;4;0.4399060010910034;Werkzaam bij Megekko. Magazijn is grotendeels Megekko, klein deel CD-ROM. Werknemers doen werkzaamheden voor beide zaken.
RX 6800;5;0.3355713188648224;CD-ROM-LAND en Megekko opereren allebei onder de koepel van Shopping pour vous waarbij CD-ROM-LAND naast de webshop i.t.t. Megekko ook nog een fysieke winkel heeft:
RX 6800;1;0.5244147181510925;Bedankt, wist geeneens dat CD-ROM-LAND en Megekko bij de zelfde horen, en is toch wat dat ze nergens al meer te koop zijn de AMD Radeon RX 6800 (XT), en als ze wel te koop zijn, ze ontzettend duur zijn. Erg mooi om te zien dat AMD met hun nieuwe Radeon RX 6800 XT, Nvidia's GeForce RTX 3080 bij kan houden, en dat ze bij meerdere spellen zelf iets of net zo snel zijn, en dat de Radeon RX 6800 sneller is gemiddeld dan de GeForce RTX 3070, het woord voor mij deze keer de Radeon RX 6800 XT, als ze weer normaal te koop zijn en de prijzen weer rond de €650,- zijn. Ben echt blij dat AMD weer is een erg snelle Grafische kaart uit brengt, zo dat er eindelijk weer concurrentie is, en er normaal voor gaat zorgen dat de prijzen omlaag gaan. Edit foutjes
RX 6800;1;0.49876636266708374;Ja ik heb het zelf ook niet geweten dat CD-ROM-LAND en Megekko onder zelfde dak zaten. Ik kwam pas erachter nadat ik zeer slechte ervaring had met after-sale service bij Megekko en mijzelf beloofde nooit meer daar iets te kopen. Later iets gekocht bij CD-ROM-LAND. Heel toevallig ook daar ik te doen gehad met zeer slechte after-sale service die zeer veel leek op Megekko. Aan de telefoon vroeg ik rechtuit of ze familie waren van elkaar. Hoezo vroeg hij? Welnu, goedkope prijzen, makkelijk te kopen maar exact zelfde after-sale service praktijken. En toen was het stil aan de overkant.
RX 6800;1;0.7777012586593628;Ja erg triest, en ik vraag me af hoe veel andere bedrijven bij elkaar (zelfde eigenaar) horen net als CD-ROM-LAND en Megekko, want dan betekent het dat we helemaal niet veel concurrentie hebben, op de tech gebied in Nederland als Tweakers ook al niet, Hardware.Info is laatst ook al over genomen door DPG Media, en is dus geen concurrent meer van Tweakers die ook bij DPG Media hoort. Ik vraag me echt af van alle winkels in Pricewatch echt nog van een eigenaar is en welke allemaal van de zelfde zijn maar onder een andere naam.
RX 6800;5;0.24219846725463867;Je kan via de KVK achterhalen welke bedrijven allemaal bij elkaar horen. Hoeft niet eens iets te kosten.
RX 6800;2;0.32529211044311523;Dat bedrijven dezelfde eigenaar hebben hoeft niet te betekenen dat ze hetzelfde zijn of dat het nadelig voor de klant is. Zo kunnen ze zich bijvoorbeeld op andere delen van de markt richten of onderscheiden met extra diensten die je bij het zusterbedrijf niet krijgt.
RX 6800;2;0.3522353768348694;Dat zal, maar het is dan nog steeds geen concurrent, dus het aantal concurrerende bedrijven is 1 minder dan als ze een verschillende eigenaar hadden.
RX 6800;1;0.7292623519897461;Het is 'gewoon' het huidige marktmodel van machtsbeluste grijpgrage investering maatschappijen, concerns en andere spelers die macht van en op de markt willen. We gaan helaas naar de tijd dat de prijzen door slechts nog een paar grote bedrijven worden bepaald. Zowel de inkoop als verkoopprijzen. Neem de supermarkten, internet, mobiele providers.. allemaal zijn deze sectoren in handen van hooguit 3 grote spelers, met daaronder wellicht wat sub merken/bedrijven. En de overheid doet daar niets tegen... en staat dit soort, ik noem het maar maffia en kartel praktijken gewoon toe.
RX 6800;2;0.43749529123306274;Ja precies, het is heel triest, en je doet er jammer genoeg niks tegen, ja wij met zijn alle maar dat gebeurd jammer genoeg niet.
RX 6800;3;0.3001909852027893;Dit staat ook op hun website.... ze zijn zo goedkoop omdat de service minimaal is... dat is dus het risico als je daar koopt. We doen niet alles. Maar wat we doen, doen we graag en goed. We kaderen graag af wat we doen. Zo kun je bij ons niet terecht voor advies of telefonische ondersteuning. Dat wil niet zeggen dat je bij ons geen service krijgt.
RX 6800;4;0.3314206898212433;Idd erg mooi als je denkt dat hij bijna het rtx geweld kan temmen. Het zijn maar minieme vergelijkingen dat de nvidia sneller is, met sommige meetingen is nvidia zelfs langzamer.Amd heeft het goed voor elkaar en is nu eindelijk ook in het hogere top segment te vinden. Ik kan alleen maar zeggen tegen amd ga zo door, gooi er nog een paar duizend cores ertegen aan en dan zijn ze misschien even snel als nvidia.
RX 6800;3;0.43618491291999817;Denk met een paar driver optimalisaties dat ze de performance wel een procent of 5 hoger krijgen of misschien wel meer. AMD heeft lange tijd gewoon de betere architectuur gehad, alleen was het niet of haast niet ondersteund worden in windows e.d. het probleem. De hardware was vaak de tijd vooruit. GCN was indien goed geïmplementeerd veel beter dan nvidia. Denk dat als het zoals in het verleden het is gegaan, AMD op den duur met deze serie na wat driver updates net wat sneller zal zijn. Ik zie dat mijn RX580 en 5700XT sinds release echt beter zijn gaan draaien, en de 2070 kaarten en 1060 kaarten juist minder of niets zijn verbeterd qua performance.
RX 6800;3;0.4592926800251007;Het is onderdeel van CDROMLand en gebruiken voor zover ik weet het zelfde magazijn, maar ze hebben niet exact het zelfde asortiment.
RX 6800;3;0.3349469304084778;Allebei onderdeel van Shopping pour vous BV, zie: voor meer info over de merken die zij onder zich hebben. Enkele merken van Shopping pour vous BV:CD-ROM-LAND - - -
RX 6800;3;0.3391985595226288;Opzich niets mis mee zolang ze deze bundeling maar gebruiken om de prijzen laag te houden
RX 6800;3;0.36862894892692566;Uiteraard. Had dit alleen voor de informatie gepost.
RX 6800;3;0.42664241790771484;Ik snapte em Overigens heb ik zelf nooit geen issues gehad met Megekko.. Aardig wat besteld en altijd goede service gehad wanneer nodig.
RX 6800;5;0.3962583541870117;Ja, ik ook. Ze zijn top. Daarom is het ook positieve reclame. Het bedrijf daarachter doet toch iets goed
RX 6800;2;0.32446831464767456;Megekko staat er toch gewoon tussen in het rijtje? Onderin?
RX 6800;1;0.3958825469017029;"Ik zie bij Alternate 2 6800XT's staan.. vanaf 999,--.. En de ""gewone"" 6800 vanaf 899,--.. Hahaha.. Ik zou nog ff wachten.."
RX 6800;1;0.522767186164856;Nee hoor, gewoon meteen bij Coolblue reserveren, voor... €9.999 voor de XT, of dezelfde prijs voor de gewone Coolblue weet heel goed hoe je scalpers tegen moet gaan, gewoon veel hogere prijzen vragen dan welke scalper dan ook zou doen.
RX 6800;1;0.5649676322937012;Reserveren is niet mogelijk bij coolblue, ook niet voor 9999. Je kunt alleen een melding krijgen als het product op voorraad is.
RX 6800;1;0.4249734580516815;Ja beetje rare invoer van prijs. Houd dan msrp aan + 50 euro ofzo. Dit schrikt alleen maar mensen af.
RX 6800;1;0.6685559153556824;"Ik denk dat ze daarmee bedoelen ""reserveer voor dat geld en krijg daaena het verscil terug."
RX 6800;1;0.7471894025802612;Shame on you Alternate!!! Adviesprijs van 669 usd, aan de huidige koers is dat 570 euro. Als ze dan nog 1 op 1, 669 euro zouden vragen dan is die 999 euro voor een 6800XT nog steeds volledig onaanvaardbaar. AMD komt met goede producten aan een goede prijs om de markt naar een hoger niveau te brengen voor iedereen en dan heb je van die winstjagers als alternate die de winst dan simpelweg bij hun verhogen zodat de consument weer de dupe is...
RX 6800;1;0.4774305820465088;USD met NL Euro prijzen 1 op 1 vergelijken gaat helemaal niet op want de USD prijzen zijn altijd ex-VAT belasting waar de NL prijzen altijd incl. 21% BTW zijn.. 669 USD = 570 euro + 21% is 689 euro.. Dat is de 1 op 1 prijs, alleen kan daar in de VS dus nog tot zo'n 8% VAT bijkomen.. MSRP (adviesprijs) in USD is altijd ex belasting omdat dat per staat en soms zelfs per stad verschilt daar.. De VAT loopt daar van 0% tot +/- 8%.. Sowieso veel minder dan de standaard 21% hier..
RX 6800;1;0.45206746459007263;Dat was ik helemaal vergeten, maar dat klopt als een bus. Zelfs als je daar in de winkel loopt (VS) dan zie je vaak prijzen zonder belasting. Daar had ik me goed in vergist, daar stond ik bij de kassa met exact het bedrag op het product. Mocht ik ineens meer betalen. Na uitleg van de medewerker werd het me duidelijk.
RX 6800;1;0.26565229892730713;Had ik idd de eerste keer in Panama ook, behoorlijk lopen bitchen dat ze gewoon de juiste prijs moeten vermelden, kreeg het mee met de belasting als korting 😂
RX 6800;1;0.38009530305862427;Kun je in zo'n geval de BTW niet gewoon terugkrijgen op het vliegveld?
RX 6800;1;0.6083464026451111;Geen idee, niet geprobeerd
RX 6800;3;0.5016815662384033;Vorig jaar in Singapore wat gekocht, en daar kregen we een speciale bon mee om belasting terug te vorderen op het vliegveld bij vertrek. EU en USA hanteren volgens mij ook dergelijke regelingen, maar je moet er wel ff wat extra voor doen. Meestal is een bon waaruit blijkt wat je aan VAT/BTW betaald hebt voldoende, maar je moet altijd ff uitzoeken hoe het in zijn werk gaat, want dat kan per land nogal verschillen.
RX 6800;1;0.25607314705848694;@trapper Nog even wat oude artikelen aan het doorspitten? Maar inderdaad, zelfde ervaring bij mij met spullen uit Japan.
RX 6800;1;0.4451516568660736;Ja, ik had van de 6900 aankondiging doorgeklikt naar dit artikel over de 6800. Even niet meer bij stil gestaan dat dit al een oud artikel was...
RX 6800;1;0.9028043746948242;Haha gekkenhuis. Is voor niets.. Doe er maar 2.. Gewoon heerlijk verschrikkelijk slecht dat ze de prijzen dan zo omhoog gooien. Ik vind het gewoon crimineel
RX 6800;3;0.34490934014320374;Zo, je kan wel zien dat Duitsland een belangrijke markt is.
RX 6800;2;0.3580911159515381;West europa staat er al niet eens meer tussen.
RX 6800;1;0.43719807267189026;"Uit het artikel; Nergens voorraad. Ik had verwacht dat AMD niet beter verkrijgbaar zou zijn dan Nvidia, maar dit is wel heel extreem. Dit is een échte paper launch. Overigens geeft Alternate aan nog in 2020 te kunnen leveren (wel voor 899). Wie persé een kaart wil kan die deal maar beter accepteren. Zo lang Nvidia leverproblemen heeft zal er uberhaupt extra vraag ontstaan. Positief punt uit de TPU review: Eindelijk de multi monitor bug opgelost ."
RX 6800;1;0.37229081988334656;De multimonitor bug was al gefixt in de AMD vega familie. Dankzij HBM2.
RX 6800;2;0.324444442987442;Navi heeft er last van, zo gaat een 5700XT idle van 8W naar 36W wanneer je er meer dan één scherm op aansluit. Zie
RX 6800;2;0.33201393485069275;"Klopt, maar Vega 64 had geen last van dit probleem: Multi-monitor power consumption seems solved, finally. With 17 W, the power increase over single-monitor idle is just 3 W, which is a huge improvement over the 15 W or so that were previously added in that state (on Polaris); the same goes for media playback. Dus onderstaande geldt niet: I didn't think I would see the day where AMD fixes its multi-monitor power consumption. Finally, the memory no longer runs at full speed in that scenario, which greatly reduces power, down to 7 W, which is now much better than NVIDIA."
RX 6800;1;0.261769562959671;Alsof iemand een Vega 64 heeft of gaat kopen. 5700XT is een populaire kaart en dus de referentie.
RX 6800;1;0.7493893504142761;Op diverse sites waren mensen er van overtuigd dat amd de launch beter voor elkaar zou hebben etc. Maar bij die mensen was het sowieso allemaal slecht wat Nvidia heeft gedaan. Had zelf nooit het idee dat deze launch beter zou zijn dan die van Nvidia. Zo is het nu eenmaal bij zo ongeveer elke release van high end zooi. Heb 1* meegemaakt dat er bij een high-end launch veel beschikbaar was. Jaren geleden bij een nvidia release, er waren geen leaks etc, opeens de aankondiging en dag erna hadden alle shops kaarten.
RX 6800;1;0.6532946228981018;Dit komt ook door de slechte communicatie vanuit AMD + het feit dat AMD later lanceerde. Tweets zoals deze werken in ieder geval niet mee.
RX 6800;2;0.4171851575374603;Klopt door die tweet dacht ik ook dat er voldoende voorraad was - nu ook wel teleurgesteld.
RX 6800;1;0.6713902354240417;En dan gaat ie gewoon nog even door met een tweet waarin hij pronkt met een screenshot waarop zijn eigen order van een Radeon kaart op de site van AMD is geslaagd. Was waarschijnlijk de enige kaart die ze beschikbaar hadden Die $10 dollar is ie wel kwijt. Dit is nog een slechtere launch dan die van Nvidia, waar ze bij AMD zo'n grapjes over aan het maken waren. En niet alleen de GFX kaarten zijn paper launches, maar ook die van de 5950x, die in geen velden of wegen te bekennen is en die ik natuurlijk weer net wil hebben
RX 6800;1;0.47344595193862915;De communicatie geeft het idee dat de beschikbaarheid goed zou zijn, maar dat is niet perse wat hij zegt. AMD verscheept op het moment een krankzinnige hoeveelheid chips, de vraag is daarentegen ook enorm. Vergeet niet dat AMD ook de hardware van de PS5 en Xbox levert. De consoles gebruiken ook RDNA2 en Zen3. Daar is gewoon niet tegenop te produceren. Is het een paper launch? Nee zeker niet. Zijn er enorme tekorten door enorme vraag? Ja. Niet zo gek, met de volledige consolemarkt naast een goed deel van de desktopmarkt.
RX 6800;1;0.4243924915790558;Bij die tweet dacht ik al wat men dan verstaat onder een paper launch. Aangezien er wel kaarten te koop waren. Maar het waren er zo weinig dat het dan toch weer dicht bij een paper launch komt.
RX 6800;5;0.3892468810081482;"Ach, op deze manier houden we allemaal een heleboel geld over aan het einde van het jaar! ;-)"
RX 6800;4;0.3964531123638153;Goed advies om een deal te adviseren die 250 euro duurder is Dan de normale prijs..
RX 6800;1;0.6455442905426025;Let even op het woordje persé . Het zit er dik in dat de situatie de komende maanden niet verbeterd. Er is nu nog geen zicht op verbetering puur omdat we niks horen. Als je bereid bent maanden te wachten dan wel een alternatief van Nvidia te overwegen (niet dat die nu leverbaar zijn, maar wellicht februari/maart) dan zou ik zeker die pre order niet plaatsen. Maar dan wil je hem dus ook niet persé hebben.
RX 6800;1;0.39951664209365845;Heb m Al binnen voor de normale prijs van 719,- als je hem persé wou hebben Dan je gepre-ordered toen de prijzen nog niet inflated waren geraakt.
RX 6800;1;0.38067105412483215;"Als we dan toch op onze woordjes letten, het is ""per se"", met spatie en zonder accent aigu."
RX 6800;1;0.47616586089134216;"Op de site van Megekko: ""AMD Radeon 6000 videokaarten: Wat is geleverd en wat wordt verwacht Officieel is vandaag, 18-11-2020 de launch-date van de nieuwe AMD Radeon 6000 videokaarten. Echter, het betreft alleen de reference designs van AMD. Deze kaarten zijn helaas nog nergens verkrijgbaar en het is ook niet aannemelijk dat deze op korte termijn beschikbaar komen. Zodra wij meer informatie hebben, updaten we jullie via deze pagina."""
RX 6800;3;0.3517521917819977;AIB's mochten kiezen om geleverde chips in de reference design te zetten of te gebruiken voor hun eigen kaarten. Het overgrote deel van de chips is natuurlijk in AIB kaarten gezet. Die zijn er vanaf volgende week, dus dat is wanneer we mogelijk wat in winkels zien (in Europa iig)
RX 6800;1;0.45370009541511536;Dit lijkt meer op een nieuw distributiemodel. De fabrikant (AMD/nVidia) verkopen de eerste batch direct aan consumenten en doordat ze een groot deel van de oude keten overslaan, levert dat veel op. Dit is een geleidelijke stap om de keten te verkorten.
RX 6800;2;0.4467008709907532;Klopt, maar er spelen een aantal zaken. Nog niet zo heel lang geleden kwam het geregeld voor dat AIB's met kaarten kwamen die niet voldeden aan de specs, te laat met nieuw kaarten kwamen of samenstellingen maakten die de bestaande line-up en marketing van de fabrikant beschadigden. Het reference model is precies dat. Een baseline voor testers, die voor iedereen hetzelfde is. AIB kaarten die slechter presteren dan de baseline worden genadeloos door de pers afgebrand. Betere prestaties kunnen worden afgezet tegenover de baseline etc. AMD heeft hier een moeilijke strategische keus gemaakt. Door AIB's meteen hun eigen kaarten te laten maken lijkt de initiële voorraad erg slecht. Aan de andere kant heeft AMD die AIB's extreem hard nodig. Anders dan NVIDIA kan AMD de AIB's niet een middelvinger opsteken en controleren. De klanten van AMD zijn nu eenmaal de AIB's, niet consumenten. Hoewel, zoals je aangeeft, ze dat misschien anders zouden willen zien.
RX 6800;3;0.254252552986145;Pre order op Alternate voor €999
RX 6800;2;0.4775889217853546;Zwaar teleurstellend dit, beschikbaarheid is een ding kun je niet veel aan doen, maar eventjes snel een extra 40% winst maken is echt diep triest.
RX 6800;3;0.3370038866996765;Ik snap je reactie, maar dit is marktwerking. Vraag en aanbod bepalen de prijs. Als verkoper kan je er maar een paar verkopen en door het gebrek aan aanbod geraken die dingen toch verkocht. Kan je als verkoper lekker cashen. Adam Smith - The invisible hand
RX 6800;1;0.5475336909294128;Daar ben ik het niet met eens. Je kan eenmalig incasseren. Maar wat betekent die 300 euro nu voor hun? Zelfs als hebben ze 10 kaarten. 3000 euro klinkt fijn voor jou en mij, maar als filiaal en al helemaal als keten is dat echt gewoon een druppel. Ik heb als student in het weekend gewerkt voor office center in België. Het devies daar was. Een klant krijgen is niet moeilijk, ons doel is om een terugkerende klant te hebben. Een winkel als alternate, die naar mijn weten nooit vecht op prijs (en als ze het doen zijn ze er belabberd in want ze zijn meestal duurder), vecht dus op basis van service en advies (die naar mijn ervaring beide wel goed zitten). Als daar je kracht zit voor terugkerende klanten en loyaliteit, is zoiets flikken niet iets dat het sentiment van je klanten positief bevordert. Nou ben ik geen adviseur voor een winkel zijn marketingstrategie en ieder kiest wat zij willen. Maar gewoon zeggen dat vraag & aanbod dit reguleren is niet van toepassing hier. Je kan genoeg artikels vinden die zeggen dat vraag & aanbod stuff stuurt, maar dit kan je niet op alles zomaar toepassen. Edit: gezien de reacties die er al komen op die prijs is dit inderdaad iets dat eerder negatief dan positief werkt. Je wilt geen toekomstige klanten een wrange smaak geven over je keten.
RX 6800;1;0.31020545959472656;Helemaal waar. Gewoon een onwetende opmerking van sommigen. Ik zou liever tientallen kaarten verkopen met wat minder winst dan een paar met veel winst. Bovendien, mensen kopen waarschijnlijk er ook nog wat extra's bij, dus meer klanten is beter dan enkelen.
RX 6800;2;0.41012895107269287;Alleen gaat je logica niet op op het moment dat het aanbod beperkt is en je te weinig product geleverd krijgt. Het is dan kiezen tussen een paar klanten iets verkopen voor een lage marge en een paar klanten iets verkopen met een hoge marge. Leuk voor de consument? Nee.
RX 6800;3;0.6964059472084045;Heb je ook gelijk in, maar de stelling dat het lekker cashen is zoals sommigen beweren vind ik wat kortzichtig.
RX 6800;3;0.3717458248138428;Nog steeds eigenlijk hoor. Je gaat misschien die paar kaarten dat je had verkocht hebben met een grotere marge. Maar waarom? Zoals ik aangaf zijn die bedragen niets voor retailers. Maar je hebt wel genoeg klanten die je prijs zien en denken wat een klootzakken. Dan kan je beter de adviesprijs aanhouden en er iets leutig rond doen naar mijn mening.
RX 6800;1;0.3493594229221344;Hoe verklaar je dan die hoge prijzen als je het niet met mij eens bent?
RX 6800;2;0.4209035336971283;Daarvoor zou ik informatie moeten hebben van de winkels zelf. Waarom sommige beslissen om ze wel duurder te verkopen weet ik niet. Het kan goed zijn dat inkoop voor hun gewoon duurder is. Dat heeft Coolblue een tijd geleden aan het begin van de pandemie gedeeld. Als ze hun marge gelijk houden gaan ze dan ook mee omhoog. Hetgeen ik het niet volledig mee eens was, is dat de retailers zelf dit doen om meer marge te pakken.
RX 6800;2;0.3529716730117798;"Om een economische stelling van meer dan 200 jaar oud te ontkrachten heb je blijkbaar genoeg aan een paar voorbeelden. Om de prijsstijging te verklaren heb je ""extra informatie"" nodig. Het is natuurlijk niet zwart-wit. Ik ben akkoord dat er winkels zijn die dit niet doen wegens reputatieschade. Maar dat de economische wetten niet gelden voor de verkoop van videokaarten lijkt me sterk."
RX 6800;3;0.36553457379341125;Dan zijn we het grotendeels eens uiteindelijk zeker :-). Uw wet houdt stand, maar niet alle winkels doen het vanwege mogelijke reputatieschade die dit gedrag met zich meebrengt.
RX 6800;1;0.616051435470581;Winkels moeten naar een totaalplaatje kijken, een paar euro's verdienen aan een paar goed verkochte graphics cards weegt wellicht niet op tegen klanten die weglopen en anders wellicht een laptop / desktop / andere onderdelen mee zouden bestellen. Ik ben zelf geen gamer, en deze kaart zou ik niet op dit moment al kopen, maar ik koop wel regelmatig andere computer onderdelen. Als ik echter veel negatief sentiment zie over een webwinkel die het onderste uit de kan probeert te halen op deze manier, ga ik wellicht wel met mijn klandizie naar een andere winkel. In die zin kan dat dus averechts werken voor een webshop.
RX 6800;1;0.43345245718955994;Ook daarboven op dat het niet is alsof ze een groot aantal hebben ingekocht en ze niet verkopen en dus de prijs omhoog moet om het verschil op te maken (of flink omlaag om verkoop te bervorderen) ze hebben een klein aantal ingekocht/binnen gekregen en als ze die voor dezelfde prijs als anders verkopen blijft de winst in verhouding hetzelfde
RX 6800;1;0.39630362391471863;Alsof mensen dat niet snappen. Ik snap het en vind het nog steeds een dick move omdat ik geen gevoelloze econoom ben maar een nerd ben die een videokaart voor een normale prijs wil.
RX 6800;2;0.3906172513961792;Dan moet je even wachten, of een oudere kaart kopen. De marktprijs is (helaas) de normale prijs.
RX 6800;1;0.7902848124504089;Ook net gezien. Echt schandalig in mijn ogen. En dat voor een adviesprijs van pakweg 700 euro.
RX 6800;2;0.3474767208099365;Ik vind 786 euro toch wel substantieel hoger als 700 euro. Vergeet niet dat de adviesprijzen van AMD in USD zijn ZONDER BTW. Dat is dan voor hun eigen kaart. Tel daar nog 100-150 euro winstmarge bij voor een aftermarketkaart (msi, gigabyte,...) en dit zijn de prijzen...
RX 6800;1;0.5134483575820923;Staat in het artikel, geen enkele Nederlandse retailer heeft iets ontvangen.. Dit is dus met recht een lancering op papier..
RX 6800;1;0.6129636168479919;Ze werden alleen via de Shop op AMD.com verkocht De 6800XT was, zoals gezegd, binnen 2 minuten weg. De 6800 geen idee, want het checkout systeem ligt nu plat edit: nu de hele shop lijkt het
RX 6800;1;0.4076458215713501;Je kan er wel allerlei landen eraan plakken Sold Out.
RX 6800;1;0.41663646697998047;Had het artikel niet kunnen lezen, vanwege het proberen te kopen, ga dat nu pas doen.
RX 6800;1;0.5804703235626221;Op de Site van AMD. De 6800XT was binnen 2 minuten uitverkocht (en staat er nu daarom ook niet meer bij)
RX 6800;1;0.7266731858253479;Ja hett slaat inderdaad helemaal nergens op al die paper releases!
RX 6800;1;0.5745462775230408;Die foto toont het enige beschikbare exemplaar?
RX 6800;5;0.26298704743385315;"Er is een oud spreekwoord. '"" Veel beloven weinig geven doet de gek in vreugde leven""' (=veel mensen zijn al blij met een belofte en geloven alles) leven de Influencers"
RX 6800;3;0.5978636741638184;Mooi overzicht, maar jammer dat er bij het testen zo weinig aandacht is voor productiviteitsbenchmarks. Hoe presteren de kaarten voor Photoshop, Premiere Pro, DaVinci Resolve, Blender, .... om maar enkele belangrijke te noemen? De klemtoon ligt vaak te eenzijdig op gaming. Leuk en interessant, maar sommigen gebruiken hun pc ook voor andere zaken dan gaming. Hopelijk wordt er bij de reviews van partner-kaarten wat meer aandacht gegeven aan niet-gaming benchmarks.
RX 6800;5;0.6831507682800293;Helemaal mee eens. Ik ben ook erg benieuwd naar performance in met name videobewerking. Zeker aangezien nVidia in het verleden altijd een streepje voor heeft in productiviteit dankzij hun CUDA cores.
RX 6800;3;0.2805962562561035;Linus heeft hierop getest. Zijn oordeel was dat als je videobewerking doet je veel meer krijgt bij nvidia. Reden waarom ik ook na deze test toch nog een 3070 heb gekocht. Encoding, Decoding en 3D renderen is vooralsnog bij Nvidia beter.
RX 6800;3;0.38718289136886597;Inderdaad, dit wou ik op tweakers ook bevestigd zien. Volgens Linus heeft Nvidia voordeel in grafisch werk (Blender, Maya, Solidworks,...) , maar heeft AMD voorsprong in medische tests, dankzij infinity cache. Daarbij mist AMD ook nog enkele features (DLSS onder andere).
RX 6800;3;0.3636438250541687;"Voor de penguins onder ons: Zou toch cool zijn als Tweakers ook kan focussen op andere platformen in de toekomst. Edit. Even een TL;DR voor de niet-Linux gebruikers. De resultaten matchen ongeveer de Windows benchmarks. Upsampling en Raytracing zijn niet AMD's sterke kant. Daartegenin zie je dat bij performance-per-dollar dat AMD wint in combinatie met de open-source drivers. Dit laatste is natuurlijk een beetje vreemd om te horen als Windows gebruiker, maar AMD heeft op Linux twee drivers. AMDGPU, en Mesa. AMDGPU is van henzelf, en die wordt officieel aanbevolen. Mesa is een samenwerking van AMD, Valve en vele anderen, en deze is standaard voor alle Linux gebruikers. Met enkele uitzonderingen daar gelaten, wint de Mesa driver het van de 'officiële' driver dus dat scheelt Linux gebruikers weer wat extra software om te installeren. Tot slot heeft AMD volledige Wayland ondersteuning. Dit is een vervanging voor het ouderwetse X11, en NVidia blokkeert opzettelijk ondersteuning daarvoor. Vandaar ook dat er een bescheiden anti-NVidia sentiment is onder Linux desktop gebruikers. Voor Linux gebruikers is dit dus goed nieuws. Dat de kaarten niet daadwerkelijk verkrijgbaar zijn... dat probleem delen we"
RX 6800;4;0.3163672089576721;Wat ik vooral geweldig vind is dat linux gamers (dual boot of niet) enifelijk een krachtige kaart heeft die ook wayland support heeft
RX 6800;3;0.3132380545139313;Wayland inderdaad maar belangrijker is toch altijd ondersteuning bij nieuwe kernels en er zijn enkele voorbeelden waarbij nvidia bij nieuwe games niet goed presteerde en je volledig bent overgeleverd aan een driver update van nvidia. Doom Eternal bijvoorbeeld. Schijnt dat de RX 5000 serie wel weer wat issues had tot enkele kernels terug. Panics en problemen na slaapstand. Heb dat niet uit eerste hand vernomen, mijn RX 480 werkt nagenoeg perfect vanaf de launch en de situatie voor de RX 6000 schijnt beter te zijn. Goede ondersteuning vanuit Valve wat betreft mesa is ook een pré.
RX 6800;3;0.2950790524482727;De mesa drivers vegen de vloer aan met de AMD drivers. Kan me goed voorstellen dat AMD die drivers ook steeds minder aandacht gaat geven aangezien het gros van de Linux gebruikers nu standaard de mesa drivers gebruikt.
RX 6800;4;0.36842605471611023;Het is niet altijd mesa die wint. Wat sowieso erg fijn is is dat je, in ieder geval bij de vulkan, kunt kiezen per game welke driver je gebruikt. Zo heb ik voor vulkan zowel de drivers mesa radv, de amd opensource amdvlk als de amd proprietaire amdgpu-pro-vulkan geïnstalleerd en kan er gewisseld worden met een simpele environment variable in steam. Gamers kunnen hun hartje ophalen en zoeken naar dat laatste extra stukje fps, zonder dat dit andere games in de weg hoeft te zitten. Of bij een launch om driverbugs heen werken zoals bij Doom Eternal.
RX 6800;5;0.3244014084339142;zoals de meme gaat: --my-next-gpu-wont-be-nvidia
RX 6800;3;0.2712405025959015;Launch option van Sway, een i3 gebasseerde tiling window manager dat draait op Wayland. Nvidia's comptabiliteit met Wayland is uiterst matig, vandaar die launch option. Vrije software heeft humor.
RX 6800;1;0.8642997145652771;Humor? Dit is dodelijk serieus! Denk aan al die kinderen die zichzelf niet warm kunnen houden in de sneeuw omdat hun NVidia Graphics Card alleen maar op base-clock draait. Deze kinderen moeten nu kiezen tussen de proprietary Nvidia drivers of de straffe winter-wind! Niets grappigs aan.
RX 6800;3;0.3071717619895935;Met AMD ben ik heel blij dat ze goede concurrentie leveren, maar als je gebruik maakt van streaming/encoding dan kan je nog steeds beter bij Nvidia zijn. Als voor mij streaming (tussen eigen apparaten, dus hoge bandbreedte) geen probleem was dan zou ik voor mijn volgende upgrade zeker AMD meenemen in de overweging, en waarschijnlijk verkiezen boven Nvidia.
RX 6800;3;0.3241477310657501;Bedoel je hiermee ook via je pc naar een tv streamen, en daar dan een game op spelen, leek mij wel leuk als ik op groot scherm rdr2 bv kan spelen.
RX 6800;5;0.41298606991767883;Naar TV's, Android TV apparaten (Shield TV), notebooks, smartphones, etc. Nvidia kaarten hebben met NVENC minder moeite (lagere vertraging) en hogere bitrates dan AMD kaarten.
RX 6800;2;0.41812264919281006;Is er een specifieke reden waarom Nvidia zoveel beter lijkt te presteren onder Linux ten opzichte van Windows? Waar bijv. de RX6800 op Windows gemiddeld genomen rondjes rent om de 3070 (die +- net zo goed presteert als een 2080Ti) is op linux ineens in vrijwel alle games de 2080ti de bovenliggende kaart van deze 2. Net als dat op Windows de 6800XT en de RTX3080 stuivertje wisselen, zie je ineens dat in de Phoronix gaming benchmarks de RTX3080 gemiddeld genomen ruim beter presteert AMD lijkt relatief gezien op Linux flink slechter te presteren in gaming dan in Windows het geval is. Verder weet ik niet of we naar dezelfde data gekeken hebben, maar Nvidia wint 5 van 9 Perf per $ graphs, waar AMD Mesa er 4 wint, al met al lijkt de perf per $ gemiddeld genomen over alle tests vrijwel gelijk te zijn tussen de RX6000 kaarten en de Nvidia RTX3080. Jammer trouwens dat Phoronix niet minimaal ook de RTX3070 meeneemt (deze presteert ongeveer gelijk als de RTX2080ti, maar met een $499 prijspunt kan deze wel een stuk interessanter zijn bij de prijs per $ metingen. Het zou mij niet verbazen als van de RX6800, RX6800XT, de RTX3070 en RTX3080 de 3070 misschien wel de beste prijs per $ kan hebben in de hier geteste games onder Linux.
RX 6800;3;0.2796073853969574;Windows vs Linux met NVidia... Daar is veel over te speculeren. Misschien is het omdat NVidia meer tijd steekt in de Linux drivers. Alle serverparken die nu uit de grond worden gestampt met Tensorcores of CUDA kaarten, draaien Linux. De perf-per-$ laat inderdaad zien dat NVidia het goed doet. Ik was vooral blij om te zien dat de mesa drivers het beter doen dan de AMD drivers, want dat is voor mij belangrijker.
RX 6800;2;0.5080851912498474;"Misschien dat ik het verkeerd verwoorde, maar ik bedoelde niet dat Nvidia in Linux beter is dan in Windows, maar dat in Windows AMD en Nvidia relatief gezien stuivertje wisselen en dat in Linux de AMD performance een flinke klap krijgt terwijl de Nvidia op niveau lijkt te blijken ten opzichte van de performance in Windows. AMD lijkt het dus in Linux relatief gezien een stuk slechter te doen op Linux dan op Windows, zeker met de eigen driver, maar ook met de opensource driver lijken ze achter te blijven. Moet wel zeggen dat ik niet direct die ""anti Nvidia"" houding begrijp, het komt op mij eigenlijk wat kinderachtig over als ik die memes zie. Ik beheer redelijk wat Linux machines (voornamelijk servers inderdaad) en er is eigenlijk nooit 'gedoe' met Nvidia hardware. Wanneer dat gebruikt wordt in een klantproject dan werkt het 'gewoon'. Ik ben wat Linux betreft wel een gebruiker / beheerder en geen evangelist, misschien dat daar het verschil zit."
RX 6800;2;0.37579894065856934;Denk dat het een kwestie van budget is. Leuk dat AMD en Valve samenwerken aan een Linux driver, maar dat kan natuurlijk niet op tegen het budget van NVidia. Dat sentiment kan ik heel goed begrijpen als ontwikkelaar. Linux heeft een heel goed driver systeem waaraan veel fabrikanten mee doen. Als je Linux installeert dan zitten alle SATA, Wireless en IO drivers ingebouwd. Dit is efficiënt en zorgt voor veel standaardisatie en comptabiliteit. Naast AMD zorgt ook Intel dat al zijn drivers meteen in de Kernel zitten. Ideaal. De enige die hier niet aan mee wilt doen is NVidia. Vanuit NVidia's oogpunt logisch: Zij hebben drivers die niet alles toestaan, en ze kunnen enterprises en hosters meer geld vragen voor hardware welke wel de juiste encryptie-sleutels heeft. Qauadro kaarten zijn NVidias goudmijn, maar dit is in strijd met de Linux licentievoorwaarden. NVidia wilt dus graag Linux ondersteunen voor servers en dergelijke, maar ze willen niet samenwerken en op veel punten geeft dit dus gezeik. Zoals ik hierboven al aan haalde, mobiele oplossingen zijn kut (Fuck you, NVidia! names Linus Torvolds) en ook ondersteuning voor Wayland en dergelijke ontbreekt. Lang verhaal kort. Als je Linux alleen op servers gebruikt, en je kunt de Quadro meerprijs door berekenen, dan is NVidia prima. Anders, zijn ze een doorn in het oog.
RX 6800;5;0.4457750916481018;Precies!
RX 6800;3;0.2835671007633209;Bekijk ook zeker de video van Wendell (L1Techs/L1Linux). Ik denk niet dat ik hem ooit zo blij heb gezien met een GPU release
RX 6800;4;0.3661244511604309;"Dit mag ook wel eens gezegd: Dankjewel voor een mooi en duidelijk review! Waar ik meest van onder de indruk ben is de transparantie en helderheid in de review. Setup configuratie is niet nieuw maar de manier waarop games zijn geselecteerd. Niet zozeer de aantallen of genres zelf maar op de gebruikte game-engines! Hoe vaak zie je wel niet bij andere reviews, bijvoorbeeld van de 10 games, 5 daarvan Unreal Engine games zijn waardoor er ongewenst scheve balans op kan treden. De optie ""Medium"" game settings diagram maakt het alleen maar meer transparant. Hierdoor heb je meerdere meetpunten om de balans op te maken. Hoe vaak komt het wel niet voor dat een game ""geintjes"" uithaalt op alleen hoge/ultra settings om bepaalde kaarten of bepaalde generatie architectuur slechter/beter uit te laten komen. Wat een verademing om weer goede onderbouwde GPU/CPU hardware review hier te zien. Ik geef toe dat ik lang niet meer hier voor PC hardware reviews kwam kijken vanwege de super belabberde en nog meer belabberde reacties van de vorige PC hardware reviewer toendertijd onder Polaris/Pascal tijdperk. Chapeau!"
RX 6800;5;0.5613728761672974;Dankjewel voor je compliment!
RX 6800;1;0.44952574372291565;2020, het jaar met de grote stappen van de titanen, waar AMD de strijd aangaat en durft te winnen, waar consoles het voortouw van snelheid nemen, maar hetzelfde 2020 waar werkelijk niets van dit ook daadwerkelijk te koop is
RX 6800;2;0.3799719214439392;Nvidia legde de lat erg laag, maar AMD lijkt er zo op het eerste oog alsnog over te struikelen Edit: ik bedoelde dat nvidia de lat erg laag legde qua voorraad. Qua specificaties en prijs /kwaliteit is het een goude tijd.
RX 6800;2;0.35576778650283813;Mwah. AMD leek tot 3 jaar geleden zowel op CPU als GPU vlak een beetje hopeloos te worden. Nu hebben ze op beide vlakken hun tegenstander ingehaald dan wel bijgebeend. De RX6800XT evenaart de RTX3080 vrij duidelijk, elk heeft een aantal wins afhankelijk van de specifieke test. Maar de Radeon doet het slimmer, aan lager verbruik en goedkoper. Da's verre van struikelen. Daarnaast zijn ze ook erg overtuigd dat de komende generaties nog meer aanzienlijke vooruitgang zullen kennen.
RX 6800;3;0.4247872233390808;Ik doelde op het ontbreken van een voorraad. De prestaties zijn indrukwekkend. En, als er voorraad was geweest, had ik graag een 6800 gekocht. Maar dit lijkt (vooralsnog) niet mogelijk te zijn. Wellicht dat het de komende dagen/weken verbeterd.
RX 6800;1;0.4141902029514313;Ah.. tja. Niet te weerleggen inderdaad. Vraag me af of bepaalde landen voorrang hebben gekregen.
RX 6800;2;0.4764891564846039;Dat zal me niet verbazen inderdaad, dat b.v. de meeste voorraad eerst naar landen als de VS is gegaan ofzo.. Grotere markten worden wel vaker voorgetrokken in dat opzicht..
RX 6800;1;0.6518594026565552;Ik vind dit eigenlijk niet. Ik vind het een heel slechte review van Tweakers omdat ze de kaarten getest hebben met SAM ingeschakeld. Alleen videokaarten en 5000 serie cpu's van AMD kunnen hier gebruik van maken en dus niet de videokaarten van Nvidia en cpu 's van Intel. Ik vind dit een oneerlijke vergelijking. Schakel SAM uit en de 6800XT presteert in meer games opeens slechter dan de 3080 daar waar hij met SAM aan nog beter presteert. Geen correcte review dus.
RX 6800;3;0.4216329753398895;In de helft van de gevallen maakt SAM niet eens iets uit. In sommige andere wel. Het is een feature die óók naar Nvidia komt, heeft Nvidia al aangegeven. Verder wordt er gewoon getest met het snelste beschikbare platform van het moment, want dat wil je toch weten als je GPUs van ~700-900 euro test/koopt? Én er staan testresulaten met en zonder zodat je de vergelijking nog enigzins kan maken. Ik neem aan dat je dezelfde opmerking maakt als DLSS getest wordt?
RX 6800;2;0.3645923435688019;"Idd SAM komt ook naar Nvidia maw de 3080 presteert in de meerderheid vd games beter dan de 6800XT en niet zoals Tweakers zegt dat ze gelijk presteren. Dit klopt dus niet. En met RT is Nvidia sowieso stukken sneller dan AMD. Techpowerup bijv. heeft een veel betere review; daar testen ze met SAM uit zoals het hoort. En daar presteert de 3080 6% beter dan de 6800XT in games. Minpunt van de 3080 is wel het stroomverbruik. Dan nog moet je met SAM uit testen want Intel cpu's hebben dit niet. Maar ja, het is algemeen geweten dat Tweakers AMD altijd bevoordeelt."
RX 6800;2;0.30239030718803406;"Dus je vindt het scenario dat iemand z'n 5 jaar oude Intel CPU wil inwisselen voor een Ryzen 5000, samen met een nieuwe GPU, onwaarschijnlijk? Dan zou een review met SAM aan toch wel ""zoals het hoort"" zijn? Tweakers heeft een pagina toegewijd aan het verschil met-zonder. TPU heeft zo te zien een apart artikel met die verschillen, met hier de samenvattende performance en hier de conclusie. Interessant aan die conclusie: Als ik naar de specifieke 6800XT review ga zie ik inderdaad dat de RX6800(XT) minder presteert bij de Intel CPU. TPU hun cijfers: 1440p Intel + RTX3080: 155fps Intel + RX6800XT: 150fps AMD + RTX3080: 164fps AMD + RX6800XT, geen SAM: 165fps AMD + RX6800XT, wel SAM: 168fps 4K Intel + RTX3080: 97fps Intel + RX6800XT: 93fps AMD + RTX3080: 110fps AMD + RX6800XT, geen SAM: 100fps AMD + RX6800XT, wel SAM: 101fps Exact dezelfde uitkomst als Tweakers: ongeacht SAM is de Intel CPU een bottleneck voor zowel Nvidia als AMD. Op 1440p is AMD ietsje sneller, op 4K dan weer Nvidia. SAM is gemiddeld genomen een procentje verschil. Dus als je tóch het maximale uit eender welke kaart wil halen moet je een Ryzen 5000 in huis halen, want dat scheelt al snel 5-10%. En dan kan je SAM evengoed aanzetten."
RX 6800;2;0.31321823596954346;Niet iedereen upgrade zijn cpu en gpu tegelijk, dus nee je vlieger gaat niet op. Met een cpu doe je meestal langer dan een videokaart. Zit je dus nog met een Intel cpu en wil je upgraden naar 6800XT dan moet je met SAM uit testen want de Intel cpu kan er geen gebruik van maken. In veel games is het verschil ts SAM aan en uit een win voor AMD of niet. Het is dus wel degelijk belangrijk dat je het uitschakelt bij de review.
RX 6800;2;0.4409332573413849;Nvidia legde de lat erg laag? De meeste mensen welke ik spreek over de 3000 serie zijn het er over eens dat de lat hoog word gelegd, zeker kijkend naar de overgang van de 1000 naar 2000 serie. Daar lag de lat immens laag. En de prijzen immens hoog. AMD zet hier een paar mooie kaarten neer, vooral kijkend naar de performance per watt - de vaste grap van het groene kamp is altijd geweest dat je een ei kon bakken op AMD kaarten, dat is voor de gein een keertje omgedraaid. Voor het gros is het vooral de vraag hoe beide kampen het in de middenklasse gaan doen, het overgrote deel van de Steam gebruikers heeft een GPU tussen de €150 en €350, een prijsgebied waar geen van beide een kaart heeft in de huidige generatie. (en ook nog geen kaarten heeft aangekondigt)
RX 6800;2;0.43159666657447815;Beter lezen, hij heeft het over de beschikbaarheid c.q. leveringen, niet over de prestaties..
RX 6800;1;0.379374623298645;Dat stond er niet op het moment dat ik reageerde, gebaseerd op het artikel kan je bij de eerste zin enkel aannemen dat het om performance gaat. (Kijk even naar de tijd dat ik reageerde en de tijd van zijn edit. )
RX 6800;1;0.48740246891975403;Vergeet dit pareltje niet.
RX 6800;4;0.5320733785629272;Precies, bijna dubbel zo snel als een 2080 vind ik zo gek nog niet. Vind dan ook niet gek dat Nvidia wat problemen had met launch als ze alles uit de kaarten weten te halen. Met die uitspraak zal ik nog voorzichtig zijn. Gezien je een 3080 flink kan undervolten dropt de wattage aanzienlijk en de kaarten kunnen dan ook iets hoger boosten. (1.025 stock 0.900 undervolt) 2000mhz steady. Maar natuurlijk is het nog afwachten of je de 6000 serie kan undervolten.
RX 6800;5;0.2889651954174042;En Apple die op technisch vlak de strijd aangaat met AMD, Nvidia én Intel, maar tegelijkertijd hun chips niet los aan consumenten verkoopt. Als Apple relatief tov inkomsten evenveel R&D uitgeeft als de anderen, hebben ze ook méér R&D capaciteit dan die drie anderen opgeteld. Gek genoeg levert het ook hard op.
RX 6800;4;0.2629912793636322;Kaarten kopen die alleen in theorie bestaan is de toekomst.
RX 6800;1;0.607357919216156;Linus z'n conclusie is totaal anders : die breekt de Radeons af: -raytracing niet op niveau -streamen tijden gamen niet ok -geen DLSS ...
RX 6800;2;0.3543289005756378;Hij breekt de radeons niet af, en zegt voor traditionele rasterized games de Radeons op gelijk niveau zijn als hun Nvidia tegenhangers. Alleen voor extra's is nVidia beter. DLSS 2 is zeker een fantastische toevoeging, alleen brede adoptie voor deze techniek is er nog niet. RTX is al helemaal een gimmick en zelfs bij Watchdogs legions niet game bepalend, hoewel het daar voor het eerst ook iets toevoegt (en maybe Control). Voor streaming raad ik zowel nvenc (Nvidia) als hevc (AMD) af, beter is de cpu encode x264 preset in obs met medium encode op een 3900x of 5900x. De kwaliteit is dan veel beter! (game performance wordt nauwelijks beïnvloedt door de hoge thread count van deze processors)
RX 6800;2;0.37977176904678345;"Ik vond Linus' review ook een negatieve toon hebben (in vergelijking met andere reviews). Een paar weken geleden stond iedereen nog te springen om de 3080, 2x de 2080 voor 700$!! Maar als AMD het doet (na jaren achterstand zelfs), met meer efficientie en voor minder geld ook nog, dan is het opeens ""verwacht"". Ja, DLSS is indrukwekkend, maar had iedereen soms verwacht dat AMD in één keer NVIDIA voorbij zou gaan op álle vlakken? Ik ben allang blij dat er weer concurrentie is!"
RX 6800;3;0.3079870641231537;"Naar mijn weten is ""RTX"" toch een onderdeel van DXR (Windows DirectX 12 Ultimate) en dat AMD ook Ray-tracing heeft? Goed, dankzij dat ""RTX"" marketing, wist ik dat eerder niet van AMD kaarten en NVidia dat alleen had. (ja ""DLSS"" stretching vond AMD (nog?)niet nodig)"
RX 6800;3;0.3465639054775238;"DXR is samen door Nvidia en Microsoft ontwikkeld. RTX zelf is niets anders een platform voor graphics en moet niet geïnterpreteerd worden als Ray Tracing X o.i.d. De rtx kaarten die we kennen maken gebruik van speciale RT cores voor Ray Tracing. AMD kaarten hebben niet zulke specialistische cores. AMD lost het Ray Tracing ""probleem"" gewoon anders op, zei het blijkbaar iets minder effectief."
RX 6800;2;0.4293655455112457;Vind het meer constructief kritiek. Zo Van AMD kom nou eens met deze features. Anders betalen we Nvidia wel 50 dollar meer.
RX 6800;5;0.3725798428058624;De 3080 is gewoon beter en je hebt nog een 3090 voor de mensen die echt het beste willen. Voor de prijs / kwaliteit lijkt een 3070 perfect te zijn, als deze nu eens voorraad hadden
RX 6800;3;0.32461002469062805;Ik had toch stiekem op een benchmark gehoopt in Microsoft Flight Simulator. reviews: Microsoft Flight Simulator - Op reis met Ryzen 5000 en GeForce 3000
RX 6800;1;0.6022338271141052;Hetzelfde als de andere grafische kaarten, medium settings heeft alles vanaf een 2070/5700xt rond de 55 fps ongeacht de resolutie en 30-35 fps op ultra settings ongeacht de resolutie. Compleet nutteloos spel om benchmarks mee te doen.
RX 6800;3;0.43781203031539917;Aan de hand van die review lijkt het mij duidelijk dat een 6800(XT) GPU niet veel verschil gaat maken, aangezien msfs liever een snelle cpu heeft met hoge single core speed.
RX 6800;5;0.5021694898605347;Linustechtips answered your prayers. 3min 20
RX 6800;3;0.2391403466463089;Thnx! Hier was ik naar op zoek. Op 1080p en 1440p scoorde de RX 5700XT nog wel goed, maar op 4k was dit ongeveer de helft van de RTX 2070 Super en ik was benieuwd hoe de RX 6000 serie ermee om zou gaan.
RX 6800;3;0.38549646735191345;Op Guru3d is FS2020 wel meegenomen:
RX 6800;5;0.5865010023117065;Dankje!
RX 6800;2;0.42370840907096863;Ze hebben hier laatst een uitgebreide benchmark van gedaan op Tweakers. Conclusie is dat het volledig CPU bound is dus geen goede benchmark voor GPUs.
RX 6800;3;0.5121813416481018;Op hogere resoluties is er zeker een verschil te zien. Zo presteerde de 5700XT op 4k ultra zo'n 15fps tegen 36fps met de RTX 3080. Een RX 6000 videokaart zou hier zeker beter moeten scoren (zie ook de linkjes hierboven). De performance op lagere resoluties zijn idd vrijwel gelijk.
RX 6800;2;0.4937407672405243;"Klopt maar tussen de kaarten in het segment van de RX 6800 (XT) waren de verschillen verwaarloosbaar, ook op 4K ultra. Ik snap daarom volledig dat ze MSFS niet hebben gebruikt als benchmark hier. Reviewers zijn vooral beperkt door de tijd die het kost de benchmarks te draaien en de data te organiseren; dan wil je uiteraard benchmarks draaien waar geen sprake is van een CPU bottleneck."
RX 6800;2;0.5228988528251648;Helaas verrast door het complete gebrek aan voorraad.
RX 6800;1;0.48417338728904724;Er zijn niet eens productpaginas....
RX 6800;1;0.49097636342048645;In de review staat ook dat geen enkele Nederlandse webshop die gepartnered is met AMD geen enkele kaart hebben ontvangen. Complete paper launch dit, zowaar nog erger dan Nvidia.
RX 6800;1;0.5683102011680603;Haha, en iedereen maar zeggen dat AMD gigantisch ging verkopen want ze hadden wel voorraad
RX 6800;1;0.6914685368537903;Helaas is de beschikbaarheid van deze ‘standaard’ videokaarten zeer slecht en verwachten we hier de komende weken ook geen verandering in te zien. Om teleurstellingen over de lange levertijden te voorkomen nemen we op dit moment dan ook GÉÉN BESTELLINGEN aan voor de AMD Radeon RX 6800 en de AMD Radeon RX 6800 XT. Azerty verkoopt ze iig niet.
RX 6800;3;0.3962593078613281;Aan het begin van het stuk begon ik bijna te balen dat ik €650,- voor een RTX3070 had neergelegd. Maar aan het eind: Toch maar blij dat ik 't wel heb gedaan. Een videokaart is beter dan geen videokaart.
RX 6800;5;0.47094419598579407;En geduld is een schone zaak
RX 6800;3;0.45592793822288513;Zeker waar. Alleen heb ik nu vakantie, dus wou ik nu gamen. Dan betaal je daar een premium voor, helaas
RX 6800;5;0.5583157539367676;3070 is een harstikke mooie kaart, kan je lang mee vooruit!
RX 6800;2;0.40544965863227844;Ik onderschrijf de conclusie dat RTX 3080 / RX 6800 XT lood om oud ijzer is. Omdat ik nu (ook) een NVidia in een AMD systeem heb zitten maakt het me niet echt uit. All AMD is leuk, maar geen must. Welke kaart leverbaar gaat zijn voor welke prijs gaat de doorslag geven.
RX 6800;3;0.519791305065155;als ik dit zo zie is de 6800XT toch echt wel de betere keus voor 1440p, vaak sneller als de RTX3090 Voor 4K is de RTX3xxx series echter vaak wel de betere keus
RX 6800;2;0.4818016588687897;Tenzij je b.v. Cyberpunk 2077 gaat spelen, dan ben je blij dat je DLSS kan gebruiken.. Het hangt echt van de games af wat beter is.. Uiteindelijk zit je met beide wel goed en zal vooral beschikbaarheid en prijs gaan uitmaken welke mensen kopen.. Op dit moment is het om moedeloos van te worden dat er werkelijk niks op voorraad is, wachttijden enorm lang en dat de prijzen daardoor ook kunstmatig te hoog zijn.. AMD moet flink gaan produceren en leveren voor dat Nvidia weer met een soort refresh komt in de vorm van een 3070Ti en 3080Ti begin 2021.. Anders pakken ze qua marktaandeel maar weinig terug..
RX 6800;1;0.6650425791740417;De AIB kaarten Komen volgende week, Dan zal amd vast een heleboel verkopen
RX 6800;2;0.38911840319633484;Prijzen zijn toch niet kunstmatig hoog? Als amd of Nvidia kon leveren, zouden ze dat echt wel doen.
RX 6800;3;0.3947795033454895;Ik denk dat mijn volgende kaart voor het eerst Nvidia wordt (ooit). Maar dan wel de 3080ti, zie de bui al hangen ja. DLSS tegenhanger is nog volop in ontwikkeling, en de AI netwerken zijn bij Nvidia uiteraard van top niveau, verwacht dan ook een minder sterk functionerend geheel bij AMD. En als je dan toch raytracing wilt, wat in sommige games (net zoals HDR) fantastisch is, wel zo handig dat je niet of nauwelijks inlevert op resolutie/scherpte.
RX 6800;2;0.46514710783958435;Het is een beetje zoals altijd het geval, afhankelijk van je use case. Linus tech tips heeft een goede review up staan. Maar productivity is amd achteruit gegaan en cuda maakt o.a. solidworks sneller. Als raytracing niks boeit is amd superieur nu en uitstekende bang for Buck. Maar als raytracing wel boeid en dlss dan is en blijft NV beter. Dlss gemaakt gebruik van tensor cores en daar heeft amd hardware matig niks voor. Ik wilde een 6800 xt halen omdat de 3080 niet beschikbaar was, maar zoals het nu gegaan is bestel ik wel een 3080. Sta ik tenminste in een lijst.
RX 6800;3;0.3596895933151245;DLSS is voor mij de belangrijkste reden waarom ik waarschijnlijk voorlopig bij Nvidia blijf.. Ik wil straks o.a. Cyberpunk 2077 gaan spelen, en die ondersteunt dat gelukkig.. Gelukkig ondersteunt mijn huidige RTX 2080 dat ook, dus het is geen hele grote ramp als ik geen 3080 voor die tijd te pakken krijg.. Ander ding is mijn CPU, dat is nog een oude i7 6700K.. Kan ook nog wel heel even mee, maar het liefst vervang ik die ook voor wat nieuws zodra ik een RTX 3080 heb want ga anders toch wel fps laten liggen ben ik bang.. Zal dan een R5 5600X worden.. Maar ook die is ook gewoon nog niet te krijgen voorlopig.. Als ook dat te lang gaat duren trek ik het door naar 2021 en ga wachten op Intel waarmee die komen als antwoord op AMD Zen3.. Mocht dat tegenvallen kan ik altijd nog rond die tijd een AMD R5 5600X (of misschien wel een non-X) kopen voor minder geld dan nu, hopelijk gewoon uit voorraad ergens.. Die moet dan wel weer +/- 5 jaar meekunnen voor gaming net als mijn i7 6700K nu..
RX 6800;1;0.41463175415992737;Ik heb nu de verschillende reviews gezien, maar als je kijkt naar Minecraft rtx, die is gewoon onspeelbaar op 6800xt (4k) de 3080 haalt net 30fps maar met dlss is het bijna 80fps. Dat is gewoon een alledaagse toevoeging die er echt toe doet en waar amd hardware matig nog geen antwoord op heeft (softwarematig zeker niet). Ik gebruik hem ook voor bedrijfsmatige doeleinden en daar regeert cuda simpelweg. Geen plugin nodig en het werkt gewoon (en sneller). Mijn 2080ti is kapot
RX 6800;2;0.47820228338241577;"Op de site van Azerty: ""Over de lancering van de AMD Radeon RX 6800 en AMD Ryzen 6800 XT videokaarten Op 18 november zijn de referentiemodellen van de nieuwe AMD Radeon RX 6800 en de AMD Radeon RX 6800 XT officieel gelanceerd. Dit gaat nu nog om de videokaarten met het standaard AMD-ontwerp qua printplaat en koeler. Binnenkort introduceren de diverse merken hun kaarten met een eigen ontwerp. Helaas is de beschikbaarheid van deze ‘standaard’ videokaarten zeer slecht en verwachten we hier de komende weken ook geen verandering in te zien. Om teleurstellingen over de lange levertijden te voorkomen nemen we op dit moment dan ook GÉÉN BESTELLINGEN aan voor de AMD Radeon RX 6800 en de AMD Radeon RX 6800 XT. Wij vinden het niet netjes naar onze klanten toe om bestellingen aan te nemen als het nog niet duidelijk is of deze kaarten uiteindelijk -waar dan ook ter wereld- leverbaar gaan zijn. In plaats daarvan richten we ons op de ‘eigen ontwerp’ kaarten die binnenkort door de verschillende fabrikanten zullen worden aangekondigd. De verwachte beschikbaarheid van die modellen zal echter ook matig zijn. Wil je toch zeker weten dat je één van de eersten bent met een van deze nieuwe videokaarten? Kijk dan eens bij onze nieuwe [Azerty Gaming Platinum AMD] en [Azerty Gaming Esports AMD] gaming pc’s. Op dit moment is nog niet bekend wanneer we ze kunnen gaan leveren, maar doordat we snel kunnen schakelen zodra de beschikbaarheid toeneemt zul je met deze pc’s zo snel mogelijk aan de slag kunnen!"" De voorraad van AMD lijkt dus hekaas nog minder te zijn dan die van Nvidia."
RX 6800;2;0.3867151439189911;Aan de ene kant netjes van Azerty, maar grote kans dat ze daardoor potentiele klanten verliezen door mensen die geen geduld hebben en daardoor bij een andere partij gaan pre-orderen.
RX 6800;1;0.6583449840545654;Zoveel winst maken ze niet op die kaarten, en ze zijn wel veel tijd kwijt met klantenservice voor mensen die bestellingen annuleren of veranderen en en mensen die bellen met de vraag of er al voorraad binnen is, en wanneer ze voorraad verwachten.
RX 6800;3;0.3651266396045685;Ja dat is ook weer zo. Marges zijn klein, want er is veel concurrentie. Ze zouden natuurlijk een pre-order met aanbetaling kunnen doen, maar dat schept ook weer verwachtingen
RX 6800;1;0.47130098938941956;Die pre-built PC's is dus ook iets waar veel videokaarten in verdwijnen.. Azerty zegt dat hier ook gewoon indirect, dat hun pre-built gaming PC's voorrang krijgen t.o.v. losse verkoop.. Ze maken meer winst op die complete systemen denk ik.. Ik blijf het jammer vinden dat je geen videokaarten rechtstreeks bij de fabrikanten kan kopen, los van AMD of Nvidia zelf.. Ik geloof dat EVGA dat als enige doet en dan alleen in de VS zover ik weet.. Dan betaal je gewoon altijd netjes de adviesprijs en geen hogere prijs omdat de tussenhandel er ook aan moet verdienen..
RX 6800;1;0.5233232975006104;"Ik lees niets in deze review over de tegenhanger van DLSS; super resolution (VSR). Is dit nog niet beschikbaar?"
RX 6800;1;0.24442729353904724;Yes, op pagina 2
RX 6800;3;0.26259365677833557;Ah thanks, ik zat specifiek de tekst te scannen op dlss en VSR/super resolution, dus had dit over het hoofd gezien.
RX 6800;5;0.3318859934806824;Ik heb een bestelling voor de 6800 kunnen plaatsen
RX 6800;1;0.6559069156646729;Heb je m via AMD gekocht? Ik ben er ook doorheen gekomen na een half uur problemen.
RX 6800;2;0.3714434802532196;"Inderdaad ook direct bij AMD, blijkt dat als je die knopjes gewoon heel vaak en snel drukt dat ze wel reageren. Ik had zelfs een 6800xt in m'n cart maar ik kon niet naar de checkout voordat die uitverkocht waren, dus dat was wel een domper. Maar hé toch nog beter dan binnen een halve seconde van ""buy now"" naar ""out of stock"""
RX 6800;3;0.431972473859787;Had precies hetzelfde met de XT, ook in de winkelwagen en vervolgens niet naar de checkout Nu maar hopen dat er geen teleurstellend bericht van AMD komt dat er toch iets teveel besteld zijn tov de voorraad.
RX 6800;5;0.5439104437828064;We zullen het merken!
RX 6800;1;0.5541725754737854;Ik had bijna hetzelfde. Ik kwam bij de checkout pagina maar kon het niet kopen. Ik ging snel door met paypal onetouch, maar het mocht niet baten. Er waren op een gegeven moment zelfs wat bugs waardoor de RX 6800 €492 was opeens 😂
RX 6800;2;0.3925759792327881;Dit laat me erg twijfelen... ik heb een 3440x1440p 120hz GSYNC only scherm.. laat nou net AMD sneller zijn in deze resolutie, maar alleen Nvidia kan me de VRR geven. Daarnaast 10GB vs 16GB. Yikes!
RX 6800;1;0.5261433720588684;2560*1440P is ongelijk aan 3440*1440P!
RX 6800;3;0.335767537355423;Ja dat klopt, zit tussen 1440p en 4k 16:9 in, maar nog steeds dichter bij 1440p. 3,6M vs 4,9M vs 8,3M pixels
RX 6800;2;0.42579591274261475;Ik heb hetzelfde. Zit hier met een Odessey G9 (5120x1440). Dan zou de 3080 beter scoren, omdat het veel meer bij 4k ligt. Aan de andere kant zijn ze allebei moeilijk te krijgen dus ik wacht wel tot ik er een krijg, en vergelijk ik het daarna als ik de andere binnen krijg
RX 6800;1;0.4658649265766144;Heb de 3080 strix oc besteld. *5 maanden later*
RX 6800;1;0.7286154627799988;Ik heb de Asus 3080 tuf niet oc besteld, begon op plek 56 en sta nu op 37, schiet niet op...
RX 6800;3;0.29421308636665344;Bij Alternate?
RX 6800;5;0.2728898823261261;Jupp
RX 6800;3;0.28245943784713745;Hoe zie je je positie?
RX 6800;1;0.25840625166893005;onderaan deze pagina. Dan je ordernummer en postcode invoeren.
RX 6800;1;0.5237327814102173;RIP 367
RX 6800;5;0.5564271807670593;Succes! Dan had je niet besteld op launch day zeker?
RX 6800;5;0.5634902119636536;Jawel, op launch day van AMD!
RX 6800;3;0.4221261739730835;Los van dat dik de prestaties indrukwekkend vind, zeker in combinatie met geluidsproductie en stroomverbruik en het feit dat er nu een geschikte test cpu is gebruikt (5950X) moet ik wel even een kleine kanttekening plaatsen. De meeste games waarbij dit mogelijk is wil je mét DLSS ingeschakeld spelen omdat de game er dan beter uit ziet dan met TAA. Bijkomend effect is dat je dan een flinke performance verbetering krijgt. Ik merkte in f1 2020 met mijn RTX 2070 een stijging van 88 naar 113 FPS. Dat scheelt 28%. Nu snap ik wel dat je als reviewer het eerlijk wil houden en dan zijn technieken als DLSS een lastig verhaal. Als consument ben ik wel benieuwd hoe de verhoudingen liggen mét DLSS, dat is immers toch de setting die ik zou gebruiken puur omdat dit de game mooier maakt. AMD heeft tegenwoordig FidilityFX dus dan krijg je ook de vraag of je die dan moet gebruiken, om het makkeliker te maken . SAM wordt overigens terecht apart getest want als ik een 6800XT koop heb ik nog geen SAM, dan moet ik eerst ook een Ryzen aanschaffen. DLSS krijg je natuurlijk bij elke RTX kaart. edit: Gamernexus doet control met DLSS on en off: Redelijk extreme verschillen: 3080FE DLSS on: 64.8 @4k / 136.9 @fullhd 3080FE DLSS off: 36.2 / 105.7 6800XT 23.0 / 70.2 DLSS is overigens prima zonder ray tracing te gebruiken, in die zin nog steeds niet precies wat ik bedoelde. Ik ben benieuwd naar de performance zonder ray racing mét DLSS.
RX 6800;3;0.43557488918304443;Ja maar geen DLSS bij de meeste games dus ...
RX 6800;3;0.33577218651771545;In deze review beschikken Control en F1 2020 en Shadow of the Tombraider over DLSS support. Mondjesmaat worden het er meer: AMD schijnt aan een tegenhanger te werken.
RX 6800;3;0.43146005272865295;"Je hebt een punt en dat Nvidia DLSS aanbiedt zou vermeld moeten worden in een review. Aangezien AMD heeft aangekondigd ook met een geavanceerdere vorm van Super Sampling te komen, en DLSS in slechts een beperkt aantal games beschikbaar is, lijkt het mij wel eerlijker om dat in de benchmarks nog buiten beschouwing te laten of hoogstens apart te testen (zoals SAM + Rage Mode; wat overigens in elke game werkt zolang je de hardware hebt, dus inclusie daarvan vind ik logischer/belangrijker)."
RX 6800;3;0.3528475761413574;Opvallend dat wanneer je van 'ultra' naar 'medium' gaat, dat je dan bij AMD relatief gezien veel meer fps terug krijgt dan bij Nvidia. Datzelfde had ik deze week al eerder in de geruchtenmolen gehoord. Als je van 'ultra' naar 'high' gaat, dan krijg je daar bij AMD meer prestaties voor terug dan Nvidia. In deze review zie je een nog groter verschil maar dat is dan ook van 'ultra' naar 'medium'. Als je dus genoegen neemt met wat mindere instellingen dan kun je behoorlijk wat extra fps uit je systeem krijgen met deze nieuwe AMD kaarten.
RX 6800;4;0.2943039536476135;Komt omdat de Ampere architectuur het beste tot zijn recht komt op hogere resoluties en zwaardere belasting. Andere render pipelines. U
RX 6800;5;0.3650332987308502;Maar dat wil niemand. Iedereen wil de pracht en praal van Ultra.
RX 6800;2;0.4378558099269867;"Ik denk dat dat zou kunnen komen doordat er op 4K meer cache-misses zijn; er passen uiteraard meer items in de cache bij 1440p dan bij 4K. Cache-misses zijn kostbaarder voor AMD want ze hebben trager VRAM. Wellicht dat toekomstige optimalisaties (bv. m.b.v. SAM) hier nog wat verandering in kunnen brengen. AMD heeft aangegeven dat de meerwaarde van SAM groter wordt als developers hiervoor optimaliseren. Daarnaast beweert AMD dat SAM 'open source' is en in de toekomst ook naar Intel en Nvidia kan komen als die producenten dat wensen. Erg mooi dat ze deze technologie werderom niet voor zichzelf houden."
RX 6800;3;0.42565420269966125;Ook al maakt AMD nu grote stappen voorwaarts, ik mis nog één dingetje ten opzichte van Nvidia: Streaming van games zoals met een shield of moonlight-embedded. Daar is naar mijn idee nog niet echt een equivalent van voor AMD. Correct me if I'm wrong .
RX 6800;3;0.5291494131088257;Je kunt op bv de Nvidia shield de AMD link app installeren en dan heb je ook streaming met AMD kaarten. Persoonlijk vind ik de nVidia implementatie beter en stabieler, maar AMD link werkt wel.
RX 6800;3;0.34762370586395264;"Hierboven ergens een comment van een medetweaker die aanraad de CPU x264 encoder in obs te gebruiken met medium profiel. Gaat blijkbaar ook snel als je een CPU met genoeg cores hebt en kwaliteit zou veel beter moeten zijn. Dat de kwaliteit beter zou zijn is iets wat ik wel vaker lees; daarom gebruik ik zelf altijd CPU encoders, maar zelf doe ik dat niet voor streaming maar gewoon offline encode."
RX 6800;1;0.44811829924583435;En AMD beweren dat het geen paper launch zou worden
RX 6800;1;0.3161469101905823;Geef ze een paar dagen ...
RX 6800;3;0.3458879888057709;Het kan nog wel. Op launch day inderdaad een paper launch maar stel dat het de komende weken wel goed verkrijgbaar is dan is het geen paper launch. Wait & See. Niet dat ik er veel vertrouwen in heb dat ik een 6800XT kan scoren voor een goede prijs binnenkort!
RX 6800;3;0.47709643840789795;Ja je hebt gelijk, het is een beetje voorbarig om te roepen dat het een paper launch is. We zullen zien Zelf was ik er bij de 3080 launch bij, maar heb toen niets kunnen bemachtigen. Ik denk dat ik iets meer naar de 3080 neig, maar pas in januari/februari weer wat ga overwegen
RX 6800;3;0.5225750803947449;Ik vind het wel jammer dat de RTX 3090 niet is meegenomen, zo kan je het beste aanbod van Nvidia vergelijken met het beste aanbod van AMD. Verder wel erg uitgebreid. Verbaast mij ook dat SAM niet veel lijkt te doen, terwijl dit eerder wel veel leek te doen (tientallen fps verschil). Oja, potverdrie. Zou bijna vergeten dat de 6900XT er ook nog is. My bad.
RX 6800;2;0.4222736656665802;de 3090 is niet te vergelijken met de getoonde kaarten. De 6900XT wordt de tegenhanger en dan pas kan je een goed beeld schetsen. 6800XT op niveau van 3080 en de 6800 op niveau van 3070. Ik ben sowieso voor Team rood op CPU als GPU .. en komen als het goed is beide volgend jaar begin volgend jaar in mijn kast.
RX 6800;2;0.4301064610481262;Naar mijn weten zijn er ook gewoon prima meetresultaten te behalen voor de 3090, en vergelijken blijkt dan ook prima te kunnen (zie o.a. reviews: Nvidia GeForce RTX 3090 - De overtreffende trap van Ampere). Ik zie niet in waarom de vergelijkingen daar niet ook gemaakt zouden kunnen worden met ook de metingen van de RX 6800 (en XT). Ik vind het echt jammer dat er geen vergelijkende meetresultaten zijn meegenomen van de GeForce RTX 3090 in de grafiek. Waarom wel de resultaten van de RX Vega 64 - een kaart van 2 generaties / 3 jaar geleden? Het lijkt me namelijk dat de resultaten van een 3090 meer relevant zijn dan die van een kaart die zelfs bij uitkomen al bijna helemaal afgeschreven was. In de Testverantwoording kan ik het antwoord daarop (helaas) niet vinden.
RX 6800;4;0.42270493507385254;De 64 was en is nog steeds een prima kaart.
RX 6800;1;0.36661651730537415;Die wordt in een aparte review vergeleken met de 6900XT die op 8 december uitkomt.
RX 6800;1;0.5326821804046631;Heb ik de 6900XT gemist? Daarnaast vindt ik ook niet dat je kan spreken van aanbod als niks verkrijgbaar is
RX 6800;1;0.4444356858730316;Het staat letterlijk in de tekst, ''terwijl de strijd tussen de echte vlaggenschepen van AMD en Nvidia pas op 8 december zal plaatsvinden, met de RX 6900 XT tegenover de RTX 3090.''
RX 6800;1;0.3902289569377899;Niet iedereen heeft de tijd om 22 pagina's te gaan lezen, en zag ze niet in de benchmarks staan, dusja.
RX 6800;5;0.3897077143192291;Je kan ook gewoon de conclusie lezen .....
RX 6800;1;0.24096858501434326;Laatste alinea was ik nog niet met lezen.
RX 6800;5;0.30664291977882385;Via AMD.com zelf toch echt even de 6800 XT in het winkelmandje gehad... Twijfelde aangezien de processoren bij alle Nederlandse bedrijven direct in backorder gingen en zag het niet zitten om bij AMD zelf op de backorder lijst te staan. Die twijfel duurde een minuut en dat was zelfs al te lang... Inmiddels staan ze op de site van AMD zelf ook op out of stock. Mooie paper launch weer dus! Op naar 2021 en hopen dat kamp Rood en kamp Groen elkaar op prijs wat concurrentie gaan bieden.
RX 6800;4;0.4471052289009094;Sowieso gewoon lekker wachten. Ik heb gelukkig op tijd een mooie MSI RTX2070 Super Trio goedkoop kunnen overnemen van een andere tweaker, ik zing het nog lekker uit met deze kaart en later haal ik wel een nieuwe vid als ze beschikbaar zijn.
RX 6800;5;0.4620463252067566;Wat heb je betaald als ik vragen man? Ik en benieuwd wat een eerlijke normale prijs is voor een 2070 super. Ik heb zelf een GTX970 dus ik zoek een upgrade. Ik heb net een 1440p scher aangeschaft en wil fatnlijk gamen met cyberpunk in mijn achterhoofd. Masr moet ook op mijn budget letten en de gedachte iets langer doorsparen voor een goede kaart die heel goed kan meekomen op 1440p upcomming games.
RX 6800;5;0.3469277620315552;Ik heb 400 eurie betaald. Die ik ook over genomen had van andere tweaker was praktisch nieuw (april dit jaar) dus vond ik een top prijs toen, en nog steeds overigens.
RX 6800;4;0.3891051709651947;Mooie prestatie van AMD. Helaas weer grote voorraadtekorten, net als met bijna alle nieuwe hardware van dit jaar. Ik ben benieuwd hoe de implementatie van SAM op Nvidia kaarten gaat werken!
RX 6800;1;0.41482681035995483;Laten we maar hopen dat AMD alle chips naar the AIB partners heeft gestuurd Dat is de enige goede reden die ik nog kan bedenken waarom AMD nu al de kaarten lanceert terwijl ze zelf geen voorraad hebben
RX 6800;3;0.4309241771697998;Prijs/prestatie helaas niet de NVIDIA killers waar we op hoopten, maar in ieder geval is er nu een echte keuze te maken.
RX 6800;2;0.27108901739120483;nvidia heeft zelf zijn vorige generatie al gekilled als het om prijs/prestatie gaat.
RX 6800;5;0.37704354524612427;Pfoe. Blauw en rood zitten elkaar op de staart. Ii ben zeer benieuwd of er nog een software/driver foefje wordt uitgehaald om bepaalde limieten te unlocken (overclockende Youtubers zoals Jayz2Cents, PaulsHardware en Nexusgaming lieten zien dat er bij de RTX kaarten op gebied van voltage nog veel te halen viel). Mooie ontwikkelingen. Zodra de voorraden normaliseren dan heb je tegenwoordig voor 1500 euro een prachtige 4k gaming pc. Dat had ik een half jaar terug, ondanks geruchten, niet verwacht. Laten we we zijn, de RTX 3000 serie voldeed boven verwachting en heeft dan voor AMD de lat ook erg hoog gelegd. Op een volledig red platform verwacht ik met deze AMD kaarten nog een prestatiewinst dankzij snel geheugen, de cpu indeling en een gen4 m2. Laat maar komen die battles, dan kan mijn 1080ti op V&A.
RX 6800;1;0.4562635123729706;Amd weet Nvidia ditmaal dus wel te evenaren en bij te houden maar zonder voorraad. Spijtig. Ik greep naast de rtx 3080, twijfelde over een rtx 3070 maar nu ik de review gelezen heb: doe mij maar een aftermarket 6800 XT voor de base prise van een FE rtx 3080 dus zeg €719. Maar nul voorraad dus. Tja, beter beetje bij beetje voorraad zoals Nvidia doet (al voelde dat ook als een papieren lancering) maar zo win je de strijd niet op gpu gebied. Corona of niet: als je dermate producten weet te releasen: zorg voor genoeg voorraad. Nu ben ik in afwachting van AM5/DDR5, morgen heb ik de PS5 in huis. Dus in die zin genoeg om te gamen. Maar ik had graag een AMD cpu/gpu combo upgrade willen doen voor mijn pc uit 2014.
RX 6800;1;0.6004435420036316;Was er vanaf de eerste tik bij en er verschenen, in mijn subjectieve ervaring, direct advertenties voor de kaart met 'uitverkocht' erbij. Dus of de bots hebben prima werk geleverd of er waren nooit kaarten te koop. Afgaande op het bericht op BH denk ik het laatste. Deze hebben precies om klokslag 15:00 (9:00 DC) een bericht geplaatst met de mededeling dat ze geen vooruitbestellingen willen accepteren aangezien er toch niet geleverd kan worden. Chapeau aan BH.
RX 6800;3;0.5197727084159851;Naast het gebrek aan voorraad vind ik het toch jammer dat ze nog niet volledig in de concurrentie mee kunnen. Er is geen kaart tegenover de RTX3090, en de RTX3080 doet niet onder voor de 6800xt en doet het veel beter op het vlak van raytracing. Echte voordeel van AMD is het ruimere geheugen zo te zien. Wel blij dat er meer concurrentie is, nu nog voorraad en concurrentie op elk vlak. Correctie, er komt nog een 6900xt aan, die kan wellicht het open vlak aan concurrentie op zich nemen. Ben benieuwd.
RX 6800;1;0.3584873378276825;Er komt inderdaad nog een 6900XT aan die waarschijnlijk tegen de RTX 3090 aan zal zitten, maar dan voor 500 euro minder.. Nvidia komt daarna echter weer als antwoord met een RTX 3080Ti (in januari al volgens de geruchten) voor ook rond de 1000 euro, en zo staan we weer nagenoeg gelijk allemaal, op features als Raytracing, DLSS en SAM na..
RX 6800;3;0.35873788595199585;Het zit dus allemaal heel dicht bij elkaar en het ligt eraan welke games je speelt welke kaart het snelste is.. Nvidia heeft nog wel mijn voorkeur vanwege Raytracing en vooral DLSS.. Maar mooi dat er eindelijk weer meer keuze is dan alleen Nvidia als je het beste wil.. Alleen erg jammer dat geen enkele videokaart op dit moment ergens normaal te koop is voorlopig en dat deze AMD launch dus nog erger lijkt dan die van Nvidia met 0 voorraad.. Beschikbaarheid en prijs kunnen wel eens gaan bepalen wie het beste verkoopt deze generatie.. Nu is trouwens ook duidelijk waarom Nvidia met een 3070Ti en 3080Ti wil komen volgens geruchten.. Daarmee pakken ze waarschijnlijk de prestatiekroon weer iets duidelijker terug.. Maar nogmaals, beschikbaarheid en prijs gaan waarschijnlijk belangrijker zijn dan weer een paar % sneller..
RX 6800;2;0.3302828073501587;Het is mij gelukt om er eentje te bestellen met paypal, Maar ik zie nergens een plek op de AMD website waar je kan checken wat de status is van je order (oftewel of ik m ook echt heb). weet iemand waar ik dit kan vinden?
RX 6800;3;0.44180047512054443;Overall zie het beeld dat AMD vooral heel veel waarde levert op het 1440p performancepunt met de 6800, en kan meekomen qua Raytracing met de vorige generatie topper van Nvidia de 2080Ti. Ik vind dat een slimme keuze van AMD om minder silicon aan Raytracing te besteden, waar ongetwijfeld de volgende generatie (RDNA3) dit geheel anders gaat zijn, bij de stap naar 5nm van TSMC waardoor de transistor density flink zal toenemen tov 7nm van circa 98 mln per mm2 naar 183 mln per mm2. En Raytracing een veel goedkopere functie zal worden. Daarnaast is het mooi om te zien dat geluid en thermals vanuit een reference design op en top zijn, they mean business :-). Ben erg benieuwd naar een tear down door tweakers.net, als we kijken naar het gebruik van de componenten in de launch versie van de 6800(XT) vs de Nvidia Grafische plankjes.
RX 6800;1;0.46477654576301575;2020 is het jaar van de paper releases. Next gen consoles, processors, en videokaarten. Er is zo weinig beschikbaar dat het gewoon paper release galore is
RX 6800;5;0.5760679244995117;zo de 6800XT insAllah wordt mijn nieuwe koop in 2021 samen met de 5900X CPU goede combo. En dan houdt ik het ook daarbij.
RX 6800;3;0.4183867871761322;Iets gekend over stroompieken? Hebben we voor deze kaarten ook een veel hogere voeding nodig
RX 6800;5;0.8136740922927856;Prachtig om te zien dat ook nVidia het lastig gaat krijgen met AMD. Deze concurrentie is geweldig voor de consument. Het feit dat deze kaarten vergelijkbaar presteren bij een veel lager vermogen, is zeer prettig. Ik denk dat een combi AMD Ryzen 5000 + AMD Zen 6000 voor velen de meeste aantrekkelijke aanbieding is van het moment. Als AMD dit momentum vasthoudt, dan belooft dat ook mogelijkheden voor VR naar mainstream te brengen. Nu alleen nog ergens te koop zien te vinden.
RX 6800;1;0.4894939661026001;Ongelofelijk dit. Ik had toch gedacht dat het bij AMD anders zou gaan dan bij Nvidia mbt bevoorrading en al helemaal na zo'n geweldige presentatie. Bij Alternate heb ik beide versies kunnen vinden waarbij voor de 6800 wel 900 euro gerekend wordt en voor de 6800 XT 1000 euro. Nu begrijp ik dat bij schaarste de prijzen omhoog schieten maar come on... Edit: merkwaardig dat bij sommige partners niks te vinden is over de nieuwe kaart. Link AMD partners toegevoegd.
RX 6800;3;0.36768051981925964;Mag je ook concluderen dat de 6800XT een hogere prestatiescore heeft tov de 3080 in 21:9, 3440x1440?
RX 6800;1;0.2779906392097473;Volgens mij waren ze al uitverkocht voor ze online waren
RX 6800;2;0.49451154470443726;Waarschijnlijk heeft AMD net als de hele (tech)wereld ook last van alle logistieke problemen op/rond China door de pandemie. Het is zelfs nu nog steeds erg lastig om betrouwbare leveringen uit China te krijgen, vooral per containerschip of trein via die zijderoute, en zelfs per luchtvracht. In dat laatste geval heb je te maken met een verdubbeling of zelfs verdrievoudiging van de kiloprijs, voor het beetje capaciteit wat er nog te krijgen is.
RX 6800;1;0.588718593120575;Ook op het gebied van levering heeft AMD deze keer nVidia weten te verslaan. Nog slechter leverbaar.
RX 6800;5;0.4933815896511078;Ineens ben ik heel erg blij dat ik de RTX 3090 heb terug gestuurd. Ben benieuwd wat de RX 6900 XT gaat presteren
RX 6800;5;0.3758291006088257;Het is een nek aan nek race dus die als eerste over de meeste kaarten online kan zetten zal winnen. Niet te vergeten dat je de laatste amd cpu en moederbord moet hebben als je het volledige potentiaal met de laatste amd kaarten wil hebben.
RX 6800;3;0.2966534495353699;Dan maar een pre-built
RX 6800;1;0.7402333617210388;AMD had de kans om Nvidia een mokerslag uit te delen, maar ze hebben de bal laten vallen... zonde.
RX 6800;5;0.7254611849784851;Via AMD.com de 6800 gekocht voor €599. Ben heel benieuwd!
RX 6800;1;0.3557998239994049;Heb je daar een link van? Heb daar namelijk nog nooit zoiets dergelijks kunnen vinden
RX 6800;2;0.3575117290019989;Maar is er ondertussen al afgehaald.
RX 6800;2;0.3710940480232239;Goed om op te slaan dan denk ik. Weet niet hoe je daar bent gekomen, maar het wordt i.i.g. niet echt heel uitgebreid geadverteerd. Tnx for the link! edit: haha, toen ik net klikte stonden er nog 2 cpu's listed en nu niks meer....
RX 6800;1;0.25395503640174866;Geluidsdruk 100% fan mist de data voor de kaarten waar de review over gaat?
RX 6800;1;0.45930540561676025;Klopt, helaas werkte de handmatige fan control voor deze kaarten niet, dus konden we de fans niet op 100% instellen om dit te meten. Overigens zegt de 'ingame' meting natuurlijk sowieso het meest, gezien de fans nooit op 100% zullen draaien tenzij je die zelf zo instelt. De grafiek is in ieder geval uit de review gehaald
RX 6800;4;0.3785412907600403;Ik ben ook wel erg benieuwd als ze gaan testen met games waar de CPU niet de bottleneck is wat het verschil gaat zijn met de nieuwste versie processors van AMD en die van Intel (dan doel ik dus vooral op het direct toegang hebben tot het werkgeheugen, wat doet dat qua performance).
RX 6800;3;0.30052459239959717;Het zou fijn zijn als er ook videokaarten van 2 generaties terug even getest worden zoals een GTX1080, waarvan er heel veel verkocht zijn en waarvan de kans heel groot is dat de eigenaren overstappen.
RX 6800;3;0.3815702497959137;Vraag me af of het niet anders kan met krachtige GPU's, als ik de grootte en hoeveelheid aan fans zie op zowel de nvida als amd kaarten is het niet anders dan bespottelijk te noemen. Het moet toch mogelijk zijn grafische wizardry op een energiezuiniger manier te realiseren?
RX 6800;4;0.5348798632621765;Hele nette prestaties. Petje af voor AMD zonder raytracing is de RX 6800 XT net zo snel als een RTX 3080 FE die wel lagere boost haalt en vasthoud dan veel AIB varianten. Maar ik neem aan dat er ook nog OC modellen van de RX 6800 XT komen. Dat word een moeilijke keuze gezien ik raytracing ook wel belangrijk vind.
RX 6800;1;0.2900966703891754;Als je raytracing belangrijk vind is er maar 1 juiste keuze. Nvidia. Als je zonder kan moet je bij amd zijn.
RX 6800;5;0.7791532278060913;AMD is terug in de race en de concurrentie is terug! Ik durf weer om voor het eerst na jaren weer een AMD videokaart te kopen. Mijn laatste AMD Radeon videokaart was de R9 290 kaart met toen Mantle. De RX 6800XT (of de RX 6900XT) zou hier mijn keuze zijn als ik nu op het punt stond een nieuwe videokaart te gaan kopen. Mooie combinatie met mijn Ryzen 7 5800X build. Ik vind de RX 6800XT erg goed geprijsd in vergelijking met RTX 3080. Ik kijk uit naar de reviews met de RX 6900XT! AMD is echt goed bezig! Ze hebben mij laatst echt weten te overtuigen, met de snelste processors met de Ryzen 5000 generatie en nu snelste videokaart. Nu nog zien wat de RX 6900XT kan
RX 6800;5;0.5623347759246826;Weer een mooie week voor de scalpers! Grootste aanbod vind je op marktplaats. Leuk dat de webshops al €900+ vragen voor 650/720 MSRP...
RX 6800;1;0.38808000087738037;Er moeten toch enkele geweldig knappe koppen zitten bij AMD tegenwoordig, met een veel kleiner budget zowel Intel als Nvidia zowat evenaren tot ronduit aftroeven, je moet het maar doen. Had ik nu toch maar mijn instinct gevolgd en die amd aandelen gekocht in juni..
RX 6800;3;0.3215027451515198;Yup eigenlijk zijn we dit jaar verwend met een bedrijf als AMD. Laten we hopen dat ze snel een level up doen met hun volgende kaart en NVidia overtreffen ipv te evenaren. Maar voor nu denk ik dat de champagne wel klaar mag staan voor deze jaren inzet en hard opboksen tegen de groten. Het bewijs dat talent, hard samenwerken als een team, je heel mooie resultaten kan opleveren.
RX 6800;1;0.41798830032348633;Ik was ook al van plan amd aandelen te kopen maar vanwege de pandemie en de fratsen van die gek in het witte huis verwachte ik een enorme beurscrash dus toch maar niet gedaan. Nu heb ik er echt wel spijt van want de boel veert weer op en AMD gaat goed boeren nu ze weer op alle fronten mee kunnen met de marktleiders.
RX 6800;1;0.8566039204597473;Dit worden de goedkoopste feestdagen ever.......niks te koop
RX 6800;3;0.399370014667511;Deze nieuwe AMD kaarten zijn erg snel en geven goede concurrentie op de high end. Echter wacht ik rustig op een 6700(XT) om een 1440p upgrade te doen. Iedereen mag vechten om kaarten van 700+ euro, de grote markt zit in de <400 euro en een 6700(XT) zal hopelijk even snel of sneller gaan zijn dan een 2080S...
RX 6800;2;0.47913551330566406;Geen 1 van de games die ik speel ondersteund Raytracing of DLSS (AC Valhalla & Apex Legends). De in alle benchmarks is de 6800 en 6800XT sneller dan de 3070 of 3080. Daarbij hebben de amd kaarten meer vram aan boord, wat in de toekomst nooit nadelig kan zijn. Er zijn genoeg mensen waar dit voor geld denk ik?
RX 6800;5;0.47776052355766296;Hear Hear Hier wacht ik ook op !
RX 6800;3;0.32183128595352173;“op 4k is het lood om oud ijzer” geen idee wat hier mee wordt bedoelt? Beeldspraak kan alleen wanneer je zeker bent dat iedereen het begrijpt imo, wel weer een leuke uitspraak om aan mijn vocabulaire toe te voegen!
RX 6800;1;0.32495084404945374;Ha, is een uitspraak welke eigenlijk zo normaal is in het dagelijkse spreken dat het me verbaast dat iemand het niet kent. Het is volgens mij dat jeugdigen de normale Nederlandse gezegden en spreekwoorden missen in hun vocabulaire. Ik zie dat ook bij mijn eigen kinderen, of wordt ik oud :-)
RX 6800;3;0.4424707293510437;Alleen jammer dat je wordT schrijft in de ik-vorm
RX 6800;2;0.5110422968864441;Tja dat is dan inderdaad jammer, ik word hier dan ook niet goed van. :-O
RX 6800;5;0.43724653124809265;Als 26 jarige hier ben ik het nog nooit tegen gekomen!
RX 6800;5;0.4514344036579132;Het betekend dat ze beide even goed zijn.
RX 6800;3;0.5663332939147949;Maar iets erbij leren is leuk... Dus wat extra beeldspraak kan graag.
RX 6800;1;0.5458003282546997;Uhm...Tweakers... Ik weet niet wat jullie hebben gedaan met die testbench, maar jullie getallen liggen echt all over the place. In jullie officiele 3080 review krijgen jullie voor shadow of the tomb raider 153,6 fps op 1080p ultra. In deze review is dat plotseling 192,5 fps. Wtf?! Voor een heleboel games zijn er gigantische verschillen. Hoe moet ik deze benchmarks nou serieus nemen...
RX 6800;1;0.4397638440132141;Ze gebruiken nu Ryzen 5000. 1080p is waarschijnlijk CPU bottlenecked.
RX 6800;2;0.41389477252960205;Heb je ook geprobeerd de verschillen te zoeken tussen beide tests? In de 3080 review werd er gebruik gemaakt van een 3900XT en nu van een 5950X op 4.65GHz, wat een enorm verschil in prestatie is.
RX 6800;1;0.4148244261741638;Er zit een super groot verschil tussen de 3900xt en ryzen5000 beetje inlezen de volgende keer
RX 6800;1;0.2250044345855713;Ben ik de enige wie altijd nog NVIDIA boven AMD verkiest puur omdat ik echt niet kan wennen aan de Software van AMD? AMD is natuurlijk al heer en meester in de processoren en dit zal bij de videokaarten niet heel veel anders worden. Ik wil ook dolgraag een AMD build hebben om geld te besparen omdat het toch goedkoper is dan NVIDIA maar de Software blijft een dingetje. Noem me ouderwets.
RX 6800;5;0.31104224920272827;Als er iets niet te doorgronden is, dan is het wel met NVidia, of je moet Geforce Experience installeren, maar dan moet je weer verplicht een account hebben. En dat is bij mij de reden geweest om van NVidia af te stappen. En nu ik zie dat de 6800XT in alle games die ik speel de snelste is (Sneller dan de 3080FE met meer dan een paar %) dan wacht ik nog even met geduld op de normale prijzen met AIB uitvoeringen
RX 6800;2;0.4381304681301117;6800XT is zeker niet de snelste, hangt zeer veel samen met wel of niet gebruiken van raytracing.
RX 6800;2;0.42945683002471924;In de spelen die ik speel is de 6800XT sneller, veel sneller zelfs. Ray Tracing is geen optie in die games omdat je dan zelfs met een SLI 3090 geen 144 fps haalt. Dus RT is geen argument, maar die 10+ procent aan extra FPS wel. Wanneer ik andere games zou spelen zou het misschien anders om zijn. Maar ik speel maar 2 spelen veel en daarbij verslaat de 6900XT in mijn resoultie zelfs de 3090 in 1 van de gevallen
RX 6800;2;0.46679195761680603;Ik schakel bij mij zelfs de nvidia software uit. Die gpu werkt prima zonder die control panel draaiend op de achtergrond. Helaas weet ik niet hoe dit bij amd werkt. Amd lijkt ook voor zijn cpu veel meer software nodig te hebben voor optimale prestaties. Waar amd echt te kort lijkt te schieten is features als rtx voice, reflex, dlss, nvenc. Dat is mij meer waard dan software.
RX 6800;3;0.4319787323474884;Ok, wat was je laatste ervaring precies? Zelf zit ik bijna nooit in de settings. In het begin paar kleine dingen zoals anti-lag aanzetten en de beeldverscherping aanpassen en dan heb ik het wel gehad. Nvidia ziet er juist nog erg ouderwets uit met hun interface, kan zo 15 jaar terug en zie weinig verschil?
RX 6800;5;0.273563951253891;Waar kan je niet aan wennen dan? Het is toch veel duidelijker dan die jaren 90 interface driver van Nvidia? Je hebt die software niet nodig ook, MSI afterburner kan alles beter dan de Nvidia en AMD drivers bij elkaar.
RX 6800;2;0.4386560022830963;Nee dat heb ik ook. 11 jaar lang AMD gehad en had best wat problemen, maar ik dacht, prima, hoort er vast bij, totdat ik er een Nvidia kaart in knalde omdat de Vega uitgesteld was een een mooie deal kreeg op mijn 2 Fury X kaarten. Toen zag ik pas wat goed drivers konde doen. Zichtbaar minder problemen en veel sneller problemen gefixt. De stap om nu terug te gaan naar AMD is wel een ding.
RX 6800;1;0.3851674199104309;Misschien lees ik er overheen... Maar de gemeten vermogen (watt verbruik). Is dat alleen de kaart of het hele systeem? #durftevragen.
RX 6800;3;0.2706943452358246;Alleen de kaart, zie onderaan de pagina 'Testverantwoording'
RX 6800;1;0.5821132063865662;check, heel erg bedankt!
RX 6800;1;0.6787142157554626;Het review NDA op hetzelfde tijdstip zetten als de verkoop can de kaarten is wel een hele grote misser van AMD. Je moet(als je dit jaar nog wilt gamen) de kaart blind gaan kopen en alleen op basis van AMD's benchmarks?
RX 6800;1;0.5907418727874756;Dan koop je toch niet? Waarom doet iedereen net alsof je verplicht bent een kaart te kopen? AMD heeft 2 data aangekondigd: - vandaag voor de reference design, waarvan er altijd een gelimiteerde voorraad is geweest in welke voorgaande launch dan ook - Volgende week voor de AIB kaarten, waar normaal 99,99% van de verkopen vandaan komen. De launch van de 5700xt was exact hetzelfde. Alleen zaten er toen geen duizenden mensen hijgend op f5 te rammen en kon men gewoon een weekje wachten totdat de echte kaarten er waren.
RX 6800;1;0.8042684197425842;Je hebt nog maanden de tijd hoor, maak je geen zorgen niks is leverbaar Ze hadden me helemaal leeg kunnen zuigen (geld-wise), heb de 3080 besteld, impuls/hype was er af heb order geannuleerd. Daarna 5900x besteld, impuls/hype begint er af te raken dus twijfel om te annuleren. Wilde een 6800 XT bestellen, maar je moet kiezen of je reviews wil lezen of dat ding wil kopen, want tegelijk gaat al niet want dan ben je te laat om te bestellen, als je al weet waar je moet zijn want waarom zou iemand daar ooit van te voren helder over communiceren? Hoezo klantvriendelijk? Het is gelukkig geen straf met een 3900x en een 2080ti te gamen, dus ik heb in dat opzicht niks te klagen maar dit geeft wel een nare smaak, en waar ik AMD 3 weken terug nog het beste wenste is dat nu omgeslagen naar een negatief gevoel.
RX 6800;1;0.5574066042900085;Mijn 3080 pre-order is vorige donderdag gewoon geleverd voor 719,- hopen dat die prijzen weer terugkomen voor je..
RX 6800;1;0.6462961435317993;Ik vind eigenlijk dat deze reviews gewoon niet geplaatst zouden moeten worden. Pas bij echte leverbaarheid online zetten. Wat hebben we hier nou aan?
RX 6800;1;0.7159917950630188;"Precies wat @Loggedinasroot dus zegt: ""Het review NDA op hetzelfde tijdstip zetten als de verkoop can de kaarten is wel een hele grote misser van AMD. Je moet(als je dit jaar nog wilt gamen) de kaart blind gaan kopen en alleen op basis van AMD's benchmarks?"" Reviews pas toestaan als het product lanceert zou illegaal moeten zijn."
RX 6800;1;0.8124937415122986;Gebaseerd op wat? Je hebt geen verplichting om te kopen? Als je het niet vertrouwd koop je het gewoon even niet. Dat allerlei mensen zich helemaal gek laten maken door hype is jammer, maar dit soort onzin is ook niet nuttig. Fabrikanten hebben geen enkele verplichting om een review toe te staan voor verkoop begint. Dat ze dat doen is een service voor consumenten. niets meer, niets minder.
RX 6800;1;0.4816906452178955;"Hoewel ik het op papier helemaal met je eens ben werkt onze maatschappij niet zo, een zeer klein deel van de bevolking ""verziekt"" (tussen aanhalings tekens want dit is uiteraard slechts mijn mening en geen feit) het voor de rest door alles weg te kopen op de minuut dat iets beschikbaar komt. Mocht jij als individu binnen 10 minuten (op basis van reviews) kunnen besluiten dat iets het waard is om te kopen dan ben je al te laat omdat het uitverkocht is, en dan word je verplicht om maanden te wachten op voorraden. Dus hoewel wachten in eerste instantie een keus is, is het dat 5 minuten later niet meer. Je word dus in essentie verplicht iets (gehyped/populairs) te kopen voordat je een weloverwogen keuze kan maken omdat je anders sowieso moet wachten."
RX 6800;2;0.4407472610473633;Niet echt, van de 3080 waren reviews de dag ervoor Al beschikbaar...
RX 6800;2;0.5364786982536316;Wel heel slecht met de beschikbaarheid. De grote vraag is hoe is het volgende week met de kaarten van de AIB merken. Hoewel verwacht is het natuurlijk heel Kwalitatief Uitermate Teleurstellend.
RX 6800;2;0.417584091424942;Is het in deze niet meer Kwantitatief uitermate teleurstellend?
RX 6800;1;0.5255882143974304;Volgens mij stond er bij Azerty dat de custom kaarten kwamen, maar dat de vooruitzichten erg slecht waren. En dat ook die amper beschikbaar zou zijn. Kan het fout hebben hoor.
RX 6800;3;0.35686635971069336;Ik weet dat Asus al aangaf dat ze weinig kaarten zouden hebben. Maar ik heb verder nog niets gehoord over andere merken. Ik hoop dat er over de komende weken meer kaarten komen maar het lijkt de RTX 30 series te volgen.
RX 6800;3;0.42398595809936523;Hele goede score van AMD, ben ik onder de indruk...nee.... Het is best jammer dat het prijsverschil niet zo groot is. Want bij het kiezen van een nieuwe GPU neem je ook de drivers mee en die zijn van Nvidia een stuk stabieler, beter. Daarnaast is de beschikbaarheid weer enorm jammer. Net als bij Nvidia. Veel mensen vielen daar over, nou...tadaaaaa, het is bij AMD niet veel beter, ook niet qua prijs. Erg benieuwd naar de 6900 XT vs 3090. Dat gaat toch wel een optie worden tenzijn Nvidia wat moois in de pijplijn heeft...de 3080 ti. Wat ik wel BS vind, dat Warzone niet wordt meegenomen in de benchmarks...is maar een game met 60 miljoen spelers...
RX 6800;1;0.5511499643325806;Altijd dat gezeik -sorry voor woordkeuze- over AMD drivers. Het lijkt wel een soort van mantra. Ik zou wel eens een onderzoekje willen zien onder Nvidia en AMD gebruikers hoe ze zelf hun drivers waarderen en wat ze van het andere merk denken op dat gebied. Het zal me niet verbazen dat het groene kamp massaal zegt dat AMD slechte drivers heeft maar dat de gemiddelde beleving weinig uitmaakt.
RX 6800;5;0.39809805154800415;Ja drivers spelen eenmaal een grote rol en daar is bij AMD nog wel wat winst te behalen. Dit zeggen de AMD gebruikers zelf ook. Ook ik heb jaren lang AMD gehad.
RX 6800;5;0.5801447629928589;De drivers zijn al een paar maanden zeer goed.
RX 6800;2;0.3372941315174103;en waar merk je dat aan ? Ik merk om me heen dat er nog steeds veel klachten zijn namelijk, Dat weerhoud mij toch wel om een AMD gpu te kopen ?
RX 6800;5;0.5935123562812805;Omdat ik zelf een amd gpu heb en een vriend van mij een 5700xt en de drivers werken sinds een aantal maanden geweldig, dus daar hoef je je geen zorgen om te maken.
RX 6800;4;0.34015563130378723;"Het is juist die 5700 XT waar ik veel problemen over lees. Maar goed, laatste update was van Aug dus wellicht is dat opgelost, Thanx ;-)"
RX 6800;5;0.4808065593242645;De power/ performance ratio is erg indrukwekkend moet ik zeggen. Als AMD een alternatief had op Nvidia's NVENC had ik er zonder twijfel één gehaald.
RX 6800;4;0.3851410150527954;Gebruik alleen NVENC als je een trage processor hebt, op een 3900x of 5900x (10900K) gebruik X264 medium preset. Zeker bij streaming geeft dit een veel beter resultaat. NVENC h265 is wel top maar CPU encoding ook. Processor encoding bij zoveel threads levert geen verlies in fps op.
RX 6800;3;0.48148059844970703;Zeker met je eens, echter ben ik ook van mening dat Nvidia NVENC (H264) en x264 maar weinig verschillen op hogere bitrate. Als je het analyseert zie je inderdaad verschil maar in een live setting valt het mij persoonlijk niet op als ik op Twitch streams kijk. Daarbij is Shadowplay toch wel een heerlijke tool imo.
RX 6800;3;0.41631683707237244;Op hoge bitrate zijn ook de avc en hevc van AMD prima, onder de 10.000 gaan echt de verschillen ontstaan. Daarom is bij streaming de verschillen het grootst... 6.000 bitrate of lager typisch. nvenc is inderdaad hier veel beter dan wat AMD kan, maar zoals al eerder aangegeven: als je 12/16 core processor hebt dan kun je veel beter over gaan naar cpu encoding en ben je dus verlost van welke videokaart beter is. Dan kun je voor een twitch streaming pc ook voor de 6800 XT gaan daar je streaming kwaliteit niet meer afhangt van de gpu. En wil je met toeters en bellen streamen dan kun je niet om programma's als OBS heen. Shadowplay of AMD live zijn dan toch te beperkt, zeker als je met je kanaal ook inkomsten belangrijk vindt.
RX 6800;1;0.604151725769043;Wil ik eindelijk weer eens een AMD CPU + GPU proberen zijn ze beiden niet verkrijgbaar
RX 6800;1;0.8555706739425659;Het is een ramp, ik wacht al sinds 7 november 2020 op mijn Ryzen 7 5800X en GIGABYTE X570 AORUS PRO moederbord besteld en betaald ik stond maandag 6e plek
RX 6800;1;0.8603259921073914;Zoveel nieuwe hardware producten en erg veel bijna of niet verkrijgbaar, mensen zijn koopziek geworden door corona lijkt het.
RX 6800;2;0.5758254528045654;Niet echt koopziek, meer het thuis zitten en vrije tijd te veel hebben denk ik. Ook kan je geld niet op tig manieren uitgeven,dus hebben we extra over waarschijnlijk.
RX 6800;1;0.7052493095397949;nergenst niks te zien?
RX 6800;1;0.49962612986564636;Alternate, die heeft een gigabyte te koop voor € 1000,- terwijl deze € 700,- moet kosten...de boeven...
RX 6800;1;0.5492105484008789;Geen verkoop zo te zien..
RX 6800;2;0.37185949087142944;Had eigenlijk meer verwacht van de Rage mode en SAM. Verder is AMD weer helemaal terug op GPU vlak.
RX 6800;3;0.5247280597686768;Denk dat met de tijd en driver/software updates het wel iets beter gaat worden dan dit. Want anders heeft het echt bar weinig nut.
RX 6800;1;0.4900057315826416;Alternate 1000 euro voor 6800xt, dan laat ik hem gaan.
RX 6800;1;0.5691193342208862;Alternate: site niet beschikbaar hahah
RX 6800;1;0.5815824270248413;De non XT is 900 euro, die lui zijn gek..
RX 6800;1;0.4236740171909332;Zou verboden moeten zijn
RX 6800;3;0.40019750595092773;Dat was het ook in de Sovjet Unie. Maar ja, dat systeem had weer andere nadelen.
RX 6800;1;0.5989280939102173;"Alternate wil gewoon geld verdienen. Als distributeur kopen ze relatief grotere partijen op en trachten het te verkopen met winst. Stem met je wallet; koop daar gewoon niet meer in het vervolg. Laat ze in het moeras stikken eerlijk gezegd."
RX 6800;5;0.3482189178466797;De boeven !
RX 6800;1;0.4814448952674866;Dit is wel het jaar van de tekorten nietwaar. Van pleepapier tot videokaarten. Ik ga me zo langzamerhand afvragen of het echt aan de levering ligt of dat het allemaal komt door onszelf (ik denk vooral het laatste). Wij consumeren ons zo gek dat er voor bepaalde groepen niets meer overblijft. Snel terugdringen die wereldbevolking.
RX 6800;2;0.34029924869537354;en geen woord over gebruik van deze kaart binnen creative suite of ander zins dan gaming. 95% van de tijd gebruik ik hem voor iets anders dan gaming. kijk wel weer bij puget voor een test die wel zinvol is.
RX 6800;2;0.5469540953636169;tja dit zijn dan ook niet echt de kaarten daarvoor. De RTX kaarten bijvoorbeeld ook niet. De Quadro's zijn de creative, render kaarten die je moet hebben. Gaming kaarten performen sowieso slechter.
RX 6800;4;0.3534289598464966;In de 3D industrie gebruiken ze ook wel de gaming kaarten voor gpu rendering hoor. Dus een benchmark daarvan was best leuk geweest.
RX 6800;3;0.5182378888130188;is wel apart dan. Heb jaren in een architectenbureau gewerkt en dat was altijd Quadro kaarten wat gebruikt werd in alle Workstations. Gaming kaarten kwamen nooit aan te pas. Daar wordt AutoCad, Revit en noem maar op gebruikt.
RX 6800;1;0.46457716822624207;dat is lang geleden of niet Duke ? RtX kaarten worden gewoon in systemen gebruikt, ook de dure Puget systemen gebruiken bij Threadripper of 5950x systemen gewoon een 3080 of 3090 kaart. of is threadripper/3090 consumenten rommel en niet pro genoeg ? voor adobe is dat meer dan logisch vanwege de Cuda's. op content creation en adobe suite gaat AMD helemaal zoek. een onverkrijgbare kaart op een onverkrijgbare cpu testen in een best case scenario, really,? waarde van dit artikel is matig at best.
RX 6800;1;0.6474102735519409;Ben benieuwd... Dear Rob xxxxxx, Thank you for ordering from the AMD.com Online Store Your order has been submitted and is currently being processed. You will receive another email with the details of your order within 48 hours. ORDER DETAILS Order Number: 6578xxxxxxx Order Date: 18 November 2020 Please note: This email message was sent from a notification-only address that cannot accept incoming email. Please do not reply to this message. Sincerely, AMD.com Online Store Customer Service
RX 6800;1;0.5512273907661438;ik kan die niet bij AMD.com bestellen, kan alleen bij een webwinkel die ze doorverwijzen
RX 6800;1;0.4491047263145447;Digital River? Heb ik toen mijn 3900X ook besteld, die nergens te krijgen was.
RX 6800;5;0.4556830823421478;Coolblue is ook goed bezig haha RX 6800 XT - 9999,-
RX 6800;2;0.5219735503196716;De echte verliezer lijkt toch Intel te zijn. Die roepen al een paar jaar mee te willen doen in de videokaartmarkt. Maar ze lijken het gat voorlopig niet te kunnen dichten. Het Duopolie blijft voorlopig nog even.
RX 6800;2;0.32300642132759094;Intel heeft ander huiswerk te doen, op CPU vlak.
RX 6800;3;0.263288676738739;Is er al iets bekend over of er budget varianten gaan komen op deze kaarten? Daar vind je nog altijd enkel de 4.5 jaar oude Polaris. Ook de groene echte low end kaarten (1050) zijn toch al weer 4+ jaar op de markt trouwens.
RX 6800;1;0.33262309432029724;€ 9,999,- bij CoolBlue
RX 6800;3;0.29000240564346313;Denk toch dat ik voor kamp groen ga voor mijn nieuwe pc. Puur om DLSS en raytracing.
RX 6800;3;0.33662623167037964;Vraag me alleen af, of mijn voeding nog wel meekomt...
RX 6800;3;0.4622996747493744;Ik heb de juiste beslissing genomen met de RTX3080. In RDR2 is de RX6800XT minder, en dat was voor mij van belang.
RX 6800;3;0.3909386694431305;Kunnen jullie in de prestatiescore games aub. ook de 99 en 99.9p gemiddelde scores erbij zetten? Die lijken namelijk zich wat anders te gedragen dan de gemiddelde en max waardes en dit is ook wel om even goed naast elkaar te kunnen zien.
RX 6800;5;0.43087708950042725;Wat een beest die 6800xt! De keuze is voor mij nu wel heel makkelijk!
RX 6800;5;0.4314239025115967;Wauw 🥲
RX 6800;4;0.2984035015106201;Ben beniewud wat de bandwidth voordelen van Nvidia welke tot uiting komt op 4K gaat betekent voor games in de toekomst op lagere resolutie. Ik ga ervanuit dat bandwidth vereisten ook daarin groter gaan worden.
RX 6800;2;0.2711176872253418;Wauw, wie had een jaar geleden gedacht dat AMD zowel de gamingprestatiekroon voor CPU's als GPU's in handen zou hebben(nou ja, wat betreft CPU's was Ryzen 3xxx al dichtbij). Niet dat je ze ergens echt kan kopen nu, maar toch...
RX 6800;1;0.38945648074150085;1) Open 2021_Ryzen_5000_Upgrade.txt. 2) Backspace... MSI RTX 3070 Gaming X Trio. 3) Typing... MSI RX6800 Gaming X Trio. 4) File.. Save... Close.
RX 6800;1;0.3531099557876587;Waarom wordt dlss niet meegenomen in de grafieken? Dus 3080 FE (dlss on) en 3080 FE (dlss off). In de praktijk zal een gebruiker deze functie gewoon aanzetten wanneer dit bij een game beschikbaar is. Weinig games ondersteunen dit natuurlijk, maar om het nou weg te laten in de games die het wel hebben haalt de prijs per fps uit verhouding. Je moet immers betalen voor die functie.
RX 6800;2;0.40968796610832214;Ik had nog gehoopt op een undervolt & overclock analyse gehoopt - volgens de geruchten doen sommige kaarten 2,5 GHz met een undervolt en minder energieverbruik. Zou tweakers dit nog toe kunnen voegen? Ik hoop het
RX 6800;2;0.4864822030067444;Frequentie is irrelevant want met hogere clocks kunnen je prestaties omlaag gaan. Testen testen testen niet je blind staren op een nutteloos iets als frequentie.
RX 6800;3;0.27906885743141174;Ik vraag toch ook om een test?
RX 6800;5;0.3833359479904175;2.65 GHz op lucht van wat ik lees en dan is hij sneller dan een RTX 3090 op stikstof.
RX 6800;2;0.44206687808036804;Ik mis hier het kopje connectiviteit. HDMI 2.1 is een belangrijk punt voor mensen die in hun vrije tijd liever niet achter het bureau zitten, maar toch de voorkeur geven aan gamen met de PC. Ondersteunen deze kaarten het überhaupt? Is de boel compatible met gangbare modellen of zijn er indicaties dat dit niet het geval is? Qua audio heeft AMD in het verleden ook vaak het nodige laten zien met hun GPU's, is dat nu ook weer het geval?
RX 6800;1;0.5144819617271423;Leuk al die grafiekjes maar uiteindelijk vind ik zowel de Ampére als de RDNA2 launch de meest teleurstellende releases in een hele lange tijd. Waarom? Omdat de kaarten simpelweg niet verkrijgbaar zijn, of tegen belachelijk hoge prijzen. Ik was van plan om voor het einde van het jaar een fikse computer upgrade uit te voeren maar met dit soort prijzen ga ik dat echt niet doen.
RX 6800;1;0.5656141638755798;2x zoveel prestaties voor je geld als 6 mnd geleden vind je teleurstellend? Het is lang geleden dat er zulke grote stappen werden gemaakt. En daarbij, als je high end wil komt dat met high end prijzen...
RX 6800;3;0.5250876545906067;Misschien moet je verder lezen dan de eerste zin.
RX 6800;1;0.4332844913005829;Tsja had ik gedaan. 2x zo snel en goed verkrijgbaar is gewoon nog nooit gebeurd in de geschiedenis van de PC. Wacht gewoon een paar weekjes en bekijk dan opnieuw de beschikbaarheid, aantallen zullen toenemen en de prijzen normaliseren. Het gaat vast lukken in 2020.
RX 6800;1;0.6770460605621338;ik zet wederom mijn vraagtekend bij deze gehele review vooral omdat bijvoorbeld gamers nexus met veel meer test ervaring heel andere resultaten laat zien in dezelfde tests. Bijna alle resultaten hier zijn slechter voor alle kaarten. Terug naar de tekentafel dus en kijken wat er in vredesnaam anders is of verkeerd gaat.
RX 6800;5;0.4447180926799774;Niet alleen GN maar ook Linus...
RX 6800;2;0.46246814727783203;Ja ik heb nog veel meer sites bekeken en bij sommige is er gewoon een enorm verschil te vinden. Die uitersten gooi ik er dus ook uit en negeer ik.
RX 6800;3;0.3803391456604004;Zou je dit concreter kunnen maken? Feedback is altijd welkom, met specifieke punten schaven we onze testprocedure regelmatig bij. Dat andere publicaties andere resultaten laten zien zal primair komen door verschillen in testsystemen, games en benchmarks. Dat alle testresultaten bij ons 'slechter' zijn betekent niet automatisch dat de verhoudingen onderling anders liggen, alleen dat onze benchmarks zwaar zijn voor de GPU's, wat ook exact de bedoeling is bij het testen van 3D-chips.
RX 6800;3;0.28185102343559265;kijk filmpje maar eens
RX 6800;2;0.4058406352996826;Als de RTX3080Ti uitkomt gaat dat de kaart worden denk ik. De RTX3080 met z'n 10GB blijkt nu het blok aan het been te zijn tegenover de RX 6800 XT Vroeg me al van in het begin af, waarom 10GB ?? En geen 12GB of ook 16GB net als AMD Prijs/kwaliteit is de RX 6800 XT de meest interessantste.. maar voorlopig blijft m'n RTX2080Ti OC de hoofdrolspeler, m'n tweede computerbuild moet dan maar tot volgend jaar wachten in de hoop dat er dan een deftige GPU uitkomt dat het verschil gaat maken in 4K.. want eerlijk gezegd is de prestatiewinst niet wat ik verwacht had..
RX 6800;1;0.4508025646209717;Als ik alle info zo lees kan ik alleen maar concluderen dat er nog teveel variabelen zijn.Beiden hebben een aanlever probleem.Je zit met third-party builds die ook zich nog moeten bewijzen.Drivers staan nog niet op punt en de bugs zullen er nog uit moeten, kwestie van tijdDe benchmarks zijn nog te beperkt en zijn meer dan ooit afhankelijk van de cpu en andere componenten.ray tracing...Dit zijn voor mij toch nog grote vraagtekens bij eender welke kaart deze periode op de markt komt. Conclusie: ik wacht tot ten vroegste maart om genoeg informatie te hebben over deze dure aankoop.Pas dan weet je wat je koopt.
RX 6800;2;0.36560750007629395;AMD doet alles goed totdat je de prijs hoort. Zaten ze vroeger nog vaak onder de prijs van Nvidia, zitten ze er nu zelfs boven! Koekoek.. De prijs zal toch echt een klap lager moeten zitten dan het equivalent van Nvidia, zoals ze wel doen met de RX 6900 XT tov RTX 3090, 1000 euro vs 1600 euro. Helaas denkt AMD dat dit niet nodig is voor de andere kaarten.
RX 6800;5;0.26932036876678467;AMD zit nu hoger in de prijs dan vergelijkbare Nvidia kaarten.
RX 6800;1;0.3976393938064575;Hoe zouden de resultaten er uit zien met een Ryzen 3600X ipv 5950, dus zonder Infinity Cache of SAM? Ik wil nml enkel m'n videokaart vernieuwen, niet m'n hele systeem.
RX 6800;5;0.41557157039642334;Nvidia draait nog steeds sneller, zie Linus Tech op youtube.
RX 6800;2;0.517743706703186;Er zijn te veel maren om die conclusie juist te noemen, hij doet dat ook niet. b.v. op 1440P kun je geen snellere kaart kopen dan de AMD
RX 6800;3;0.5460795164108276;Mijn eigen voorkeur blijft toch Nvidia, heeft meer voordelen. Ik vond de conclusie van Tweakers nogal kort door de bocht om voor alle toepassingen AMD te kiezen.
RX 6800;5;0.4955567419528961;Je kunt natuurlijk altijd je eigen voorkeur hebben, ieder zijn smaak toch.
RX 6800;3;0.310680627822876;Rare conclusie want hier wordt toch een andere conclusie getrokken.
RX 6800;3;0.4227117896080017;Ik vind het niet zo gek dat de AMD GPUs niet te krijgen zijn. AMD bouwt de CPU en GPU voor de Xbox, voor de PS5 en voor de desktop. Dat zijn héél veel chips.
RX 6800;3;0.40433937311172485;Zijn er manual OC resultaten?
RX 6800;2;0.29456403851509094;Wat misschien een goede racegame is, is Assetto Corsa Competizione, deze vraagt veel van een GPU. En de simwereld springt om benchmarks qua GPU's om dat de 6800(XT) en de 3xxx serie te zien. Misschien afgezet naar een 1080Ti, 2080Ti n 5700xt zodat men ook ziet hoe ze performen tov oudere generaties. Dit is bericht 372 oid, en staat op p3, geen hond die dit leest.
RX 6800;5;0.2561784088611603;Jawel hoor
RX 6800;4;0.23950062692165375;En ik ook!... Want dit is precies waarom ik geïnteresseerd ben in de 6800. Draai op mijn PC vooral (niet enkel, maar wel 90%) racegames, en tegenwoordig liefst ook in VR met de Quest 2, naast mijn ultrawide op 3440x1440. Draaide dat voorheen op een GTX 1660 Super, en dat draaide ok-ish, maar veel getweak nodig met de instellingen. Nu recent van een andere Tweaker een GTX 1080ti overgenomen, die het met grote stappen beter doet. Gezien het prijsniveau kijk ik nu ondertussen naar de 6800 (en heb ook een soort van bestelling ervoor, maar weet niet of t gaat lukken) en de RTX 3070. Er is wel 1-en-ander op YouTube te vinden, maar ik probeer nu adhv de standaard benchmarks (1080 / 1440 / 4K) te bepalen wat het ongeveer voor mij zou betekenen op die ultrawide en in VR. Er is een vuistregelen hier ooit geplubliceerd: 19% aftrekken van 1440p resultaten, of 40% optellen bij 4K resultaten voor de 3440x1440 indicaties. Daar houd ik me maar even aan vast.
RX 6800;1;0.849811315536499;allemaal irrelevant als zowel nvidia als amd niets leverbaar heeft
RX 6800;5;0.24562475085258484;Ik zie inmiddels wat prijzen in de webshop van Alternate verschijnen: ASRock Radeon RX 6800 16GB EUR 899 incl BTW (preorder) GIGABYTE Radeon RX 6800 XT 16G EUR 1049 incl BTW (uitverkocht) ASRock Radeon RX 6800 XT 16GB EUR 999 incl BTW (preorder)
RX 6800;1;0.37410274147987366;"Haha, ""Rage mode"", back to 2001? Maar zijn de AMD drivers nog even ruk als tien jaar terug? Wat een ellende was dat zeg."
RX 6800;1;0.7710349559783936;nVIDIA biedt zoveel meer prestatie winst met raytracing, cuda, alle andere opties die nVIDIA biedt. AMD heeft een kaart die kaal meekomt maar verder totaal niets anders biedt. Sterker, de media encoders e.d. zijn zo slecht van AMD dat het eindresultaat onbruikbaar is. LinusTechtips heeft hier een aardige video over gemaakt. AMD vraagt een stuk meer voor zijn kaarten maar levert een karig product af. Wat er in de toekomst door AMD nog toegevoegd wordt is maar af te wachten.
RX 6800;5;0.2905336320400238;Eindelijk concurrentie!! Maar langs de andere kant weet intussen iedereen dat de 3070/3080 standaard een te hoge voltage mee krijgen. De meeste Tweakers onder ons undervolten deze kaarten waardoor ze makkelijk 100W minder verbruiken en beter presteren zonder de powerlimit aan te tikken. Dus in feite minder stroom gebruiken dan de AMD tegenhangers en dan weer meer fps genereren. Overigens stel ik me ook vragen bij de SAM benchmarks, ik zou de benchmark overdoen op een Intel systeem waar SAM momenteel niet mogelijk is dan zie je toch andere resultaten dan op Tweakers. Het is wel nice van AMD dat SAM opensource is waardoor dat SAM of een variant er van naar Ampère komt waardoor de cijfers nogmaals gaan veranderen. Ik persoonlijk ben voor een Intel + 3080 gegaan want ik heb geen zin om nog allerlei software te moeten installeren en gebruiken om AMD's CPU's deftig te laten werken, waar je met Intel meteen klaar mee bent. Ook hun Radeon software geraak ik niet wijs van, ik ben mijn Nvidia control panel zo gewoon, het ziet er misschien outdated uit maar alles is zeer duidelijk. Maar dit is gewoon persoonlijk. Again: top dat er concurrentie is, eens de kaarten van Nvidia en AMD goed verkrijgbaar zijn, kunnen ze aan de prijzen oorlog beginnen!
RX 6800;2;0.3794441819190979;Ga er van uit dat als ik dus een raytracing kaart wil voornamelijk voor dxr/rtx aangedreven productivity (voornamelijk lighting baken in unreal en unity) nog steeds beter af ben bij het groene team vanwege een volwassenere raytracing chip?
RX 6800;3;0.38513192534446716;De afgelopen week veel benchmarks voorbij zien komen ook op alle YT kanalen en overall komt AMD met de 6800XT wel als winnaar uit de bus ten opzichte van de Rtx3080 als het gaat om de FPS score. Zelfs indien men geen gebruik maakte van een Zen3 CPU. Tussen de 6800 en de 3070 ligt dit toch wel dichterbij elkaar en is het op benchmark niveau toch wel de 6800 welke triomfeert icm Zen3. Dit uit zich niet altijd even uitdrukkelijk in de fps score want indien er geen Zen3 systeem word gebruikt zijn er ook games waarbij de 3070 het aanzienlijk beter doet. Hierin blijft staan dat Nvidea überhaupt beter presteert op het gebiedt van Ray Tracing. Hier tegenover staat dan weer het verschil in Vram, 8GB bij de 3070 vs 16GB bij de 6800. In theorie zou je verwachten dat de 6800 hierdoor in de toekomst een groter gat gaat slaan ten opzichte van de goedkopere 3070.
RX 6800;5;0.5368270874023438;Inmiddels komen de RTX3070's goed op gang qua voorraad. Ik heb er iig 1 kunnen bemachtigen, en het is een beest op 1440p ultra.
RX 6800;5;0.30858051776885986;Welke heb je?
RX 6800;2;0.3737355172634125;Corona, dus dichte fabrieken en zieke medewerkers En AMD moet ook nog eens leveren voor zowel de PS5 als de XBOX Series S/X En ik denk helaas dat de consoles in ieder geval tot aan kerst echt de prioriteit hebben Die worden namelijk gewoon veel en veel meer verkocht
RX 6800;2;0.31842154264450073;En hebben veeeeeeel minder marge, maar het zullen vooral afspraken zijn waar ze zich aan moeten houden, want je verkoopt liever 10 apparaten met 100 euro winst dan 1000 chips met 50 cent winst
RX 6800;2;0.375349223613739;Ja inderdaad De marges zijn veel lager, maar de contracten zullen ze waarschijnlijk dwingen om aan Sony en Microsoft te leveren Overigens is dat voor AMD ook wel weer chill, want dat betekent dat een groot deel van de games dadelijk al geoptimaliseerd is voor Big Navi, en de games dus op PC waarschijnlijk ook beter presteren op de nieuwe AMD kaarten
RX 6800;3;0.4477013647556305;Dat wordt vaker geroepen maar dat was met de PS4 en Xbox ook al zo die waren ook RDNA (1) gebaseerd. Dus persoonlijk geloof ik er niet echt in dat dat verschil maakt, maar ik hoop dat je gelijk hebt
RX 6800;2;0.31190136075019836;RDNA is pas in 2019 uitgekomen De PS4 en XBOX waren hier dus niet op gebaseerd En bovendien was dat ook nog eens in een tijd waarin AMD niet kon concurreren met Nvidia in de high-end Nu kunnen ze dat wel, en als de prestaties normaal gesproken redelijk gelijk zijn krijg je dan misschien net het voordeel van die optimalisatie
RX 6800;2;0.33241236209869385;Ter nuance: men gaat er al nu direct vanuit dat er weinig voorraad is waardoor 90% van de kopers direct bij launch zit te F5-en. Verkopen die normaal over de eerste weken uitgesmeerd worden komen nu allemaal direct de eerste minuut binnen. Feit blijft inderdaad dat de vraag groter is dan het aanbod. Ik denk persoonlijk dat dit te maken heeft met het feit dat er dit jaar eindelijk weer significante stappen gemaakt worden qua performance waardoor iedereen wil upgraden. Dat in combinatie met de coronaciris waarin men sowieso al veelal thuiszit zorgt voor een flinke toename in vraag.
RX 6800;2;0.37829142808914185;Vlak ook de problemen in de distributie niet uit. Ik hoor op veel plekken over voorraad problemen. Niet alleen in de IT, maar electronica in het algemeen. En niet alleen over eindproducten, maar ook over reserve onderdelen en halffabrikaten.
RX 6800;2;0.4090813100337982;Ja als je net een 3080 hebt gekocht lijkt me het raar om dan gelijk over te stappen naar een andere kaart, zou wel heel snel wezen.
RX 6800;2;0.3443606197834015;Was ook niet van plan om over te stappen maar zoveel mensen roepen omdat ze geeen 3080 krijgen.. Ik wacht wel op die van AMD!!! en nu is die nog schaarser en minder performance dan de 3080. te grappig
RX 6800;2;0.45207884907722473;Ik weet niet welke benchmarks jij hebt gekeken maar de 6800xt is sneller dan de 3080 en de 6800 is sneller dan de 3070, zo grappig is het niet.
RX 6800;4;0.3630521297454834;"Dit jaar zeker; veel mensen die normaal gesproken op vakantie gaan die dit nu niet doen en hierdoor vakantiegeld/kerstbonus aan iets anders uit kunnen geven. Daarnaast zijn veel mensen bereid om grof geld neer te tellen voor hun favoriete hobby, maar dit is natuurlijk per persoon verschillend."
RX 6800;3;0.49849167466163635;Ja, blijkbaar...zal wel te oud zijn om het te begrijpen
RX 6700 XT;1;0.6348952054977417;Eerlijk? Ik lees deze reviews niet eens meer. Ik raak er altijd enthousiast door, krijg neigingen een nieuwe kaart te kopen maar vind de huidige prijzen zo irreëel, laat maar. Daar kunnen jullie uiteraard niks aan doen, maar 't is echt jammer, ik pluis al 20+ jaar GPU reviews helemaal uit.
RX 6700 XT;2;0.4508042633533478;We begrijpen je gevoel volkomen, ook voor ons bij Tweakers voelt het op het moment enorm dubbel om reviews over videokaarten te schrijven. Een videokaart kopen valt nu amper aan te bevelen, ondanks dat zowel AMD als Nvidia met deze generatie relatief grote stappen hebben gezet. Het enige wat op dit moment leuk is aan videokaarten, is er vanuit technisch oogpunt naar kijken. De verkrijgbaarheid is uiteraard een enorme dooddoener als je werkelijk in de markt bent voor een nieuwe kaart, maar de echte tweaker vindt het hopelijk toch nog ergens leuk om te lezen over gpu's zonder dat 'ie een koopintentie heeft (of überhaupt kan hebben). Het probleem oplossen kunnen wij natuurlijk ook niet, maar we hebben gisteren wel een uitgebreid achtergrondartikel gepubliceerd over hoe naast de leverbaarheid, nu ook de prijzen totaal uit de hand lopen. Zo schijnen we in elk geval wat licht op de huidige situatie en geven we de problematiek aandacht. Ik hoop dat de huidige prijzengekte je interesse in en enthousiasme over gpu's niet voorgoed verpest - laten we hopen dat de beschikbaarheid en prijzen van videokaarten zo snel mogelijk weer normaliseert
RX 6700 XT;3;0.36859768629074097;Ik beschouw deze recensies als archiefstukken: op dit moment niet actief bruikbaar, maar in de toekomst mogelijk zeer nuttig. Dan halen we ze weer uit de dozen. Dus ga ermee door.
RX 6700 XT;2;0.2599515914916992;Hoeveel nut heeft dat? Eigenlijk zou je over 6 of 12 maanden (of in ieder geval als de prijzen weer een beetje normaal zijn) alles weer opnieuw willen testen met de dan courante drivers. Dan weet je pas echt goed of een videokaart aantrekkelijk is. Gelukkig besteden sommige website daar ook aandacht aan, zodat we dan nog eens kunnen kijken wat we moeten kopen.
RX 6700 XT;3;0.49728044867515564;Het zal dan wat drivers betreft niet helemaal actueel meer zijn, nee. Maar toch grotendeels nog wel bruikbaar, denk ik.
RX 6700 XT;3;0.3255704343318939;Van de software/spellen die je erop wilt draaien zullen ook wel weer nieuwe versies verkrijgbaar zijn tegen de tijd dat de verkrijgbaarheid van gpu's weer normaal is.
RX 6700 XT;3;0.4960280954837799;Exact, ik wacht wel todat de bubbel barst en de tweedehandsmarkt puilt van goedkope videokaarten .
RX 6700 XT;1;0.4892174005508423;Vandaag stonden er een 100-tal 2080Ti voor $99/stuk. Ze waren in een paar uur weg. De vraag lost zichzelf wel op zodra de NVIDIA en AMD nieuwe kaarten verkoopt aan de miners. Een 3080 brengt momenteel wel EUR500 per maand in het laatje, zolang dit doorgaat ga je geen kaarten vinden onder de EUR2000, zelfs RTX4000 en 6000 is nu tekort omdat die voor hetzelfde geld net zo goed kunnen hashen als een 2080 of een 3070. Eenmaal kaarten richting de 5000 gaan, zul je de Tesla’s ook zien verdwijnen.
RX 6700 XT;3;0.35072794556617737;Waar vind je zulkte 100 tal 2080's voor 99/s ? ik ben wel benieuwd waar ik moet zoeken voor dit soort dealtjes.
RX 6700 XT;2;0.3362492322921753;EBay deze morgen. Je moet wel snel zijn, ze waren binnen een paar uur verdwenen. Ik denk dat het batches zijn van warenhuis of bedrijfsliquidaties. Soms kun je kaarten goedkoper krijgen adhv model nummer, bijvoorbeeld een NVIDIA 900-1G180 ipv 2080, vooral van recycleerders.
RX 6700 XT;5;0.44969549775123596;Thanks voor de info! Vandaag een 2080Ti ROG STRIX OC voor 87 euro gehaald. Nu hopen dat het geen scam is...
RX 6700 XT;3;0.2587593197822571;Mag ik vragen welke zoekterm je gebruikt hebt op ebay (neem ik aan)
RX 6700 XT;1;0.4719482958316803;Bedoel je dat de miners 500 euro per maand verdienen met zo'n kaart??
RX 6700 XT;4;0.42284393310546875;Laten we het hopen. Gelukkig doet mijn gtx1080 het nog prima....
RX 6700 XT;2;0.4563014805316925;Die ben ik nu aan het zoeken maar kan hem nergens meer vinden.Tenminste niet nieuw, ja ik weet het het is een oudere kaart maar nog wel snel genoeg voor mijn nieuw systeem. ik had een asus kaart gezien deze week die kosten bijna 3000,00 euro maar wel met waterkoeling stock erbij.
RX 6700 XT;1;0.36868974566459656;Ga maar uit van 2022.
RX 6700 XT;2;0.4584287106990814;Als het dat al gaat worden...Vaak zie je dat juist in crisistijden prijzen omhoog gaan, en uiteindelijk ook niet of nauwelijks omlaag gaan. Ik verwacht niet dat ze met de RTX3000 of AMD 6000 serie omlaag gaan met de prijs wanneer de yields beter gaan, althans niet tot het niveau van de adviesprijs. Helaas is uit vorige crisissen telkens gebleken dat de verhoogde prijzen als nieuwe standaard gelden voor opeenvolgende producten. Prijzen worden nou eenmaal hoger en dan niet alleen door inflatie. Ik zit al ja-ren te kijken voor een naar-ratio-presterende vervanger voor mijn GTX970 die ik voor 319 euro heb gekocht in 2015. Dat gaat nog wel even duren, vermoedelijk tot ver in 2022. Na de GTX 900 series kwam voor mij gevoelsmatig die ommeslag naar ineens veel duurdere videokaarten en datzelfde geldt ook voor smartphones. Never waste a good crisis komt helaas nooit goed uit voor de minder bedeelde mensen. Niet dat ik mezelf daar onder schaar, want een videokaart is zeker geen noodzaak, maar het maakt me wel steeds afgestompter eronder ja. In ieder geval zou ik met een RTX2060 2 jaar geleden of nu een RTX3060 (ti) toch wel bereid zijn om de adviesprijs te betalen, maar ja, ik ben bang dat dit niet meer terug komt. Ik hoop van wel. Tot die tijd, staar ik me wel blind op grafiekjes en zet ik wel een emmertje onder mijn gezicht. Hoop doet leven, nietwaar?
RX 6700 XT;5;0.7302539348602295;Ik heb een Xbox gekocht, beste alternatief voor een game pc 😉
RX 6700 XT;2;0.4361226558685303;Het vervelende is dat die ook (op de series S na) overal uitverkocht zijn. Maar ik snap je beslissing hoor.
RX 6700 XT;3;0.2927825152873993;Kostte inderdaad wat moeite hier in DK ook, paar maanden lang iedere dag weer kijken of er niet toevallig voorraad was 😉
RX 6700 XT;3;0.45697221159935;Same here, ik kan nog aardig gamen op mijn GTX 970 ook op 1440p. Vorig jaar wilde ik eigenlijk een 2070s hebben maar dacht slim te zijn om te wachten. Overigens 8 gb vind ik anno nu wat weinig daar ik wat langer med videokaarten doe. Juist cpu’s vervang ik sneller. Nu 9900k gaming en een 10900k 128gb voor vmware. Gezien de prijzen ga ik nu voor een laptop 4k 3080 en een Ryzen cpu. In verhouding zijn die nog redelijk geprijst. Minder stroom en kan ik nog eens remote werken(gfx design).
RX 6700 XT;3;0.43335357308387756;Beetje raar dat ze een nieuwe processor maken ipv meer van de bestaande waar al tekorten van zijn
RX 6700 XT;5;0.5197067260742188;Ik denk dat heel de community jouw gevoel begrijpt en voor een groot deel hetzelfde heeft. Maar deze schaarste gaat ooit voorbij (en daarmee die absurde prijzen ook) waardoor een artikel zoals deze nog altijd heel erg nuttig kan zijn.
RX 6700 XT;1;0.6014159321784973;"Het probleem is denk ik groter dan we nu denken. De absurde prijzen zijn nu immers betaald, dat zien producenten ook. ""Mensen kochten in de ""crisis"" voor 2x retail een kaart? Dan kunnen we net zo goed zelf de prijzen x2 doen"". Bedenken de aandeelhouders van Nvidia dit en die van AMD ook dan zien we de prijzen die we hadden echt nooit meer terug. Denk ook aan wat er gebeurde met de iPhone. Het topmodel 6S kostte 699 euro en verkocht mega. De X kostte 999 euro en verkocht mega. De 12 uit mijn hoofd 1299 euro, en verkocht nog steeds mega. De volgende iPhone zal daarom niet goedkoper zijn dan 1299 euro. Ik denk zelfs duurder met als excuus silicon tekorten en corona. Dat zal tot op zekere hoogte ook allemaal waar zijn maar zoals in het uitgebreide artikel over de recente prijsstijgingen ook al bleek, niet alleen fabrikanten, maar ook distributeurs en webshops hebben hun marges drastisch opgeschroeft. Zolang mensen toch wel blijven kopen gaan die echt niet meer omlaag."
RX 6700 XT;1;0.4799865484237671;De enige kanttekening die ik daarbij zou willen zetten is dat de mensen die Apple spullen kopen standaard geen kloot lijken te geven om de prijs en prestaties, zolang het nieuw is en er n appeltje op staat wordt er desnoods een tweede hypotheek voor afgesloten. Hoewel deze trend zich ook steeds meer naar andere merken/vlakken doortrekt hoop ik toch dat dit gigantisch gaat backfiren als ze die truc proberen uit te halen met GPUs of andere tech.
RX 6700 XT;3;0.26688259840011597;De trend in de GPU markt is ook dat topmodellen alsmaar duurder en duurder worden.
RX 6700 XT;1;0.6131067276000977;Dat hoeft in dit geval niet op te gaan: ik denk dat de crypto miners erg veel kaarten uit de markt trekken. Houdt die gekte weer eens op dat zakt die vraag in. Komt daarnaast de productie weer fatsoenlijk op gang, dan is een prijsverlaging vrijwel onvermijdelijk - wanneer dat gaat gebeuren is de vraag, ik zou hopen op later dit jaar, maar vrees dat we dan al dik in 2022 zitten!
RX 6700 XT;2;0.4803329110145569;"Beetje kort door de bocht. Apple heeft tot de 5S alleen een type telefoon uitgebracht, Bij de 5S hebben ze gekeken of er ook interesse was in een type telefoon dat lager gepositioneerd is, de 5C. Een paar generaties verder, ten tijde van de iPhone 8 hebben ze gekeken of er interesse was in een model gepositioneerd boven hun normale type, de X. Fast forward 4 jaar, en je kunt nog steeds een iPhone SE kopen voor ~500 euro, een normaal model (12 mini) voor ~700 euro, en het topmodel met alle toeters en bellen voor een belachelijk bedrag. De meeste concurrenten zijn hun daarin gevolgd, als je perse het duurste wil hebben kun je voortaan >1000 euro stukslaan op een telefoon. Om in videokaart termen te spreken; een beetje alsof je de eerste Titan met een 'normale' consumentenkaart van de generatie ervoor vergelijkt"
RX 6700 XT;3;0.5683702826499939;De vergelijking gaat hier niet helemaal op. Voor mij is de afweging voor een videokaart veel 1 dimensionaler dan bij een telefoon. Misschien kunnen features zoals dlss, ray tracing en encoding doorslag geven maar voornamelijk de performance bepaalt de prijs. Dlss is eigenlijk ook een performance feature. Als de gekte weer voorbij is zullen AMD en Nvidia gewoon met capaciteit blijven zitten als ze niet of de prijzen verlagen of een lager range model aanbieden.
RX 6700 XT;3;0.33426737785339355;Ik denk dat we na deze crypto bull-cycle wel weer een gpu kunnen kopen voor normale prijzen, dat kan nog een paar weken duren of een paar maanden, dat weet niemand.
RX 6700 XT;2;0.43501219153404236;Ik zie sommige bedrijven er wel voor aan om vanaf nu hogere prijzen te handhaven dan in het verleden, het wordt (ook door gamers) toch wel gekocht. Wanneer het minen niet meer rendabel is zal de markt echter overspoeld worden met goedkope tweedehands GPU's van miners die nog even de laatste centen er uit willen wringen, dus dat zullen voor webshops/leveranciers waarschijnlijk even wat magere tijden worden.
RX 6700 XT;3;0.5690211057662964;"Dat verwacht ik ook, daarom heb ik me (alvast) voorgenomen om uit principe geen tweedehands videokaart te gaan kopen die vermoedelijk uit een mining rig komt. Wordt natuurlijk moeilijk te weerstaan voor velen, zeker gezien de markt massaal videokaarten wil aanschaffen en ze straks opeens op Marktplaats verschijnen voor ""redelijke"" prijzen. Zal interessant worden om te zien of er vanuit officiële kanalen een tegenreactie komt (het zij lagere prijzen of reguleren van aanbod); doet me ergens wel een beetje aan het bewegen van de OPEC denken. We gaan het zien. Ik vind het tegelijk ook wel lastig hoor; ik wil heel graag mijn eigen systeem bouwen voor 3D modelling en render werk maar het is nu gewoon niet slim om te investeren in deze hardware."
RX 6700 XT;5;0.31593650579452515;Een ex mining kaart hoef je niet te laten staan. Mining kaarten draaien 9/10 met een undervolt en underclock om zo de hoogst mogelijke efficientie te halen. Daarnaast worden ze vaak goed gekoelt en blijven constant een stabiele temperatuur. Dit is anders dan een intensief gebruikte gaming kaart die afkoelt, heet wordt, afkoelt etc. Zijn de fans erop nog goed is er absoluut geen reden om een mining kaart links te laten liggen.
RX 6700 XT;3;0.562874436378479;"Interessant, dank hiervoor! +3 Heb het nog nooit zo bekeken maar klinkt wel logisch. Toch ook nog even een nuancering op mijn tegenstand tegen ex-miningkaarten; het is niet alleen de zorg voor de wear-and-tear (die mogelijk wel meevalt begrijp ik nu) maar ook de vieze bijsmaak die ik van crypto mining krijg. Het ""voelt"" niet okee om een kaart te kopen van iemand die a) daar al geld mee heeft verdiend, b) waarbij veel stroom is verstookt, c) de beschikbaarheid van kaarten voor de rest van de mensen zeer beperkt heeft gemaakt en d) door mijn eventuele overkoop nu nóg eens een keer geld vangt. Ik weet niet of ik de enige ben die zo principieel is of dat andere mensen zelfde gevoelens hebben maar ik baal enorm van de verstorende machten die het voor de doorsnee gamer of hobbyist zeer moeilijk en kostbaar maken om op een fatsoenlijke manier aan hardware te komen."
RX 6700 XT;2;0.39084598422050476;Op zich eens, maar dat het niet controleerbaar is of een kaart gebruikt is voor mining, maakt dit helaas niet tegen te houden. Je kunt die mining kaart zo op marktplaats zetten wegens beëindiging hobby, of heb het geld nodig voor wat anders, doe er toch niet meer genoeg mee, heb wat nieuws gekocht dus overcompleet. Is in goede staat, nooit overclocked geweest, bon erbij. Nog even uitgaande dat je bij doorvragen eerlijk antwoord krijgt, zullen heel veel mensen al niet eens meer doorvragen, gewoon laten opsturen.
RX 6700 XT;1;0.29059359431266785;Helemaal mee eens, ik moet soms wel grinniken als iemand zijn kaart te koop zet en in zijn/haar andere advertenties duidelijk spullen voor mining rigs te koop heeft of zoekt. De echte slimme Marktplaats verkopers kopen via het ene account in en verkopen via een ander (ik hoop niet dat ik iemand op ideeën breng hier). Ik check bij Marktplaats ook altijd wat de andere beoordelingen zijn en wat voor soort spullen het betreft: als iemand in korte tijd veel videokaarten heeft gekocht/verkocht, dan weet ik wel waar al die kaarten vandaan komen. De realiteit dat dit niet te stoppen is, behalve doordat het crypto minen zélf totaal onrendabel wordt. Dat hebben we bij Bitcoin een paar jaar geleden ook gezien maar helaas is dat ook weer compleet opgeleefd. Doet me denken aan de situatie met wietplantages: zo lang het genoeg oplevert en er onvoldoende reden is om het niet te doen, zal men er in blijven investeren.
RX 6700 XT;3;0.5520257949829102;"Goede tekst, Vooral punt b valt op. Waar zijn opeens al die ""Gretas"" die op de straat gaan om te demonstreren tegen mining, Een van de grootste stroomvretende hobbies die op deze wereld bestaan. Zeker niet CO2 neutraal."
RX 6700 XT;3;0.5505156517028809;Daarentegen moet je er wel op letten hoe de verdere staat van de kaart is. Een mining rig kan namelijk in een kantoor staan, maar ook op zolder of in de kelder of in de schuur. Heb al videokaarten gezien waarvan de uitgangen al redelijk begonnen te roesten.
RX 6700 XT;4;0.546672523021698;Goed punt om in de gaten te houden inderdaad. Bedankt voor je toevoeging.
RX 6700 XT;1;0.645223081111908;Dan nog hebben deze kaarten wel 24/7 alleen maar gedraait.. Lijkt mij ook niet gezond voor zo`n kaart.. Zeker als je beseft dat ze zeker 1 jaar non stop hebben gedraait. Daarbij koop je een kaart waarbij je nooit 100 garantie hebt dat deze 100% stabiel zal draaien in je systeem.
RX 6700 XT;5;0.5411501526832581;Ik gebruik sinds juli 2018 een rx570 ex-mining kaart, vele honderden uren heb ik er al mee gegamed en hij gaat als een trein.
RX 6700 XT;1;0.26579245924949646;Ik heb voor kennissen een paar systemen gebouwd, daarbij wijk ik momenteel uit naar GTX 1060's 3GB en GTX 970's. Daarmee kunnen ze in ieder geval even vooruit en deze modellen komen af en toe langs voor €100-150. Wanneer de hype voorbij is kunnen die kaarten weer worden verkocht zonder 100'en euro's verlies en kan de GPU die ze eigenlijk wilen worden ingebouwd
RX 6700 XT;2;0.34214067459106445;Ja het blijft inderdaad nog de vraag of als minen niet meer rendabel is, hoe lang het gaat duren tot de tekorten aan chips zijn weggewerkt tot een punt waarop ook de gaming markt weer verzadigd is.
RX 6700 XT;1;0.42475831508636475;"Laten we zeggen dat er straks een jaar amper GPU's geleverd zijn voor de gaming markt. De 20-serie van Nvidia had ook al een niet te beste reputatie qua prijs performance. Ik denk dat heel veel mensen nog in de 10xx generatie zitten. Dat zal betekenen dat het grootste deel van de GPU consument nu zo;n beetje al 3 jaar de hand op de knip houdt. Misschien 2 jaar 200% draaien. Om weer op een normaal peil terecht te komen?"
RX 6700 XT;1;0.378853976726532;"Je hebt helemaal gelijk, de prijzen van nu worden betaald. Dat zien fabrikanten ook. Ik hoop dat je gelijk hebt wat betreft de mining kaarten maar waren die kaarten de nieuwe mining kaarten van Nvidia dan kunnen ze direct de kliko in omdat niemand er meer wat aan heeft. Zo heeft Nvidia de ""schaarste"" van nu kunstmatig een flinke tijd verlengd."
RX 6700 XT;2;0.35329487919807434;Je kan je afvragen hoeveel gamers de huidige prijzen betalen.... Volumes zijn volgens mij ongelofelijk laag.
RX 6700 XT;2;0.3827323019504547;Het is natuurlijk ook niet alleen crypto maar een sterk lager aanbod, je ziet ook op markten waar miners alleen indirecte invloed hebben (consoles bijvoorbeeld) dat het aanbod maar een fractie is van normaal bij een gemiddeld iets hogere vraag (20% biivoorbeeld). Ik snap overigens de winkels best, die hebben hun marges rondom volumes gebouwd. Ik heb mn pc gebouwd zo rond 2011, had een budget dit jaar van 2-3k voor een nieuwe maar heb dat nu gewoon maar ergens anders aan uitgeven, van utistel komt nu eenmaal afstel, voordeel is meteen dat ik over een aantal jaar alle games die ik nu niet kan spelen via een sale binnenhaal (ik ben benieuwd hoezeer het beperkte aanbod van GPU's de gamedevelopers treft)/
RX 6700 XT;5;0.6236349940299988;Sluit ik me helemaal bij aan. Hoe sneller iedereen op Tweakers stopt met het kopen van nieuwe videokaarten voor belachelijke prijzen, en hoe minder aandacht er aan paper launches geschonken wordt, hoe sneller we weer terugkunnen naar normaal. Lijkt de politiek wel!
RX 6700 XT;2;0.3802128732204437;Tja, alleen lijkt het probleem op dit moment niet zozeer bij de fabrikanten van de kaarten te liggen, als wel bij hun (onder)leveranciers, vanwege de enorme chiptekorten? Dus AMD of Gigabyte kunnen er ook niks aan doen.
RX 6700 XT;1;0.7095633745193481;Even advocaat van de duivel maar zou jij dat ook niet beweren? Het idee van schaarste in stand houden is voor iedereen in de keten gunstig behalve voor de consument. Je kunt om meer winst te maken of een nieuwe fabriek bouwen, of je prijzen omhoog gooien. Het eerste kost tijd en geld. Het tweede kost tot de markt de prijzen gaat afwijzen en je verkoop inzakt helemaal niets en levert alleen maar geld op.
RX 6700 XT;2;0.4847286343574524;Nou, volgens mij is het wel algemeen bekend dat er overal ter wereld enorme tekorten aan chips zijn: die informatie komt niet van Nvidia of AMD of zo. De prijs is bij AMD nu weliswaar hoog, maar het verkochte aantal is miniem. De totale omzet wordt daardoor ondanks de hoge prijs neem ik aan flink lager dan normaal. Hogere prijzen resulteren vaak helemaal niet in hogere winst: er is ergens een evenwicht met een optimale prijs voor maximale winst, en dat is niet een extreem hoge prijs.
RX 6700 XT;3;0.32528984546661377;Daarom snap ik niet dat ze productiecapaciteit opofferen voor deze gpu
RX 6700 XT;2;0.4852749705314636;"Is dit niet al zo'n beetje het gevoel na de 1000 serie van Nvidia? Na de 900-serie lijkt elke performance-verbetering ook gepaard te zijn gegaan met een prijsverhoging. In ieder geval had ik met mijn R9 390 -> RTX 2070 upgrade niet het gevoel dat ik er vooruit op was gegaan. Ja, in pure performance wel, maar de kaart was ook flink duurder. De RTX 3000 serie zou daar verandering in hebben moeten brengen - prijzen minder belachelijk als de RTX 2000 serie, maar die nieuwe kaarten zijn nergens te verkrijgen. De 6000 serie van AMD brengt in die trend(s: verkrijgbaarheid en performance/euro) geen verandering blijkt nu. Wat is het de afgelopen jaren toch vermoeiend geworden qua GPU-""upgrades""... Het is hopen dat de tekorten teruglopen en alles minder exorbitant wordt, maar naar mijn gevoel is dit al sinds de nVidia 900 series aan de gang."
RX 6700 XT;4;0.39874351024627686;Keerzijde is natuurlijk wel dat je door het huidige tekort ook veel meer kunt krijgen voor je oude videokaart, als dat tenminste een redelijk recent model is. Dus als je een nieuw model voor niet te lomp veel geld kan krijgen, is dat het al snel waard.
RX 6700 XT;3;0.310390442609787;Prijsverhogingen zijn er altijd (er wordt tegenwoordig zoveel geld bijgedrukt door overheden dat geld steeds sneller minder waard wordt), maar het aanbod is ook een stuk breder geworden en de prijs/performance van de juiste setup is relatief goedkoper geworden.
RX 6700 XT;1;0.48694244027137756;Nog nooit op Tweakers een reactie gezien waar ik mij zo erg bij aansluit, probeer al zo lang een nieuwe videokaart op de kop te tikken. Het erge is dat de prijzen met de dag gekker worden , zo lijkt het. In december had ik de kans een 3070 te kopen voor 825 euro, toen niet gedaan omdat ik dat nog te hoog vond. Achteraf had ik er gewoon voor moeten gaan. Diezelfde videokaart is nu op zijn goedkoopste 1300 euro..
RX 6700 XT;1;0.6684390306472778;ik heb via stockdrops op de een of andere manier een rtx 3070 voor 650 euro kunnen kopen en prijs mezelf supergelukkig dat ik meteen heb toegeslagen. De prijzen nu slaan helemaal nergens meer op
RX 6700 XT;2;0.44493988156318665;Ik probeer netjes te blijven, maar heb je daarbij niet gewoon het gevoel dat ze hem overdwars a... in mogen brengen? Een 70 model voor 1300 is nog schofteriger dan het massaal opkopen van ps5 en nog erger dan Apple prijzen. Zolangzamerhand kunnen de computerwinkels en de producenten de rambam krijgen.
RX 6700 XT;1;0.4425976276397705;Yep, de prijzen zijn voor mij echt te hoog om serieus te overwegen. In januari nog een 2070 Super weten te krijgen voor 380, ze staan nu te koop voor 750 Hardware Unboxed heeft ook nog onderzoek gedaan naar welke partij de prijzen nu eigenlijk aan het opdrijven is, maar iedereen wijst naar elkaar. Webshops kijken constant bij elkaar welke prijzen worden aangehouden, je zou gek zijn om MSRP aan te houden met kleine marges terwijl bij de concurrent de biljetten in het rond vliegen. Wanneer de koers daalt en minen niet meer winstgevend is zal de vraag volledig instorten en moeten ze gaan concurreren met voormalige klanten die de markt overspoelen met 1000'en tweedehands kaarten.
RX 6700 XT;2;0.37183618545532227;Laat ik het zo zeggen, bij mij wekt €1300 geen goodwill op, een kaart die wat meer betaalbaar geprijst is zou voor mij wel een trigger zijn om wat vaker bij die webshop te shoppen.
RX 6700 XT;3;0.3622150719165802;Tweakers heeft het zelf ook uitgezocht: reviews: Waarom zijn gpu's opeens bizar duur? - Na tekorten nu ook prijsstijg... Conclussie overal doen ze er wel wat bij. Vraag die onbeantwoord blijft, waarom leverd AMD zelf nog wel MSRP kaarten?
RX 6700 XT;2;0.496366411447525;Computerwinkels krijgen ook minder kaarten binnen, dus die worden ook lichtelijk gedwongen om dit soort prijzen te hanteren. Ze hebben de kwantiteit gewoon niet om te verdienen wat ze doen met de marges van pre releases van Nvidia/AMD. Helaas, maar je kunt niet helemaal met het vingertje wijzen naar winkels.
RX 6700 XT;1;0.43682271242141724;Heb begin dit jaar via V&A een 3090 voor €1600 gekocht. Zou hem nu voor €1000 meer kunnen verkopen . De wereld is gek geworden.
RX 6700 XT;5;0.5026152729988098;Om dan van je winst vervolgens als je geluk hebt een gtx 1050ti te kunnen kopen! Big brain v&a trading.
RX 6700 XT;1;0.7941274642944336;Ga hem niet verkopen hoor. Gebruik die kaart niet als beleggingsobject, maar 'alleen' maar waar hij voor bedoeld is . En nee, dat is geen mining. Zou niet eens weten hoe dat moet.
RX 6700 XT;3;0.4448166787624359;Als je een jaartje ofoz op de ingebouwde graphics van je CPU kan werken of nog een oude kaart hebt liggen die je zo lang kan gebruiken is het wel makkelijk verdiend...
RX 6700 XT;1;0.6506820917129517;Als je 1600 euro (over) hebt betekend dat niet dat je 2600 euro (over) hebt... Dus nee die redenering klopt natuurlijk niet.
RX 6700 XT;2;0.3425551950931549;De kans dat dat dat wél betekenT is groter dan bij iemand die er maar 300 voor over heeft
RX 6700 XT;1;0.8198390007019043;Idem, eerste wat ik bij deze reviews doe is doorskippen naar beschikbaarheid. En ja, die is er niet, dus de kaart zal mij eerlijk gezegd een worst wezen. Ik ben deze papieren releases helemaal zat en snap ook niet waarom Tweakers überhaupt er tijd aan besteed.
RX 6700 XT;1;0.4564511477947235;Ik heb er blijkbaar een slecht jaar uitgekozen om te gaan upgraden.. Ik heb nog een GTX 980, ik heb onlangs bijna alles vernieuwd en wil dus nu uiteraard ook een nieuwe GPU.. maar dat wordt dus zeer moeilijk..
RX 6700 XT;1;0.6915174722671509;Same... Zit hier met een GTX980TI welke k graag zou willen upgraden als de RTX3080TI uitkomt. Maar in de huidge situatie qua prijzen gaan ze aan mij geen geld verdienen. Pure diefstal en te zot voor woorden. PC gebruikers moeten AMD en NVIDIA eens een tijdje links laten liggen. Enige middel om die lui een boodschap duidelijk te maken want dan zijn de aandeelhouders niet zo blij als de verkopen instorten.
RX 6700 XT;5;0.41831550002098083;Als jij nog een andere bouwer van goede GPU's weet hoor ik het graag Ik kan nog wel even vooruit met m'n 2060 Super (die dus nu gek genoeg meer waard is dan toen ik hem kocht 1 jaar geleden. Bloody crazy)
RX 6700 XT;1;0.5036889910697937;Ik begrijp dat hier wellicht commerciële belangen meespelen, maar ik zou het echt een goede zaak vinden als reviews van videokaarten gewoon uit protest niet meer geplaatst zouden worden, want ze zijn eigenlijk domweg niet leverbaar. Telkens weer worden we lekker gemaakt met uitgebreide reviews, maar het is in feite nutteloos.
RX 6700 XT;2;0.39897462725639343;"Ja ze zijn niet leverbaar, maar als je geen idee hebt hoe hij presteerd wanneer je de kans krijgt om hem te kopen, dan zit je ook in de knel. Een lange tijd na de review kijken hoe goed hij presteert als hij daadwerkelijk leverbaar zou zijn is een veel beter idee dan überhaupt geen reviews krijgen en vervolgens niet weten hoe goed hij presteert. Daarnaast is het toch voor mijn mening toch fijn om reviews te hebben, krijg je een beetje meer enthousiasme dan alleen maar ""Niet leverbaar"" zien op alle webshops."
RX 6700 XT;3;0.28102177381515503;Dan zoeken mensen de reviews elders. Je kan het de reviewers niet kwalijk nemen dat ze nu al hun resultaten willen publiceren.
RX 6700 XT;2;0.3393141031265259;Ik denk dat het ook niet kwalijk te nemen is naar tweakers, het is een dubbeltje op z'n kant en als ie ook maar op één kant komt dan gaat de fik er in. Disclaimer bij elke review en link naar een uitgebreid artikel over de situatie is een nette oplossing als je het mij vraagt. Dat het gelinkte artikel plus is kan wellicht nog ongelukkig uitpakken mits dat niet wordt opgepakt.
RX 6700 XT;1;0.29539772868156433;Nou ja, als je iets met auto's hebt: die zijn nog altijd duurder dan video kaarten, dus als je leest over een auto van €80.000 of zo kan ik dat met interesse volgen - zonder ook maar een moment eraan te denken zoveel geld uit te geven! Zo lees ik nu ook de videokaartenreviews.
RX 6700 XT;2;0.3653410077095032;Die vergelijking gaat ietsjes mank. Je hebt die 80.000 euro voor een auto maar door marktontwikkelingen kost diezelfde auto opeens 160.000 euro.
RX 6700 XT;3;0.26402580738067627;En als ze leverbaar zijn, dan zijn ze meestal te koop tegen woekerprijzen. Ik had graag een nieuwe PC willen kopen, maar ben nu blij dat ik ook voor een Xbox Series X gegaan, op het moment dat ik de kans kreeg.
RX 6700 XT;1;0.465492308139801;Ze zijn niet voor iedereen niet-leverbaar. Er is natuurlijk een selectie mensen die eerder is dan anderen.
RX 6700 XT;1;0.5608663558959961;Absoluut mee eens. Uit protest!
RX 6700 XT;2;0.35877326130867004;"17. Raytracing en DLSS Ligt het aan mij, of zie ik hier alleen de Raytracing benchmarks, en niets over DLSS? En waar is het kopje ""Geluidsproductie en koeling""?"
RX 6700 XT;3;0.6374654173851013;Gaat momenteel een beetje moeilijk op een AMD-kaart inderdaad, titel van die pagina is aangepast
RX 6700 XT;2;0.41488468647003174;Is het dan niet eerlijk om alsnog een grafiek te maken met de Nvidia kaarten die DLSS draaien, en de AMD kaarten zonder? Eigenlijk zit er dus nog veel meer in de 3060Ti met DLSS aan.
RX 6700 XT;4;0.3360755741596222;Met collega's hebben we hier al een aantal keer over gesproken. Die vergelijking ligt inderdaad voor de hand, maar de rabbit hole is dat dan je niet meer appels met appels aan het vergelijken bent. De beeldkwaliteit is met DLSS (in ondersteunde games) naar wens aan te passen, en het verschilt per persoon hoe de afweging tussen beeldkwaliteit en framerate wordt gemaakt. Input vanuit de community is uiteraard altijd welkom, ook op dit vlak
RX 6700 XT;2;0.48965340852737427;Deels eens, echter aan de andere kant zijn het naar mijn mening nu juist de features die in veel gevallen de doorslag zullen geven (mits verkrijgbaar, vandaag de dag telt alleen kan ik überhaupt een kaart kopen). Je ziet immers dat rasterization performance zo dicht bij elkaar liggen dat het voor pure rasterization performance eigenlijk niet uitmaakt of je nu een 3070, 6700XT of 3060TI zou kopen, alle games zijn wat dat betreft perfect speelbaar op al deze kaarten op bijv. 1440P, dan zou ik als mogelijk koper al snel gaan kijken naar wat biedt de AMD of de Nvidia kaart mij naast de rasterization performance en dan is denk ik toch een vergelijk van DLSS v.s. geen DLSS ineens vrij relevant, ook al is het niet 100% meer appels met appels.
RX 6700 XT;1;0.24873510003089905;"Dat snap ik helemaal. Ik zou dan ook meer zien in een gestapelde staafgrafiek. Dus in één grafiek de scores voor ""standaard"" instellingen, en bij kaarten die het ondersteunen nog twee extra datapunten in dezelfde balk voor Performance en Quality DLSS. Uit deze review bijvoorbeeld. Een RTX3060 (niet eens Ti!), die een hogere framerate behaald met DLSS Quality en RT Ultra, dan de 6700XT op RT Medium."
RX 6700 XT;3;0.5172020792961121;Zijn toch beide videokaarten (appels met appels). Een uitgebreidere feature set mag gerust flink mee gewogen worden als je het mij vraagt. De prestaties en de prijs bekijkend is dit helaas niet zo'n goede beurt voor AMD. Een prijs gelijk of lager aan een 3060Ti kan het dan wel interessant maken, mits bechikbaar.
RX 6700 XT;5;0.35269299149513245;Helemaal mee eens, dat kan een spel als cyberpunk speelbaar maken met 60+fps (haalde ik met mijn 3060TI met DLSS aan), of net niet zoals hij nu in de benchmark staat.
RX 6700 XT;3;0.35700681805610657;AMD ondersteund geen dlss daarvoor moet je bij Nvidia zijn, dit is een kaart vooral geschikt voor lagere resoluties dan mis je deze functie ook minder.
RX 6700 XT;3;0.2921834886074066;Waarom zou je dit bij deze kaarten minder missen? Je hoort juist vaak dat DLSS op een kaart als de 2060 of bijvoorbeeld de nieuwere 3060 juist sommige games speelbaar maakt op bijv 1440P, waar ze zonder DLSS geen speelbare framerate laten zien.
RX 6700 XT;2;0.3195148706436157;Langs de andere kant gaat AMD een gelijkaardige feature (FX super resolution) aanbieden in de toekomst. Natuurlijk is er altijd het risico dat dit niet gaat gebeuren, maar indien ze het klaarspelen valt dat argument weg,
RX 6700 XT;2;0.39615413546562195;Dat AMD daar mee bezig klopt, echter zolang het er niet is, is DLSS gewoon een verkoopargument voor Nvidia, of dat zo blijft zal afhangen van een aantal zaken, bijvoorbeeld wie zal de bredere adoptie gaan krijgen, DLSS of FidelityFX Super Resolution? Daarnaast zal ook de beeld kwaliteit belangrijk zijn. Of dat argument wegvalt zal nu dus nog moeten blijken, al hoop je er natuurlijk wel op, hoe meer AMD en Nvidia feature technisch aan elkaar gewaagd zijn, hoe beter het is voor de consument.
RX 6700 XT;5;0.5582167506217957;Lekker uitgebreid stuk weer! Top.. Wat ik heb gemist is hoeveel geluid deze kaart maakt. Dat is voor mij belangrijker dan hoe snel die is. Concreet voorbeeld is Metro Exodus, hoor je de kaart daar? Als ik deze game draai op mijn MSI GTX 980 in combinatie met een 10700K is mijn GPU (oudje) op voll vermogen de hele tijd druk bezig.. En dat hoor je. Ik zoek een GPU die een mooie performance realiseert maar die vooral ook lekker stil is
RX 6700 XT;3;0.47114819288253784;"Dat is bij kaarten nogal een uitdaging, en ik hoop dat Tweakers misschien ook wat meer vergelijkingen gaat doen juist hierom tussen diverse board partners. De praktijk is (bij AMD) dat de reference kaarten doorgaans het minst interessant zijn. Maar stiekem geld dat ook bij nVidia, hoewel die eigenlijk geen reference meer maken, maar zelf een custom board partner zijn geworden (er zijn OEM's die nog wél reference maken, founders edition is géén reference echter!). Ik vind dit artikel goed, omdat het goed aangeeft wat de chip kan doen. Wat drivers betreft en chip is het dus vrij ""gelijk"". Qua features ook. Dit is echter géén kaart-review. Stuk context, als voorbeeld: ik heb zelf een RTX3090 die zelfs onder full load uit zichzelf 0db produceert: géén fan op de kaart (het is een pre-waterblocked variant, die gewoon mijn loop in gaat, ik speel vals door de herrie te verplaatsen naar een zestal laag-RPM fans op een tweetal chonk radiatoren). Tegelijk, en die heb ik niet kunnen testen, zijn er kaarten met blower-style koelers. Qua decibel misschien nog steeds niet veel, maar wel hoog frequent... We weten nu wat de chip kan, ben benieuwd naar wat voor kaarten er gaan komen 😁"
RX 6700 XT;3;0.40100976824760437;Daar was hardware.info een leuke site voor om de verschillende ontwerpen van fabrikanten te vergelijken, hopelijk neemt tweakers hier de fakkel over. Op het moment lijken de meeste reviewers echter toegang te hebben tot niet bijzonder veel kaarten. De tekorten treffen niet alleen de gewone consument.
RX 6700 XT;2;0.4577777683734894;"""Ik zoek een GPU die een mooie performance realiseert maar die vooral ook lekker stil is :)"" Ik zoek een GPU. (punt) Hier verscheen voor mij ""ineens"" de review van de 6700 XT op Tweakers... waarom ineens: omdat ik mij er totaal niet meer mee bezig hou gezien de schaarste en de prijzen. Ontopic: ik mis de benchmarks van de ""zuinige/geknepen"" 6700 kaart die ook zou uitkomen en minder stroom zou verbruiken. Zo'n 220 watt versus zo'n 170 watt van de RTX 3060 trekt mij dan eerder naar de RTX 3060 toe. En ik mis net als anderen ook geluidsproductie benchmarks..."
RX 6700 XT;3;0.5156880021095276;Je gaat een vergelijk doen met een GPU omdat die 50W minder verbruikt... je vergeet er wel even bij te zeggen dat deze dan ook 20-25% gem lagere fps score heeft... en dus eigenlijk op het randje is om 1440p te spelen. Als je op 1080p zit kan je evengoed naar een kaart kijken van 150W.
RX 6700 XT;2;0.4355064630508423;"Je gaat een vergelijk doen met een GPU omdat die 50W minder verbruikt... Klopt, dat is dus belangrijk voor mij en mijn usecase. Zoals je kon lezen had ik het ook over ""zuinige/geknepen"" kaart. je vergeet er wel even bij te zeggen dat deze dan ook 20-25% gem lagere fps score heeft... Dat is minder belangrijk voor mij persoonlijk dan het verbruik en dus de warmte ontwikkeling. Waarom concludeer jij dat ik dat ""vergeet"" te melden? en dus eigenlijk op het randje is om 1440p te spelen. Als je op 1080p zit kan je evengoed naar een kaart kijken van 150W. Dat is allemaal per persoon verschillend, ik begrijp niet dat je dat zomaar durft te roepen... De ene persoon wilt een kaart om op 1080p 240hz/fps te kunnen gamen, de ander wilt dezelfde kaart om op 4k 60hz/fps te kunnen gamen. Roepen dat je voor (ik noem maar één van de twee door jouw genoemde resoluties) 1080p evengoed naar een kaart van 150W kan kijken vind ik dus wat kort door de bocht. Op met moment game ik op 4k i.c.m. een Vega 56 en gereduceerd stroomverbruik (max 200 watt) i.v.m. een zeer compacte behuizing. Dus ik wil het liefste een kaart die beter presteert tegen een lager stroomverbruik (ook weer i.v.m. die compacte behuizing en de warmte afgifte van de videokaart). Oftewel de RTX 3060 die beide punten haalt (t.o.v. mijn Vega 56) en dus niet deze 6700 XT die alleen betere prestaties haalt (t.o.v. Vega 56). De prestaties per watt van de RTX 3060 en 6700 XT liggen overigens zeer dicht bij elkaar, zoals je in het artikel had kunnen lezen Wie weet is dus het stroomverbruik van de 6700 XT reduceren een optie en geeft dat hetzelfde effect (lagere prestaties voor een lager stroomverbruik)."
RX 6700 XT;1;0.4518625736236572;4k 60hz en dan een slideshow in aantal FPS je kan hier een hele boek zitten schrijven maar er is geen enkele review site of marketing die je viesie volgt... niemand zegt dat dit de kaarten zijn om degelijk 4k mee te spelen. En als wat zo een probleem is ga je gewoon in de radeon settings en verlaag je de vcore en de ghz zullen evenredig mee dalen en dan heb je ook een veel lager verbruik. Dit akn je zelfs instellen voor 101 profielen.
RX 6700 XT;3;0.4118165671825409;Ik zou bij het stroom verbruik graag ook een totaal zien, ipv alleen per onderdeel. Dat een 3060TI FE maar 1 connector heeft, en alle niet FE's zo'n beetje 2 geeft een beetje een vertekend beeld. Zo lijkt de 6700xt per grafiek vrij zuinig, alleen moet je dan zelf nog maar alles op gaan zitten tellen om tot een eerlijke vergelijking te komen. Vooral ook omdat de FE's van nvidia maar 1 stekker gebruiken, maar mogelijk met 2x8 pin voeding erachter.
RX 6700 XT;4;0.40391016006469727;Als je dan toch bezig bent is total system power consumption ook een leuke om mee te nemen. Gezien Nvidia kaarten de CPU meer belasten doordat hun scheduler voornamelijk softwarematig is.
RX 6700 XT;1;0.5202433466911316;"De grafiek ""opgenomen vermogen ingame"" is al het totaal. Was het nu even aan het na rekenen en er zit 1 of 2 W verschil tussen de som van de individuele onderdelen en het totale ingame vermogen, zal toch net op een ander moment gemeten zijn dan."
RX 6700 XT;3;0.45366382598876953;ah, dat was me niet helemaal duidelijk, had alleen naar de titel van de grafiek gekeken, en bij de PCIE/PEG grafieken staat er heel specifiek videokaart boven. Zonder naar de waardes te kijken dacht ik bij de bovenste even dat het om de volledige systeem load ging. Al is dat gezien de waardes dus wat laag. Neemt niet weg dat een vergelijking op PEG connectors met de FE modellen wat scheef is.
RX 6700 XT;3;0.6615151762962341;Oké maar er wordt toch ook niet echt een conclusie aan die grafiek verbonden. Ik denk dat die vooral nuttig is om te zien of één van de aansluitingen kort bij de limieten zit, dan denk ik voornamelijk aan de RX 480 die meer dan 75W uit de PCIe 12V kon trekken en zo sommige moederborden kon beschadigen. Totaal vermogen is die grafiek net expliciet niet en ik zou het maar bizar vinden dat je verbruik van videokaarten hier gaat vergelijken in plaats van de grafiek iets eerder. Overigens staan er in deze lijst geen RTX 3000 FE modellen die 2 x 8 pins connector aan de 12 pins connector hebben. Dat zijn alleen de RTX 3080 en RTX 3090, en in die reviews is wel degelijk het verbruik van elke aangesloten 8 pins connector meegenomen: reviews: Nvidia GeForce RTX 3090 - De overtreffende trap van Ampere. De RTX 3070 en 3060 Ti FE uit deze review hebben slechts 6 van de 12 pinnen in gebruik, dus die trekken wel degelijk net geen 150W over één kabel (wat nog net binnen spec is).
RX 6700 XT;1;0.5712976455688477;Ik zou er pas over schrijven als ze beschikbaar zijn. Elke letter die je erover intikt, is momenteel 1 teveel.
RX 6700 XT;3;0.36888018250465393;Gelukkig toch 12GB en niet 6GB wat sommige leaks lieten zien.
RX 6700 XT;1;0.7125766277313232;Er kan evengoed 128GB in zitten ...even onwaarschijnlijk als dat je'm kan kopen
RX 6700 XT;2;0.3416566848754883;Honderd kaarten voor de Benelux. Ik had dit nog als een optie voor de nieuwe build staan maar vermoed dat ik die wel kan doorstrepen. Mooi hoor die AMD kaarten. Alleen nergens te koop.
RX 6700 XT;2;0.4305865466594696;Mooi hoor, die Nvidia kaarten, maar nergens te koop. De gehele GPUmarkt is verziekt door productietekorten van beide fabrikanten en de mininghype van nu.
RX 6700 XT;3;0.42632296681404114;Tja 100 voor hele benelux vs een paar honderd per webshop vind ik nogal een verschil.
RX 6700 XT;3;0.40006211400032043;Het resultaat is echter exact hetzelfde.
RX 6700 XT;1;0.484684556722641;Niet noodzakelijk... ik heb een 3080 in November aangeschaft zonder pre-order
RX 6700 XT;1;0.4242945909500122;Volgens hardware unboxed zou de beschikbaarheid een pak beter zijn als bij de vorige launches. Maar waarschijnlijk zal het niet genoeg zijn Gezien er niet echt iets anders beschikbaar is, zal deze kaart ook compleet uitvoerkocht zijn. Zelfs als ze erin slagen significant grotere aantallen aan te slepen... Want als je kunt kiezen een RX 6700XT nu of één van de nu niet leverbare kaarten ergens in de toekomst denk ik dat er veel zullen zijn die dan toch de 6799XT zullen kopen. Zelfs al is het niet de kaart die ze willen.
RX 6700 XT;3;0.49546968936920166;Wel goed nieuws ivm de 6700XT in elk geval. Lokale computerwinkel had er meer dan 25 op voorraad deze morgen, en op het moment van schrijven nog 7. Wel aan 900+€ En de voorraad 6900XT's is ook aangevuld geweest blijkbaar. Meer dan 50 van verschillende fabrikanten in voorraad. Maar hier zijn de prijzen ook wild.. 1800 tot 2000€ De beschikbaarheid is dus nu wel beter maar de prijs is buiten alle proportie...
RX 6700 XT;5;0.4885442554950714;500 euro voor een 192 bit kaart het wordt steeds gekker
RX 6700 XT;2;0.3225958049297333;Ja en dan maar 330 mm2
RX 6700 XT;2;0.47899582982063293;Met maar 40CU's op de 6700XT verwacht ik op langere termijn wel een betere beschikbaarheid. Ik ben alleen benieuwd wat die morgen in de webshops kost, ik gok tegen de 900€ aan. Ik wacht nog gewoon rustig even af. Ik laat me niet gek maken, ja er is schaarste maar er wordt ook wel extra kunstmatige schaarste gecreëerd.
RX 6700 XT;2;0.48257705569267273;Huidige markt of niet, blijkbaar willen niet alleen de webwinkels maar nu ook de fabrikanten lekker meeprofiteren van de rotzooi op de grafische kaarten markt. Mij niet gezien, AMD kan doen wat ze willen en deze kaar mogen ze wat mij betreft lekker houden. Ik heb vaak genoeg commentaar gehad op Nvidia met hun praktijken, maar blijkbaar is AMD geen haar beter. Het probleem voor AMD is dat men toch wat features mist ten opzichte van Nvidia. DLSS 2.0 is toch eindelijk een volwassen features en de raytracing performance van Nvidia is gewoon beter. Ik ben blij dat AMD enigszins kan mee concurreren, maar in het huidige klimaat heeft concurrentie geen enkele invloed. Wat mij betreft moet iedereen zich afvragen of men echt een grafische kaart nodig heeft en zich op deze manier wil laten oplichten. Want dat is het gewoon, iedereen kan wel roepen dat dit normaal is maar dat is het absoluut niet. Het wordt tijd dat de cryptomarkt weer instort en dan moet je eens kijken hoe de markt overstroomt wordt met grafische kaarten. AMD en Nvidia kunnen zich wel rijk rekenen, maar ze gokken toch echt op het verkeerde paard, de cryptomarkt is bij lange na niet zo stabiel als de gamingmarkt en misschien moeten ze nu wel pakken wat ze kunnen. Iets wat mij bij de benchmarks opvalt is dat er maar een game is die echt nieuw en next gen is en dat is de extra test van Cyberpunk 2077. Alleen het verschil in prestaties tussen deze game en al die andere games van 1 tot 4 jaar oud laten mij wel zien dat de kaart helemaal niet zo bijzonder is, helemaal niet voor het geld. Dat geldt trouwens ook voor de Nvidia kaarten. Ja er is wel degelijk prestatiewinst t.o.v. vorige generaties, maar ook deze generatie zit gamen met raytracing op native resoluties boven 1080p er niet in. Gelukkig bied DLSS nog enige uitkomst.
RX 6700 XT;1;0.37117981910705566;Waarom alleen maar Gameresultaten en geen video-editing/grafische toepassingstests? Games hebben minder toegevoegde maatschappijwaarde maar krijgen toch alle aandacht.
RX 6700 XT;3;0.37333589792251587;"Dit is een Radeon kaart, voor professionele doeleinden is er de Radeon Pro. Zelfde verhaal voor GTX vs Quadro. Uiteraard kun je deze kaarten ook gebruiken voor gelijkaardige doeleinden als de prosumer lijn, maar daar zit je echt wel al al in een klein niche, je kan niet echt verwachten dat er voor iedere x% nichetoepassingen een benchmarksuite toegevoegd wordt. Ik zie niet in wat 'toegevoegde maatschappijwaarde"" hiermee te maken heeft."
RX 6700 XT;2;0.4414103925228119;Ik ben het toch met @aquinox eens, foto en videobewerking of grafische toepassingen is tegenwoordig al lang geen niche meer. Er zijn genoeg gratis foto en videobewerking software en hetzelfde geldt voor grafische toepassingen. Juist tegenwoordig gebruiken mensen die deze grafische kaarten kopen de computer voor veel meer dan alleen gamen. De professionele lijn waar je over praat is juist een niche markt, ik werk al meer dan 10 jaar als engineer en alleen de echte grote bedrijven gebruiken quadro kaarten. Bijna alle mkb bedrijven gebruiken gewone reguliere lijnen, omdat deze juist prijstechnisch veel interessanter zijn. Daar waar de professionele lijn in uitblinkt is compatibiliteit en ondersteuning. Alle grote namen en software is uitvoerig getest bij de professionele lijn. Maar prijstechnisch zijn deze kaarten voor kleinere firma’s oninteressant, want voor de kosten van de goedkoopste pro kaarten kun je zo goed als twee high-end modellen kopen uit de geforce lijn. Gelukkig zijn er andere sites die dit soort benchmarks wel doen, en daar blijkbaar wel de meerwaarde van inzien.
RX 6700 XT;5;0.4296043813228607;Precies, ik gebruik mijn GPU 'zakelijk' maar tevens voor gamen.
RX 6700 XT;1;0.4323182702064514;Ik blijf het knap vinden dat AMD videokaarten nog slechter verkrijgbaar zijn dan Nvidia.. Wel 100 stuks beschikbaar bij launch, wauw!
RX 6700 XT;2;0.37567898631095886;"""Verkrijgbaar"" wordt dan wel erg abstract. Antieke Bugatti's zijn ook verkrijgbaar voor een paar duizend euro, maar dan moet je wel net een verlaten schuur weten waar zoiets onder het stof te vinden is."
RX 6700 XT;1;0.7459988594055176;Voor wat betreft de prijzen van GPU’s in het algemeen, die gaan nu door de schaarste over de kop. Maar over een aantal maanden als de normalisering weer een beetje intreedt, dan raak je die kaarten aan de straatstenen niet meer kwijt. Nu betalen mensen 1000 tot 2000 euro en meer voor een kaart, als er aanbod is dan krijg je never nooit meer een “redelijk” bedrag als je deze dure kaart aan de man wilt brengen in “vraag en aanbod”. Ik vindt het voor nu gevoelsmatig echte kapitaal vernietiging. Een tweedehands kaart vliegt qua prijs ook enorm omhoog. Ik ken het principe van vraag en aanbod, maar aan het einde van de rit heb je knetter veel betaald voor iets wat nog minder dan een kwart waard is. Of de fabrikanten moeten hier op aansturen om zo kunstmatig de prijs op te schroeven en dat deze prijzen het nieuwe gewoon is.
RX 6700 XT;3;0.3548525273799896;"En... de kaarten van AMD Direct Buy zijn op 2 minuten na tijd (hoe weinig dat er ook waren), en de partner kaarten kosten richting 1000 euro. Wat een verrassing . Nu was dat wel te verwachten in het huidige techklimaat. Maar, het feit dat AMD zo lang het ""ja hoor ze worden beschikbaar voor MSRP""-spelletje vol hield, zit me niet echt lekker."
RX 6700 XT;5;0.7001713514328003;Look at all this nice stuff nobody can buy
RX 6700 XT;1;0.4113870859146118;"""bij de RX 6700 XT betreft het ongeveer honderd kaarten voor de Benelux.""...."
RX 6700 XT;2;0.5034500956535339;Dat is best een tegenvaller! Deze kaart had minimaal net zo snel als de GTX3070 moeten zijn, maar valt daar wel onder nu. Ik vind het wel top dat Tweakers deze kaart een dag voor release reviewed. Zo heeft de potentiele klant nu de kans om te beoordelen en daarna te bestellen! Laat AMD maar opschieten met de SuperResolution functie om DLSS te tacklen.
RX 6700 XT;1;0.6712398529052734;Dit is een test op grafisch gebied, niet mining. Dat gehele mining is de doodsteek voor de normale consument. En voor die normale consument is deze review bedoeld.
RX 6700 XT;2;0.36129945516586304;Als hij net zo snel was geweest als de 3070, had hij te dicht in de buurt van de 6800 gezeten, denk ik.
RX 6700 XT;2;0.4231508672237396;lijken alle signalen er dus op te wijzen dat ook Navi 22 voor vrijwel alle gamers als een verdampende oase in de woestijn is Beetje misleidende ondertitel dan...
RX 6700 XT;1;0.5193139314651489;De ondertitel is toch een vraag? Hoe is die vraag misleidend? Heb je wel eens van een fata morgana gehoord? Nou dat is dus die oase
RX 6700 XT;3;0.32031142711639404;Is het de referentiekaart die getest is? Want in dat geval ben ik tevens best benieuwd naar de geluidsproductie.
RX 6700 XT;3;0.4870632290840149;Het lijkt me handig om het verbruik te tonen met twee schermen en met video bekijken.
RX 6700 XT;1;0.31704333424568176;Oase? Eerder een fata morgana met de huidige productie en markt omtrent videochips/kaarten...
RX 6700 XT;1;0.6594838500022888;Lees het artikel eens ipv alleen de titel Hint: allerlaatste zin.
RX 6700 XT;3;0.6081429719924927;Ik heb het artikel wel degelijk gelezen, alleen de laatste alinea overgeslagen. Wel grappig dat het daar ook zo staat.
RX 6700 XT;3;0.37753617763519287;Waarom staat er in de Prestatiescore niet de 2070 super als van de 2060 en 2080 wel de super er van bij staan?
RX 6700 XT;4;0.27497729659080505;@Trygve Is het mogelijk om ook wat serieuzere benches toe te voegen? Met name Davinci Resolve, Premiere Pro, CUDA/OpenCL.
RX 6700 XT;5;0.41947612166404724;Nog even en ik kan doorsparen voor een ferrari xD 1799 bij release,,, 3000 is gewoon heel normaal nu...
RX 6700 XT;5;0.25554874539375305;Waar ik benieuwd naar ben is of deze kaart op kan boxen tegen een 3060ti in VR. Nvidia ondersteunt bijvoorbeeld SPS. Kan AMD hier in meekomen of blijft NVidia nog steeds de beste keus?
RX 6700 XT;1;0.3440808057785034;Waar is de kaart te bestellen? En hoelaat kan hij besteld worden iemand enig idee?
RX 6700 XT;1;0.4780643880367279;"Kan er bij de volgende reviews het onderwerp ""Beschikbaarheid"" bij komen? Dat scheelde een boel leeswerk en desillusie in het zoeken bij de pricewatch?"
RX 6700 XT;1;0.44607192277908325;Je spreekt met een 6800 of daarboven niet meer van instap-4K gaming. Je kunt met een 6800 4k 60 op max settings danwel 4k 60 met minimale compromissen qua beeldkwaliteit halen. Alles dat 4k max settings continu 30+ haalt is geen instap meer. Bij instap-4K denk ik aan een kaart die 4k 60 op medium haalt.
RX 6700 XT;5;0.49757540225982666;Team Rocket!
RX 6700 XT;1;0.910777747631073;Absurd. Die scalpers bij Azerty vragen 1000 tot 1100 euro. Megekko zit op de 1200 tot 1300 euro voor de 6700 xt serie. Deze zaken hebben echt afgedaan bij mij.
RX 6700 XT;1;0.4763928949832916;Idioot. Ik wil mijn RX580 updaten. Lijkt me een 'normale' update. 2 Jaar geleden nog geen € 300 voor betaald. Nou dan nog maar even wat settings wat lager in mijn games want dit trek ik niet.
RX 6700 XT;1;0.60329669713974;Rx580 vragen ze nu 600 euro voor....
RX 6700 XT;1;0.8730517029762268;Haha, Heb op Trustpilot geprobeerd mijn mening te geven over elke GPU webshop hier in Nederland. Werd door Trustpilot meteen in de kiem gesmoord. Het is gewoon crimineel gedrag van deze shops. Ze doen e allemaal aan mee, en beweren dan dat ze niet anders kunnen.
RX 6700 XT;1;0.8572670817375183;Gaat toch helemaal nergens over, moet je die prijzen zien. Belachelijk.
RX 6700 XT;1;0.5090121626853943;tot zover dus al het geblaat hier op het forum over paper launch en beschikbaarheid... ze zijn 8u na launch nog steeds te koop. Wel natuurlijk aan een belachelijk hoge prijs, maarja daar kan AMD niks aan veranderen dat zit in doorverkoop en heel wat verschil in prijs per webshop tweakers info.. een 100tal in de belux.....jaja blijkbaar veel connecties
RX 6700 XT;5;0.339459091424942;"Een bekende uitspraak is: ""Er bestaat geen slechte videokaart, enkel een slechte prijs."" Ja super bekend.... Ik gebruik 'm dagelijks Die artikels hier soms, wenkbrauwen fronsend"
RX 6700 XT;3;0.2336122691631317;Hoe presteert deze kaart met de adobe tools?
RX 6700 XT;1;0.5923186540603638;"Het grootste probleem is dat AMD en NVidia een ""Stereopool"" stelling hebben. Het is aan de tijd dat er minimaal nog 2 GPU chip ontwikkelaars op de wereldmarkt komen, om ten eerste de prijzen te dumpen, en ten tweede de navraag te kunnen voorzien. Geen idee waarom niemand op deze trein spingt. Bijvoorbeeld Samsung, produceren sowieso al de geheugens voor bepaalde Nvidia GPUs...."
RX 6700 XT;3;0.4317058324813843;Ik vind het wel bizar. Wil dat zeggen dat ze bewust minder kaarten produceren? Of dat er wereldwijd gewoon te veel vraag is bij een normale productie?
RX 6700 XT;1;0.48286089301109314;Ik heb het in die 20 seconden dat ik er vluchtig doorheen las niet op gelet, maar hoe is de mining performance? Dat lijkt mij vandaag de dag een belangrijke benchmark om er achter te komen of deze kaarten uberhaupt in de winkel verschijnen. Als de retailprijzen zodadelijk ook weer 3x over de kop gaan dan kunnen we met zijn allen deze kaarten ook op de buik schrijven voor gaming. Kleine moeite om ook even een hashrate van wat populaire crypto's te testen (of ETH in het bijzonder). Edit: Geen idee waarom dit nou een -1 moet opleveren, lijkt mij een hele legitieme vraag toch? Ik zie de bui alweer hangen. En als T.net het niet test dan lezen we het een week later wel op de FP dat misschien deze kaarten meer bang for bucks leveren en dan zien we het schip al weer in de haven zinken voor de consumenten...
RX 6700 XT;2;0.36470648646354675;Het staat eigenlijk in de inleiding: De kaarten zijn rendabel om mee te minen, dat hoef je helemaal niet te testen want is gewoon af te leiden uit de specificaties. Hoe de kaart zich verhoudt tot de concurrentie wbt hashrate en efficiëntie, doet er pas toe als er een keuze gemaakt zou moeten worden bij onbeperkt aanbod, maar dat is niet de realiteit.
RX 6700 XT;4;0.30788692831993103;Zonder het ook maar op te hoeven zoeken kan ik je vertellen dat de mining performance subliem is van deze kaart.
RX 6700 XT;2;0.4168078303337097;nee eigenlijk niet want de geheugenbus is beperkt net als de navi22 minder interessant.. (lees minder)
RX 6700 XT;3;0.516635000705719;Doet er toch niet toe dat de mining performance minder is dan van een Vega VII of een RTX 3080? Zolang de mining performance genoeg is om winstgevend te zijn, en dat is meer dan zo, zijn het interessante kaarten om te minen.
RX 6700 XT;3;0.39734911918640137;Ik weet eigenlijk niet waarom je weggemod wordt. Het is een fact of life dat deze kaarten ook vaak ingezet worden voor mining en voor velen zal dat een reden zijn tóch meer te betalen voor de kaart. Ik vind daar ook vanalles van, maar feit blijft dat het een populaire usecase is. Anyway, volgens mij is de mining performance per watt wat lager door de 192-bit geheugenbus.
RX 6700 XT;5;0.37113794684410095;Nou nou zo geweldig minen die nieuwe AMD's niet, dan kan je beter 1 van de vorige serie kopen, zuiniger en sneller met minen... Hoewel het alsnog rendabel is is een 3060 dat ook gewoon hoor, duurt alleen wat langer
RX 6700 XT;5;0.29672491550445557;undervolt, overclock en deze kaart mined prima hoor
RX 6700 XT;2;0.5263503789901733;Ja maar lang niet zo goed als de vorige generatie en lang niet zo goed als de nvidia's... Prijs/prestatie verhouding (mbt minen!) is het slechtste van de 6xxx serie En ja ik reken UV/OC ook mee hoor, dat kan je met de andere kaarten ook gewoon doen dus dat maakt het verschil niet.
RX 6600 XT;2;0.39827391505241394;En nog steeds is er géén aanbod in de sub-300 euro markt. Gewoon van beide partijen niets van een nieuwe generatie, teleurstellend. Wel goed voor de backlog, die games draaien op een aardappel, en zijn net zo leuk als de nieuwste triple-A titels. Zodra grafische kaarten weer voor een normale prijs te koop zijn, kunnen we beginnen aan de grafische pareltjes. -Edit: nog even gecontroleerd. Zowel de GTX260, 560Ti, 960, 1660Ti die ik heb gekocht waren volgens de Pricewatch allemaal verkrijgbaar voor rond de 225-280 euro. Alleen de 1660Ti was een uitschieter naar 340 euro (en dat vond ik toen al duur!). En in 2021 brengen ze kaarten voor het x60 segment uit met vanafprijzen rond de 350 euro, en vervolgens betaal je 150% MSRP... Dikke doei voor deze generatie
RX 6600 XT;2;0.4055399000644684;Eigenlijk behoort deze kaart gewoon in het sub-300 segment, probleem is dat álle kaarten overpriced zijn door tekorten op dit moment en AMD (en Nvidia) dus makkelijk wegkomen met deze prijzen.
RX 6600 XT;1;0.8062050342559814;Deze kaart hoort niet te bestaan. En pci-e 8x kaart kan gewoon echt niet meer... Ook is de geheugen bus veel te slecht voor deze kaart. Hadden ze minder cores/units op de kaart gepropt en een betere geheugen bus hadden ze minder silicon nodig voor een beter presterende kaart en had hij goedkoper geweest. Het is gewoon een slecht product. Vind het wel vermakelijk dat amd nu ze weer op prestatie pairity zijn de prijzen te hoog houden voor wat het is in vooral het budget segment is het erg. Beide deze kaart en de 5600x en de apu's zijn gewoon te duur.
RX 6600 XT;1;0.37133103609085083;Want? Het gaat uiteindelijk toch gewoon om hoeveel prestaties je voor je euro's krijgt? Wat boeit een technisch detail als dit? DM de engineers van AMD even, daar hebben ze vast niet over nagedacht...
RX 6600 XT;2;0.4373495876789093;De prestaties worden in bepaalde games alleen BEHOORLIJK gebottlenecked door die 8x tot op het punt dat de kaart dermate traag scoort dat de prestatie onder het punt ligt die de kaart zou moeten halen. DOOM is daar een voorbeeld van.
RX 6600 XT;2;0.43950355052948;Ik vraag me af hoe groot die bottleneck dan maximaal kan zijn, aangezien in deze review is gebenchmarked dat je met PCIe 3.0 x8 al 99% van de performance haalt als van PCIe 4.0 x8. Zet je het in een PCEe 4.0-systeem, dan is er van een bottleneck dus echt geen sprake. En op 3.0 is met een verschil van hooguit procenten ook niet echt te spreken van 'bottleneck'. Die bandbreedte is dan wel zo ongeveer in lijn met wat de GPU ongeveer kan presteren. Tuurlijk, ik snap ook niet waarom x16 hierop niet aanwezig is, maar het lijkt weinig verschil te maken.
RX 6600 XT;2;0.4712531268596649;Het is erg applicatie afhankelijk. Duidelijk te zien dat dat VER ondermaats is. En dan kan je zeggen dat moet je maar PCE-E4.0 nemen maar deze kaart gaat voor bijna 100% op systemen geprikt worden die niet de laatste generaties hardware hebben en dus PCI-E 3.0 en dus is het gewoon een zeer domme fout van amd en waarom? En de kans is groot dat dit soort games meer en meer gaan komen nu de nieuwe generaties games uit gaan komen di geschreven zijn met de nieuwe consoles en hardware in het achterhoofd.
RX 6600 XT;2;0.4513610303401947;Ik verwacht dat de Radeon RX 6600 XT €500+ gaat kosten, aangezien de goedkoopste Radeon RX 6700 XT €713 kost, en weet bijna wel zeker dat hij niet in de €300+ gaat kosten. Het is echt triest, we kunnen nu al ruim een maand de nieuwste grafische kaarten kopen, en zijn nu bijna overal op voorraad bij de winkels en anders bij de leveranciers van die winkels, maar de prijs gaat zo goed als niet omlaag, ik denk dat we nu gewend moeten raken blijkbaar aan absurde hoge prijzen voor grafische kaarten, ook de rede dat ik nog steeds geen nieuwe grafische kaart gekocht heb, ze zijn gewoon veel te duur.
RX 6600 XT;1;0.8980126976966858;Overal word gewoon misbruik van gemaakt. Olie prijs s 7% gezakt maar de benzine prijs gaat vrolijk door omhoog. Het heeft echt niets meer met vraag en aanbod te maken gezien de prijsverschillen met andere landen hoger en hoger worden. Ik kan niet wachten tot ik weer naar Duitsland kan voor al mijn boodschappen want ik gun ze in Nederland niets meer.
RX 6600 XT;1;0.6486319899559021;Ja klopt, het was eerst omdat er te veel vraag was, ik ben benieuwd wat nu de excuus is waarom de prijzen (na een maand dat ze te koop al zijn en op voorraad) nog steeds zo hoog zijn, en ben bang dat Intel hun grafische kaarten ook duur woorden, als ze ooit uit komen, maar de enigste rede dat brandstof zo duur is, is vanwege de regering, 95% wat jij betaal aan de pomp gaat rechtstreeks naar de regering.
RX 6600 XT;1;0.428301066160202;Belastingen op brandstof is vast procentueel dus nee dat ligt niet bij de regering. De tarieven zijn namelijk niet gewijzigd het zijn de pomphouders die meer windt pakken en daar komt oom belastingen boven op ja maar de oorzaak van de prijzen ligt bij de pomphouders. Als de niet zouden graaien zouden we ook die extra belastingen niet hoeven betalen.
RX 6600 XT;3;0.5123894810676575;Ok ik was te hoog met procenten. Maar het meeste voor wat jij bepaald aan de benzinepomp gaat dus naar de regering en niet naar de pomp eigenaar.
RX 6600 XT;1;0.4968412518501282;Dat het meeste naar het rijk gaat doet er niet toe de oorzaak van onze ten opzichte van de olieprijs en prijs van de buurlanden ligt volledig bij de pomphouders. Stel ik betaal aan de pomp 1,537 euro per liter benzine dan is accijnzen en btw zijn dan pakt de pomphouder dus 45 cent en gaat er 1,087 naar de staat. Stel de pomphouder verhoogd de prijs naar 72 cent per liter dan is dat na accijnzen en btw euro. Ja er gaat dan betaal je ineens 1,864 euro per liter waarvan 1,144 euro accijns en btw. Oftewel de BTW gaat maar met 5.76 cent omhoog terwijl de pomphouder 28 cent meer pakt. dat kan je ook nog procentueel uitrekenen maar ik denk dat je de oorzaak zo wel snaps. belastingen zijn zelfd bij een goedje waar we te veel belasting op betalen voornamelijk productprijs gerelateerd omdat een groot deel van de belasting vast is en niet mee schaalt. Het is altijd benzineprijs gevraagd door de pomp houder + accijns + btw over de 2 samen. Zo komt de prijs tot stand en dus als de pomp de prijs verhoogd krijgt de overheid vooral meer door de btw maar dat is een feit bij alles. zo ook eten in de supermarkt of een concert kaartje of wat dan ook. De echte prijsverhoging komt door de verkoper van het product. We gaan immers niet ineens 30 cent meer accijns betalen.
RX 6600 XT;1;0.36108261346817017;Vergeet ook niet dat de wegenbelasting van Duitsland bij de brandstof zit (zo was het altijd), en toch was de brandstof goedkoper bij de pomp, dus ja nog steeds is de grootse rede voor de prijs van brandstof de schuld van de regering, aangezien we daar het meeste aan betalen.
RX 6600 XT;5;0.39823997020721436;De benzine die van olie gemaakt is waar wel de volle mep aan olieprijs voor betaald is moet natuurlijk ook eerst op. Verder is er ook een vraag/aanbod markt voor benzine. Met een globale economie speelt verder de raffinage capaciteit enorm in op het aanbod gedeelte. ( en dus de uiteindelijke prijs ). Raffinage capaciteit is krap, globaal gezien. Verder transport etc.
RX 6600 XT;1;0.5286503434181213;Doom is de uitschieter in het verschil, met nog steeds een average framerate van 180fps. Dat zijn de soort situaties waarbij je eerder nog meer eyecandy aanzet en de FPS aan je reet kunen roesten. De rest zit ergens tussen 0-5% : En dit is getest op een Ryzen 5950X. Goeie kans dat de markt voor een 6600XT een tragere CPU heeft, zéker als ze een systeem met PCI-E 3 hebben. De CPU wordt dan sowieso meer een bottleneck en het verschil verdwijnt grotendeels. Dit is een geval waarbij een reviewer met de beste setup inderdaad zo'n verschil kan opmerken in één specifieke game, maar het voor de doelgroep totaal irrelevant is.
RX 6600 XT;2;0.43448513746261597;Nog wel ja maar waar er 1 is kunnen er meer komen en dan zit je met een kaart opgescheept die niet in staat is bepaalde games fatsoenlijk te spelen. De markt staat niet stil en nu we 2 consoles hebben met hoge bandbreedte en 10+ GB geheugen kan dit in de nabije toekomst toch wel problemen gaan opleveren. En een pc met PCI-E 3.0 hebben betekend niet dat je een cpu bottleneck heb. Het kan ook juist het probleem vergrote maar dat zou getest moeten worden door iemand die de hardware heeft. Misschien is met een ryzen 3600x bijvoorbeeld het probleem nog veel extremer. Dat weten we niet tot het getest is. Toekomst van deze aart is dus verre van duidelijk en ik beveel hem daarom af.
RX 6600 XT;2;0.2799121141433716;Het is een midrange kaart met voldoende VRAM voor zijn core. Ik begrijp echt niet waar je over zanikt, eerlijk gezegd. Zet deze naast een RTX2060 of een RTX3060ti en de balans is dikke prima of zelfs beter te noemen (6GB vs 8GB is veel meer impact, denk aan textures etc.), de prestatieverliezen vanwege pcie x8 zijn nagenoeg te verwaarlozen en waar ze er wel zijn is je FPS al meer dan hoog genoeg, zéker voor dit prijssegment. De toekomst van deze kaart is heel duidelijk. 1080-1440p gaming en daar voldoet ie prima voor. Is het een geweldig product, deze 6600xt? Nee, maar normaliter had deze ook voor rond de 230-260 euro in de markt gestaan, ook AMD wil een graantje van de schaarste meepikken. En voor een normaal MSRP zijn dit soort 'keuzes' ook heel gebruikelijk. Er is geen x60 die dat niet heeft gedaan... De 2060 die voor gelijke core perf als een 1080 toch 2GB VRAM moet missen bijvoorbeeld. Of een asymmetrische VRAM bus zoals op vele x60s van Nvidia de norm is. En die raken je performance ook, zo niet harder. Been there done that... Je transplanteert nu de 10GB - RTX 3080 discussie naar het prestatieniveau van een 6600XT. Heb je ze nog op een rij verder? De 6600XT is geen kaart die over 5 jaar nog alle courante games lekker gaat draaien. Dan lever je inderdaad in omdat de core gewoon niet snel genoeg is. Dus daarom: het gaat om de balans.
RX 6600 XT;1;0.6480042338371277;dat jij het niet begrijpt is een jij probleem. Het probleem is simpelweg dat er games zijn die gewoon NIET GOED gaan presteren op deze kaart en dus ver onder het prijspunt gaan zitten qua prestatie waardoor een oudere RX5700 beter presteert. Als je dus deze kaart koopt en deze games speelt heb je dus een kaart met belabberde prestaties. Misschien moet je je gewoon wat beter inlezen over het gevonden probleem ipv venten op het internet omdat je het denkt beter te weten. De rest van je post lult dan ook gewoon om het probleem heen en blijkbaar weet je daar dus niets van af. En nee je snapt duidelijk mijn hele punt niet qua geheugen. Die geheugen bus gaat gelimiteerd worden door de PCI-E bus en dat gaat problemen opleveren. Die kaart draait gewoon niet optimaal op PCI-E 3.0 8x en dat is toch het systeem waar deze kaart vaak op zal belanden. AMD heeft duidelijk gedacht dat iedereen al PCI-E 4.0 heeft of maar even een nieuwe pc koopt en dat is juist inhet segment waar deze kaart in gedropt word gewoon niet waar. heck ik heb een 6900 XT en ik heb nog PCI-E 3.0 Het gaat om balans zeg je maar ondertussen is de PCI bus te traag voor de kaart mooie balans. Van mij mag jij de kaart lekker kopen hoor maar ik raad het iedereen af.
RX 6600 XT;2;0.41533657908439636;"Je hebt ondertussen met al je praatjes alleen nog DOOM kunnen vinden als voorbeeld, verder doe je een boel aannames over kennis van zaken maar ondertussen... 1 voorbeeld. Dat voorbeeld draait echter al op zo'n hoge FPS dat het er totaal niet toe doet. Verder is het historisch allang bekend wat PCIE bottlenecking kan doen, en dat is... nagenoeg niets. 5% is een uitschieter. En dat is niet uniek voor deze 6600XT. Maar omdat het jou zo om kennis van zaken gaat, zal ik je een mooie bron aanbieden. Ik hoef denk ik niet toe te voegen dat een 3080 een tandje sneller is dan de 6600XT waar het hier over gaat. En nu terug je hok in, jij. Je verkoopt onzin en dat verdient een correctie. Ik snap je punt prima; je hebt er geen."
RX 6600 XT;1;0.6467511057853699;Doom is als enige getest dat betekend niet dat er niet meer games zijn. Denk je echt dat reviewers de duizenden games die uit zijn gaan testen? het feit dat er maar 1 case gevonden is uit 10 games betekend niet dat het ook maar 1 game op de duizenden is al lijken jou hersenen zich wel zo te beperken. Ik heb zelf deze kaart gelukig niet maar ik heb wel een aantal games waarbi ik verwacht dat ze hetzelfde probleem laten zien. Rage 2 bijvoorbeeld en games die beide vulkan en unreal engine 4+ gebruiken. maarja zover had jij ng niet eens nagedacht over de mogelijke oorzaken van dit probleem gezien jou manier van schijven. En wat een houding heb jij zeg mijn god wat een persoontje moet jij in het echte leven zijn. Dat kleinerende bullshit kleuter gedrag heb je mij niet meer hoor maar je laat goed zien wat een persoon jij ben. valt niet mee re argumenteren en gaat uit zijn weg om anderen te kleineren of dat in ieder geval te proberen. Zielig niets meer niets minder. Maar verkoop/adviseer jij lekker slechte kaarten aan mensen ik adviseer ze liever compleet en vertel ze over de mogelijke risico's. Gelukkig ben ik en anderen om mij heen niet afhankelijk van jou kennis van zaken want dan word het treurig. Zelfs de grote reviewers zijn het niet met je eens maarja wie zijn zij (behave mensen die het product daadwerkelijk getest hebben) als we jouw superieure mening hebben. Bij deze einde ''discussie''.
RX 6600 XT;3;0.42146027088165283;Al eens op de link geklikt of ga je nog steeds obv je eigen aannames door op de oude weg? Je 'verwacht' het ook elders. Dat is lekker betrouwbaar Daar worden al die engines namelijjk wel getest. Je moet niet zo zielig doen. Ik corrigeer je met een bron. Thats it. Maak het niet groter dan het is. Ondertussen over tone of voice had ik een 'jij' probleem maar ondertussen weten we wel beter, gelukkig, wie er echt een heeft... Iets met de bal kaatsen
RX 6600 XT;1;0.6172800660133362;De 5950X is zowat de snelste consumenten-CPU die er is. Op nagenoeg elk andere CPU zullen games trager lopen. Misschien 0,5%, misschien 5%, misschien 20%. Afhankelijk van de andere CPU. Het ding kost het dubbele van deze GPU. Als je eigenlijk op zoek bent naar een upgrade voor je huidige GPU in de categorie 200-400 euro, is de van je computer hoogstwaarschijnlijk ook een stuk ouder dan zo'n 5950X. En nee, ik kan me écht geen scenario inbeelden waarbij een 3600X sneller is dan een 5950X. Écht niet. Dat is een van de pot gerukte, op niets gebaseerde veronderstelling. De realiteit is dat dit een weliswaar meetbaar (op de meest bloedsnelle CPU), maar hoogstwaarschijnlijk irrelevant gegeven is voor de markt. De overlap van top-of-the-line CPU en budget GPU komt zelden voor. Net als dat je waarschijnlijk geen 8600k met RTX 3090 combineert.
RX 6600 XT;1;0.6247932314872742;In de meeste spellen zit je maar op 10fps verschil tussen een Ryzen 9 5950X en een Ryzen 5 5600X, soms heb je uitschieter maar niet met de meeste spellen, maar dat woord met 1440p al een stuk lager of zelf verwaarloosbaar, en met 4k al helemaal. 1080p/1440p 1440p/4k En als je wat streamed en op YouTube zit en internet en je gamed er een hoop op, is het echt zonde voor je geld om voor een AMD Ryzen 9 5950X te gaan, de AMD Ryzen 9 5950X is bijna 3x zo duur. AMD Ryzen 5 5600X €275 AMD Ryzen 9 5950X €797 Het is wat anders als je er mee gaat video renderen en en CAT en meer van dat soort dingen, maar als je het meeste doet wat ik boven geschreven heb is het zonde voor je geld om een AMD Ryzen 9 5950X te kopen.
RX 6600 XT;2;0.4940967857837677;Ik had het over oude systemen die zelfs geen PCI-E 4 doen. Een 5xxx Ryzen heeft weer een behoorlijke single-thread voordeel over de 2xxx of 3xxx reeks. Maar waarom zou je die nieuw kopen met een niet-PCI-E 4 moederbord? De benchmarks die computerjunky aanhaalt worden allemaal op 1080p gedraaid, net waar het meer uitmaakt. De 6600XT wordt ook niet bepaald voor 4K ingezet. Ik weet ook best dat het op hogere resoluties minder meespeelt. Je verdere argumenten zetten m'n punt alleen maar meer kracht bij: de benchmarks die computerjunky gebruikt om zijn verhaal aan te sterken hebben niks te maken met de doelgroep voor deze kaart en zijn argumenten. Of je hebt een nieuwere CPU die PCI-E 4 heeft en het is geen probleem, of je hebt een ouder systeem met PCI-E 3 waarbij de CPU toch ook al geen krachtpatser meer is en de framerates waarbij het verschil zichtbaar zou kunnen worden niet eens op kan brengen. De combinatie nieuwe supersnelle CPU én PCI-E 3 én deze kaart én een game die het verschil zou kunnen tonen is gewoon een randgeval dat er niet toe doet.
RX 6600 XT;3;0.4344286620616913;Aha ok, ja een Radeon RX 6600 XT met een AMD Ryzen 9 5950X koppelen is de grootse onzin die er is, dan ga een goedkopere Grafische kaart (dat hoort het te wezen) koppelen met een dure CPU, maar ach ik wil een goedkope CPU (AMD Ryzen 5 5600X) koppelen met een dure Radeon RX 6800 (XT), als de prijzen eindelijk wat lagen woorden, en meer richting de MSR zijn, omdat ik geen grafische werk doe en zo vind ik de AMD Ryzen 5 5600X meer dan snel genoeg.
RX 6600 XT;1;0.2561354339122772;Je doet net alsof er geen oplossing voor is...nml. gewoon de kaart in een PCIe 4.0 slot prikken, als je die extra performance in dat handjevol spellen toch nodig hebt. Enne, 182fps gemiddeld. O o oww, wat verschikkelijk! lol
RX 6600 XT;1;0.868126392364502;Je mist net als die andere totaal het punt. mensen die een ''goedkope'' kaart kopen kopen juist geen nieuwe pc... Even in een PCI-E 4.0 slot stoppen kost wel even minimaal 400 euro meer. Als ze 400 meer uit hadden willen geven hadden ze beter een 6800 kunnen kopen. die heeft geen pci belemmering en dan hebben ze meer waar voor hun geld zelfs op PCI-E 3.0 Je suggestie slaat dus werkelijk nergens op. En deze game heeft 182 fps ja maar dat is NU en bij DEZE game het sluit NIET uit dat er andere games zijn waar je MINDER fps haald en dus die dip krijgt. Jullie blijven je blind staren op 1 ding en zoeken werkelijk naar manieren om dit goed te praten terwijl het compleet scheef is.
RX 6600 XT;2;0.4958896338939667;Jij staart je blind op een bus width die voor dit prestatieniveau prima past bij het product Jij bent degene die een enkele uitschieter de norm laat zijn voor wat de kaart in allerlei in jouw hoofd bestaande edge cases gaat doen. Daarom is men het niet met je eens. Tests wijzen zelfs uit dat je ook met high end kaarten nog rustig met een oudere PCIE versie en x8 prima wegkomt, nu, gisteren, morgen en ook tien jaar geleden,.. En als je dan toch de PC gaat upgraden pak je je laatste 2-3% misschien nog mee. Het is niet zo ingewikkeld.
RX 6600 XT;1;0.5660009980201721;Deze game heeft dus een ver van ernstige bottleneck, daar zijn we het iig over eens. Dat er andere spellen komen die dit verschijnsel ook laten zien, is niet uit te sluiten maar ook niet dat dat een echte bottleneck gaat worden. Het druipt bij je af van aannames en bangmakerij, die eerlijk gezegd, tot nu toe, ongegrond lijken te zijn. Nee, jij staart je blind op één game-engine, van één spel. Zo kan ik je ook wel helpen en zeggen dat het zéér wel mogelijk is dat deze engine zich nog verbetert met pcie-3. Ook dat kan je niet weerleggen maar schijnbaar niet aan gedacht. Hoe veel breder wil je het hebben? Alle andere spellen laten zulke grote verschillen, ALLE andere spellen, honderdtallen waarschijnlijk. Breed genoeg?
RX 6600 XT;1;0.32912003993988037;PCI-E 8x op een 4th Gen is even snel als een 16x op 3rd Gen PCI-E. Das nog altijd verre van een bottleneck voor full-HD.
RX 6600 XT;1;0.4103407859802246;De meeste mensen die deze gpu kopen zullen alleen geen pcie 4.0 hebben en dus is er een bottleneck. En dan hebben we het nog niet eens over de problemen die 4.0 op veel systemen met zich mee brengt.
RX 6600 XT;1;0.49327552318573;Ah bullshit, ik kan recente high-end kaarten draaien op veel oudere mobo's met maar PCI-e 2.0 en dan nog werkt het prima zonder merkbaar verschil. Dat er synthetische benschmarks zijn die het verschil aantonen is logisch maar in de praktijk boeit het weinig. Je staart je echt blind op die PCI specs, is niet relevant. (Dit staat overigens los van het feit dat deze kaart niet boeiend is en ik het absoluut zou afraden om deze te kopen voor de huidige prijzen.)
RX 6600 XT;3;0.318562388420105;dan heb je dus duidelijk mijn eerdere post niet gelezen waarin een reviewer BEWIJST dat je verhaal dus niet waar is. Lagere pci bus bandbreedte kan je prestaties op zeep helpen. Iedereen die zegt van niet is blind voor feiten. maar zal het nog een keer voor je linken:
RX 6600 XT;2;0.34630048274993896;Kijk dan ook even verder naar In 11 van de 12 games is het verschil tussen PCIe 3.0 en PCIe 4.0 5% of minder, en dus eerder verwaarloosbaar. Enkel in Doom Eternal is het verschil een gigantische 25%. Het verbaast me trouwens waarom geen enkel moederbord de optie biedt om PCIe 4.0 lanes uit te splitsen naar meer PCIe 3.0 of zelfs 2.0 lanes. Ik heb een netwerkkaart en storage controller met PCIe x4 en x8 aansluiting, maar moet het doen met PCIe x1 slots, weliswaar met PCIe 4.0 dat die kaarten dan terugschakelen naar 2.0
RX 6600 XT;2;0.49503251910209656;Ik durf zo niet te zeggen of dit überhaupt mogelijk is bij consumenten mobo's, maar zou dit mogelijk zijn dan zal het waarschijnlijk vrij duur zijn, omdat je dan aparte chips moet hebben die de lanes uit elkaar kunnen trekken qua bandbreedte. Daarnaast zal dit iets zijn waar weinig vraag naar zal zijn vanuit de consument. Ook dit maakt het duurder. Stel dat dit dan mogelijk gaat zijn, zal je dit alleen gaan zien op zeer dure moederborden en dan ben je mogelijk goedkoper uit door gewoon de onboard nic en storage controller te gebruiken of om een nieuwe PCIE 4.0 storage controller te kopen.
RX 6600 XT;2;0.40843671560287476;Tja wat mij betreft mag 5.0 er snel komen nog voordat ik mijn pc moet upgraden naar een 4.0 bord. Dan heb je veel minder lanes nodig en kom je met 20 lanes ineens heel ver. en dat 11 van de 12 games het prima doen doet er niet toe. Als er in de toekomst nog meer games komen met dit probleem dan heb je dus een kaart die onder zijn kunnen presteert door een vreemde keuze van amd die naar van wat ik kan zien niet eens geld bespaard.
RX 6600 XT;1;0.6544125080108643;Ja want slechte 182 fps is een groot probleem
RX 6600 XT;2;0.4313375949859619;Waarom is de 5600x en 5600G te duur? ze staan aan dezelfde prijs als de 11600, is die dan ook te duur? Als je kijkt naar de performance is de 5600G iets minder sterk kwa cpu dan de 11600 maar is kwa GPU buiten categorie, de 5600X is dan in meeste gevallen weer sneller dan de 11600 in cpu. En dit alles met 10-20eur verschil dus peanuts en niet de moeite om op prijs te bekijken want neem al een ander moederbord of een verplichte high end koeler voor de intel om het maximum piek performance op te nemen en je zit al op exact dezelfde prijs.. (dit volgens de stats van tweakers review....)
RX 6600 XT;2;0.5429928302764893;De 11600 is te duur ja je kan net zo goed de 10600 kopen. De 5600x is ook gewoon te duur voor de prestaties die je krijgt net als de 5800x waar je beter een 10700k kan kopen qua prijs zeker nu upgradability een non issue is met een nieuwe socket en ddr5 op komst. De 5600g is aanzienlijk trager (10-15%) voor mensen die later een dedicated gpu willen kopen wat ik dus niet aan kan raden. En diegenen die dat niet willen hebben de igpu waarschijnlijk helemaal niet nodig op de manier dat een gamer dat heeft of maakt de extra gpu snelheid ten opzichten van een 10400 niets uit voor hun usecase. AMD is gewoon in de budget segmenten niet interessant in beide cpu en gpu land met de laatste generaties.
RX 6600 XT;1;0.5446294546127319;nu ben je wederom alles naar pro intel aan het draaien zoals steeds in uw geval. dan kan je evengoed vergelijken met een veel goedkopere 3600 of zelfs massa 2dehands versies. met de 500 series amd moederborden zijn er meer features en keuzes op alle prijzen, dit is niet bij intel. je geeft eol platformen alsof amd en intel gelijk zijn, effe realiteit check, er is niks high end intel.... enkel amd en als je nu een 6 - 8 core 3600 - 3700 koopt of een 5600 kan je nog wel even een kuch upgrade doen naar een 16core... dat is zeker een optie ook bij intel? er is trouwens bevestigd dat er nog een update op komst is voor de 5000 series met vcache en ghz boost, dat is er zeker niet bij intel op dit platform, dus het is zeker geen eol, enkel dit intel platform is zo dood als een pier, was al dood voor de release.... de alder lake is nog afwachten, maar als je graag fake news en theoretische benchmark schaling volgt doe gerust.... .. rarara it ain ' t going to happen. en als amd zo onder de indruk was van de alder lake dan hadden ze hun 5000 upgrade wel van de planning gehaald en hun am5 wel eerder gepushed want ook die staat al klaar en er is al productie op de 5nm. en als laatste nog effe een reality check voor de die hard intel fans, waaronder tweakers review site... kijk even naar top sales van de laatste maanden : of naar de sales deze maand : amd : 4550, 87. 5 %, asp : 320. 33 ( euro ) intel : 650, 12. 5 %, asp : 229. 89 amd revenue ( euro ) : 1 ' 457 ' 482, 90. 7 % intel revenue : 149 ' 427, 9. 3 % niemand koopt intel cpu in byo, als je hier dan komt prediken dat het de beste keuze omdat ze gelijke performance hebben in benchmarks met een tientje verschil of omdat beide eol platformen zijn en de amd te duur
RX 6600 XT;1;0.8392965793609619;En bij deze zijn we uitgepraat. Ik word doodmoe van dit boxjesplaatsen. Zoek maar iemand anders om daarmee lastig te vallen ik zeg toch ook niet dat jij alles pro amd praat. Je beschuldigd mij van bevooroordeeld zijn terwijl je zelf extremistischer hetzelfde doet. De groeten.
RX 6600 XT;2;0.40344393253326416;"Ben nog lang niet uitgepraat. Snap niet goed waarom jij een videokaart ""specialist"" ben met al je opvattingen en aannames."
RX 6600 XT;1;0.8443986773490906;Blijkbaar snap je de videokaarten Tag net zo min als het probleem met deze videokaart. Geeft niet maar zeg dan gewoon niets.
RX 6600 XT;5;0.4227944016456604;Nog even voor de bevestiging, TPU heeft nu ook de 6600XT getest. PCI 3.0 x8 is een compleet non-issue - sterker nog, zelfs met 2.0 kom je nog goed weg.
RX 6600 XT;2;0.5621404051780701;Tja toch apart dat ze totaal andere waardes halen. De 1080p fps is 10% lager enbdus totaal niet vergelijkbaar. Afwijkingen die zo groot zijn kan je niets mee en verdienen een hercontrole op een ander systeem. Ergens gaat er iets niet goed. Enige duidelijk verschil is 5800x vs 5950x en 4000 cl19 vs 3200 cl14. Misschien is het dus de latency van 9.5 ipv 8.75 die de fps crippled maar de hogere bandbreedte van de 4000 cl19 set die de pci bus compenseert. Maar zonder meer data en testen is dat lastig te zeggen. Duidelijk nowhere near 200 fps. Maar als je dan deze weer bekijkt dan is de fps weer torenhoog. En het verschil enorm: In dit soort situatie zou ik maar wat graag 4 of 5 test systemen hebben en verschillende modellen van de kaart om dit dieper te testen. Beide de fps en de prestatie verschillen zijn groot tussen reviews.
RX 6600 XT;3;0.43286997079849243;Niet apart, dit is perfect in lijn met alle PCIE testjes ooit. En niet nieuw. Maar op Youtube is men erg goed in spijkers zoeken op laag water want dat levert lekker veel clicks op.
RX 6600 XT;3;0.5177987813949585;Niet helemaal eens. Voor mij persoonlijk kan deze kaart bijvoorbeeld een interessante upgrade zijn zonder gelijk mijn mobo/cpu(4770k)/memory combo te moeten upgraden. Tot op heden was de beste kaart icm voor mijn rig een GTX 1070, wellicht kan ik met deze kaart verdere upgrades nog iets langer uitstellen PS met deze kaart zou ik tevens waarschijnlijk weer MacOS kunnen draaien
RX 6600 XT;2;0.5398421287536621;Als ze maar genoeg van deze kaarten leveren, komt er hopelijk wel wat meer beweging in de tweede hands markt. Totdat de tekorten opgelost zijn is het gewoon niet zinvol om nieuwe kaarten te ontwerpen voor die prijsklasse.
RX 6600 XT;3;0.25307023525238037;Die beweging komt er als het aan mij (en aan de prijs) ligt: De 6600 xt (8GB) is 2x sneller dan mijn RX 480 (4GB) en verbruikt marginaal meer stroom: +10W
RX 6600 XT;1;0.29822131991386414;Tja en die 480 8gb gaan op eBay nog steeds voor 275 euro 😎😅
RX 6600 XT;5;0.5207608938217163;Helemaal met je eens. Ik had mijn 1660 super voor ca. 240 euro gekocht en dat vind ik een prima maximale prijs voor een mainstreamvideokaartje. Ik hou hem daarom ook zo lang mogelijk tot ik een waardige opvolger voor ongeveer dezelfde prijs kan kopen. Waarschijnlijk moet ik dan wel lang wachten, maar dat is dan jammer.
RX 6600 XT;2;0.5022845268249512;"De 2060 was ook al zo. Begin 2019 schreef Tweakers: Nog steeds geen budget-Turing. Het was al duidelijk dat NVIDIA graag de ""RTX"" feature voor dikke premium verkocht. De 1600 serie vond ik persoonlijk niet zo'n sterke serie. Ik had eind 2015 een GTX970 gekocht voor ~350 euro incl. gratis leuke game + Asus muis. 3.5 jaar later komt de 1600 serie uit, en in perspectief met een GTX970 was dat de naam ""upgrade"" niet waard. De laatste sub-300 euro x60 kaart was de 1060 6GB. En dat is inmiddels ook al ruim 5 jaar geleden. Ik ben het met je eens dat het teleurstellend is.. Ik denk dat het ook een beetje komt omdat relatief nog maar weinig mensen zijn die een dedicated GPU vereisen. De integrated graphics zijn voor velen toereikend. Low-end GPUs maken kan bijan niet uit. Kaarten zoals de 1030 wordt lacherig over gedaan en krijgen bijzonder weinig aandacht.. Tevens, er zijn maar weinig mensen die een dedicated GPU in hun HTPC of kantoor PC stoppen voor betere media playback. Want een 1030 voegt weinig tot (haast) niets toe op een iGPU. Dus focussen beide bedrijven op de kerndoelgroep gamers die als alternatief evenveel geld zouden spenderen aan een console (zeg 300 - 500 euro -- die prijzen kruipen ook omhoog!). De stelling dat je een PC hebt voor studie/administratie/werk, en een GPU daarop toevoegt voor gaming, rechtvaardigt dat waarschijnlijk voor velen... Ik kijk nog graag terug op de tijd van een degelijke mid-high range GPU van 200 euro. Maar dan ga ik al 10 jaar terug.."
RX 6600 XT;5;0.30535027384757996;we we orden oud
RX 6600 XT;2;0.3860500156879425;Chips zijn nou eenmaal complexer geworden de laatste 10 jaar. Het algemene oppervlak alleen al terwijl de wafers zelf nauwelijks goedkoper zijn geworden. Daardoor is qua resources zelf de GPU al prijziger. Stroomverbruik is omhoog gegaan ondanks het procede wat wel flink kleiner is geworden. Hierdoor zijn er ook betere VRM's etc nodig wat ook prijziger is. Zp zijn er nog meer verschillen wat het gewoon duurder maakt.
RX 6600 XT;4;0.3896870017051697;Gebruik hier eigenlijk nu een 3400G (Vega 11) en heb 3000-DDR4 op een stabiele 3200 OC draaien. Ik ben in de toekomst van plan om 3600-DDR4 stickjes te kopen zoals deze: pricewatch: G.Skill Ripjaws V F4-3600C14D-32GVK - x570 chipset, kan nog een kleine OC proberen natuurlijk. Maximale bandbreedte kan tot ~20% meer FPS opleveren, als ik de zoekresultaten op Google mag geloven. Voor mijn gebruik is dat voorlopig echt genoeg en kan zo prima wachten tot de tweedehandsmarkt voor GPU's stabiliseert. Echter kan je voor de prijs van die dingen ook een 1070ti of 1080 kopen inmiddels.
RX 6600 XT;3;0.5490525364875793;Beetje goed zoeken Hans. Ik heb van een Tweaker in 2019 (?) een GTX1660 voor €150 weten over te kopen. Dan is best een flinke upgrade over de GTX970 en 3,5GB vs 6GB memory voor een appel en een ei. Je moet natuurlijk wel vaak V&A in de gaten houden en helaas is dit sinds oktober vorig jaar niet meer realistisch..
RX 6600 XT;2;0.23322975635528564;Een 6600XT is een opvolger van 5600XT, en haalt met RDR2 in 1080p 22fps meer. Wat gelijk is aan een RTX2070 Super. als ik het goed heb kwam de 5600XT voor $ 279.- begin 2020, en de 6600XT is $100.- duurder
RX 6600 XT;2;0.43803882598876953;Ik snap niet zo goed wat je hiermee wil verduidelijken?
RX 6600 XT;5;0.39314761757850647;Dat de GPU qua prestaties niet duur is. Met prestaties boven de RX5700XT.(1080-1440p)
RX 6600 XT;1;0.7564706802368164;Die 5700 is dan ook al ruim 2 jaar op de markt en was bij momenten verkrijgbaar voor onder de 300€ en je kreeg 3 gratis games erbovenop ! We spreken hier nog steeds over een MSRP prijs die opgegeven word van 389€ die uiteindelijk zal het dubbele worden en lang wachten. De Sapphire 580 RX kon je bv kopen voor 175€ ook met de 3 gratis games. De prijzen van videokaarten zijn gewoon absurd hoog mede dankzij mensen die redeneren zoals jij dat het nog meevalt. Nee het valt echt niet mee en is zwaar kut voor jongeren die hun vrije tijd willen vullen met een potje FPS games waar je een redelijk deftige kaart voor nodig hebt. Geen enkele werkgever bied me een loon aan van 1600€ MSRP en stort er 3200€ op mijn rekening
RX 6600 XT;2;0.4053110182285309;We praten over de prijs bij op de markt verschijnen, niet wat ie ergens afgelopen periode heeft gekost. Ook niet welke acties er toen aanhingen, Gewoon E459,- ten tijde van op de markt komen. Moderne spelen vragen meer van de videokaart. ga maar eens met je RX580 RDR2 spelen, op 1080p haal je nog geen 40fps, net zoveel als de 6600XT op 4K haalt MRSP van de RX580 was $250.- en dat is ook niet dubbel geworden,je fantasie is wel rijk zeg
RX 6600 XT;5;0.3955334424972534;Ah, weer iemand die niet snapt dat elke generatie altijd al sneller is geweest dan de generatie daarvoor. Als we jouw methodiek aanhouden, dan kost een GPU over een paar jaar minimaal 1500 euro, want dan zal het basismodel net zo goed zijn als de RTX3090, en dus ook net zoveel mogen kosten.
RX 6600 XT;2;0.38858821988105774;Wat je niet wilt begrijpen, is dat je nu een RX6600XT kunt kopen, ipv 5700XT. Wat kostte een 5700XT ? En dat een 6600XT de opvolger is van 5600 XT. Verder wordt alles duurder. Je kunt wel nagaan, dat je niet jaren achtereen voor €250,- een nieuwe GPU kunt blijven kopen, die alsmaar beter presteert, en de prijs gewoon gelijk blijft. De 6600XT komt voor hetzelfde bedrag op de markt als een RX5700 in july 2019.
RX 6600 XT;2;0.4192162752151489;En dat ligt aan wat? De inflatie? Leuke middenklasser voor tussen de 2 en 300 zou ik verwachten. Hoezo moet dat omhoog schuiven dan? Zou verwachten dat ik steeds meer prestaties koop voor gelijk geld. Okay de software wordt dan ook wat zwaarder, nou en. Maar nu moet ik puur naar fps gaan kijken om te zien wat ik zou moeten betalen of wat een goede prijs is?
RX 6600 XT;2;0.3509465157985687;Wat je niet wilt begrijpen, is dat je nu een RX6600XT kunt kopen, ipv 5700XT. Wat kostte een 5700XT ? En dat een 6600XT de opvolger is van 5600 XT. Verder wordt alles duurder. Je kunt wel nagaan, dat je niet jaren achtereen voor €250,- een nieuwe GPU kunt blijven kopen, die alsmaar beter presteert, en de prijs gewoon gelijk blijft. De 6600XT komt voor hetzelfde bedrag op de markt als een RX5700 in july 2019. Toch bijzonder, aangezien dat precies is wat je jarenlang kon doen sinds de GTX260. Maar ik zal wel gedroomd hebben dat de GTX560, GTX960, GTX1060 en GTX1660 allemaal beter dan de vorige generatie presteerden, en toch dezelfde prijs hadden.
RX 6600 XT;1;0.508711040019989;Een RTX3060 en RX6600 zijn alleen geen low-end kaarten. Denk je dat de komende RTX3050 ook voor $159,- in de planken ligt, net als de 1650 Super ? Zelfs je benoemde GTX560 was met $199.- Duurder dan 1650s. En de GTX1660 kwam uit voor $219.- RTX2060 is al geen low-end kaart meer en kwam uit voor $349.- RX6600XT kun je niet vergelijken met RX580/590 als je over de GTX1060/1660 praat
RX 6600 XT;5;0.4824754595756531;Een RTX3060 of 6600XT is net zo low-end/high-end als een GTX260/GTX560/GTX960 ooit was.
RX 6600 XT;2;0.5343874096870422;"Er is toch wél een verschil en meteen een goede reden waarom het toch allemaal duurder gaat worden: De kaarten die je noemt waren amper bedoeld voor 4k, alles wat nu komt en er al is moet 4k aankunnen en als het ff kan ook raytracing. De eisen worden hoger, wij willen meer en dat zal, en dat zie je nu al; meer gaan kosten. Jarenlang kom de inflatie beteugeld worden, met steeds kleinere transistors o.a.. Dat proces gaat in verhouding steeds trager met als gevolg dat die's groter worden. En dus prijziger. Méér RAM onboard; helpt ook al niet."
RX 6600 XT;1;0.4561172425746918;Ik vermoed eigenlijk dat er bijna geen markt meer voor is. Voor games spelen op medium / low voldoet een geïntegreerde GPU al. Waarom zou je dan nog een losse low-end kaart kopen?
RX 6600 XT;3;0.38727036118507385;Tussen een IGP en een RTX3060/6600XT zit toch nog een wereld van verschil. Alleen moeten die kaarten een fatsoenlijke prijs krijgen, en niet vanaf 350+ euro.
RX 6600 XT;1;0.465260773897171;Een RTX 3060 is geen low-end kaart. De meeste games draaien daarop op de hoogste instellingen. Vroeger kocht je een kaart voor €100, en dan kon je misschien net de nieuwste games draaien op medium met 30FPS. Ik denk dat mensen die een videokaart kopen daar gewoon niet meer op zitten te wachten.
RX 6600 XT;2;0.5035598278045654;Als ik naar de 2dehands markt kijk, dan lijkt er echt wel een markt voor te zijn. Als je ziet wat voor hoge bedragen er gegeven worden voor eigenlijk oude hardware. Het blijft toch een relatief oude kaart zonder garantie. Je weet immers nooit zeker wat de vorige eigenaar er mee gedaan heeft.
RX 6600 XT;5;0.601616621017456;Er is echt nog wel een markt voor. Ik denk dat de meeste mensen staan te springen om een kaart te kopen tussen de 200-300 euro, zoals dat mogelijk was met de 570 of 1060.... Die kaarten presteren vele malen beter dan een geïntegreerde GPU doet. Helemaal als ze weer kaarten in de range uitbrengen, vele jaren dat ze expres dat gat kunstmatig hebben gecreëerd.
RX 6600 XT;3;0.6376975178718567;Ik zou denken dat een beetje middenklasser qua GPU gezien tussen de 200 en 300 eurie zou kosten. Mijn 1060 doet het nog leuk maar wordt een beetje net te traag geloof ik. Maar zie nu maar eens een upgrade te scoren voor dat geld.
RX 6600 XT;4;0.45436424016952515;Mwah een hele uitgebreide iGPU zoals in de 3400G of 5600G doet misschien niet onder voor een 1030gt, maar er zijn echt wel veel kaarten low-end gaming kaarten te noemen die sneller zijn.
RX 6600 XT;2;0.4582323729991913;Huidige IGPs vechten het uit met de GT 1030. Dat is nou niet bepaald een beest. Een RTX 3050(Ti) voor desktop zou echt wel nog op zijn plaats zijn, net als een RX6500(XT). En ga er maar niet van uit dat RDNA2 IGPs dat gat goed gaan dichtten. Al zijn ze 2x sneller (echt onwaarschijnlijk) blijft het gat met budget-class GPUs als de 50 serie van NV en 500 serie van AMD gewoon eenvoudig groot genoeg om hun een bestaansrecht te geven. Wat er vooral weer zal moeten gebeuren is dat de ruimte onder de €300 weer gevuld wordt. Dat gebeurt vanzelfs als de dit huidige tekort over is. Zou me niks verbazen als zowel de 3060 als de RX6600XT op termijn veel verder onder hun MSRP zakken da gebruikelijk. Misschien duurt het nog tot de volgende generatie, maar dit enorme tekort door cryptogekte gaat niet oneindig door.
RX 6600 XT;2;0.25704288482666016;De prijzen zijn vooral bizar tweedehands nu. Dezelfde rx5700 (non xt) die ik voor 325 euro gekocht heb ik 2019 staat nu 600 euro op marktplaats. Bitcoin is weer aan het stijgen dus de miners gaan ook weer los. Deze nieuwe kaarten zijn zeer efficient kwa mining performance per watt. Vergeet dus maar om hem voor de normale prijs ergens te vinden.
RX 6600 XT;3;0.4321569800376892;Ik zou het wel interessant vinden wat de 'Prestatie per Watt'-score zou zijn. In langdurige game sessies tikt dat wel aan. 4u per dag in game, á 300W (6900XT) = 1.2kWh. Dat enkel voor de kaart. Als ik mijn Steam gemiddelde neem, 120h/2weken = 72kWh * 0.25c = €18 per week om te gamen. Stel de 6600XT is nét iets langzamer, maar vreet maar 160W in game, zou ik daar eerder naar neigen. Ik mis daarom een beetje deze statistiek in de review.
RX 6600 XT;2;0.4093988239765167;Eh.. je kan je FPS limiteren, je kan je resolutie omlaag gooien.. je kan lichte games spelen, hij trekt echt niet elk gamende uur de volle 0.3 kWh uit de voeding! Edit: het voelt alsof het een nadeel is dat een Porsche 911 zeg 1 op 10 loopt en een Clio R.S. wel 1 op 14 haalt, lijkt me toch ook alle 2 voor totaal andere doelgroepen met andere bedoelingen? 😅
RX 6600 XT;3;0.32753822207450867;Woah.
RX 6600 XT;3;0.44780272245407104;Wat ook kan schelen in stroomverbruik is je refreshrate lager zetten. Tussen 144hz of 60hz zit vaak zo'n 100watt of meer verschil.
RX 6600 XT;5;0.47528815269470215;Wat ook kan schelen is een stoel kopen met pedalen en een generator eronder
RX 6600 XT;1;0.4940005838871002;Jij gamed 60 uur per week? ...
RX 6600 XT;1;0.6176095604896545;Ik kan dat wel eens aantikken ja. 2 weekenden volle bak gamen, en nog een paar avonden. Dan zit je er zo aan. Op het moment sta ik op 25H in afgelopen 2 weken. Het dan ook maar een rekenvoorbeeld. Je zou maar een jong op je zolder hebben zitten die wel letterlijk iedere week 60 uur aantikt Edit: Dit is ook verder totaal off-topic. Ik wil graag zien in de review wat de prestaties per Watt zijn.
RX 6600 XT;3;0.3204736113548279;Je wordt op pagina 17 op je wenken bediend. offtopic:Met een kind dat 60 per week gamet lijken me die paar euro's aan stroomkosten me wel het minste probleem
RX 6600 XT;5;0.2946929633617401;Ruim 8 uur per dag dus. Het leven van een 15 jarige...
RX 6600 XT;1;0.6008487939834595;Doubtful, ik ken niet veel 15 jarigen die zelf de elektra moeten betalen...
RX 6600 XT;5;0.27412936091423035;Ik ken wel veel 15 jarige die nooit hun pc afleggen en 24/7 laten draaien met de kaart waar ouderen hun krom voor gewerkt hebben. En ze zorgen er ook nog eens voor dat de oudjes zeer hoge elektriciteitsrekening erbovenop als geschenk terug krijgen
RX 6600 XT;1;0.5825405716896057;aaaah, de goede oude tijd.... Tray energy drank, verzameling worstebroodjes en de hele dag geen zonlicht zien.
RX 6600 XT;3;0.326901912689209;same here
RX 6600 XT;1;0.4199807345867157;En als ik nu ga slapen ga ik nooit op tijd opstaan, dus we trekken nachtje door...
RX 6600 XT;4;0.49833977222442627;Good old dayssss
RX 6600 XT;3;0.4440464675426483;Als Eve in de achtergrond loopt, ik wel ja. Anders nooit niet. Maar dat is ook meer af en toe, nu bijvoorbeeld is het TEST bashen. Anders is die 12 uur ook de max per week.
RX 6600 XT;1;0.548757791519165;Je kan van die 60uur per week ook uurtje gaan werken, dan heb je die 18€ alweer terugverdiend 😉
RX 6600 XT;1;0.4762502908706665;Op de eerste pagina staat de 6600 XT in de tabel met 8 GB geheugen. Wat is het nu, 6 GB of 8 GB?
RX 6600 XT;5;0.3376357853412628;8GB volgens AMD:
RX 6600 XT;1;0.49448519945144653;Het is inderdaad 8GB, stond verkeerd geformuleerd in de conclusie, fixed.
RX 6600 XT;1;0.29141125082969666;Er staat ook nog TBP i.p.v TDP als ik het goed is, in de inleiding. waarbij alle kaarten op een rijtje worden gezet.
RX 6600 XT;1;0.44170239567756653;8GB, de 5600XT heeft 6GB Vram
RX 6600 XT;2;0.5356943607330322;Raar om te zeggen dat gamers zich richten op 1080p en daarom deze kaart uitbrengen, gamers spelen op 1080p omdat het gewoon nog niet soepel genoeg gaat met de kaarten van nu op hogere resoluties. Niet andersom. Speel nu zelf 1440p met een 3060 en dat gaat net, wel fijne resolutie voor gamen, maar als ik echt serieus in competitie zou spelen ging ik op 1080p spelen denk ik, nu wil het oog ook wat.
RX 6600 XT;1;0.749756932258606;Monitoren worden ook niet goedkoper. Een 1080p monitor kost net zoveel als 10 jaar geleden, en als je een hogere resolutie wilt betaal je ruim het dubbele. De ontwikkeling van mainstream monitoren staat eigenlijk volledig stil.
RX 6600 XT;5;0.29861342906951904;Onzin. Mijn 1080p gaming monitor was 200 euro voor een 75hz tn paneel nu heb je die al voor 100 high refresh is ook veel goedkoper geworden. Daarnaast zijn ze veel beter geworden. Ook grotere schermen zijn snel goedkoper geworden.
RX 6600 XT;4;0.397149920463562;4K is anders flink omlaag gegaan. En dus heb ik er een gehaald. Geen spijt van, lekker veel ruimte, wel de scaling op max 125%.
RX 6600 XT;2;0.5225801467895508;Ik heb een heel nieuw systeem gebouwd, en alleen de gpu mis ik nog. Heb me pijlen gericht op een 6700xt ivm 34inch widescreen. Deze 6600xt zal net niet krachtig genoeg zijn voor de toekomst. Vind het knap hoe mensen het toch voor elkaar krijgen om een kaart te bemachtigen via AMD. Mij is het nog niet gelukt
RX 6600 XT;1;0.36110103130340576;Probeer anders een 3060, die komt regelmatig nog voorbij voor 600 euro. Vergelijkbaar met een 6700 XT.
RX 6600 XT;1;0.5410139560699463;Ja die hou ik ook in de gaten, van wat ik gelezen heb ligt de 6700xt tussen de 3060 en 3070. Via MP probeer ik het ook, maar wil eigenlijk niet meer dan 50 euro meer betalen dan de website prijs.
RX 6600 XT;3;0.46524158120155334;Dan kan je beter de 3060 TI halen die is sneller en kost vaak bijna even veel.
RX 6600 XT;1;0.46327483654022217;Zou iemand mij willen uitleggen waar 99p en 99,9p voor staan?
RX 6600 XT;1;0.36232975125312805;Staat op pagina 2, bij Testverantwoording. Het heeft dus te maken met de rendertijd van het langzaamste frame, waarbij je de langzaamste 1% en 0,1% weglaat.
RX 6600 XT;1;0.4342482388019562;Pagina 2... ik heb mijn huiswerk dus niet goed gedaan Dank voor de verwijzing. Maar wat is nou precies het punt van het weglaten van de 1% langzaamste frames? Bij het meten van framerates worden toch ook niet de 1% laagste FPS weggelaten... Het zal vast een scheve vergelijking zijn maar wat mis ik hier?
RX 6600 XT;4;0.31527912616729736;Dit was voor mij eerlijk gezegd een prima kaartje geweest. Ik heb gewoon een full-time baan, ben recent vader geworden dus ik heb geen zeeën van tijd om te gamen. Voor de games die ik speel, Warzone, Apex, was dit meer dan voldoende geweest (overigens wel op 1440p). Ik heb via een AMD drop een 6700 XT weten te bemachtigen, maar als ze deze hadden aangeboden was ik voor de 6600 gegaan.
RX 6600 XT;4;0.3003547787666321;Console 😉 staat binnen 15s aan en je kan ook spelen als die kleine in je armen ligt te slapen omdat ze anders ‘s nachts mama wakker houdt 😉
RX 6600 XT;1;0.3422216475009918;Voor Warzone en Apex? Dat is echt geen doen met een joypad en dan druk ik me nog erg positief uit. Knaagdier en ramplank.
RX 6600 XT;3;0.22410598397254944;Zo erg? Ik heb ook een PS5 en heb daar echt nooit problemen mee gehad. Enige minpuntje is de FOV in Warzone
RX 6600 XT;2;0.5378429293632507;Deze kaart moet dus concurreren met de 3060 maar is iets sneller maar ook iets duurder? Kan je dan niet beter direct voor de 3070 gaan? Heb net een 3060 12GB gekocht. Teveel betaald maar ja met niks draait game ook moeizaam. Toekomstig 4k gamen op hoge settings blijft toch nog voor de gemiddelde gamer ver weg als je kijkt wat je nodig bent . Komt nog eens bij dat veel games voor de pc dramatisch geoptimaliseerd zijn. Kijk naar rdr2, wat een drama. Op 1080p is het een groot blur of oversharpning festijn
RX 6600 XT;2;0.44446277618408203;AMD is nogal bezig om te cashen uit de huidige situatie met chiptekorten en te dure videokaarten. Wat natuurlijk begrijpelijk en hun goed recht is maar toch. Het uitbrengen van alleen dure APU's zoals de 5600G en 5700G met matige grafische chips is ook zo'n voorbeeld. Waar is bijv. de 5300G voor zo'n 160 euro? Jammer want een APU koop je juist om een lekker goedkoop systeempje voor erbij in mekaar te zetten.
RX 6600 XT;2;0.3693087100982666;Inderdaad, AMD richt zich nu ook meer op mid-high range ipv low-budget wat ze eerst deden, goed recht dat geef ik toe, maar jammer dat de budget gamer nu niks kan kopen (qua pc gaming) Ben zeer benieuwd waar Intel mee komt met hun gaming videokaarten Zou zomaar de nieuwe budget koning kunnen worden
RX 6600 XT;1;0.3543659448623657;De eerste recente kaart die ik me van Intel kan herinneren (Xe) was alleen te gebruiken in Intel systemen. Misschien omdat het OEM only was. Dan zal Intel die truc moeten laten varen ook al willen ze Intel CPU's op de desktop zien.
RX 6600 XT;2;0.3842114210128784;Ik kan me niet voorstellen dat dit een hardloper wordt wanneer de marktprijs hiervan richting de 500€ of meer gaat. Tenzij je per sé direct een GPU nodig hebt zou ik m'n geluk beproeven om via de AMD website voor dat geld een 6700 of 6800 te bemachtigen, dan maar wat meer geduld ophoesten.
RX 6600 XT;3;0.2833029329776764;4 uitgangen dus 4 schermen lukt?
RX 6600 XT;3;0.41854971647262573;250€ zou een uitstekende prijs zijn voor deze videokaart maar met de huidige vraag verre van realistisch.
RX 6600 XT;3;0.3315310478210449;Ik zie RDN2 met 20% minder CU's dan een playstation 5, maar je betaalt méér dan voor een complete PS5+controller. Tjaaaa. Keuze snel gemaakt lijkt mij.
RX 6600 XT;2;0.4379521310329437;Conclusie slaat de spijker op zijn kop. Ik heb nu een RX580 wat op zich prima is voor 1080p gamen, maar je merkt toch dat een solide 60fps er niet altijd meer in zit, in ieder geval niet zonder de grafische settings flink terug te schroeven. Maar als ik nu dus wil upgraden naar een nieuwe mainstreamkaart om lekker te blijven gamen op 1080p/60fps dan mag ik meer dan €400 aftikken. Dan ben ik eerlijk gezegd meer geneigd om eens flink in de buidel te tasten voor nieuwe monitor en een 4K waardige GPU. Dan weet je ook dat je wat hebt.
RX 6600 XT;3;0.5418925881385803;Let wel even op dat des te hoger je resolutie des te sneller je de gpu kan upgraden. Met andere woorden als de hoogst mogelijke resolutie op je scherm hebt zitten dan heb je relatief vaker een duurdere gpu upgrade nodig. Wellicht is 3440x1440 wellicht ook een optie voor je. Dan heb je een PPI vergelijkbaar met een 1440p scherm maar wel meer in de breedte.
RX 6600 XT;5;0.4584463834762573;Voordeel van 4k is dat het 4x 1080p is - je kan dus vrij makkelijk terugschalen naar full-hd resolutie. Één pixel worden er 4 op je scherm, dat blijft scherp. Veel nieuwe games ondersteunen ook al een lagere render resolutie dan je scherm is, en technieken zoals DLSS maken dat nog beter. Zo game ik veel spellen op medium/lage settings op een oc'ed R9 290 icm een 4k scherm (ik vind de huidige prijzen even wat te zot om te upgraden). Kortom: nu top GPU + 4k scherm kan ook best een lange tijd mee.
RX 6600 XT;2;0.5476348400115967;Dat dacht ik ook, maar het valt vies tegen om 1080P content op een 2160P scherm te tonen, zonder verlies van detail en/of scherpte. Om een de een of andere idiote reden, willen de videokaart makers, Windhoos 10, en de monitor makers, maar niet goed 'integer scaling' (1:2) implementeren. Daar is jarenlang al voor gelobbyed, maar het is nog altijd een bron van ellende. Nvidia heeft die mogelijkheid zover ik weet nog steeds niet. AMD heeft sinds kort wel een integer scaling optie, maar dat ziet er nog altijd niet zo goed uit als 1080P native.
RX 6600 XT;2;0.44942060112953186;vergeet niet dat veel 1080p schermen 22 of 24inch zijn, en 4k 27inch of soms zelfs 32inch, dus dan is de 1080p als sowieso een stuk minder scherp dan wat je gewend was.
RX 6600 XT;1;0.3588929772377014;Waarom is dat zo?
RX 6600 XT;2;0.45753106474876404;Evenveel pixels over meer centimeters, dus grotere pixels, dus minder scherp.
RX 6600 XT;5;0.5305846333503723;Op een native 1080p monitor worden subpixels gebruikt voor een zo mooi mogelijk plaatje. 1080p op een 4k monitor heeft geen subpixels nodig maar 4 pixels om mee te spelen. 1080p op een 4k scherm kan er veel beter uit zien dan op een native 1080p monitor . Helemaal als je voorbij de 24inch gaat.
RX 6600 XT;2;0.5183740854263306;Daar heb ik ook rekening mee gehouden. Het kartelrandje / pixalatie is niet zozeer het probleem, de leesbaarheid van de tekst door contrast vervaging en fringing wel. Met integer scaling is dat héél goed te beperken, maar toch kijkt native 1080p dan prettiger (imho). Je kan het effect van het verschil in pixelpitch / PPI beperken door met kijkafstand te variëren. Met andere upscaling technieken zoals bilineair, of bicubic is het gewoon een ramp. Voor films met late bitrate kan dat een voordeel zijn (foutjes worden ook vervaagt / weggepoetst), maar niet voor games of tekst. Het komt erop dat je dus alleen met een AMD videokaart uit de voeten kan wil je 1080P goed upscalen naar 2160P.
RX 6600 XT;2;0.5022993683815002;Tja ik zou daar toch echt een andere keuze in maken. Een betaalbare 4K monitor zit op 60 Hz en dat is toch echt wel een stuk minder fijn dan een 100+ Hz monitor. Wil je een 100 Hz+ 4K scherm dan zit je meteen boven de 650 euro. Om vervolgens dan op low/medium te gaan spelen op 1/4e van de resolutie vind ik geen logische uitkomst. Je bent dan véél beter af met een resolutie waarbij je native wél voldoende fps kunt halen in combinatie met 100+ Hz. Vergis je overigens niet in dat scherpte verhaal. Een 28 inch scherm zoals deze pricewatch: Samsung U28E590DS Zilver, Zwart heeft een PPI (pixels per inch) van 78. Een ouderwetse 1920x1080 22 inchmonitor zit op 100 PPI. Met andere woorden je 4k monitor is dan onscherper.
RX 6600 XT;4;0.5104841589927673;Precies wat ik gedaan heb, alleen dan wel gecombineerd met een 3070 ti zodat ik net wat minder compromissen moet sluiten qua instellingen.
RX 6600 XT;3;0.5091825723648071;ik heb een 1440p en speel het meeste op ultra met mijn 3060ti. Ben toch wel blij dat ik die meteen gekocht heb voor <500 euro. Overgenomen van een tweaker die er 2 had. Ik kwam van een 1070ti, dus meer dan genoeg vooruitgang en net voor de enorme prijsstijgingen. Ik vind ook wel dat de settings tussen high, very high en ultra lang niet altijd heel duidelijk zijn in alle games. In ieder geval niet bij alle settings. Ik vind persoonlijk dat hele raytracing niet zo heel boeiend.
RX 6600 XT;4;0.5377070903778076;3060 ti heb ik ook overwogen, maar vanwege beschikbaarheid/ toekomstbestendigheid toch de 3070 ti gekozen. Ook omdat ik met ultrawide werk. qua graphics heb je zeker gelijk. verschillen zijn soms niet eens heel groot. je kan ook heel erg tweaken met specifieke instellingen. Shadows kost vaak veel performance dus als je die laag zet dan kan misschien de textures weer wat omhoog.
RX 6600 XT;1;0.44479236006736755;Vooral een lege portemonnee. Ik heb de mazzel dat ik een 280Ti voor 565 2e hands kon overnemen ten tijde van de pre-order gekte van de 3080. Zit nu op 900 euries. 3080(Ti) zit je nog steeds 1000+. Moet de monitor er nog bij. Op dit ogenblik adviseer ik mensen de keus uit 2 dingen. Wachten totdat de gekte over is, of voor 1 van de nieuwe consoles gaan.
RX 6600 XT;3;0.3493882715702057;Inderdaad, beetje jammer weer van die prijzen Maar je zult zien dat ze morgen toch als warme broodjes over de toonbank gaan Ben benieuwd welke prijzen er gehandteerd worden bij, Megekko, Azerty, Alternate, etc Maar je hebt gelijk, dan beter flink investeren (als je dat kunt natuurlijk) en dan meteen een snel en kachtig systeem neerzetten waar je weer jaren mee vooruit kunt
RX 6600 XT;2;0.3649943768978119;Ja dat is leuk een top kaart kopen. Ik kocht in 2014 een gtx 780ti. Ik dacht, hier kan ik wel lang mee vooruit. (Met 3 gig vram zou ik snel tekort komen, maar het gaat om het idee. Als je nu een kaart met 8 gig vram koopt als voorbeeld. Daar heb je heel lang wat aan) Je betaald dus voor een kaart met specificaties die misschien wel 10 jaar mee kunnen. Maar als je kaart het begeeft bij 4,5 jaar dan heb je er nog niks aan. Daarom koop ik niet meer de top kaart (ook al zijn die prijzen nu veel hoger dan toen ik de 780ti kocht) Ik heb destijds veel betaald voor een kaart die niet heel lang mee ging. Ik heb nu een gtx 1070ti. Was een mooie prijs en er zat een spel bij (call of duty black ops 4) omdat ik dat spel wel leuk vind haal ik dat van de prijs van de kaart af. En ook zijn de meeste nieuwe kaarten voor 4K of die daar onder. Dus geen 3090 nodig voor 1080p. Ik wou altijd de beste graphics, maar dat is niet meer te doen. (Betaalbaar) En games worden steeds mooier. Dus een spel tegenwoordig kan op medium ook al super mooi zijn.
RX 6600 XT;5;0.7263239026069641;Herkenbaar, altijd de beste graphics, ik had een 1070, toen nog net op tijd (achteraf) een 2070 Super kunnen kopen, die blijft nu wel even Je kunt games ook aanpassen zodat je toch vloeiend blijft spelen met minimale grafisch impact, of je resolutie omlaag zetten, als je op 1440p zit naar 1080p bijv.
RX 6600 XT;2;0.4903090298175812;Als je nu een kaart met 8gig vram gaat kopen, ga je al heel snel achterlopen. 12GB is naar mijn idee nu wel een minimum als je 3+ jaar met je GPU vooruit wilt. Steeds meer games tikken bij mij de 8GB VRAM aan op 1080p. Er zijn zelfs al games die door 8GB gebottlenecked worden en dat gaat de komende jaren alleen maar toenemen nu we langzaam de volgende console generatie instappen.
RX 6600 XT;3;0.39307934045791626;Misschien is dat waar ze op hopen met deze prijsstelling.
RX 6600 XT;1;0.8201422691345215;Dit is idd waar ze op hopen De redenering is vrij simpel met die lockdowns geweest had iedereen geld over. Mensen die iedere maand alles uitgaven voor corona aan horeca uitgang en gebruik drugs hebben dan plots mega veel geld over in de maand. Fabrikanten weten dit en investeerders en gaan er maximaal van profiteren ! De prijs is volgens mij gezet en zolang de gekken er de prijs ervoor blijven betalen zal het niet veranderen. Het zit zelfs zo ver dat zelfstandigen massaal kaarten opkopen op bedrijf en ze voor de volle pot verkopen op 2dehands aan dubbele prijs.
RX 6600 XT;2;0.41388487815856934;Die nieuwe 4 K monitor + de GPU om hoger dan 60fps te zitten continu zal idd wat kosten ( de prijs van een volledige gamepc eind 2019). Dan kom je waarschijnlijk een bottleneck tegen van je CPU en je bent vertrokken Ikzelf ben het upgraden moe omdat het vaak veel geld kost en nooit stopt , maar vaker teleurstellingen gehad op gebied kostprijs prestatiewinst. Bijvoorbeeld 500€ uitgeven voor 10fps + is gewoon je geld wegsmijten.
RX 6600 XT;1;0.4929928183555603;Dus voor een 1440p monitor heb ik minimaal 12gb aan vram nodig ? dat zal wel de reden zijn waarom , mn youtube/primevideo zo vreselijk haperen als ik aant gamen ben Edit: ik heb het al uitgevonden in de display settings moet je variable refreshrate uitzetten, anders krijg je dus de problemen zoals hierboven op je non-gsync/freesync monitor
RX 6600 XT;1;0.4218065142631531;"Dat ligt eerder aan youtube denk ik, op 1080P ;agt de video ook met 8GB Vram"
RX 6600 XT;1;0.4590105712413788;Euh? Ik draai 2x 4K 120hz op een RX Vega56 met 8gb vram, éne scherm FF14 aan 120fps, andere scherm YouTube/Netflix in 1080p 60Hz. Ik zie nooit haperingen ...
RX 6600 XT;3;0.5234012603759766;Dat lijkt mij sterk. Kan ook zijn dat de prio verlaagd wordt wanneer je in de game window gaat zitten.
RX 6600 XT;2;0.3147474229335785;Nee, ik speel division 2 op 1440p ultrawide met een gtx1070 met 8GB.
RX 6600 XT;3;0.399184912443161;Concluderend dus een compleet kansloze kaart, maar goed de markt zal het uiteindelijk uitwijzen.
RX 6600 XT;2;0.32427486777305603;Een kansloze kaart die wel beschikbaar is vanwege weinig vraag, verkoopt ook, vermoed ik. Eigenlijk best jammer, want zo geeft de markt nog geen signaal af
RX 6600 XT;1;0.3554656505584717;Is er iemand op dit forum die mij wegwijs kan maken in hoe je een kaart voor een normale prijs kan bemachtigen? Ben best bereid enkele weken/maanden te wachten erop. Ben iets te trots om voor een 3070 meer dan €700 te betalen.
RX 6600 XT;2;0.5263379812240601;Niet veel anders dan wat je al zegt, even wachten. Het zal dichter bij maanden dan weken zitten tot de tekorten volledig weg zijn en de prijzen zijn genormaliseerd.
RX 6600 XT;4;0.23110538721084595;Prebuilt game PCs.
RX 6600 XT;2;0.33609291911125183;Ja ik heb dus een redelijke pc waar eerst een 1660 in zat. Maar had die aan mijn broer vergeven omdat ik met mijn naïeve hoofd dacht dat ik wel even een 3070 zou scoren. Misschien ergens voor kerst dan. Heb zelf ook niet te veel tijd vanwege twee kids.
RX 6600 XT;2;0.3401406705379486;AMD dropt op zijn eigen site iedere week voor de adviesprijs. Zie ook het forum topic: [AMD Radeon RX6000 series] Levertijden & Prijzen. Moet je er wel snel bij wezen. Voor de 6800XT is bijna niet te doen.
RX 6600 XT;5;0.5788605213165283;Dit is exact wat ik zocht. Zal er eens naar kijken en geduldig afwachten. Heeft Nvidia ook zoiets dergelijks?
RX 6600 XT;1;0.3248257040977478;nvidia verkoopt volgens een duitse site naar nederland geloof ik, maar daar is ook een topic voor
RX 6600 XT;3;0.4029304087162018;Doe het toch maar. Tenzij je wilt wachten tot doomsday. Mining is weer helemaal terug, dus dan weet je het wel.
RX 6600 XT;3;0.26531776785850525;Tweakers laat weer, geheel in traditie, na om de text te laten corrigeren vóór publicatie. Bij mijn weten is 2*8 nog steeds 16.
RX 6600 XT;1;0.6534323692321777;Inderdaad belachelijke prijs (werkelijk w.s. iets van 800 euro) voor een zeer basic kaartje...
RX 6600 XT;1;0.472277969121933;De 6700 XT heb je vanaf 779,- euro dus hij zal iets van 559,- euro worden in de webshops gok ik.
RX 6600 XT;1;0.28853416442871094;Puur uit interesse, kan je deze GPU ook koppelen aan je IGPU? waarbij je radeon + radeon gpu power koppelt als het ware. reviews: AMD Ryzen 5 5600G en Ryzen 7 5700G Review - Eindelijk moderne Ryzens... Intel had er een stukje over een tijdje terug:
RX 6600 XT;2;0.4274427592754364;En weer geen VR test gedaan, terwijl we tegenwoordig meer VR games zien... vooral met Quest 2 over link. Waarom testen jullie niet VR? Is het totaal niet interessant omdat doelgroep te klein is? Eigenlijk is VR dé reden waarom betere videokaart nodig is. VR is wat zwaarder en dat kan je niet doen met GTX1060 bijvoorbeeld, ook niet zoveel met GTX1070. Als je géén VR doet, is er niet echt reden om gelijk RX 6600 XT te kopen, omdat games nog steeds vlot lopen met GTX1060 / GTX1070. De framerates zijn prima te doen met deze kaarten. Veel gamers hebben niet echt reden om gelijk te upgraden. Zeker met 1080p nog lang geen probleem. Bovendien is GPU markt nog lang niet gunstig, velen wachten nog even op verder zakken prijzen. Pas als je meer met VR gaat doen, dan moet je wel betere kaart nemen. Dus, waarom hebben we weer geen VR test gedaan? Wat is er toch met VR?
RX 6600 XT;2;0.6007089614868164;VR is een niche. Te weinig (goede) content en teveel devices met hun eigenaardigheden. Kortom, hoe ga je een goede test opzetten die nuttige informatie blijft geven? De aantallen zijn over de hele breedte te laag.
RX 6600 XT;3;0.4078969359397888;"1080p voor die kaart? Ik krijg een prima 1440p ervaring op mijn 5600xt in de meeste games, sure niet de nieuwste AAA maar wel games van een jaar of 3 oud, meeste mensen spelen toch alleen GTA V, CSGO, Apex etc ; ik ken niet veel mensen die vaak de nieuwste AAA games koopt en speelt voor een redelijke tijd"
RX 6600 XT;3;0.2650783956050873;Is waar de marketing zich op richt. Je kan elke kaart wel promoten als 1440p kaart als het om CSGO gaat Next gen kaarten richten zich op next gen games, dus games die misschien vorig jaar, maar vooral dit jaar en in 2022 uitkomen en triple A zijn.
RX 6600 XT;3;0.6694729328155518;Deze kaart komt aardig in de buurt van een RTX 2080 als ik techpowerup mag geloven dus een 1080p kaart lijkt me inderdaad wat negatief. Overigens zijn er wel games zoals control waarbij een 3060 Ti vereist is om 60 fps te halen op 1440P terwijl een 6700XT de 60 fps nog niet haalt. Dus als je naar die games gaat kijken wordt het wellicht een ander verhaal.
RX 6600 XT;2;0.5414620041847229;Beetje overbodige kaart. Meer dan 250 uitgeven voor een 1080p gpu lijkt het mij niet waard, en in de praktijk zie ik deze niet onder de 500 verschijnen. Behalve voor enkele gelukkigen die de tentjes opzetten rond de AMD website. Ben zelf uiteindelijk naar maanden wachten toch maar overstag gegaan. 449 voor een nieuwe 2060 gp oc, heb je in elk geval garantie + zekerheid dat er geen mining mee is gedaan. Als de prijzen in augustus/september niet dalen lijkt het wachten op de volgende black friday sale.
RX 6600 XT;1;0.5789403915405273;Tenenkrommend die verwijzing naar een uitleg in een plus-artikel zonder die uitleg in dit artikel ook te noemen. Bah!
RX 6600 XT;1;0.8882172703742981;Nudgen noemen ze dat geloof ik. Voor mij is het steeds een druppeltje richting account deletion. Ik ga echt never nooit geld betalen voor de content die hier gemaakt wordt. Onzinnig.
RX 6600 XT;2;0.39869484305381775;Kwaliteit neemt iig niet toe hier op tweakers... Copy/paste much?
RX 6600 XT;1;0.48730993270874023;Hoe is het minen van verschillende coins met deze kaart?? ik vraag het voor een vriend...
RX 6600 XT;1;0.7145910859107971;"Ik vroeg me het zelfde af. Het antwoord kan je hier vinden; na aftrekken van energiekosten (€0.25/kWh). RX6800 XT €3.31/dag RX6800 €3.31/dag, 181 dagen terugverdientijd RX6700 XT €2.32/dag, 210 dagen terugverdientijd RX6600 XT ? RX5700 XT €2.97/dag RX5700 €2.61/dag, 128 dagen terugverdientijd RX5600 XT €1.71/dag, 178 dagen terugverdientijd Als je gaat minen zou ik de 5700 pakken. Bij Nanopool moet je nu minimaal 0.2 ETH hebben voordat ze het overmaken. Dat was tot vorige week 0.05 ETH. Daardoor moet ik nu met 6x 1060 2 maanden minen in plaats van 2 weken. Ik ga dus een andere pool zoeken maar misschien stop ik ook wel."
RX 6600 XT;5;0.46002694964408875;Het is misschien wel de beste mining kaart. Je moet hem alleen wel zien te krijgen.
RX 6600 XT;5;0.31609639525413513;En nu ontdekt iemand dat het ideale mining kaart is. Beter dan RTX3060, meer efficiency in mining. Ik verwacht straks out of stock meldingen nu de nieuws eruit is. Maar aan andere kant: laat de gamers kans hebben om kaart te kopen. Als iemand vraagt of kaart goed is voor mining, ga ik negatief voten. We hebben er genoeg van, want dankzij deze miners zijn veel kaarten uitverkocht en kunnen we, als gewone gamers, geen kans maken om kaart te kopen. Je wordt bedankt. Dit moet eens ophouden met al die vragen over mining. We zijn er gewoon zat van.
RX 6600 XT;5;0.578819215297699;Een erg fijne kaart om mee te minen. Zeer efficient, misschien wel de beste. Gamers hoeven zich dus niet druk te maken. Voor hun is deze kaart waarschijnlijk niet te koop.
RX 6600 XT;1;0.5307573080062866;Ik denk dat die duren prijzen vooral komen omdat men is omdat men bitcoins is gaan minen. Er is voor de gemiddelde gebruiken geen goeie middenmoot kaart beschikbaar voor soort van normale prijs. Of je koopt hele goedkope kaart die niks kan of je valt gewoon buiten de boot voor 1000 euro of nog gekker.
RX 6600 XT;1;0.5196132063865662;Geen enkele kaart op msrp, een voor je het zegt dat het msrp + 21% belasting is, de 6700 xt is ongeveer 500€ op amd.com inclusief 21% belasting.
RX 6600;1;0.6031104922294617;Als ik het goed begrijp hoor ik de recensent in de video zeggen dat dit chips zijn die normaal in een duurdere videokaart zitten, maar gedeeltelijk defect zijn en dus als minder goede kaart / goedkopere kaart worden verkocht. Verkopen ze dan niet gewoon een defect product? Gaat een kaart als deze dan niet eerder stuk, of kun je last krijgen van issues met zo'n chip die niet helemaal lekker is.
RX 6600;1;0.47668007016181946;Op al je vragen is het antwoord: Nee In de chip zitten allemaal losse cores, en die cores zijn individueel aan/uit te zetten (door AMD) en zodoende worden alleen cores aangezet die gewoon helemaal in orde zijn (dat kunnen ze namelijk per core doormeten). Je moet het gewoon zien als een bandenfabriek waar 1 op de 10 banden defect zijn, volgens jouw redenatie zouden dan alle 10 de banden defect zijn omdat er 1 band daadwerkelijk defect is. Maar nee, je haalt de defecte band er tussen uit (en dat doen ze niet fysiek bij cores/chips want dat kan dan weer niet maar ze snijden de verbindingen wel door met een laser) en je houdt dus 9 perfect werkende banden over die je gewoon kan verkopen als goed werkende banden In de chips zelf vindt ook (bij normaal gebruik) geen slijtage plaatst dus die chips die al goed werkte gaan niet slechter werken omdat cores ernaast uit staan. Het is dan ook geen defect als het als RX6600 want het voldoet dus aan de specificaties daarvan... PS dit is ook niks nieuws dit gebeurd al decennia lang bij alle soorten chips, hierdoor is er dus meer bruikbaar en dat zorgt ook dat de prijs lager blijft dan dat ze dit niet toe kunnen passen.
RX 6600;3;0.5089111328125;Bedankt voor je uitgebreide reactie en opheldering. Jammer dat dit gemarkeerd wordt als off topic. Het gaat tenslotte over iets wat in de video wordt genoemd.
RX 6600;3;0.30522769689559937;Hier en daar is het daarom soms mogelijk om dmv een bios mod uit een hogere klasse de lagere klasse te flashen. Want het kan nl zo zijn dat in dit geval de XL chip gewoon prima is (dus geen geen kapotte shaders) waarbij het dus mogelijk een XT versie te maken.. Vaak worden de verschillende custom varianten besproken waarin wellicht een firmware upgrade afdoende is.. Dus houdt het nieuws in de gaten zou ik zeggen.. Ter illustratie. Ik had vroegah een AMD HD6950 die ik via een bios mod naar een HD6970 geflashed had.
RX 6600;1;0.5128756165504456;Er was een tijd dat ze de cores softwarematig uitschakelden omdat ze toen niet genoeg defecte Fury X kaarten hadden en die kon je ook weer inschakelen met een beetje moeite en daardoor had je in feite een Fury X in plaats van non-X of bijna. Daarna zijn ze cores express kapot gaan maken zodat men niet gewoon maar de goedkopere kaart kocht en unlockte
RX 6600;1;0.5343756079673767;Dit is een normaal onderdeel van chips die verkocht worden. In elke chip zitten imperfecties, in sommige chips zitten zoveel imperfecties dat een deel niet meer functioneert. Dat zijn dan CU of cores. Deze kunnen in de software uitgeschakeld worden en alle werkende delen kunnen dan gewoon gebruikt worden. Het is niet mogelijk om het stukje eruit te zagen of te repareren dus is uitschakelen de oplossing. Daar heeft de chip verder ook helemaal geen last van.
RX 6600;3;0.37839704751968384;Ik heb niet de technische kennis om dit als een feit neer te zetten, maar ik kan me zelfs voorstellen dat je vanwege de defecte cores wellicht de wél werkende cores beter kunt laten presteren (overclocken) omdat er minder warmte vrij komt. Zal uiteraard GPU-specifiek zijn of dit wel of niet zo werkt.
RX 6600;2;0.371351420879364;Volgens mij gebeurt zoiets in de AMD CPUs Ryzen 5900X en 5950X. De 5900X heeft 2 chiplets met elk 6 cores (dus twee disabled), en de luxere 5950X heeft 2 chiplets met elk 8 cores (de max). De advies-kloksnelheid is voor de 5900X 3.7 GHz, en voor de 5950X slechts 3.4GHz. In totale rekenkracht is de duurdere 5950X dus wel sterker, maar op single core performance is de goedkopere 5900X sneller. Deze chips hebben overigens een gelijkwaardige energieproductie, volgens AMD.
RX 6600;1;0.5027499794960022;Dit verhaal geld eigenlijk voor bijna alle kaarten, Navi21 (De chip in de 6700xt, 6800xt en 6900xt) heeft 80 compute units. De enige kaart die deze 80 allemaal aan heeft staan is de 6900xt. Hiervoor hebben ze dus een chip nodig waarin daadwerkelijk alles werkt. De 6800xt heeft 72 compute units, dat betekend dat er 8 zijn uitgeschakeld. Dat kan heel goed komen doordat er bijvoorbeeld 2 compute units defect zijn, die schakelen ze dan uit waardoor ze het product niet kunnen verkopen als 6900xt, dan maar een 6800xt. Niet alle 6800xt's zullen defecte compute units hebben, een deel van de verdeling van chips is natuurlijk ook segmentatie, er is nou eenmaal meer vraag naar een 6800xt in verhouding met de 6900xt.
RX 6600;2;0.6104208827018738;Ik vind de RX 6600 nog veel te duur om nieuw te kopen. Ik wacht wel tot de laagste mid-range videokaarten weer rond de 300 euro kosten meer vind ik ze ook niet waard.
RX 6600;1;0.5103315114974976;Sorry maar dit noem je een mid-range kaart? Een mid-range kaart is wat mij betreft een RTX3060-Ti of een RTX 3070. Nu positioneert Nvidia de RTX 3070 niet als mid-range kaart om zo meer geld te kunnen vragen, maar realistisch gezien is het dat wel en is het dat ook altijd geweest. Deze kaart had niet meer mogen kosten dan €260,- als adviesprijs. Ik denk dat mensen beter af zijn met een (gebruikte) RTX 2070 dan deze kaart, vind het zelf jammer dat deze niet in het overzicht te zien is.
RX 6600;3;0.3520118296146393;Ik vrees dat je niet meer bij de tijd bent en te lang in the 'good old days' periode blijft hangen. Fabrikanten zijn de positionering van hun kaarten aan het verschuiven. Kijkende naar de huidige tekorten hebben zij het perfecte moment gevonden om dat re doen en om eerlijk re zijn neem ik het ze ook niet kwalijk. Je kunt dan wel aangeven dat je zo lang mogelijk met je huidige kaart blijft werken maar op gegeven moment zul je aan de max van je kaart oplopen.
RX 6600;2;0.37305909395217896;Hopelijk kan de concurrentie van Intel dat voorkomen. Het zou fijn zijn als er een prijsstrijd kwam. Natuurlijk niet mogelijk zolang er tekorten zijn.
RX 6600;1;0.45118847489356995;De huidige prijzen hebben niets te maken met bij de tijd zijn, AMD en Nvidia maken graag gebruik (misbruik) van de huidige tekorten. De meest populaire kaarten zijn altijd de mid-range tot low-range kaarten. Echter bestaan er op dit moment geen enkele sub €300,- kaarten, wat betekend dat low-range er helemaal niet is. De enige reden dat men dit kan doen is omdat er een gebrek aan concurrentie is en er extreem veel vraag is naar nieuwe videokaarten.
RX 6600;2;0.37143373489379883;Het heeft nog steeds te maken met bij de tijd zijn. Prijzen gaan omhoog wat elke fabrikant graag wilt doen alleen was het wachten op de juiste samenloop van omstandigheden. Hoe je het went en/of keert...het aanschaffen van een aparte videokaart is een luxe...het is geen basisbehoefte. Het bouwen van een eigen pc is een luxe anders had je net zo goed voor een pc kunnen gaan waar er een iGPU aan boord zit maar door de zelfbouw gooi je jezelf in een markt waar er meer geld aan verdiend kan worden dus moeten wij er niet raar van opkijken als in dat segment de prijzen gaan stijgen.
RX 6600;2;0.6175971627235413;De low range is nu IGP (APU's in AMD termen). De price/performance van een low-end discrete GPU is te slecht om de concurrentie aan te gaan. Je hebt teveel kosten aan de productie, aan de printplaat, het losse GDDR, de PCIe controller, etcetera. Voor minder geld heb je dezelfde GPU cores op het silicium van je CPU.
RX 6600;1;0.4669380187988281;"Ik heb zelf een GTX1080 en eerlijk gezegd vind ik 260 euro voor een spel dat Cyberpunk op bijna max settings kan spelen wel erg goedkoop klinken. Naar mijn mening worden er geen low-endkaarten meer gemaakt, en waarom zouden ze ook; iedereen die chips kan bakken print praktisch geld, dan kun je net zo goed dure chips bakken. De 30-serie van Nvidia heeft volgens mij enkele kaart die de plaats van de GT1030 bij introductie zou innemen. De goedkopere, slechtere GPU's van vandaag zijn eigenlijk allemaal midrange tot topmodellen van de vorige generaties."
RX 6600;2;0.47370994091033936;Je vergeet daarbij dat ontwikkelaars natuurlijk een zo groot mogelijke afzetmarkt wil hebben. Al bovenstaande games afgezien van Cyberpunk 2077 zijn al meer dan een jaar oud en zijn ontwikkeld voor voornamelijk de oude gen (consoles) in gedachten. De echte next-gen games moeten nog komen, maar die zullen steeds langer op zich laten wachten omdat nieuwe hardware zo slecht beschikbaar is. Videokaart fabrikanten gaan wel door met de doorontwikkeling van hun kaarten, afgezien van hoe de kaarten beschikbaar zijn. Misschien dat ze de release data iets gaan verschuiven, maar ze kunnen niet stil blijven hangen in de huidige techniek. Vergeet niet dat de adviesprijs die ik noem een prijs is voor de reference kaart van AMD, 3rd party kaarten zullen dan eerder rond de €300,- liggen.
RX 6600;1;0.6482704281806946;Cyberpunk is nou net een spel dat compleet niet werkt op de vorige generaties. Het hele spel is nota bene uit onlinewinkels gehaald en de versie die op de huidige generatie consoles te krijgen is, is de versie die origineel op de vorige, veel tragere consoles had moeten werken. De wet van Moore is praktisch dood en nog weten fabrikanten enorme rekenkracht uit nieuwe generaties te trekken, dat vind ik heel knap. Ik had zelf nooit de verwachting dat de MSRP van de 30-serie en haar concurrenten ooit in de praktijk gehaald had kunnen worden, en voor een kort moment leek het erop dat ze het nog voor elkaar konden krijgen ook (voor de transportindustrie naar de tyfus ging).
RX 6600;1;0.606770396232605;"De game was natuurlijk technisch gezien een grote ramp, maar de game is niet ontwikkeld met de nieuwe consoles in gedachten. Dat is ook onmogelijk gezien de game bijna 9 jaar lang in ontwikkeling is geweest. Ik heb bij release een GTX 970 aangeschaft voor €365,- dat was naast de GTX 980 de beste kaart die beschikbaar was. Dit was geen Reference kaart maar een Asus OC variant die toch al snel een stuk duurder is. Met de inflatie van de afgelopen jaren zou deze kaart €398,68 kosten. Ja ze hebben hogere ontwikkelingskosten en ook de prijzen van grondstoffen zijn gestegen, maar daarvoor heeft men tegenwoordig ook een veel grotere afzetmarkt. Zelfs zo groot dat men massale tekorten heeft en dat is niet alleen maar door productieproblemen. Al met kost een RTX 3070 (reference) €519,- wat voor een 3rd party kaart rond de €549,- zou zijn. Dat betekend dat men €184,- meer vraagt voor dezelfde ""soort"" kaart. Hoe kun je zeggen dat de prijzen tegenwoordig goedkoop zijn. Blijkbaar doen de marketing mensen iets heel goed, dat ze inderdaad mensen zover krijgen om te denken dat de prijzen van grafische kaarten (afgezien van de huidige actuele prijzen) goedkoop zijn voor de prestaties."
RX 6600;5;0.6536127328872681;Nvidia en AMD hebben jaarlijks ook gewoon record winsten behaald...
RX 6600;2;0.5603727698326111;De 580GTX was bij release zon 400 euro. Dat je 4 jaar later dezelfde prestaties kon kopen voor de helft was volstrekt normaal. Het is grotendeels chiptekorten wat de prijs opdrijft. Toegenomen prestatie niveau heeft er weinig mee te maken.
RX 6600;2;0.5102649331092834;Ik denk dat mensen beter af zijn met een (gebruikte) RTX 2070 dan deze kaart, vind het zelf jammer dat deze niet in het overzicht te zien is. Ooh ik snap dat wel, want het is voor de tweakers redactie niet interessant. Het is dan ineens zichtbaar dat je beter voor een 2e hands product zoals de 2070, lees goedkoper kunt gaan dan voor een nieuw product zoals dit.. Ik mis tevens de 2080 in het rijtje...
RX 6600;3;0.2868247628211975;Dan kun je nog jaren gaan wachten. Dit zal nog jaren doorgaan.
RX 6600;1;0.5536789894104004;Dan koop ik gewoon jaren geen videokaart meer Ik las ergens dat de tekorten tot begin 2022 aanhouden, ik hoop dan dat ergens in 2022 de prijzen weer verlagen.
RX 6600;5;0.4724687933921814;Dat is een zeer optimistische schatting. Ik wil je adviseren om rekening te houden met eind 2023. Daarnaast wil ik je meegeven dat je direct van AMD en partners van nVidia 'gewoon' kaarten voor MSRP kan kopen.
RX 6600;1;0.2425244152545929;Bijv. bij AMD is alles out-of-stock:
RX 6600;5;0.4478103220462799;Elke donderdag om 16:00 is het feest bij AMD.
RX 6600;2;0.367209792137146;Dat is zeer waarschijnlijk inderdaad, en ook de reden dat ik een jaar geleden een RX 5700 XT heb gekocht om mijn RX 480 8GB te vervangen. De schaarste was toen in zicht (voorspellingen uit verschillende richtingen) maar de prijzen nog normaal, daarna schoot het keihard de lucht in... Dat het wel eens 3 jaar kon gaan duren was toen echter nog geen pijl op te trekken, maar nu wel.
RX 6600;1;0.3234272301197052;Voordat dit begon, heb ik nog een 1660 Super voor 90! Euro opgepikt, kan je je dat nu nog voorstellen?
RX 6600;1;0.6301897168159485;Nauwelijks... Zo'n '300 en een paar tientjes' voor een nieuwe RX 5700 XT ook trouwens.
RX 6600;5;0.3037058413028717;GTX 1080 voor 250 gekocht op de valreep
RX 6600;4;0.2846148908138275;
RX 6600;5;0.32244256138801575;RTX 2080 bij release voor 666,- via Amazon
RX 6600;1;0.47933638095855713;De technieken zijn vooruit gegaan, alleen voor die 666 euro koop je geen kaart die net zo snel is dan de 2080... (2e hands buiten beschouwing gelaten).. Dus is de techniek dan wel echt vooruit gegaan??? Ik zie de prijzen niet meer zakken btw, dit is het nieuwe normaal..
RX 6600;5;0.7437421083450317;Ik ben ook zo blij dat ik nog net een nieuwe build heb gemaakt toen de RX5700 XT uit kwam. De Sapphire Nitro+ heb ik toen voor rond de 500€ gehaald, nu bekeken de beste investering tot nu toe
RX 6600;2;0.400214821100235;Zo is het. Kwestie van vraag en aanbod. Heel weinig aanbod nu, prijs hoog. En dan mensen die daar misbruik van maken om er rijk van te worden. Als iedereen jouw instelling heeft, hebben die lui die de prijs extra opdrijven geen reden meer. En de prijzen blijven op iets hoger niveau dan normaal vanwege de tekorten. Ja als iedereen een run op de componenten gaat doen wanneer het weer leverbaar wordt, duurt de tijd nog een wat langer, maar daarna normaliseert de boel weer. scripts en al die andere poppenkasten uithalen voor een component tegen een hogere prijs dan wat het waard is, vind ik waanzin. Lekker in de (web)winkel laten liggen.
RX 6600;3;0.34590473771095276;Heb je toevallig nog het artikel waar je dat in gelezen hebt? Ik zou het graag ook willen lezen.
RX 6600;5;0.2671583294868469;Zoeken naar tekorten op tweakjes en je vindt een hoop, hier iets van Intel, Micron, AMD
RX 6600;5;0.37277162075042725;Inderdaad succes met wachten
RX 6600;1;0.3835403025150299;Je stuurt je berichtje naar de verkeerde .
RX 6600;2;0.31992107629776;Wacht gewoon totdat RX7000 serie komt en zorg dat je er op tijd bij bent, naar verwachting zullen eerste gegevenssets beschikbaar komen Q4 2021, Q1 2022. Waarbij de prijscategorie van RX6800/6900 op zelfde niveau komt als een RX 7700 (ongeveer 450-500 adviesprijs). RX 7800/7900 en de XT modellen zullen oplopen startend vanaf 550-1500?
RX 6600;1;0.43817055225372314;"Punt is dat het ""op tijd bij zijn"" nu ook relatief is. Alles wordt opgekocht door bots. Dus een product kunnen halen voor de MSRP prijs is bijna utopie geworden."
RX 6600;5;0.5064317584037781;Je kunt voor een zacht prijsje een Bot inschakelen die dit voor je doet. Je kunt je ook aanmelden bij Telegram channels, discord groepen, whatsapp groepen, slack channels. Er zijn veel mogelijkheden tegenwoordig! Je zou zelfs als je zo ver wilt gaan zelf een scriptje laten draaien (die een ander al gemaakt heeft) die continue scant op voorraad bij groot aantal webwinkels.
RX 6600;3;0.5112406015396118;Al gedaan, dan nog is het vrij lastig om er één te bemachtigen. Meestal is zo'n voorraad binnen 10 seconden opgekocht.
RX 6600;3;0.40853258967399597;En toch lukt het iedere week genoeg mensen. Kwestie van je aandacht op de juiste stappen richten en volhouden, dan lukt het vanzelf een keer.
RX 6600;1;0.844408392906189;Het is complete debiliteit dat er vanwege miners om videokaarten gevochten moet worden en de prijzen absurd geworden zijn. Een mid-range kaart als de GTX 1070 kostte 5 jaar terug €450 euro, en zelfs dat was al aan de wat hoge kant. Zelfs met de inflatie erbij zou een GTX 3060, die een stap lager ligt, goedkoper moeten zijn dan €450. Daarnaast probeert 3/4de van de wereld een klimaatprobleem op te lossen, mede door het terugdringen van energieverbruik, en hier hebben we mensen die zonder blikken of blozen dag en nacht een systeem laten draaien dat verscheidene duizenden watts stroom gebruikt, om niet-bestaand geld te minen. Wat mij betreft mag de hele crypto-valutamarkt simpelweg verboden worden. Hetgeen mensen aan het doen zijn is het maken van hun eigen geld, in de hoop dat het ooit verkoopbaar c.q. inzetbaar wordt op de plek van echt geld, en het kost duizenden watts per systeem per uur aan energie, en drijft de prijzen op voor mensen die hardware op een normale manier willen gebruiken. Het is van de zotte dat ik >= 650 euro zou moeten uigeven voor een midrange-kaart om een beetje op een normale manier een nieuw spel te kunnen spelen. Nog niet heel lang geleden kon je voor dat bedrag een cmplete PC kopen om een nieuw spel te spelen.
RX 6600;1;0.3836457431316376;"Speciaal voor 'ons' is de volgende video gemaakt: Natuurlijk is het satirisch gebracht al zit er een grote waarheid in verscholen. Crypto brengt ons niet wat het belooft. De belofte is prachtig maar meer dan een beleggingsproduct is het niet. Te volatiel om mee te betalen, te ondoorgrondelijk om er iets mee te doen. Het kost bakken energie om te 'vervaardigen' in een tijd waar landen alles op alles zetten om genoeg energie te kunnen genereren. De inkoop van grondstoffen is niet alleen duur, brandstof is schaars op het moment. Miners minen lekker door terwijl de energie die zij verbruiken op andere plaatsen harder nodig is. Dit is een beleggingsproduct dat populair is terwijl het ernergie opschrokt die elders nodig is. Wat begon in China ""Koop kolen in tegen elke prijs"" zie je nu over de rest van de wereld gebeuren. China werpt niet voor niets steeds hogere barriëres op tegen Cryptogeld. Ze laten het lijken alsof te maken heeft met het feit dat het speculatief is. Als je iets dichterbij kijkt dan kom je erachter dat het komt omdat China energie wil besparen omdat er grote tekorten zijn. Cracked mag het dan een soort van lollig brengen, deze video is de eerste waarvan de satire niet alleen satire is, er zit zowaar een duidelijke boodschap in. Een boodschap waar ik het helemaal mee eens ben."
RX 6600;1;0.7707068920135498;Een groot deel van die problemen wordt opgelost met proof of stake. En gewoon geld is ook behoorlijk ondoorgrondelijk. Zo krijgen banken gratis geld van de centrale bank als ze reserves aanhouden, met een factor van minimaal tien. Dus als ze 1 miljoen toevoegen aan hun reserve, krijgen ze minimaal 10 miljoen om uit te lenen. Als mensen een deel van dat geleende geld weer stallen bij de bank, kan die dus weer meer geld krijgen van de centrale bank, etc. Als mensen een keer massaal hun geld gaan opnemen kan zo het hele systeem eenvoudig instorten, want dan kunnen banken minder uitlenen (en een flink deel van de leningen die worden afgelost worden direct weer omgezet in nieuwe leningen, dus krijgen veel bedrijven dan kredietproblemen). Omdat men geen leningen meer kan krijgen is men dan gedwongen om geld op te nemen van de bank, waardoor die bank minder reserves heeft en nog minder kan uitlenen. Vervolgens zijn nog meer mensen gedwongen om hun geld weg te halen bij de bank, waardoor die weer minder geld heeft om uit te lenen, etc.
RX 6600;2;0.3278602361679077;Ff advocaat van de duivel spelen, de miners zijn het probleem niet, het zijn de kopers van de cryptos die zorgen dat mining interessant is. Zij denken, nog erger, bepalen dat gebakken lucht wat waard is. De miners voorzien slechts in die vraag.
RX 6600;1;0.5497827529907227;"een stapje is een understatement. in de praktijk is de 3060 een instapmodel, geen midden ranger. dat is de 3070. ik zie op mindfactor de prijs van de 3060 as 499 euro... en dat zal 9 / 10 kans nog meer omhoog schieten. het probleem is dat die dit te laat is. in china is het verboden geworden. buiten een kleine dip, is de prijs van de crypto zelf nog hoger dan ervoren. en ja, het is een verspilling van energie dat gpu ' s staan te mijnen voor iets dat eigenlijk lucht is. maar ik kan hetzelfde zeggen van pc ' s dat 70 a 100w in idle gebruiken, gewoon omdat : * amd hun kaarten niet deftig naar beneden clocken met bepaalde frequenties of resoluties of multi monitor en die 6w idle effen naar 30 a 40w duwen. en de meeste mensen hebben geen idee dat dit gebeurt. * amd chipsets zoals de x570 uitbrengt dat 7w gebruiken voor eigenlijk niets te doen. * amd weeral met hun io op de cpu dat constant 17 a 21w zuipt. feit is dat men het kan om enorme efficiente pc ' s te maken maar gewoon niet wilt voor de verkoop cijfers. je moet te veel als klant weten hoe je een efficient systeem kunt bouwen ipv dat het standaard is en je uit je weg moet gaan voor een power systeem. zet een watt counter op iedere desktop en je zal zien dat de industrie onder druk zal komen als mensen hun idle enz gebruik zien. we moeten van die eco labels echt hebben voor de pc industrie. a, b, c... verbruik in idle, load, enz. waar is een wil, is vaak een weg maar zo lang er geen druk is blijven we gewoon verdergaan met een eigenlijk 1960 design. en ja, laptops en tablets zijn zuiniger maar daar zit je met gans de "" wegwerp product "" gedoe. dit is een stap in de goede richting... maar dat het moet komen van een kleine indi firma, zegt genoeg over de grote jongens. note : kijk hoe lang het zal duren eer een bedrijf zoals dell.. dat opkoopt en de boel zal laten sterven."
RX 6600;3;0.24372877180576324;Ik heb eind maart dit jaar op de dag van de launch een RX 6700XT weten te bemachtigen. Ik heb er toen wel 1100 euro voor betaald maar bedacht mij achteraf dat het gelijk is aan hoeveel mijn nas pc heeft gekost en de helft van de prijs van mijn desktop pc die ik toen 1,5 jaar geleden gekocht had. Ik ben blij dat ik 'm heb gehaald en heb geen spijt maar het blijft toch veel geld voor mid-range videokaart.
RX 6600;1;0.6593706011772156;En de miners zeggen: Complete zotheid dat mensen met zo'n videokaart spelletjes zitten te doen (en net zoveel energie verbruiken per tijdseenheid) en daar geen eens geld voor krijgen.... Alsof andere produkten niet door (gefingeerde?) schaarste, niet duurder worden? Zoals xbox/ps5 consoles. Maar zelfs 2e hands produkten kunnen opeens belachelijk kostbaar worden. Allemaal door schaarste en gewildheid. En dankzij een wat vreemd verdeelde rijkdom in de wereld kunnen sommigen dat ook nog betalen (en doen dat ook). Wat de prijzen verder opdrijft. En verder geeft het een gevoel van: je moet nu wel instappen/kopen want over een tijdje is de prijs helemaal hoog => nog meer schaarste ondanks hogere prijzen. Want dat telt vast wel mee: zelfs mensen die normaal geen 500 euro zouden uitgeven voor een video-kaart, kopen dat nu maar wel ook al hebben ze 'm nu niet echt nodig en is het eigenlijk niet hun idee van value-for-money. Dat zorgt voor weer een toename van kopers. Een gok op niet meer dalende prijzen. Feitelijk hebben we ook te maken met verschillende effecten tegelijkertijd die elkaar allemaal versterken:chip tekortenminersiets van spaargeld overschot dankzij minder vakantie/uitgaan (vanwege Corona)overgang van HD naar UHD (dus noodzaak voor snellere hardware)scalpers (want de prijzen stijgen toch wel...)fear of missing out (dus koopt men nu toch maar een kaart)
RX 6600;2;0.45771360397338867;"gelukkig heb ik een goed werkende gtx 1070. die is ondertussen 5 jaar oud. echter, ik speel mijn spellen op 1920x1200, en dat blijft nog wel even zo. ( vanwege een zichtbeperking die ervoor zorgt dat ik niet zomaar naar een 27 - 32 inch monitor met een hogere resolutie kan overstappen, en dus wacht op een eizo van 24 inch met iets van 2880x1800 of 3840x2400, voor een scaling van 1. 5 of 2. ) tel daarbij op dat ik weinig spellen speel, en mijn nieuwste spel the witcher 3 ( en dvinity original sin 2 ) is, en je snapt dat ik ( gelukkig ) niet zit te springen om een videokaart. of zelfs maar een upgrade van mijn i7 - 6700k. ik wacht lekker tot amd uitkomt met zen4 / am5 in 2022. dan zal ik zien wat intel ertegenover zet, want ik wil voor mijn volgende systeem minstens 16 snelle cores. ik ga ervan uit dat dit niet eerder is dan oktober volgend jaar. ) dus ik zie me niet eerder overstappen op een ander systeem dan augustus / september 2023 ; dan zie ik op dat moment wel verder. als graphische kaarten dan nog steeds belachelijk duur zijn, dan zet ik een of andere tweedehands kaart in mijn i7 - 6700k ( die ik dan voor een habbekrats verkoop met enkel een 256 gb ssd er nog in ), en dan gaat mijn 7 jaar oude gtx 1070 uit 2016 zonder blikken of blozen in die nieuwe computer uit 2023. ik blijf die kaart gewoon gebruiken zolang die in een nieuw systeem werkt, en ik niet tegen een spel op loop dat ik wel wil spelen, maar er niet meer fatsoenlijk op draait. mocht die kaart onverhoopt kapot gaan, dan ben ik natuurljik wel de sjaak ; maar ik heb nog nooit een kapotte grafische kaart gehad. al mijn defecte hardware in 25 jaar computergebruik was 1 hdd, 1 ssd, 1 geheugenchip, en 1 moederbord ( maar die laatste was wel heel erg gaar, want dat was een dual - xeon bord, net 4 maanden buiten de garantie van 3 jaar )."
RX 6600;1;0.6520684361457825;"Waarom een downmod?? Want dat is het oorspronkelijke idee - valuta onafhankelijk van overheden die er een spelletje mee doen. Nu lijkt de discussie zich te stigmatiseren naar ""Stoute miners stelen onze videokaarten en verspillen energie en daarmee werken ze mee aan een milieu ramp"""
RX 6600;1;0.6394540071487427;Ik zelf probeer al maanden een rtx 3080 founders editie te bemachtigen maar dat is vrijwel onmogelijk. Zelfs als ik de wachtrij omzeilt op notebookbillinger ben ik altijd te laat.. Het zijn voornamelijk mensen die gepersonaliseerd scripts hebben die iedere keer raak schieten. De scalpers verdienen simpelweg te veel aan waardoor zelfs de kopers die discord en telegram groepen volgen ook voornamelijk misgrijpen. Misschien dat het bij AMD wel lukt, Nvidia is toch stuk lastiger
RX 6600;3;0.43395841121673584;Maar dan ga je toch AMD proberen? Ik snap dat je graag een 3080 wil, maar die zijn inderdaad onmogelijk om te krijgen. Een 6800XT is ook niet makkelijk, maar de 6900XT is redelijk te doen en niet veel duurder dan een 3080.
RX 6600;2;0.4649434983730316;Ik zelf ben gegaan voor de AMD Radeon RX 6800, wou toen ze uitkwamen een AMD Radeon RX 6800 XT maar kon er nooit aan eentje komen, en de prijzen zijn absurd, en dat viel met mijn AMD Radeon RX 6800 redelijk mee, ieder geval ver onder de €1000, vind het erg jammer dat Tweakers niet de GeForce GTX 1080 mee genomen hadden, dat was de meest gekochte high end grafische kaart 5 jaar geleden.
RX 6600;3;0.4479864537715912;Tja, da's dan ook het probleem met die 1080, hoe lang wordt die nog ondersteund? Mijn Fury X is maar een beetje ouder en daarvan is de ondersteuning al gestaakt. Lekker moment wel, ik kan dus geen BF2042 spelen tot ik aan een recentere kaart kan komen. Ik weet niet of een 1080 dan überhaupt nog het overwegen waard is, zeker gezien de prijzen die ze er nu voor vragen (rond de 500 euro is bizar voor een kaart van die leeftijd).
RX 6600;1;0.5103257298469543;Ja dat is heel veel geld voor een ruim 5 jaar oude grafische kaart, ik heb mijn GeForce GTX 1080 verkocht voor €350, bijna de helft van de prijs wat ik er voor betaald had toen ruim 5 jaar terug, maar BF2042 moet het met gemak doen op 1080p met een GeForce GTX 1080, zou heel vreemd wezen als hij niet op die resolutie minimal 60fps kan halen op 1080p, als de GeForce GTX 1070 alle nieuwste spellen op Ultra op 50+ fps kan spelen, behalve Cyberpunk 2077 met 31fps (maar dat is een van de slechtste geoptimaliseerde spel die er is, in ieder geval van de AAA spellen), en Red Dead Redemption 2 die max 43fps haal, maar dat is allemaal op de hoogste (Ultra) settings, als je het op High zou zetten zou de fps enorm omhoog schieten.
RX 6600;1;0.7670558094978333;"Dit heeft niets met ""aandacht"" te maken. Het is gewoon te belachelijk voor woorden dat je op elke voorraad, waarin de GPU voor een MSRP prijs wordt aangeboden, je maar een aantal seconden hebt om de bestelling te plaatsen, omdat er mensen zijn die op de één of andere manier door wachtrij komen, en alles opkopen."
RX 6600;1;0.4701426029205322;"Het is toch absurd dat je door al die hoepels moet springen om een vrij normaal product te kunnen kopen? Ik vind het verre van normaal, je hebt er een dagtaak aan om een product te ""mogen"" kopen."
RX 6600;1;0.2913423776626587;Voor sommige plantjes bij de Aldi moet je om 08:00 al voor de deur staan.
RX 6600;3;0.4288202226161957;Maar er zijn genoeg plantjes te koop in andere winkels zonder daarvoor in de rij te hoeven staan, dus die analogie met videokaarten gaat niet echt op.
RX 6600;4;0.4745592474937439;Ook genoeg videokaarten, wel wat duurder.
RX 6600;2;0.47597044706344604;"De vraag is (veel) groter dan het aanbod, dus niet ""genoeg videokaarten"". ""Wat duurder""? Grapjas..."
RX 6600;1;0.54228675365448;Volgens mij is een plantje bij de intratuin ook gewoon 2x MSRP hoor
RX 6600;1;0.4989407956600189;Je bent ook niet verplicht om een nieuwe kaart te kopen.
RX 6600;1;0.4863628149032593;Ik ben het hier niet mee eens. Ik wil geen reclame maken voor het bedrijf maar bijvoorbeeld bij Alternate moet je gewoon een bestelling plaatsen op dag 1 via de website of als dat niet lukt via de telefoon. Je komt dan op een wachtlijst. Bij nieuwe releases is die wachtlijst dus nog niet zo groot hoewel die met een paar dagen natuurlijk ontploft zal zijn. Dus, niet wachten op benchmarks maar zoek een kaart die je wilt en op dag van de release bestellen. Anders zit vis je achter het net. En begrijp me niet verkeerd. Ik vind de situatie ook belachelijk zoals deze nu is maar er zijn plekken waar je niet zomaar met een bot kan bestellen op dag 1 van release waarmee alles opgekocht wordt. Ook omdat er maar 1 kaart per klant geleverd wordt. Dus hou de releasedatum in de gaten en zorg dat je je handen vrij hebt om je bestelling te plaatsen.
RX 6600;3;0.23309621214866638;Moet je er ook echt wel op tijd bij weten te zijn. Het zou je niets verbazen als het hele zooitje weer opnieuw begint. En de oudere kaarten die dan ook lekker weer boven de nieuwprijs worden verkocht, net zoals we nu zagen met onder andere de GTX16xx serie. Als je daarvan een stapel had ingekocht kon je de afgelopen anderhalf jaar dikke winst maken. Maar nu gaat dat dus ook gebeuren met de RTX3000 serie en de Rx 6000.
RX 6600;4;0.3869090974330902;Dat ga ik ook zeker doen
RX 6600;1;0.4569355249404907;Je kunt dan wachten tot je een ons weegt. De prijzen gaan voorlopig nog niet zakken. Er is gewoon niet genoeg productiecapaciteit.
RX 6600;2;0.43925240635871887;Ik ook, zoals Steve van GamersNexus ook zegt, dit is abnormaal. En ik vind dan ook niet dat je als media de indruk moet wekken dat dat het wel is.
RX 6600;1;0.5918685793876648;De 6600 is op dit moment net zo snel en net zo duur als mijn Vega 64 drie jaar geleden was. Of te wel de GPU markt heeft 3 jaar stilgestaan
RX 6600;5;0.5037012100219727;en de vega56 was een tijdje zelfs nieuw te krijgen voor rond de 200 euro, wat het nog grappiger maakt.
RX 6600;1;0.7732152342796326;De markt slaat echt helemaal nergens meer op, ik heb medelijden met de mensen die nu noodgedwongen in de markt zijn voor een nieuwe kaart. Zelf zou ik er principieel niet aan beginnen in ieder geval. Bijna twee jaar geleden heb ik mijn Vega64 (nieuw) gekocht voor zelfs maar €250, ik zou hem op dit moment voor het dubbele kunnen verkopen krijg ik het idee...
RX 6600;2;0.31402885913848877;Al een tijdje, pakweg 2 jaar geleden mijn game pc verkocht, zat GTX 1080 Ti in, ben een leek maar de prijzen zijn flink omhoog gegaan, maar is zo'n kaart als deze beetje qua kracht beter als de1080 Ti?
RX 6600;4;0.2846148908138275;
RX 6600;4;0.29654866456985474;Dat is niet de GTX 1080 Ti, maar de normale versie. De Ti versie zal vergelijkbaar of zelfs beter presteren dan de 6600.
RX 6600;3;0.4127584397792816;Ah dom linkje van hierboven gekopieerd, maargoed het idee is duidelijk het is prima uit te zoeken
RX 6600;1;0.3096710741519928;Die draait rondjes om deze 'next gen' 6600 ...
RX 6600;2;0.44241201877593994;Darn,shit dat die prijzen zo belachelijk zijn geworden, dacht na twee jaar dan zal het toch wel weer een stuk sneller zijn en hoopte ongeveer op dezelfde kwaliteit uit te komen. Komen er betere tijden aan of is dat glazen bol kijken?
RX 6600;1;0.49810096621513367;Ziet er uit dat bitcoin naar 100k gaat ergens begin 2022, maak je borst maar nat
RX 6600;2;0.47187188267707825;De prijzen zijn volstrekt ridicuul op dit moment en ik hoor al dat de verwachting is dat dit de komende 2 jaar nog wel eens zo kan blijven mede door de chip tekorten. We gaan het zien de komende jaren
RX 6600;5;0.34086835384368896;Nou lekker dan, hij is nu al 600 euro.
RX 6600;1;0.7557704448699951;Dit soort prijzen zijn zo belachelijk, dat zelfs de belachelijke prijs die ik in de coronapiek heb betaald voor een 5 jaar oude kaart het nog in verhouding waard is.
RX 6600;1;0.4267750382423401;al niet meer leverbaar
RX 6600;4;0.2605803906917572;579
RX 6600;3;0.30259978771209717;"Ik wou net vragen ""En hoe hard zal deze in prijs stijgen?"" Maar men is al druk bezig .."
RX 6600;3;0.36099329590797424;Weet je wat SU-PER handig zou zijn? Dat je ook een extra videokaart zelf kon toevoegen aan de benchmark scores. Vermoed dat voor de meeste titels wel resultaten zijn ergens in een database van bijvoorbeeld een GTX1080, ben zeer benieuwd hoe mijn kaart nog presteerd tegenover de nieuwere generaties. Een GTX1070 staat er wel tussen, dus kan al wel een beetje inbeelden waar mijn kaart zou staan.
RX 6600;3;0.38573768734931946;Dat kan al - niet bij de grafieken helaas, maar wel als je een vergelijking aanmaakt in de Pricewatch: We zetten 'm er niet zelf bij omdat hij niet is meegegaan in deze test, en de getoonde resultaten dus met een iets oudere driver zijn.
RX 6600;5;0.5764855742454529;Dat moet ik eens gaan bekijken! Thanks!
RX 6600;3;0.45420974493026733;Mag ik het wat gek vinden dat de RX580 in de vergelijking wordt meegenomen maar de Vega56/64 niet? Ik hoop alvast dat mijn Vega56 het nog even mag volhouden.
RX 6600;1;0.43666326999664307;"Volgens mij zijn er verhoudingsgewijs veeeel meer 580s verkocht dan vegas, en van een vega naar een ""budgetkaart"" gaan is ook geen voor de hand liggende stap Huidige marktchaos daargelaten zou deze kaart precies gemikt zijn op 1060/580 eigenaren die sneller willen"
RX 6600;3;0.5985618233680725;Ik vind FPS per euro wel een wat vreemde metric. Ergens is het goed om zo te vergelijken of de prijs relatief voor de prestaties gunstig is, maar ik denk dat iemand zelden een kaart koopt specifiek voor die metric. Je wilt een kaart die moderne spellen kan spelen, en afhankelijk van hoeveel je beschikbaar hebt kijk je wat op dat moment de beste kaart is. Als de kaart significant goedkoper is dan de RX 6600 XT maar een iets slechtere fps per euro dan kijk je toch vooral of het verschil in prijs de extra prestaties van de XT waard is. Hoeveel FPS krijg je erbij per EXTRA euro. Bij de fictieve prijs van 500 euro voor de RX 6600 geeft een RX 6600 XT van 619 euro je 0.099 fps per euro meer dan de normale RX 6600. Bij een prijs voor de RX 6600 van 550 euro geeft de RX 6600 XT voor 619 euro je 0.17 fps per euro die je meer betaald voor de XT. Ik verwacht dat een prijs tegen de 550 euro dan ook veel logischer zal zijn als de XT verder een gelijke prijs houd.
RX 6600;2;0.5010152459144592;Inderdaad. Als je het budget hebt voor een high-end kaart ga je niet ineens een midrange kaart als deze halen omdat ie betere bang for the buck bied. Ook zijn de prijzen momenteel zo instabiel dat deze vergelijking nu al niet meer opgaat.
RX 6600;2;0.3758343756198883;Prijzen van videokaarten volgen de koersontwikkeling van cryptomunten als Bitcoin en Ethereum. Zo lang het rendabel blijft om te minen zal de prijs van videokaarten hoog blijven. Afgelopen zomer, toen Bitcoin even flink lager stond, waren ook videokaarten opeens beter verkrijgbaar en goedkoper. Een tweedehands RTX 3080 bijvoorbeeld ging in een paar weken van €1.800 naar €1.200. Nu lijken de prijzen van deze kaarten weer naar €1.500 te stijgen. Als de koersen van cryptomunten dalen kunnen de prijzen van videokaarten wel eens erg hard omlaag gaan, maar of dat snel gaat gebeuren?
RX 6600;3;0.4224958121776581;Bitcoin zal EOY tussen de 75 en 110k zitten, dus verwacht een nog hogere stijging voor deze kaarten.
RX 6600;2;0.3742099106311798;Bitcoin wordt tegenwoordig gemined met ASICs, niet met videokaarten. Er wordt vooral Ethereum gemined met GPUs.
RX 6600;1;0.5779228806495667;Ik kon mijn 3 jaar oude miner (6x1060) nu ook volgens mij voor 2000€ verkopen, terwijl ik er destijds 2500€ voor heb betaald. Het is echt van de gekken! Het goede nieuws is dat Eth (waar 90% van de mining GPU’s voor worden ingezet) op termijn over gaat naar PoS en dat die videokaarten dan dus niet meer rendabel kunnen minen. Rond juli 2024 gaat de dag epoch size naar 6 gb en dan worden alle 6 GB GPU’s overbodig voor ethereum. September 2022 zal dit 5 gb passeren.
RX 6600;1;0.7915889024734497;Wat een prijzen al weer zeg, niet normaal. Ik heb mijn RX 6800 voor minder gekocht dan deze dingen nu kosten. Wat een onzinnig gedoe.
RX 6600;1;0.5739675164222717;Dit. Dit heeft er voor gezorgd dat ik nog steeds game op een amd 3700x + 32GB ram + 1TB PCI-E SSD op een amd RX 470 8GB game kaart 2 weken geleden de kaart helemaal schoongemaakt en voorzien van nieuwe koelingspasta. Ik weiger absurde prijzen te betalen voor een videokaart, ik game nog wel even verder op 1080p op mijn amd freesync monitor tot dat de prijzen weer wat normaler worden.
RX 6600;1;0.2692405879497528;ja ken het, radeon 5850.... was van plan na 2x overslaan nieuwe pc ook nieuwe videokaart en beeldscherm te geven
RX 6600;2;0.41507282853126526;Ik weiger ook.. maar zolang alles continue uitverkocht is, liggen ze daar bij amd en nvidia niet wakker van. En ergens kan je niet meer weigeren als de oude stuk gaat of niet meer ondersteund wordt.
RX 6600;2;0.4404798150062561;Dit jaar maar een laptop en een xbox s gekocht voor net 600 euro, na jaren met plezier op de bruutste game pc,s te hebben gespeeld en bouwen was ook leuk.Gaat dit me toch echt te ver voor het geld van een betere gpu had je eerder een complete game pc.
RX 6600;3;0.3149901330471039;next-gen video kaart > 1080P Wat? Dat is een last gen kaart, wat ook terug te zien is in de games selectie.
RX 6600;4;0.35015836358070374;Want de hele wereld gamed al op 4k? Prima next gen kaart met nieuwe technologie aan boord en over het algemeen dik boven de 60fps op 1080p ultra.
RX 6600;2;0.4643062949180603;Niets primas aan voor die prijs, 60 fps 1080p, lachwekkende situatie. Gpu van een xbox series s is nog sneller en dat hele ding is goedkoper dan deze gpu
RX 6600;1;0.46482405066490173;Dat was ook niet het punt. Ik ben het met je eens dat de prijs absurd is voor wat je krijgt. Neemt niet weg dat het wel een next gen kaart is.
RX 6600;2;0.4540077745914459;Ik ben het met je eens dat het een next gen kaart is, maar jij noemde het een prima next gen kaart, dat was het deel waar ik het niet mee eens was, want of die slecht/prima of goed is hangt in mijn ogen ook zeker af aan het prijs kaartje.
RX 6600;2;0.3585883677005768;"Ik zat na het zien van de figuur al op de bekende websites om te kijken of hij daar voor 500 euro te bestellen was. De indruk wordt via de figuur gewekt dat deze kaart een gunstige prijs/prestatie heeft terwijl dit een volledig fictief scenario betreft. Ik raad aan dit in de figuur zelf aan te geven. Er staat nu immers 'Prijs gebaseerd op goedkoopste uit voorraad leverbare videokaart op 13 oktober 2021.' Daar kan nog even bij dat de 6600 een fictieve prijs heeft meegekregen. Overigens lijkt het er sterk op dat veel gamers zoeken naar de goedkoopste kaart met acceptabele performance. Om dit even te duiden; afgelopen week vroeg de goedkoopste winkel 2100 euro voor een RTX 3090. Bij EVGA staat hij voor 1619 euro, dat komt neer op circa 480 euro verschil. Kijk je naar een RTX 3070 dan zie je EVGA 559 euro vragen terwijl een winkel 999 vraagt. Bij de 3060 is het 349 tegen 649, een verschil van 300 euro. Voor de goedkopere kaarten betaal je dus procentueel veel meer extra. Je zou een 3090 bijna een goede deal kunnen noemen . Op basis van bovenstaande verwacht ik dan ook niet dat het fictieve scenario uit de tabel gaat uitkomen. Immers als de RX 6600 beschikbaar is voor een gunstige prijs dan springt iedereen er bovenop en gaat de prijs vanzelf omhoog omdat er geen voorraad is."
RX 6600;1;0.4060019254684448;Voor de Belgen onder ons, Alternate.be heeft een Powercolor staan voor 419 euro: Voor zolang het duurt dan...
RX 6600;3;0.337185800075531;Alternate heeft mij een MSI 3060 Ti Ventus voor 449 geleverd (besteld 15-12-2020) maar lijkt me niet dat je daar de tabel op moet baseren. Die prijs is alleen voor wie de mazzel had binnen een paar minuten te bestellen.
RX 6600;1;0.8311138153076172;Dus, reference wise, zou de rx 6600 net zoveel kosten als een rx 5700 was, bijna in alles net zo snel zijn tit ietsje sneller. Dat is, vrij kansloos, je krijgt met msrp niet perse meer voor je geld, behalve rsy tracing support. Als deze kaart €280 was, dan was het al iets minder ernstig. Maar dit, is gewoon niet leuk meer. Slechte kaart.
RX 6600;1;0.5813199281692505;"Met andere kaarten is het niet leuker; allemaal slechte kaarten, of misschien: allemaal slechte prijzen."
RX 6600;1;0.6564188003540039;Sorry maar zelfs afgezien van de huidige prijsgekte vindt ik de laatste releases van grafische kaarten gewoonweg lachwekkend. Als ik kijk naar de game titels die gebruikt worden voor de review dan is er eigenlijk maar 1 echte titel die next-gen is en dat is cyberpunk 2077. Zelfs op 1080p en Ultra settings is die game niet eens met 60fps te spelen. Nu is alles boven de 30fps natuurlijk gewoon speelbaar op papier, maar in de realiteit is dat meestal niet zo gezien de framerates variabel zijn. Wat betekend dat 1440p op die game al niet mogelijk is. En dat met een adviesprijs van €356,- Nu zijn de consoles slecht beschikbaar, maar wanneer deze weer normaal beschikbaar zijn dan zijn gamers toch compleet gek wanneer ze een kaart als de 6600 kopen voor zelfs de adviesprijs. De meeste kaarten zullen bij een normale markt zelfs met adviesprijs geen enkel bestaansrecht hebben. Ik zie niet in waarom een gamer zou kiezen voor een grafische kaart in plaats van een console.
RX 6600;3;0.43833890557289124;Ik denk dat je niet zo veel aan een console hebt als je bepaalde game genres wilt spelen.
RX 6600;1;0.299064040184021;Waarom niet, welke games bedoel je dan, fps? Je kunt namelijk prima met muis en toetsenbord spelen op de nieuwe consoles.
RX 6600;1;0.5299219489097595;RTS games, Minecraft (java) zijn bijvoorbeeld populaire games. Ook niet alle FPS kan je zomaar naar console switchen met muis. Daarnaast alle games die je al hebt voor PC en niet opnieuw wil kopen. Ik heb zelf een 1070 tweedehands moeten kopen omdat mijn GPU stuk ging. Hoop geld maar ik wil wel mijn PC gebruiken. Ik heb wel een xbox SX waar ik andere games op kan spelen.
RX 6600;2;0.42779916524887085;Strategy, city builders en simulators zijn doorgaans alleen op PC/Mac beschikbaar. De grootste titels komen af en toe nog weleens naar consoles, maar ook vaak genoeg niet.
RX 6600;2;0.31876492500305176;Dat klopt inderdaad, al zijn dit tegenwoordig zo goed als een niche genres.
RX 6600;5;0.3848991096019745;Dat zal je verbazen. Als ik door de top 50 van Steam blader zie ik een behoorlijk aantal games in deze genres. Hearts of Iron IV heeft naar schatting meer omzet behaald op Steam dan Skyrim. Of wat dacht je van Euro Truck Simulator 2, op plaats 15? Dat is geen niche meer. Deze genres zijn enorm populair op de PC.
RX 6600;2;0.436352401971817;Ik denk dat als je de totale aantallen bekijkt van de games die tegenwoordig gespeeld worden dat er maar weinig mensen dat type games spelen. Nu zijn veel mensen door corona misschien ook begonnen met het spelen van games of men heeft het weer opnieuw opgepakt dus de aantallen zullen wel meer zijn als voorheen maar nog steeds kun je het volgens mij een niche noemen. Het is niet voor niets dat er maar weinig grote ontwikkelaars zijn die zich hier op richten, terwijl ze hele populaire franchise hiervan in bezit hebben.
RX 6600;2;0.39667990803718567;Uit die link: - Civilization VI - Hearts of Iron IV - Euro Truck Simulator 2 - Total War: WARHAMMER II - Europa Universalis IV Samen meer dan 100.000 actieve spelers. De data bevestigd jouw claim niet, die games zijn enorm populair. Ja, de grote multiplayer games zijn populairder, maar multiplayer heeft nogal de neiging om iedereen naar één game te trekken.
RX 6600;1;0.4330032467842102;De eerste 3 games hebben al meer dan ruim 1 miljoen spelers, wat dus een factor 10 is ten opzichte van de games die jij noemt. Reken daar alle overige games die niet in het genre vallen die jij noemt nog bij en het is minder dan 5 a 6%.
RX 6600;2;0.4769119322299957;Ik speel sommige games met mods zoal Citties Skylines, dat doe ik veel liever op een PC dan op een console. Maar ik betaal nu ook liever niet de hoofdprijs voor een videokaart die later flink in waarde zakt.
RX 6600;3;0.434286504983902;Daarnaast is het een stuk eenvoudiger een next gen console te halen voor adviesprijs. Met een beetje inzet lukt het prima om een PS5 te halen voor 500 euro. Ben dan ook best blij met die van mij, ook perfect voor PS4 spellen en de komende maanden komen er wel een paar leuke spellen bij. Als de videokaarten ooit weer normaal zijn geprijsd bouw ik wel weer eens een game pc, mijn huidige is al 10 jaar oud of zo.
RX 6600;1;0.3409990668296814;Wat is er zo next gen aan Cyberpunk dan?
RX 6600;3;0.6183047294616699;Eigenlijk niet zo heel veel, al ondersteund de game wel de laatste technieken. Maar je hebt gelijk dat de game eigenlijk nog ontwikkeld is met last gen in hun achterhoofd. Maar laten we eerlijk zijn op de oudere consoles is de game zo goed als onspeelbaar of de game is niet zo als het zou moeten zijn.
RX 6600;3;0.30456048250198364;Omdat je al een computer hebt? Dan is de meerprijs soms alleen die van de kaart de rest heb je al Spellen kosten vaak ook een stuk minder (maar kan je weer niet doorverkopen) dus daar kan je weer op besparen. Daarnaast vind ik achter een computer gamen veel fijner dan een console, al heb ik die ook (PS4) voor als ik vrienden over de vloer heb of met mijn vriendin wil gamen... Denk dat iedereen wel zelf kan bepalen wat ie fijn vindt (of niet fijn vindt) en welke prijs daartegenover moet staan. Dat zie je met ongeveer alles, de ene koopt huismerk hagelslag en de ander liever die van De Ruiter, dat hoef je ook aan niemand te verantwoorden maar om te stellen dat iedereen die De Ruiter hagelslag koopt gek is omdat ik tevreden ben met huismerk hagelslag zou ook raar zijn. Speel(de) voornamelijk maar 2 spellen, COD:MW en Rimworld. Moet/mag ik nu naar het gekkenhuis?
RX 6600;3;0.4074403941631317;Ik heb ook een computer, die is echter al 10 jaar oud. Alleen de grafische kaart aanschaffen zal dus weinig meerwaarde hebben. En ik weet zeker dat er veel mensen zijn die nu al meer dan een jaar wachten om te upgraden en die ook een systeem hebben van meer dan 5 jaar oud. Dan is alleen de grafische kaart upgraden niet erg slim. Ik game ook liever op een pc, alleen met nieuwe games wordt hem dat niet. Maar prijstechnisch als iemand graag wil gaan gamen dan kan die beter een console aanschaffen dan deze grafische kaart. De consoles zijn nog maar net nieuw en betere hardware ga je voor zo'n prijs niet krijgen.
RX 6600;2;0.455941766500473;Ik wel hoor, heb ondanks de veel te hoge prijzen toch bewust voor een 3060Ti gekozen i.p.v. een console. Weinig andere opties als je ook spellen speelt die niet op console beschikbaar zijn of graag gebruikt maakt van de uitgebreide modding mogelijkheid in games als Cities: Skylines of Skyrim wat er op consoles gewoon niet of veel beperkter is. Wat ik zelf zo fijn vind aan de PC is dat al je games gewoon blijven werken na hardwareupgrades terwijl je bij consoles echt met die generaties zit en overgeleverd bent aan MS of Sony of je oude games blijven werken of dat je vrolijk een remaster (opnieuw) mag kopen. Heb zelf één keer Skyrim gekocht op release, inmiddels gratis bijgewerkt naar de Special Edition en nu op m'n huidige hardware een stuk mooier dan in 2011. X360/PS3 eigenaren hebben hem opnieuw moeten kopen op een latere console. Daarnaast heb ik natuurlijk al een aardige backlog aan games op de PC/Steam, terwijl de switch naar een console betekend dat ik weer op 0 begin. En op een tweede scherm iets anders open hebben staan terwijl je aan het gamen bent kan ook niet op een console Neemt niet weg dat als jij een gamer bent die alleen maar de laatste AAA games wil spelen, geen behoefte hebt aan modding of het (opnieuw) spelen van oudere games een console momenteel wel aantrekkelijker is.
RX 6600;2;0.38535383343696594;Ik praat ook voornamelijk over gamers, mensen die hun pc voor veel meer zaken gebruiken zullen altijd eerder voor een nieuwe grafische kaart gaan, wat ook logisch is. Maar mensen die alleen maar upgraden om een spel te kunnen spelen kunnen zich beter richten op een console. De meeste games zijn tegenwoordig te downloaden of digitaal speelbaar, maar ik heb nog genoeg games in de kast liggen die ook niet zomaar speelbaar zijn op de pc. Dus die gedachte gaat ook niet helemaal op, terwijl ik de cd gewoon in mijn oude console kan stoppen en dan werkt die
RX 6600;1;0.6121127605438232;Daarbij komt nog eens, dat doordat grafische kaarten onbetaalbaar worden, de market voor spellen die de kaarten nodig hebben minder aantrekkelijk is. Ik koop in ieder geval geen videokaart meer. Heb nu een RX5500XT omdat m'n RX480 stuk ging en die draait D2 Res. De rest dan maar op de xbox (na 4 maanden F5'en eentje kunnen kopen ).
RX 6600;2;0.4609944820404053;Nu ben ik niet echt een fan van moord,- en doodslag spelletjes en sim racen doe ik nog maar zelden. Mij interesse ligt feitelijk bij vliegsimulators en vraag me derhalve af hoe deze kaart het doet icm MSFS2020 en DCS world. Ja idd , DCS world is er eigenlijk ook 1 uit de categorie moord,- en doodslag maar het gaat me persoonlijk meer om het vliegen...
RX 6600;1;0.4916157126426697;Dit is de reden dat ik enkel oude games speel .. de groeten , ik wil geen midrange troep voor 800 euro, 290 euro zou een goede prijs zijn Deze markt is knettergek geworden. gewoon goedkope rotzooi voor mega prijzen . Want zo erg is het ECHT geworden Zou toch minimaal 2x GTX 1080ti performance zien voor 800 euro
RX 6600;1;0.7792091965675354;Slechter dan de vorige generatie, lachwekkend dit.
RX 6600;1;0.2839425802230835;En ze worden blijkbaar ook nog (duurder) verkocht. Dat is het frappante.
RX 6600;5;0.3201802670955658;Dat wordt nog even ganzeborden voorlopig voor mij.
RX 6600;3;0.2819366455078125;Nou ja hij is goedkoper maar nog steeds 500 + euro
RX 6600;1;0.41595950722694397;Normaal gezien is dit een goedkoper alternatief voor de RX5700. Dat zou normaal ca een 199 euro videokaartje betekenen. Nu ben ik nog opzoek naar een videokaartje voor mijn 2e PC maar 345 euro vind ik nog steeds teveel 'weggegooid geld'.
RX 6600;2;0.37976521253585815;Inderdaad, dit zou de upgrade moeten zijn voor mensen met een rx470/480/570/580, maar niet voor deze prijzen.
RX 6600;3;0.3750056326389313;De reden dat ik lang geleden op dit mooie forum kwam was dat ik ATi kaarten bewonderde, en ze vervolgens ook nog eens van 9500 naar 9500pro geflashed konden worden, en met wat mazzel ook nog eens daarna naar een 9700pro, ik werd door een collega op dit artikel gewezen, en zie ineens weer hele leuke mods in het verschiet, HD6950 naar 6970 was ook zo'n pareltje, hopelijk is AMD nog altijd zoals ik ze bewonderde en laten ze hierin nog wat tweakersfun ruimte over.. Mmhh de geheugenbus is al niet zo breed, en deze worden toch wel wat geknepen tov XT varianten, vraag me af of dat hem te snel mank zal laten gaan lopen of niet.?!
RX 6600;5;0.2869762182235718;en dan heb je een AMD 6800XT voor dezelfde prijs
RX 6600;1;0.7579523921012878;Adviesprijs 300 euro maar word niet voor onder de 580 euro verkocht... Is gewoon belachelijk
RX 6600;4;0.34276482462882996;‘Voor wie nog gamet op full-HD’ Dat zijn er nogal wat, het is niet bepaald een niche markt ofzo. Zeker op monitoren onder 27” is full-HD gewoon nog erg gangbaar
RX 6600;1;0.3391621708869934;Ik snap al dat gedoe rond grafische kaart problemen niet.... Ik heb binnen een paar seconden een grafische kaart. En de allernieuwste/meest actuele....
RX 6500 XT;1;0.9520092010498047;Ik kan helaas niet andere concluderen dat AMD een dikke Intel/Nvidia doet en daar zelfs goed overheen durft te gaan! Deze videokaart is ronduit schandalig! 350€ op voorraad voor een laptop videokaart in een ander jasje! Zelfs een oude 570 4gb is vaak genoeg sneller op pci-e 3.0! Nagenoeg niemand zal deze kaart in een PCI-E 4.0 systeem zetten door de waardeloze performance en het overgrote deel van mensen die nog een pci-e 3.0 hebben moeten zo een kaart weren in elk opzicht! Dit kopen is gewoon je geld weggooien! 4GB vram en dan deze schandalige prijzen! OMG. Ik heb zelf en 5900X in mijn systeem, maar bah...dit is walgelijk! Ook het pcb met maar 4 lanes is veel goedkoper te maken! Deze GPU zou nooit meer dan 100€ in de winkel nieuw mogen kosten, en zelfs dan is het maar de vraag! 2 jaar geleden kocht je een RX 580 8GB voor 179€ nieuw! Deze bubbel gaat uiteindelijk gewoon barsten of gamers stappen over op consoles want zo kunnen mensen dit allemaal niet meer betalen! Zeer teleurgesteld in AMD! Waar ze veel mooie producten zoals de Ryzen series cpu's en Radeon gpu's hebben gemaakt is nu de GREED helemaal losgeslagen! Men mag zich diep schamen!
RX 6500 XT;1;0.5433844327926636;€ 212,00 inclusief btw, exclusief € 3,95 verzendkosten Voor deze exacte prijs valt er momenteel weinig of niks negatief te zeggen over deze kaart, want zowel Nvidia als Intel leveren momenteel geen alternatief voor € 212,00.
RX 6500 XT;1;0.7285601496696472;Op die link kost hij €329 voor mij, wat natuurlijk een belachelijke prijs is voor deze kaart
RX 6500 XT;3;0.41085243225097656;De link is intussen aangepast. Aardig wat mensen hebben hem voor € 212,00 kunnen aankopen. Maar ze kunnen geen € 212,00 blijven vragen als de andere winkels minimum € 80 extra vragen, en vaak nog meer. Hier zie je het bewijs dat je een degelijke versie van deze kaart nu nog steeds voor € 239 kunt aanschaffen en dat hij zelfs nog in voorraad is: De goedkoopste GTX 1650 in dezelfde winkel kost € 259,00 en volgens de review van tweakers is een GTX 1650 33.4% trager, wat redelijk veel is. Het is nog steeds duur in vergelijking met de prijzen van 3-4 jaar geleden. Maar nog niet zo lang geleden was iedere RX 570 4GB op Newegg minimum 550 dollar en zag je dat er (op Newegg) voor een RX 570 frequent meer dan 1000 dollar gevraagd werd. Zo gezien is deze prijs best goed te noemen, relatief gezien dus.
RX 6500 XT;1;0.9453470706939697;Zelfs € 212,00 is teveel voor deze kaart. Je hoeft niet altijd te vergelijken met Nvidia en Intel. AMD levert hier gewoon een mega slecht product. Op reddit zie je talloos aan benchmarks voorbij vliegen waarbij de kaart zwaar ondermaats presteert en je er eigenlijk geen enkele euro aan wil wagen. Dus er valt niks positiefs te zeggen over deze kaart. Een schande, dat is het!
RX 6500 XT;2;0.3491387665271759;Hm. Ik zou het met je eens zijn in de oude wereld, waar een RX 580 8GB nog rond 250 EUR was. Maar dat is ondertussen al heel heel lang gelden die tijden. Welkom in de nieuwe wereld thisisjazz. Hier zie je wat de GPU prijzen van dit moment zijn: Over de prestaties van de RX 6500 XT, die zijn in deze tijden uitermate goed te noemen voor de prijs: En op Medium settings is er zelfs nog minder verschil met de PCIe 4.0 versie: Excluding those unusual outliers, the Radeon RX 6500 XT is an average of 6.4 percent slower across our test suite when running in PCIe 3.0 mode. Zijn al die boze posts over de 6500 XT werkelijk ergens voor nodig wanneer AMD moeite doet om jullie een betaalbare kaart te leveren waar je prima mee kunt gamen op 1080p? Voor de tragere 1650 wordt vaak 400 EUR gevraagd in veel winkels in de Benelux. De 6500 XT was te koop voor 212 EUR en 239 EUR op alternate.be En het enige dat jullie kunnen doen is klagen over iets waar AMD geen schuld aan heeft en probeert op te lossen.
RX 6500 XT;1;0.6587203145027161;Well, het leest wat lastig zo. Maar @Mindfree bevestigt dat hij deze kaart echt een ongelooflijk slechte prijs-prestatie punt laat zien. Mensen zijn op Tweakers winden AMD vaak wat sympathieker dan Intel/Nvidia, maar hier je dat AMD ook enthousiast meedoet aan de geld graaierij. Dat is een schandaal en wel een paar uitroeptekens waard.
RX 6500 XT;2;0.3517676591873169;Ik vind het straf dat AMD het aandurft om deze kaart te lanceren voor een adviesprijs van 215 euro, terwijl de twee jaar oudere Geforce 1650 Super maar een adviesprijs had van 174 euro en deze kaart even snel is (in 1440p zelfs sneller) met een lager energieverbruik. Wat is dan de bestaansreden van deze kaart?
RX 6500 XT;2;0.2636331021785736;"De grote bestaansreden van dit ding is het niet beschikbaar zijn van alternatieven. Er is amper een GPU te krijgen, laat staan voor adviesprijzen, dus alles dat enigszins pixels op het scherm kan zetten is nu geld waard. Als het Eth-hashjes kan draaien is het zelfs goud geld waard. AMD kan op deze manier voordelig produceren, want deze GPUs zijn vrij klein met z'n die size van 107 mm^2 (tegenover 230+ voor de 6600-versies), en dat betekent dat je er veel uit een wafer kunt snijden, met aardig rendement ook. Dat is ook waarom ie 'maar' 4 GB heeft; dat is ook vrij voordelig, en uit die minimale hoeveelheid silicium pompen ze dan zoveel mogelijk prestaties. Ik vind het nog aardig gelukt ook. Dit is de minimumkaart nu, en als je toch een discrete GPU nodig hebt wordt het (nieuw) niet snel goedkoper dan dit."
RX 6500 XT;1;0.6634555459022522;Bij megekko zijn heel veel kaarten uit de recente RTX 3k reeks gewoon de volgende dag leverbaar. De prijzen daarentegen... Het is dus niet meer dat er geen kaart te krijgen is. Ik denk dat mensen het zat zijn veel te veel te betalen voor een videokaart. Dit is onhoudbaar.
RX 6500 XT;2;0.500017523765564;Ik wordt steeds banger dat de PC-gaming markt zo'n grote klap krijgt dat het grote gevolgen heeft. PC-gaming was voor corona weer terug van weggeweest en tijdens corona is het alleen maar populairder worden. Mensen zoeken een nieuwe hobby en volgen influencers op Youtube, Tiktok, etc. Door het gebrek aan videokaarten (en ook PS5's en Xbox X), moeten mensen overgaan op diverse capriolen. Het is of een (relatief dure) 2e hands kaart kopen, of veels te veel betalen voor een nieuwe, of het opgeven. Hierdoor verwacht ik dat het marktaandeel van PC-gaming nu en straks na corona veel hoger had kunnen zijn. Waardoor de noodzaak van uitgevers ook lager ligt om voor PC uit te brengen (terwijl Sony en Microsoft juist meer zijn gaan uitgeven).
RX 6500 XT;1;0.5773402452468872;PC gaming was voor Corona op zijn retour? Hoe kom je daarbij? Sinds 2010 is PC gaming alleen maar gegroeid en in 2021 is PC gaming in revenue net zo groot als xbox en playstation bij elkaar. Alleen mobile gaming is groter (50% van de totale markt). PC gaming zat in een dip, maar dat was ruim 10 jaar geleden (de 00's) het geval.
RX 6500 XT;2;0.3941355347633362;Ja dat zeg ik toch? Misschien verkeerde woordkeuze. Maar grotere revenue dan de consoles? Heb je een bron? Ik dacht dat dit altijd wat lager lag. En dat beeld klopt ook met mijn omgeving.
RX 6500 XT;2;0.4127918779850006;"Niet groter, maar net zo groot. En niet de hele console markt, maar specifiek het aandeel van xbox/playstation. De totale console markt is altijd groter geweest. Met de zin ""PC-gaming was voor corona op zijn retour"" geef je aan dat de PC markt aan het krimpen was, maar dat is niet de trend van de afgelopen 10 jaar Maar dan nog is het geen gigantisch verschil waar PC revenue op 35 miljard zit en de totale console markt op 49 miljard. Gecombineerd zijn ze nog steeds kleiner dan mobile met een whopping 90 miljard."
RX 6500 XT;2;0.39513054490089417;"ik vrees wel dat wat Bliksem ook zegt, dat die hoge GPU prijzen een gevaar zijn voor PC-gaming op de lange termijn. Een XBOX series S koop je voor 300 euro en de Series X is ook spotgoedkoop in vergelijking tot een PC. Overigens zie je in jou grafiek de PC markt nog wel stijgen, maar minder hard dan de console markt. De ""YoY growth"" van consoles is 8,9% vs 1,7% voor PC gamen. Ik ben een PC gamer vanaf de 8088, dus doe al even mee en het doet zeer te zien dat enerzijds Game studios er een zooite van maken (BF2042 kuch...) maar vooral nu de HW prijzen zo gigantisch stijgen vrees ik het ergste."
RX 6500 XT;2;0.40189874172210693;Nouja, aanbieders zoals Stadia en Geforce Now profiteren momenteel ook van de GPU schaarste. Ook het gebrekkig aanbod van o.a. een PS5 helpt hieraan mee. Alleen al in mijn eigen kring zie ik dit gebeuren dat er overstappers zijn van de PS4 op Stadia. Trouwens.. correct me if i'm wrong, maar de grafiek laat toch een negatieve YoY zien? -1.7% en -8.9%? Dat betekent toch dat console revenue met 8.9% is afgenomen? Scheelt overigens dat Microsoft een visie heeft van één eco-systeem onder game pass. Geen PC vs Console meer, maar alles XBOX ongeacht hardware En ook Sony probeert de meest succesvolle franchises naar pc te porten om de concurrentie met MS aan te gaan.
RX 6500 XT;1;0.31117719411849976;oeps, ik zie het... er staat een - voor.... Er is nog hoop voor de pc gamers...
RX 6500 XT;3;0.3960076868534088;is maar 4GB dus geen ETH miner...wel andere coins natuurlijk.
RX 6500 XT;2;0.49110397696495056;Met de huidige situatie rondom ETH en ETH2 is heel ethereum niet de moeite waard. De hoop is ingeslagen, ethereum wordt aan het eind geholpen, tenzij het netwerk inspeelt op de toekomst, maar met de huidige samenstelling van de wereld en economische stelsels is ethereum aan zijn eind. Een bron hoef ik denk Ik niet te noemen, heel het internet puilt uit van deze informatie!
RX 6500 XT;2;0.40916186571121216;Tja, ik dacht al dat dit een deprimerend artikel zou gaan worden, maar dit is echt schandalig (de kaart, niet het artikel). Leuk voor de aandeelhouders, dat dan weer wel. Wel fijn dat hij raytracing heeft, b.v. patience kun je dan de reflectie van de kaarten nauwkeurig weergeven!
RX 6500 XT;1;0.4111497700214386;Dat ligt nou ook weer niet bij AMD, maar eerder bij de miningindustrie. Zo'n 25% van de kaarten gingen in Q1 2021 naar hen toe: Tel daarbij op dat er een tekort aan chips is en een groter aandeel aan gamers. AMD vist met pricewatch: ASUS Radeon RX 6500 XT OC Edition (€399,-) met name in de vijver met 2e hands (4GB) kaarten en misschien in die van de pricewatch: Inno3D GeForce GTX 1650 GDDR6 Twin X2 OC (299)(de 1650 Super is niet ineens leverbaar). Want als we naar de 2 hands kaarten kijken, dan levert een V&A aangeboden: Asus Dual Radeon RX 580 4GB €230 en een V&A aangeboden: Gigabyte Radeon RX 580 GAMING 8GB minimaal €275,-. Deze kaarten zijn ongeveer net zo snel als de 6500 XT of 1650 super.
RX 6500 XT;2;0.5952802896499634;Het is waarschijnlijk meer een actie om een kaart in de markt te zetten die ook goed leverbaar moet zijn. De betere kaarten zijn over het algemeen slecht leverbaar. De modellen die wel geleverd kunnen worden zijn wel erg duur geworden. Voor AMD is het natuurlijk ook een manier om de winst een beetje op te krikken.
RX 6500 XT;1;0.270463764667511;Tot dat je wint en je de animatie krijgt, dan schiet je FPS onder de 30.
RX 6500 XT;5;0.3660622835159302;Ik hoop dat die bitcoins binnenkort door de grond zakken zodat we allemaal weer voor een normaal tarief gpu's en andere producten kunnen kopen die het echt nodig hebben. Kan de energie (lees stroomvoorzienig) ook eindelijk weer naar sectoren die het echt nodig hebben.
RX 6500 XT;1;0.803737223148346;"Oh ja? Doordat dat dat belachelijke Ponzi scheme gestoorde hoeveelheden energie verbrand kunnen mensen die er iets leuks mee willen doen geen spelletjes spelen. De enige manier om een grafische kaart ook te kunnen betalen is dat ding te gebruiken voor meer van die viezigheid. Hoe bestaat het dat we ons zorgen maken over het klimaat maar het vierkant vertikken zelfs de meest gestoorde energieverspillingen aan te pakken en ja, je gaat vast wijzen naar andere dingen die ook niet helemaal OK zijn. Ik heb op Tweakers al een tijdje alle argumenten van de voorstanders gevolgd en nog nooit iets gezien dat ook maar een beetje hout snijd, behalve het argument van ""geld overboeken"" naar mijn grootmoeder die in een klein dorpje in centraal Afrika leeft. Als we daar het elektriciteitsgebruik van Argentinie voor nodig menen hebben dan kan je op je vingers natellen dat elke maatregel die echte toepassingen bemoeilijkt al helemaal geen schijn van kans maakt. Dat moet je niet de fabrikanten verwijten maar overheden die te beroerd zijn dit soort dingen aan te pakken!"
RX 6500 XT;5;0.6911699175834656;Exact. Perfect verwoord. Daarnaast doel ik ook op het feit dat chips gebruikt worden in veel meer dan alleen gpu's. De bouw van medische apparatuur ligt ook op zijn gat en de transport sector.
RX 6500 XT;1;0.5605655908584595;Er zijn allang allerlei initiatieven die de restwarmte van de 'mining gear' gebruikt voor het verwarmen van huizen etc. je argument gaat dus totaal niet op omdat je blijkbaar de markt en ontwikkelingen totaal niet volgt.
RX 6500 XT;1;0.48565101623535156;Dat gebeurt sowieso: als een grafische kaart 24/7 draait, verwarmt die dus het huis waar die staat, dus 's winters heb je er een elektrische verwarming bij en dat geldt voor vrijwel al het elektrische verbruik in je huishouden: vrijwel alles eindigt als warmte op b.v. uitgestraald licht naar buiten na, dat is meer een natuurwet dan een ontwikkeling van de markt...
RX 6500 XT;3;0.582777738571167;Voor een wetenschapper aan de TU had ik wel wat meer verwacht:
RX 6500 XT;2;0.45566967129707336;Ja, leuk ideetje, maar dat gaat de komende jaren nog geen zoden aan de dijk zetten, zie problemen bij restwaardebenutting van datacenters. De verwarming in prive huizen door gamers zet voorlopig meer zoden aan de dijk!
RX 6500 XT;1;0.5409280061721802;Wat de adviesprijs van hardware twee jaar geleden was is natuurlijk niet zo heel relevant, als je ook die hardware niet meer voor die prijs kan krijgen. Niet dat ik er blij van wordt, maar als je een nieuwe GPU zoekt voor 300 euro heb je nou niet bijster veel alternatieven. Behalve natuurlijk gewoon geen GPU kopen (wat mij verstandiger lijkt dan dit apparaat kopen, want hier wordt niemand blij van ).
RX 6500 XT;3;0.4655659794807434;Het is inderdaad niet relevant voor je portemonnee. Maar kan het eventueel ook gezien worden als wat de fabrikant het product waard vindt? En is het in die context misschien toch relevant?
RX 6500 XT;1;0.5602308511734009;Mijn RX580 8GB heeft me evenveel gekost dan de adviesprijs van dit ding, jaaaren geleden en die zal nauwelijks onderdoen, gezien de RX 570 4GB al vaak sneller is. In de praktijk zal je voor de 6500XT dus nog veel meer betalen, wat het helemaal absurd maakt. Even snelle kaart, 5 jaar later uitgebracht, voor 30% hogere prijs?! Ik was gewend dat je na 5 jaar een kaart kreeg die 2x zo snel was, tegen gelijke prijs... De voorganger 5500xt was goedkoper én sneller! Schiet mij maar lek...
RX 6500 XT;1;0.4994574785232544;@ShaiNe: De 1650 super is niet meer verkrijgbaar. En de 1650 is nu verkrijgbaar vanaf! 300 euro, deze kaart is echter ~25% langzamer. Dus ik begrijp het probleem met deze kaart niet. Laten we aub actuele winkelprijzen met elkaar vergelijken, adviesprijzen van een paar jaar geleden zijn nu niets meer waard. En laten we nieuwe kaarten met nieuwe kaarten vergelijken aub. Ja, deze kaart heeft zeker tekortkomingen, maar als! deze kaart over een tijdje nog te verkrijgen is voor onder de 250-300 euro is het wel de beste low-budget kaart. Alles hangt dus af van de werkelijke winkelprijs!
RX 6500 XT;3;0.41481491923332214;Goede video. Het stomste vind ik nog wel, dat ondanks het gigantische knipwerk in deze chip, hij nog steeds boven de 75W uitkomt. Dat was misschien nog wel de enige kans voor deze kaart: als beste GPU voor OEM PCs e.d. zonder PCI-E PSU kabels... maar nee hoor
RX 6500 XT;2;0.43120846152305603;Inderdaad dat is wel een gemiste kans van AMD, maar ik vermoed dat de idioot hoge clocks van deze kaart (waarmee ze het gebrek aan letterlijk al het overige proberen te compenseren) een streep door dat plan hebben gezet.
RX 6500 XT;3;0.42641931772232056;Ja dat denk ik ook. Normaal gesproken zijn de RDNA2 kaarten zo'n beetje de efficiënte die je kunt kopen, zelfs net iets beter dan de RTX30 series op dat vlak. Dit ding daarentegen, zelfs op PCIe4 is hij niet sneller dan een stokoude GTX1650s maar verbruikt wel 25% meer energie dan diezelfde kaart
RX 6500 XT;1;0.7083772420883179;Als je geen spellen speelt, is zo'n kaart helemaal overbodig en ben je beter af met een IPGU, of zie ik dat verkeerd? Beter het geld van de kaart in een zwaardere processor gestoken dan, met minder totaal stroomverbruik.
RX 6500 XT;4;0.36074569821357727;Zeker waar, je kan het sowieso overwegen ipv dit mormel te kopen. Een 5600G presteert ~ als een GT1030. Wil je wat meer koop een 5700G, overclock je iGPU en neem high speed geheugen. De FLCK van een 5700G kan soms zelfs 2200 MHz halen en om 1:1 te runnen heb je dan 4400 MHz geheugen nodig. Heb je gelijk een flinke CPU en het presteert volgens mij beter dan een 6500XT met pci-3 als een game >4GB wil hebben.
RX 6500 XT;2;0.37536686658859253;"das toevallig, een GT1030 is wat ik nu heb met een 2600 cpu. Games hoeft ie niet te spelen, wel een 38"" monitor(3840x1600) aan te sturen. Zit erover te denken om naar een i5 12500 te switchen, zonder grafische kaart. Sneller en minder stroomverbruik schat ik in."
RX 6500 XT;5;0.5294771194458008;Als je switcht naar een 5600g of 5700g kan je mooi je moederbord houden en heb je een flinke boost in cpu-performance.
RX 6500 XT;1;0.5642484426498413;De iGPU van Intel houdt een 5600G niet bij, dus een GT1030 ook niet.
RX 6500 XT;3;0.7153300046920776;Aardig vergelijk, de AMD heeft veel meer cpu % nodig om sneller te zijn, en de gpu clock van 1900 itt 1450 scheelt ook natuurlijk.Maar ik gebruik m'n pc niet voor spellen.
RX 6500 XT;1;0.7498332858085632;dit is toch echt een kaart voor de casual gamer die wat oudere games heeft die hij nog speelt maar voor de nieuwere games echt waardeloos is... rare move van kamp rood.
RX 6500 XT;2;0.3379473388195038;Ik heb nog een 2gb 1050, zou deze kaart uberhaupt een meerwaarde zijn of kan ik beter naar een 1060+ gaan kijken?
RX 6500 XT;4;0.4302024245262146;Je gaat zeker verschil merken. Zie de review van de 1050 2GB. De benchmark die hetzelfde is, is de TimeSpy van 3DMark, dus die pak ik even ter vergelijking. De 6500XT is 26% sneller dan de 1060 6GB, die 100% zo snel is als de 1050 2GB. Ook al is de 6500XT met 4GB krap bemeten qua geheugen, het is dubbel zoveel als je nu hebt. En als je niet de eis hebt om op ultra-instellingen met 120+ fps te spelen, lijkt het me een prima kaart. Afhankelijk van de prijs, je budget en geduld (als de prijzen óóit weer dalen).
RX 6500 XT;5;0.46782809495925903;Dank voor je reply
RX 6500 XT;2;0.45994582772254944;"Dan mag je in dat scenario wel hopen dat je die oude games op een heel nieuw systeem speelt (11th of 12th gen Intel of Ryzen 5xxx), want anders zit je aan maximaal PCIe gen3 met 4 lanes en is de performance nog slechter. Dus een oude pc met PCIe gen 3 of, nog erger gen 2, hiermee upgraden is dus een heel slecht idee. En wat noem je dan oud, wel zelfs Ryzen 3xxx of iets als een 9900K systeem noem ik niet bepaald ""oud en achterhaald""."
RX 6500 XT;3;0.3279876112937927;Deze kaart is toch PCIE3. Dus dat maakt niet echt uit. Dat was een van de punten uit de review waarom die zo slecht scoort dacht ik. Hieronder: helemaal correct! Ik moet niet zo snel lezen.
RX 6500 XT;1;0.4869745969772339;Nee, deze kaart is gewoon PCI-e 4.0, maar met slechts 4x lanes. Je stopt je videokaart in een x16 PCI-e slot (het lange slot op je moederbord). Daar gebruikt deze videokaart dus slechts 4 lanes van, de rest is onbenut. Als je meer wilt weten over de exacte snelheden per PCI-e versie en hoeveelheid lanes, in dit nieuwbericht stond een tabelletje. Het korte verhaal: per versie verdubbelt de bandbreedte per lane. Zodra je deze videokaart in een PCI-e 3.0 slot (op je moederbord) steekt, is de bandbreedte lager dan als je hem in een 4.0 slot steekt. Blijkbaar verlies je dan ruim 12% aan prestaties. En @Moi_in_actie heeft gelijk: pas vanaf de nieuwste chipsets wordt PCI-e 4.0 op moederborden ondersteunt. Dus alleen AMD's B550 en X570 moederborden en Intel's B560 en Z590 (LGA1200) of de nog nieuwere LGA1700 moederborden ondersteunen dat. Die zijn maximaal een jaar oud. Er zijn heel veel personen met bijvoorbeeld een AMD B450 moederbord. Die ondersteunt prima de nieuwe 5000 Ryzen-cpu's, maar hebben geen PCI-e 4.0 en hebben dus last van deze 4x lanes bottleneck.
RX 6500 XT;1;0.6375569701194763;Nee, het is een PCIe 4 kaart. In het tweede deel van de conclusie legt men juist uit dat wanneer men een systeem wat alleen PCIe gen3 heeft gebruikt, de prestaties NOG slechter worden. En aangezien de meeste systemen niet eens PCIe 4 hebben en, ik gok, de doelgroep voor deze GPU dat al helemaal niet heeft, maakt het de kaart des te waardelozer.
RX 6500 XT;5;0.4198794662952423;Ik hoop dat intel succesvol word met hun graka's. lekker concurrentie.
RX 6500 XT;2;0.4382307231426239;En jij denkt dat intel de prijs gaat drukken en performance weg gaat geven? dan ken je intel nog niet goed denk ik. Voor de review, wel straf dat deze nog niet eens mee kan met de 5500XT dat was toch wel de minimum, ik heb zo nog een 5500xt 4gb gekocht voor 175eur.... deze nieuwe is veel te duur voor zijn performance, laat staan dat de scalper winkels nog eens hun prijs erbovenop gaan zetten.
RX 6500 XT;2;0.30171826481819153;Ik hoop dat er meer concurrentie en innovatie komt. Een derde partij is altijd goed. Of beter dan 2. Ze willen allemaal zover mogelijk centjes van ons maar ze moeten ook gaan concurreren.
RX 6500 XT;1;0.43426084518432617;De prijzen van Intel's Cpu's zijn al jaren vriendelijker dan die van AMD. Dus wat bedoel je eigenlijk?I5-12600 (met of zonder K en/of GPU onboard) ⚖️ Ryzen 7 5800x? (€80 verschil)I5-12400 (met of zonder GPU onboard) ⚖️ Ryzen 5 5600x (€100 verschil)Dat AMD niet mee kan gaan kwa prijzen met Intel of Nvidia op dit moment zegt meer over de zorgelijke positie waar AMD zich wederom in lijkt te bevinden dan iets anders... Maar men kan altijd hopen op een wonder...
RX 6500 XT;2;0.3535102605819702;haha voor de eerste keer in 2-3 tal jaren dat intel eens een betere CPU prijs heeft als AMD ga je dit verhaaltje boven halen? Vergeet trouwens de uber dure moederborden niet want je vergelijkt enkel de CPU op z'n tweakers.... Als AMD echt niet mee zou kunnen kwa concurrentie hadden ze al lang een reductie gedaan van de verkoop prijs.. want het proces is al lang in gebruik. Echter doen ze dit niet dus wil zeggen dat iedereen niet zo blind is om zomaar een intel te kopen, want de enige die intel altijd als de beste koop vindt is tweakers website.... maarja dat was zelf bij de 10 en 11e generatie ook nog altijd effe een realiteits check voor diegene dat zeggen dat Intel steeds de beste koop is en meeste cpu verkoopt, de DYI markt is het niet met je eens: 1/3 revenue, dus amper intel te zien, in de volgende maanden zal er wel wat meer balancering in komen als de midrange mobo wat betere prijs hebben. Maar als die nieuwe cache versie uitkomt van de 5000 serie zal die zeker snel op nr 1 staan want iedereen heeft al een AM4 mobo en kan dan nog gemakkeljik een upgrade doen van CPU.
RX 6500 XT;2;0.37682685256004333;"Zucht... Er zijn meerdere chipsets voor Intel CPU's van elke generatie. In verschillende prijsklassen. Het lijkt er meer op dat jij je blind staart op de Z-varianten, want als je een stapje doet naar een B- of H-variant dan dan praat je over 2/3 tientjes verschil t.o.v. een B350 of een B450 voor AMD, die overigens ook niet echt ideaal geschikt zijn voor een Ryzen 5xxx. Met die paar tientjes overbrug jij niet het prijsverschil tussen de CPU's van €100. De meeste mensen kopen bij de Action en er zijn daar echt vaak prima spullen te vinden. Ik denk dat de Action méér tissues verkoopt van hun merk, dan de Kruidvat. Maar wat zegt dat over Action en wat over de mensen? Zegt dat iets over de duurzaamheid van wat er verkocht wordt? Zegt dat iets over de kwaliteit van de tissues? Zijn de tissues van Action automatisch ""De beste koop, puur omdat deze op 1 staan op de ""bestselling list""?"" Het feit dat amazon een grote speler is die overal de markt weet te veroveren met een aggressieve marktaanpak, zegt uiteindelijk ook wat over de gemiddelde koper die graag gemakkelijk op Amazon bestelt. De gevaarlijkste mensen zijn domme mensen die niet weten dat ze dom zijn. Zoals schapen ook niet weten dat ze schapen zijn."
RX 6500 XT;1;0.43670740723609924;en den 550 chipset? ah nee het is geen intel dus soewieso niet relevant... Die paar tientjes amper B660 moederborden beschikbaar, laat staan H.... en dan B met een prijs 150-200eur met amper keuze. leuk profiel btw... komt overeen met uw reacties. typsiche grote mond achter een internet ID, veel blabla. Maar als je high tech verkoop al vergeljikt met een low budget handel dan zijn we al ver gekomen.
RX 6500 XT;1;0.6379315853118896;"$TargetName = ""Conces"" $ImNotHappy = ""D3X"" $Greetings = -join (""Hi, Mister"", $TargetName, ""..."", ""Everything flex?"") $Introduction = -join (""You can call me D3X."") $Admit = ""I was intrigued by your post."" $FakeCompliment = ""Nice Tweakers profile you've got there!"" $NotAdmit = ""I do not believe it's real though. I think you are a fraud!"" $BigEgo = ""I use different Internetprofiles all the time. Everybody uses fake profiles, right?"" $TheEnd = ""So, yours has to be fake too!"" $ItsOk = ""No problem man! You are entitled to have an opinion!"" $LetsPartWays = -join (""Have a good future and sleep"", $ImNotHappy) $MessageArray = @($Greetings, $Introduction, $Admit, $FakeCompliment, $NotAdmit, $BigEgo, $TheEnd) $ReplyArray = @($ItsOk, $LetsPartWays) Add-Type -AssemblyName System.Speech $Speech = New-Object System.Speech.Synthesis.SpeechSynthesizer $Speech.SelectVoice('Microsoft Zira Desktop') ForEach ($Message in $MessageArray){ $Speech.Speak($Message) Start-Sleep 0.5 } Start-Sleep 0.5 $Speech.SelectVoice('Microsoft David Desktop') ForEach ($Message in $ReplyArray){ $Speech.Speak($Message) Start-Sleep 0.5 }"
RX 6500 XT;2;0.4432379901409149;Ze gaan hopelijk op MRSP concurreren, maar dan hopelijk met een veel betere verkrijgbaarheid. Hierdoor daalt de vraag voor Nvidia/AMD kaarten, en zullen de scalpers hun prijzen moeten verlagen. Zodat we, wie weet ooit, RTX kaarten mogen kopen tegen een prijs waar ze eigenlijk voor bedoeld waren (wat overigens nog steeds tamelijk hoog is, maar beter dan de debiele marges die er nu op zitten)
RX 6500 XT;5;0.27472642064094543;de scalpers zijn de winkels...... diezelfde winkels die ook intel in de portfolio nemen voor verkoop. prijzen gaan enkel dalen als er veel stock is.
RX 6500 XT;1;0.29254740476608276;En zelfs dan nog, dat doelpubliek targeten is niets mis mee, maar dat dan doen met een duurdere kaart die minder prestaties biedt dan zijn voorganger... yikes
RX 6500 XT;1;0.33499473333358765;Nee dit is voor het betere Office werk maar ach alles verkoopt nu.
RX 6500 XT;1;0.9228858947753906;Wat een shitshow op GPU gebied zijn 2020 en 2021 ook geweest. In 2019 voor net iets meer dan 300 euro een GTX1660Ti gescoord (en dat vond ik toen al belachelijk als iemand die gewend was dat een x60 nooit meer dan 240/270 euro hoefde te kosten). En nu in 2022(!) lanceert AMD een kaart die zwakker is dan die GTX 1660Ti, voor méér geld. De GPU wereld staat niet stil, ze holt zelfs achteruit!
RX 6500 XT;1;0.8547537326812744;En die 1660 Ti zou je met winst kunnen verkopen Maar ja wat koop je terug ? Vraag me echt af, AMD, waarom breng je überhaupt deze kaart uit..... Plank flink mis geslagen
RX 6500 XT;1;0.39888396859169006;Zo heb ik mijn 1660-Ti in november 2020 verkocht voor 175 euro omdat ik er snel vanaf wilde toen ik mijn 6800 XT had
RX 6500 XT;3;0.3350745141506195;Misschien inderdaad de 1660 maar verkopen, en er een console voor in de plaats zetten. Alleen tjah, de meeste leuke non AAA games vind ik toch op de PC.
RX 6500 XT;1;0.8192055821418762;Tja, ik heb mijn zoon maar mijn RX580 gegeven. Ik zag dat er €350-400,- (!!!) gevraagd wordt voor een tweedehandsje!. Dat is zo'n €200,- meer dan ik een dikke 3 jaar terug NIEUW voor mijn RX580 betaald heb! Het moet niet gekker worden! ....En dat soort idioterie geeft dus het bestaansrecht aan deze zéér kreupele, veel te dure RX6500 XT. Rare tijden.
RX 6500 XT;3;0.4712092876434326;In 2019 lopen twijfelen, maar toch voor wat de luxere variant van de 1660 super gegaan. Dat kostte me dan wel 80 euro meer dan de budget modelletjes, maar achteraf is dus deze nu nog sneller dan wat AMD in 2022 voor 200 euro adviesprijs uitbrengt. Dat was dus een aardig goede beslissing gezien de huidige markt. Nou maar hopen dat ie niet de geest geeft en ik alsnog dus zoiets als deze prut van AMD zou moeten kopen.
RX 6500 XT;2;0.461533784866333;Je zou eens moeten kijken wat de 1650 kost 🧐 Die is in deze review zelfs wat zwakker en met een introductie-prijs van ongeveer 200€ is die al een keer verdubbeld en twee jaar later zit die rond de 300€ 🤨🤨 Kwa prijs en product vind het niet zo gek dat AMD deze stap durft te maken al vind ik het dan wel weer raar dat in deze tijden van “tekorten” deze chip wel kan worden geproduceerd op TSMC 6nm. Daar is het laatste waar je overschot verwacht. Als ik dat 2 jaar geleden had geweten had ik wel een extra hypotheek genomen om kaartjes in te slaan 😄
RX 6500 XT;1;0.39745795726776123;Ik snap niet waarom er een PCIe 16x op zit, als maar 4x aangesloten is? Wat is de meerwaarde, aangezien ik 'm nu niet zomaar in een 4x slot op mn mobo kan prikken?
RX 6500 XT;1;0.5601463317871094;Als je wat plastic van je x4-slot affreest kun je hem er zeker wel in prikken. (Ja, werkt v.z.i.w. technisch wel gewoon, al heb je dan een stuk PCI-connector los rondzweven. Nee, niet serieus doen.) AMD had hier inderdaad prima een klein low-profile kaartje van kunnen maken, dan had 'ie nog een doel gehad. Maar ook dat hebben ze niet gedaan. Nu is deze kaart geschikt voor ongeveer niks.
RX 6500 XT;3;0.4862010180950165;Ook al is pci-e een generieke standaard, het is toch een beetje het idee om altijd de grote sleuf van 16 lanes voor je GPU te gebruiken. Elk moederbord heeft sowieso een 16x slot, maar niet automatisch ook een 4x slot, dus ik snap de keuze wel.
RX 6500 XT;1;0.6803686022758484;En een x4 kaart past gewoon in een x16 slot. Dus dat sluit dat gebruik niet uit. Nu past ie niet in een x8 of x4 slot.
RX 6500 XT;5;0.39514756202697754;Mijn GTX970 van voor de oorlog is nog best courant als ik dit zo zie
RX 6500 XT;2;0.30379053950309753;GTX970 is een beest!! Die ouwe kan nog aardige meekomen. Gamers Nexus vergelijkt de 6500 XT met oudere GPUs: 6500XT vs GTX 970 Tomb Raider: 66.7 / 53.6 fps (+13.1) RDR 2: 53.1 / 40.6 fps (+12.5) GTA V: 73.2 / 80.6 fps !! (-7.4) Ja ok de winnaar is duidelijk. Maar niet echt wereldschokkende toename in performance. En niet futureproof. Alleen als ik hem gratis krijg zou ik het doen.
RX 6500 XT;3;0.2642270624637604;Een tweedehandse 970 heb je tweedehands vanaf 150 euro, de 6500XT kost al gauw 300-350 nu... makkelijke beslissing inderdaad
RX 6500 XT;1;0.919152557849884;Alternate heeft er 1 online staan, niet verkrijgbaar en geen info... voor de lieve som van € 499,00... helemaal de weg kwijt.
RX 6500 XT;3;0.28275859355926514;Dan ben ik toch blij met mijn 5500XT van 2 jaar oud. Was toen 185 euro (de 4GB uitvoering) en dat vond ik al veel geld. Mooi om te zien dat deze kaart 2 jaar later alleen in het stroomverbruik verslagen wordt door de nieuwe generatie.
RX 6500 XT;1;0.5199953317642212;De markt voor videokaarten is helemaal verziekt. Ik zag net een post van Megekko, die heeft er een aantal in stock voor de lieve som van 350 euro. Daar kocht je vroeger (ja ik weet het, inflatie, chiptekort en miners) een dikke midrange kaart voor, denk aan een RTX 2060. Ik heb zelf nog voor 250 euro een GTX 1660 Super gekocht die rondjes om deze 6500XT rent. Dit is in basis een prima kaartje voor een bepaalde doelgroep, maar dan voor 120 euro en niet voor megalomane bedragen.
RX 6500 XT;1;0.4426744878292084;Onvoorstelbaar dat mijn rx580 8gb van 2017 dus soortgelijk presenteert voor minder geld. Prijs was toen iets van 250 euro volgens mij. 4-5 jaar later...
RX 6500 XT;2;0.44190117716789246;Nou, ik vind het onvoorstelbaar dat er nu een nieuwe redelijke gpu voor deze prijs komt. Voor iets meer dan 250 euro zit je nu vast aan maximaal een GTX 1050 TI, hiermee moet je deze gpu dan ook denk ik vergelijken. Als je deze videokaart in de huidige markt plaatst tegen een rx580 zal dit eerder de opvolger van een rx550 zijn. Helaas zijn door diverse oorzaken de prijzen nu zo hoog dat deze prijs (als ie daadwerkelijk hiervoor beschikbaar komt) nog wel meevalt.
RX 6500 XT;1;0.6908960342407227;Alleen heb ik de rx550 voor 60 euro gekocht nieuw en deze kost 350 bij megekko dus 199 is het al niet waard bij 350 helemaal niet AMD heeft hier een mooi stuk ewast gemaakt
RX 6500 XT;4;0.3230615556240082;Ik kan ze nu via ali krijgen voor 143 euro. Nog steeds niet spotgoedkoop, maar goedkoper dan de alternatieven.
RX 6500 XT;5;0.3484184145927429;Dus echt de limiet van 1080p, de prijs gaat alles uitmaken. En waarschijnlijk kan FSR in de driver niet snel genoeg komen.
RX 6500 XT;1;0.24171218276023865;€799,- bij alternate voor de MSI Radeon RX 6600 XT Gaming X 8G
RX 6500 XT;1;0.3623741567134857;Lol... Voor <250 zou je nog kunnen zeggen dat je de GPU tekorten even uitzit met een 6500XT. (Zelfs dat is een slechte deal, maar op het moment misschien wel de beste deal.)
RX 6500 XT;4;0.5018461346626282;In feite dus vooral een kaart die het goed zou kunnen doen voor degenen die een nieuw systeem in elkaar zullen zetten en geen godsvermogen uit willen geven aan een GPU, maar toch een spelletje willen kunnen spelen. Top dat in het vergelijk de RX570 ook wordt meegenomen. Het maakt het echter wel pijnlijk duidelijk dat in 5 jaar tijd je voor dezelfde Euro's grofweg dezelfde performance krijgt. Omdat het voor de GTX1060 / RX480/580 eigenaren sowieso een sidegrade zou zijn in plaats van een upgrade, snap ik de opmerkelijke keuze van AMD wel. Het enige wat ik echter mis in het vergelijk, is hoe deze kaart presteert in vergelijk met de huidige AMD onboard GPU's. Voor low-end gaming lijkt dat me namelijk nog het meest relevante alternatief.
RX 6500 XT;1;0.553958535194397;Wat een afgang, ik wilde mijn RX 470 8GB een upgrade geven in mijn HTPC/Couch Gaming pc. Ik denk dat deze nog een aantal jaartjes blijft zitten. Had de kaart 125 euro gekost en 6400 XT als naam gehad met fatsoenlijke encoding dan was het een goede deal. Ik had serieus beter verwacht dan dit.
RX 6500 XT;2;0.5767762660980225;"Dit is wel heel matig. Los van de prijs/prestaties, waar genoeg mensen natuurlijk over klagen, dat dat anno 2022 niet zo best is was wel te verwachten. En ook los van de absolute prestaties, want dat dit een low-end kaart zou worden met wat flinke bottlenecks was ook duidelijk. Maar ook in technisch opzicht valt dit wel vies tegen. Voor een nieuwe kaart op een geoptimaliseerd ""nieuw"" procedé mag je toch in ieder geval verwachten dat het ding energie-efficient is, maar zelfs dat is niet heel best. Qua prestaties zit 'ie op 1080p tussen de GTX1650 en de GTX1660 in, en op 1440p zit hij daar flink onder, maar het verbruik ligt er ook tussenin. Ten opzichte van de 2 jaar oude kaarten, geproduceerd op TSMC 12nm, dus geheel geen verbetering, eerder een verslechtering... Ik verbaas me er ook over dat deze kaart 120W verbruikt terwijl de 6800U, die naar verwachting richting 2/3 van de grafische prestaties zal hebben, een TDP heeft van 28W - voor de CPU en IGP samen! Ik vraag me dus echt af wat hier nou precies misgaat."
RX 6500 XT;1;0.6696886420249939;Mijn rx570 heeft een powerlimit van 130w, en een pcie 6 pin connector, dus max 150w tbp En dat op glofo 14nm, presteert beter dan dit onding in mijn pcie 3.0 systeem zou doen... Wat een afgang
RX 6500 XT;1;0.44491302967071533;Schaarste is wat er misgaat. Nu voor 199 dollar te koop zou al heel knap zijn vanwege die schaarste. Zonder schaarste zou je hooguit rond de 100 euro betalen. Op medium draait hij wel alle games op 1080.
RX 6500 XT;2;0.5113869309425354;"Mijn punt is dus dat schaarste niet het enige is wat hier misgaat, want schaarste zou toch vooral de uitwerking moeten hebben op de prijs. Niet op de kwaliteit van de chip op zich. Zoals ik al zei: als je niet kijkt naar de prijs is dit in technisch opzicht ook maar een zeer matig product. AMD heeft er ook in tijden van schaarste geen baat bij om matige producten te ontwikkelen - dit is een proeftuintje voor de volgende generatie kaarten, en tegen die tijd wordt een matige videokaart misschien niet meer voor elke mogelijke prijs verkocht. Als dit een voorbode is voor andere kaarten op ""6nm"", dan is daar nog flink wat werk aan de winkel... Ik kan me nog van dit soort proeftuintjes herinneren - zoals de HD4770 heel wat jaartjes geleden - die regelrechte successen waren omdat ze vanwege een nieuw procedé echte knallers waren terwijl de chip toch lekker compact en zuinig was. Dat is hier wel anders. Andere kaarten, zoals de RX6600XT, hebben ook geen fatsoenlijke prijs maar los daarvan zijn het in ieder geval geen producten die voelen alsof ze in 2017 thuis horen..."
RX 6500 XT;1;0.3386398255825043;Ik vermoed juist dat AMD heel verstandige keuzes heeft gemaakt. Door deze kaart een sterke anti mining handicap mee te geven zal dit een van de weinige zijn die geen last heeft van mining opkoop. De adviesprijs van 199 euro is dan nog heel scherp voor de prestaties, bijna alles op 1080 goed laten draaien. De HD4770 was een heel mooie tussenoplossing. Deze is mogelijk nog een heel stuk slimmer ontworpen. De enige echt vraag is, hoeveel kan AMD ervan aanbieden?
RX 6500 XT;2;0.4453170895576477;Da's volgens mij volledig gelegen aan het feit dat AMD de prestaties iets hoger had ingeschat en dus maar heeft proberen compenseren door de efficiëntie compleet overboord te gooien voor 5-10% hogere prestaties. Volgens mij krijg je het verbruik van deze kaart perfect 30-50% omlaag door de clocks 10-15% te laten zakken en daarbij nog eens te undervolten. Zelfs de 6700XT krijg je gezakt tot rond de 135W door de clock target amper 100MHz te doen zakken en de spanning 0,060mV lager in te stellen, dus dan zal hier hoogstwaarschijnlijk ook veel winst te behalen zijn.
RX 6500 XT;4;0.3365837633609772;Ik zou prima met deze kaart uit de voeten kunnen. Games die ik graag speel zijn gta 5, fortnite, beamng, cities skylines en eurotruck simulator. Dus niet allemaal games waar je wereldprestaties voor nodig hebt. We moeten nog even de prijs afwachten, maar ook hier is een breed publiek voor, zeker voor de juiste prijs: Ik zet wel eens vanuit mijn werk game pc's voor jongeren in elkaar die niet te veel centjes mogen kosten, maar meestal kom je al tegenwoordig snel op een RX-560 gpu uit om maar iets te noemen. Een verouderde gpu, maar nèt speelbaar met de nieuwere games. En nog (new old stock) voor een goede prijs te vinden op o.a. Aliexpress als je goed reviews checkt en weet waar je op moet letten.
RX 6500 XT;3;0.24234448373317719;Waar moet je dan op letten? Ik heb mijn RX-560 aan mijn zoon gegeven (die in mijn trouwe ouwe overspecced MacPro desktop beestje zat) en zou graag weer een vervanging willen vinden.
RX 6500 XT;5;0.4845965802669525;Let vooral op reviews op Aliexpress en dat je het juiste product selecteert. Vaak kun je bij een productpagina van een 1 terabyte ssd ook modellen van 60gb aanklikken om maar iets te noemen. Vooral het merk Kllisre heb ik goede ervaringen mee, ze maken o.a. ram, mobo's, ssd's en grafische kaartjes. Alles is incl. belasting (incl. VAT staat er dan bij) en vaak hebben ze ook nog eens 10 daagse levering.
RX 6500 XT;2;0.425740510225296;Tegen de tijd dat Zen 4 uit komt krigj je standaard IGP's die hier zomaar eens best dicht in de buurt kunnen komen. Sterker nog, de 6800U (en varianten) zijn al aangekondigd en zitten waarschijnlijk al op 2/3 van de prestaties van deze videokaart. Goed, die 6800U is waarschijnlijk pas vanaf de lente/zomer te koop, en dan niet in budget-systemen, en Zen 4 is pas beschikbaar in het najaar. Maar toch zie ik deze RX6500XT minder lang meegaan dan zijn voorganger.
RX 6500 XT;2;0.5683706402778625;Valt toch behoorlijk tegen, maja wat verwacht je met maar 4GB en maar 4x pcie lanes dikke nee, zelf voor 1080p kan je deze kaart maar moeilijk aanraden... Hopen dat het snel beter wordt in de GPU wereld.
RX 6500 XT;4;0.4721314013004303;Leuke kaart voor CSGO mits de prijs op MSRP blijft, maar niet meer dan dat.
RX 6500 XT;3;0.49039047956466675;Een goede GPU kan een voordeel zijn, maar CSGO is een DX9 game, Dan haal ik mezelf wel een GTX580. Daar kan ik er meerdere van kopen voor dit bedrag.
RX 6500 XT;1;0.7367264032363892;Wat een drol van een product, AMD moet zich zwaar schamen.
RX 6500 XT;2;0.3530622720718384;Wat je nu ziet is dat er nu op alle segmenten van de GPU markt tekorten zijn, dus ook mensen die normaal in het ene segment zouden kopen zullen in een ander segment iets kopen omdat het gewoonweg beschikbaar is. Specifieker, mensen die eigenlijk met een entry level kaart genoeg hebben zullen soms uit pure noodzaak een duurdere kaart aanschaffen omdat ze niets anders kunnen vinden. Andersom zal ook wel gebeuren al eindigt men dan natuurlijk met een systeem dat onder de beoogde specificaties zit. Als deze kaart goed beschikbaar wordt voor die entry level doelgroep ben je dat probleem alweer armer en creëer je ook wat minder druk op de andere segmenten. Aan de andere kant kan ik me ook voorstellen dat een goede beschikbaarheid juist mensen uit de hogere segmenten zal verleiden om eieren voor hun geld te kiezen. De traditionele marktsegmentatie is een beetje gaan vervagen in deze hele toestand en ik ben benieuwd of deze kaart daar in de praktijk iets aan gaat veranderen.
RX 6500 XT;1;0.45765867829322815;Precies wat ik al verwachte, langzamer dan een rx480 van 6 jaar oud, schandalige kaart deze 6500xt. Verkopen zullen ze ze toch wel in deze gpu schaarste, mensen met minder verstand ervan zien gewoon een nieuwe gpu met 4GB voor onder de 300 euro dus die kopen ze.
RX 6500 XT;2;0.5649020075798035;Ik heb zelf al enge tijd een ITX systeempje klaar staan en mis enkel nog de CPU en GPU gezien de gekte op dit moment, nu had ik gehoopt dat deze 6500XT een leuke tijdelijk kaart zou zijn, maar ik denk dat ik beter af ben om er maar een APU als de 5600G/5700G erin te proppen.. had er graag een wat meer potent systeem van willen maken maar dit gaat het hem niet worden..
RX 6500 XT;3;0.270612895488739;Ik ga gewoon wachten op de volgende AMG gen zelfs. Hopelijk zijn ook de gpu's dan weer betaalbaar.
RX 6500 XT;1;0.6180952191352844;Dat zou ik ook normaal gesproken ook doen, maar ik heb die ITX- X570 compleet klaar staan al 1,5jaar? zo zonde om het niet te gebruiken, nu een 5600G/5700G er in zetten dan is voor nu mi. nog de goedkoopste oplossing en zien we later wel weer…
RX 6500 XT;2;0.4414152503013611;Je hebt weinig keuze dan, de volgende AMD heeft andere socket. Ik wou ook eerst voor een G gaan maar heb niet echt nieuwe nodig(5 tal pc in huis )
RX 6500 XT;1;0.49979373812675476;Tja dit verbaast me niets. Dit hebben we aan ons zelf te danken. Moeten we maar niet zo achterlijk zijn en deze prijzen betalen voor kaarten die 5 jaar oud zijn. Welkom in 2022 waar we 300 euro betalen voor een lowend kaart.
RX 6500 XT;3;0.6074535846710205;Prestaties iets minder dan verwacht, had hem net boven RX 5500 XT verwacht en niet er onder en dan ook nog met een veel hoger verbruik.... Voor de 300-350 euro die er nu voor wordt gevraagd, zou ik hem links laten liggen, zelfs als tijdelijke kaart voor een nieuw te bouwen game pc, waarbij je later een echt game kaart koopt als deze verkrijgbaar zijn.... Dan kun je nog beter een APU nemen, zoals de AMD r5 5600G/ r7 5700G, met snel systeem geheugen in dual channel opstelling.
RX 6500 XT;3;0.46331679821014404;Nou, bij een APU moet je eerder denken aan 720p gaming op low, wat op vandaag de dag wel een beetje karig is. Maar voor die ''gratis graphics'' die je bij je cpu krijgt, mag je ook niet klagen lijkt mij.
RX 6500 XT;2;0.4697181284427643;Hebben jullie de enhanced edition of de normale versie van Exodus genomen? De normale vraagt namelijk veel minder (mijn pc met 9900k en 2060 vindt de enhanced edition namelijk niet leuk).
RX 6500 XT;2;0.33375176787376404;Wij hebben de normale versie gebruikt, niet de enhanced version.
RX 6500 XT;1;0.5872788429260254;"@ schrijver artikel : je finale zin van je conclusie is veelzeggend : "" in veel gevallen zul je dan ook beter af zijn met zo ' n ouder model als je dat voor een schappelijke prijs tweedehands op de kop kunt tikken, dan met een nieuwe rx 6500 xt. "" hier zitten nl. zoveel haken en ogen aan : 1. je vergelijkt nu ineens een nieuw product met garantie en support met een soms wel 6 jaar oud product ( bijv een rx 570 of rx480 ) welke gelijkwaardig presteert. 2a. het woord "" schappelijk "" is heel misleidend! de goedkoopste kaart met vergelijkbare prestaties op dit moment in de pricewatch ( binnen 70km van mijn woonplaats ) is 299 euro! 2b. zeker i. c. m. de woorden : als / kunt.... zo ken ik er nog wel een paar... als je een rx6600 voor 300 euro kunt kopen dan moet je dat zeker doen. refereer aub aan werkelijke prijzen, en niet aan fictieve geidealiseerde prijzen. en dat is niet reeel in de huidige gpu markt. de enige reele vergelijking welke je kan maken van alle review kaarten is denk ik met deze 2 kaarten : geforce 1650 voor 299 euro geforce 1660 super voor 479 euro ( van de rx 5500xt is er nog maar 1 te koop voor een absurde prijs van 730 euro ) en al de kritiek op het geheugen en de pcie bus doet er niet toe, dit zie je gewoon terug in de prestaties. daarin doet hij het ~ 20 - 30 % beter dan de 1650 en 20 - 30 procent minder dan de 1660 super. op de 1080p medium resolutie waarvoor hij bedoeld is. ( 1440p met 4gb geheugen is nooit een goed idee geweest ) en idd. deze kaart mist bepaalde codec encryptie / decryptie, en als je die gebruikt dan moet je hem zeker niet kopen, maar veel mensen zullen deze codecs niet missen. dus hangt het volledig af van de winkelprijs van deze kaart en jouw gebruik of dit een goede koop is of niet."
RX 6500 XT;1;0.7523506879806519;"Deze GPU is een aanfluiting en was met normale winkelprijzen niet eens review waardig geweest. Dit had dan in de categorie ""het geeft beeld"" gevallen (zoals de MX-en en GT`s van Nvidia), het is niet eens een volwaardige RDNA2 GPU, er is zoveel weggehakt en gefused dat er niet eens actieve encoders in zitten en er nog maar 2 display pipes over zijn. Een RDNA2 chip doet AV1 decoding en dat doet dit ding ook niet, het is ontworpen als een laptop GPU of in ieder geval iets wat een checkbox op de specs lijst van een laptop fabrikant invult. AMD had dit beter een RX 5450XT of iets dergelijks kunnen noemen, er is notabene voor OEM`s ook nog iets als een RX 6400...als het maar beeld geeft en een logo op de doos... Deze chip hadden ze enkel in laptops moeten aanbieden, als losse kaart is dit stompzinnig, het word qua prestaties aan alle kanten door jaren oudere kaarten overtroffen en zelfs de RX 5500XT komt ie niet tot nauwelijks voorbij. Als AMD dit in een ""gewone"" marktsituatie had uitgebracht voor deze prijs, was het internet ontploft met met negatieve reacties, maar nu gaan mensen dit nog serieus overwegen ook. Deze kaart is alleen zinnig met een prijs rond de 100 euro, voor het bedrag van de adviesprijs koop je liever een 2e hands kaart, dat is altijd zinniger dan dit ding, zeker met werkelijke winkelprijzen van 350 tot 400 euro, dat is gewoon waanzin en weggegooid geld."
RX 6500 XT;2;0.3499660789966583;Ik was aan het wachten op benchmarks. Maar bij deze de druppel, ik stap over naar console om te gamen. Lekker gamen, nergens omzien naar en initiele aanschaf investering 300 tot 550 euro ipv 1500 euro voor degelijke PC. Dan nog misschien het belangrijkste, totaal systeemverbruik van 60 tot 200 watt. Ivp 300 watt of meer bij een Game-PC. Als je dat ding een paar honder uur per jaar gebruikt, telt het ook aardig mee. Ik ga niet blijven wachten ! Zie ook niet meteen verbetering in de situatie, misschien verbetering op het einde van dit jaar (maar worst case pas zelfs 2023 )
RX 6500 XT;2;0.4732075035572052;Als het op prijs aankomt, ja, dan heb je gelijk. Maar het argument van stroomverbruik klopt niet, omdat moderne consoles gewoon dezelfde hardware gebruiken als een game-PC. Een PS5 gebruikt een Zen2-octacore, een modernere 5600X zal typisch juist efficiënter zijn in termen van stroomverbruik. De GPU van een PS5 is vergelijkbaar met de RX6600XT of RX6700XT. En een game-pc met een 5600X en RX6600XT verbruikt echt geen 300W, het zal niet ver boven het verbruik van een PS5 zitten. Hooguit is console-hardware iets lager geklokt om het stroomverbruik laag te houden maar dat kun je bij een PC prima tweaken. En als we dan even op de RX6500XT terugkomen, die is juist absurd hoog gelokt met inderdaad een dramatisch energieverbruik voor de matige prestaties die je ervoor terugkrijgt.
RX 6500 XT;1;0.4800666570663452;De XBOX Series X doet 180 a 200 watt (bij XBOX One games 80 watt) in full load (de Playstation vergelijkbaar). De XBOX Series S doet maximaal 80 watt wat je ook speelt. Vooral dat laatste haal je met een desktop nooit. De 6500 XT is een product dat nooit had mogen bestaan in deze vorm. Verziekte markt, maar zelf kan ik het AMD niet kwalijk nemen. Ze kunnen er waarschijnlijk veel van produceren en willen gamers te hulp schieten. De praktijken die Nvidia nu doet zijn nog erger.
RX 6500 XT;3;0.434935986995697;Klopt, omdat die wel flink wat GPU/CPU cores heeft maar die gewoon heel laag klokt. Dat is een recept voor goede prestaties met toch een laag stroomverbruik. En voor relatief hoge hardware-kosten voor de prestaties, maar dat wordt flink gecompenseerd doordat MS en Sony natuurlijk fantastische bulk-contracten kunnen afsluiten zonder extra OEM's ertussen. Als je een 5600X + RX6600XT flink underclockt en undervolt zou je wel eens heel dicht bij het verbruik van een XBOX series S kunnen komen. Dat zie je alleen nooit in desktops. Wel in laptops, en die zijn niet geheel toevallig ook erg energiezuinig voor hun prestaties vergeleken bij desktops.
RX 6500 XT;2;0.3817170262336731;Was het nou een single slot kaart was geweest zou het opzich kunnen als een upgrade voor een 1030. Maar helaas.
RX 6500 XT;4;0.31677791476249695;Een GT1030, daar noem je mij ook wel wat. Als je echt een redelijke upgrade wil hebben, kijk dan eens in het rode kamp voor een single slot RX550/560. Prestaties zijn bijna 2 keer zo snel, terwijl ze uit hetzelfde tijdperk komen en vrijwel een gelijkwaardige prijs hadden. Het enige wat in het voordeel is van een GT1030 is dat het de goedkoopste kaart is die hogere resoluties op veel relatief hertz kan weergeven.
RX 6500 XT;3;0.32061606645584106;Je moet toegeven dat daarvoor een niche zou zijn.
RX 6500 XT;1;0.92917400598526;Echt belachelijk dat deze kaart slechter dan een 570 draait op PCIe 3.0 Het is een budget kaart maar met een CPU ouder dan een jaar is het een stront kaart want je hebt geen PCIe 4.0 en zonder PCIe 4.0 is dit een zeer slechte koop.
RX 6500 XT;1;0.5642527937889099;Je leest de review niet eens, Tweakers heeft ook op PCI-E 3.0 getest.
RX 6500 XT;1;0.4908613860607147;Hij zegt toch nergens dat tweakers dat zou moeten doen/niet gedaan zou hebben?
RX 6500 XT;2;0.3528013825416565;Zijn bericht was vervolgens snel aangepast. Er stond eerder dat de kaart niet een op PCIe 3.0 is getest en dat wel had moeten gebeuren.
RX 6500 XT;1;0.24791181087493896;Asus Dual RX 6500 XT € 349,- bij Megekko
RX 6500 XT;5;0.46006226539611816;Lekker weer terug naar 1280x720…
RX 6500 XT;5;0.4227299690246582;Wat ben ik blij dat ik een 6800 heb kunnen scoren voor mrsp
RX 6500 XT;4;0.4487106502056122;Grappig kaartje voor Candy Crush. Ik bied 1000 euro
RX 6500 XT;1;0.8323891162872314;Waarom is de kaart zo groot? Lijkt hij expres op een High-End videokaart zodat ie beter verkoopt? (stunt van Gigabyte?) Dit had toch makkelijk een single-fan mini-videokaartje kunnen zijn? En die 4x PCI-e lanes begrijp ik ook niets van. Helpt dat nou werkelijk de kosten te drukken? Wat een belachelijke kaart van AMD, ziekelijk. Volgens mij is de werkelijke waarde in productie nog niet eens 100 Euro, zullen ze lekker winst op maken.
RX 6500 XT;1;0.40679213404655457;Klopt het dat de totale prestatiescore op 1080p medium verkeerd staat? (pagina 14). In vrijwel de meeste games is de 5500XT individueel telkens hoger op die setting dan de 6500XT, maar de totaalgrafiek geeft het omgekeerde beeld aan.
RX 6500 XT;1;0.4990001618862152;Wat een grap is AMD soms… na het zien van dit zou ik er geen ene euro voor neerleggen. Schofterig bedrag Ik weet dat de meeste tweakers pro AMD zijn maar ik neig echt meer naar Intel/Nvidia voor mijn toekomstige build
RX 6500 XT;1;0.5212694406509399;Hoewel ik best wel fan ben van wat AMD het afgelopen jaar heeft neergezet (zeker performance per watt) is dit gewoon echt een lachtertje...
RX 6500 XT;1;0.5236481428146362;Ik heb een paar jaar geleden ooit een 6GB 980 Ti gekocht van MSI om op 1080p ultra settings te gamen (wat nog steeds geen enkel probleem is met de allernieuwste games) en ik ben gewoon shocked hoe de nieuwste kaarten die juist voor 1080p gemaakt zijn anno nu gewoon onderpresteren. Alsof de markt is achteruit gehold.
RX 6500 XT;1;0.5050060749053955;Is AMD soms bezig om zo straks hun nieuwe amd ryzen 7 5800X3D beter voor de dag te kunnen laten komen? Ik ben nieuwsgierig of 'toevallig' deze CPU de beperkingen verzacht. Als deze GPU er niet van profiteert kan deze samen met 5800X3D in het bakje onsuccesvolle AMD producten van vroegah.
RX 6500 XT;1;0.5462817549705505;Hoe moet een CPU geheugen- en bandbreedte-bottlenecks wegnemen? Dat is ronduit onmogelijk. Het enige wat een 5800X3D toevoegt t.o.v. bestaande CPU's is extra CPU-cache. Dus nee, ook met een 5800X3D blijft dit ding gewoon ruk.
RX 6500 XT;3;0.3261021375656128;Die cache zou het verkeer tussen de videokaart en de CPU beter kunnen bufferen en dat lijkt me extra nuttig voor een videokaart die zo 'ruk' is als de 6500XT (Het was dan ook geen poging om AMD te verdedigen... juist niet) Het zou iig de mooie performance boost van de 5800X3D die AMD in z'n eigen cijfers laat zien kunnen verklaren. nieuws: AMD Ryzen 7 5800X3D krijgt 64MB aan 3D V-Cache voor betere gamepresta...
RX 6500 XT;2;0.4056822657585144;De videokaart heeft niks aan bufferen in de CPU van verkeer vanaf de CPU. Een high-end CPU levert zijn data snel genoeg aan om geen bottleneck te zijn. De bottleneck ligt puur bij het ophalen van data die niet in het geheugen past en die te traag door de weinige PCIe-lanes komt, de CPU kan daar in geen enkel opzicht iets tegen doen. De performance-cijfers die getoond worden over de 5800X3D slaan op de situatie waarin de CPU de bottleneck is. Dat is een combinatie 5800X + RX6500XT zeker niet het geval. Tenzij je dwarf fortress speelt.
RX 6500 XT;3;0.3848452866077423;Ik weet niet wat je achtergrond is, maar ik geloof dat je de computer architectuur van een PC niet helemaal snapt, Het renderen mag dan wel op de GPU zelf gebeuren, maar heeft wel degelijk te leiden onder een verder 'stroperig' systeem. Als dit het geval zou zijn dan was er ook geen '3D cache' nodig op de CPU. Door middel van cache de communicatie met de CPU te verbeteren met de GPU en het systeemgeheugen loopt simpelweg het gehele systeem soepeler.
RX 6500 XT;2;0.4981646239757538;"En ik weet niet wat jouw achtergrond is, maar deze redenering slaat als een tang op varken. CPU cache is helemaal niet specifiek gericht op een grafische pipeline, dus de conclusie dat een 3D cache niet nodig zou zijn als een GPU er geen winst uit haalt slaat nergens op. Algemene programma-instructies die niet voor graphics bedoeld zijn (algemene logica, AI, netwerkafhandeling) heeft allemaal baat bij extra CPU-cache, en dat is waar 3D-cache winst oplevert. Het ""3D""-gedeelte slaat dan ook puur op de fyieke locatie van de cache op de chip, functioneel is het gewoon standaard CPU-cache, die in zijn algemeenheid helemaal niet op graphics gericht is en waar de GPU helemaal niet direct bij kan. Een GPU heeft pas te leiden van een stroperig systeem als die zijn instructies/data niet snel genoeg aangeleverd krijgt, niet meer en niet minder, en dat is bij een 5800X gewoon niet het geval. Hoe veel cache er in de CPU zit is voor de GPU geheel irrelevant, net zoals hoeveel cores hij heeft, wat zijn kloksnelheid is, etc. Zolang de GPU maar snel genoeg instructies en data krijgt om er niet op te hoeven wachten. Maar los van over hoe dingen technisch in elkaar zitten: trek gewoon eens een CPU-benchmark open waarin de GPU de bottleneck is. CPU's met veel of weinig cache, met veel of weinig cores, met hoge of lage kloksnelheid: alles levert gewoon even veel FPS. Bijvoorbeeld deze, en dan vooral de hogere settings/resoluties. 3D-cache is geen magische technologie die dat even gaat veranderen. Om het maar even met een populaire auto-vergelijking te illustreren: als een er file staat op een tweebaanssnelweg omdat die te krap is, dan lost het verbreden van de opritten weinig op. De bottleneck van de RX6500XT ligt duidelijk bij het weinige geheugen en bij de krappe bandbreedte op de PCIe-interface."
RX 6500 XT;2;0.39300400018692017;Lees het artikel over de 5800X3D nou eerst eens, want je bakt er niet veel van met je lange niet kloppende verhaal. Na dat je het gelezen hebt en eerst dus de onzin uit je reactie gehaald hebt reageer ik misschien nog wel een keer. Tip : je vergeet iig het scenario waarbij zowel de GPU als de CPU bij het systeemgeheugen moeten.
RX 6500 XT;2;0.4896051585674286;"Ook in dat geval gaat ""het verkeer tussen de videokaart en de CPU [...] bufferen"" geen performancewinst geven. Verkeer van de CPU naar de GPU moet hoe dan ook via het RAM, een GPU kan niet direct bij de CPU-cache dus het bufferen daarvan in de CPU-cache zorgt alleen dat de CPU wat minder lang hoeft te wachten - maar de CPU is bij zo'n low-end kaart nou net niet de bottleneck voor de framerate. Die CPU mag best even wachten, dat zorgt er niet voor dat de GPU zijn data sneller of trager krijgt. In theorie kan het RAM wel de bottleneck worden als zowel de GPU als de CPU veel tegelijk uit het RAM moeten lezen (dus juist bij het cachen van ander geheugen dan wat er van de CPU naar de GPU moet, want dat moet hoe dan ook via het RAM) en dat zou je met extra CPU cache inderdaad kunnen verminderen. Maar of wat extra CPU-cache het RAM dermate veel zal ontlasten dat dat merkbaar is in de benutte bandbreedte naar het RAM vraag ik me af. Oh, en ik heb het artikel nog eens snel doorgespit, maar daar staat niks specifieks in over het bufferen van verkeer tussen CPU en GPU, dus je mag even wat specifieker zijn. Verder kun je beter gewoon wat preciezer zeggen wat je bedoelt en meteen met je ""tip"" komen in plaats van eerst te klagen over mijn achtergrond. Zeg gewoon wat er volgens jou niet klopt en waarom niet, in plaats van dat de ander ernaar moet graven, want anders kan ik het moeilijk anders ontkrachten dan door een lange uitleg te geven van wat ik bedoel. Algemene stellingen als ""Het renderen mag dan wel op de GPU zelf gebeuren, maar heeft wel degelijk te leiden onder een verder 'stroperig' systeem"" zijn in hun algemeenheid simpelweg fout omdat het voorbij gaat aan het hele concept van bottlenecks, die mij de kern lijken van computer-architectuur. Component A heeft in zijn algemeenheid helemaal geen last van de stroperigheid van component B als B geen bottleneck is."
RX 6500 XT;2;0.4523850083351135;Het zijn zulke lange verhalen met als je het met wat in het door mij verkondigde artikel verkondigd wordt vergelijkt het gewoon soms gewoon letterlijk tegenspreekt wat in dat artikel staat over de 5800X3D. Wat mijn probleem hier is... aan de ene kant lijk je over een hoop populaire technologie kennis te beschikken , maar aan de andere kant lijk je elementaire kennis over het functioneren van cache binnen een computersysteem te ontberen. Cache voorkomt juist vaak concurrentie tussen computer resources en daar mee dus de bottlenecks, Daar kan ik een lange discussie overgaan voeren, maar als je dus elementaire kennis mist heeft dat weinig zin. En bedankt : je hebt me nu wel genoeg overtuigd dat dit het geval is en met dus name dat je zelfs mijn Tip niet goed snapt
RX 6500 XT;2;0.48406603932380676;""" cache "" voorkomt juist "" vaak "" concurrentie tussen "" resources "" is nogal vaag. zonder specifiek te kijken over welke bottleneck precies en welk component nou waarom aan het wachten is dit wederom gewoon veel te algemeen. component a wordt niet magisch sneller omdat b wat extra cache heeft, dat kun je pas stellen als je de hele data flow van a naar b bekijkt, waar die datadoorvoer nou stokt, en waarom een cache op die precieze plek dat precies oplost. en we hebben het hier over een specifiek geval : een low - end gpu met als bottleneck te weinig geheugen en weinig geheugenbandbreedte, met daarnaast een cpu die veel meer fps aan kan. wederom : pak er gewoon eens wat benchmarks van verschillende gpu ' s bij waarbij steeds dezelfde bottleneckende gpu gebruikt wordt. resultaat : bij elke cpu precies dezelfde fps. meer cache in je cpu ( bijvoorbeeld een 5700g t. o. v. 5800x )? maakt geen bal uit. dat extra cpu - cache helpt op 1080p als je er een gtx3090 naast zet, ja, goh. andere bottleneck, ander verhaal. vage en veel te algemene stellingen als "" cache voorkomt juist vaak concurrentie tussen computer resources en daar mee dus de bottlenecks "" en "" het renderen mag dan wel op de gpu zelf gebeuren, maar heeft wel degelijk te leiden onder een verder ' stroperig ' systeem "" geeft eigenlijk niet echt een beeld alsof je zelf ooit wel eens tegen een bottleneck aan hebt zitten programmeren / engineeren. je hebt altijd ergens een bottleneck. die haal je niet weg door als in een shotgun - approach gewoon maar op willekeurige plekken wat caches tussen te plakken. oh, en ik werk genoeg met micro - optimalisaties gericht op cpu - caches en het daarbij meten van performance - effecten, dank je hartelijk voor je overtuiging."
RX 6500 XT;2;0.3620828092098236;Het is een verdorie een reactie op tweakers... ik probeerde het nog een beetje kort te houden. Get a life.
RX 6500 XT;1;0.5852874517440796;"Nogmaals: in je gelinkte artikel staan welgeteld twee alinea's over performance, en er is geen enkele indicatie dat dat over de situatie met een GPU-bottleneck gaat, dus spreekt helemaal niks tegen wat ik zeg over een 5800X3D i.c.m. met een RX6500XT. Dan kun je wel blijven hameren op ""discussie voeren heeft weinig zin"" en ""je mist kennis"" maar je geeft niet eens aan wat je nou bedoelt. En als die ""reactie op tweakers"" niet klopt, mensen leggen dat uit en je blijft reageren met ""je snapt het niet"" en ""je bakt er niks van"" terwijl je op geen enkel moment zegt waarom dan niet dan wordt een discussie vast heel kort, ja. Komt er zo nóg een reactie terwijl je het zelf zo kort wil houden, omdat je toch eigenlijk echt wel het laatste woord wil hebben?"
RX 6500 XT;3;0.34583863615989685;"nog maar een poging nu ik wat meer tijd heb en je verhaal wat concreter vind : het artikel heeft specifiek over ' game prestaties in 1080p ' dan klopt het dat ' 3d ' niet op de rendering slaat of ook maar iets met de interne werking van de gpu te maken heeft. daar neem je dus ook eigenlijk aan dat ik die kennis niet zou hebben terwijl ik zelf met het artikel met de uitleg daarover kom wat dat 3d inhoudt. dat verhaal over de piperendering hoeft ook helemaal niet gezien mijn eerdere opmerking dus over de langzamere pci - e bus. maar genoemde verbeterde gamesprestaties worden wel door amd afgemeten aan de fps van de videokaart en die zijn volgens amd zelf in het artikel beduidend beter met dus extra cache bij de cpu. als je daarbij dus een cpu en een gpu hebt die concurreren op de bandbreedte van het geheugen en de gpu meer tijd nodig heeft om in het geheugen zijn data op te halen door de langzamer pci - e bus is het daar belangrijk dat de cpu gegevens niet op hetzelfde moment moet ophalen om door te kunnen blijven werken. ik zeg geen schokkende dingen ofzo en toch probeer je ze te ontkennen? waarom? het leek iig eerst dat je er geen bal verstand van had ( omdat je oa de resultaten van amd tegensprak ) en ik me echt afvroeg : waar moet ik beginnen met uitleggen? ik ben geen leraar informatica iig en heb nog steeds moeite te geloven dat je "" met micro - optimalisaties gericht op cpu - caches en het daarbij meten van performance - effecten "" in een professionele organisatie werkt, maar goed dat is mijn probleem. mijn eerste opmerking was er vooral eentje in de trend van ' ik ben benieuwd welke marketing stunt amd nu weer uithaalt ' daarom was mijn enthousiasme om hier inhoudelijk diep op te reageren niet zo groot. het is even afwachten tot daadwerkelijk meer bekend is over de 5800x3d en met welke videokaart de gaming! benchmarks zoveel resultaat opleveren."
RX 6500 XT;2;0.39456477761268616;Aangezien AMD die cijfers gebruikt om de 5800X3D in een goed daglicht te zetten hebben ze er vooral baat bij om de bottleneck bij de CPU te leggen, want met een trage GPU ga je minder tot geen verschil zien. Dat is gangbaar bij elke CPU-benchmark op game-prestaties. Dus het lijkt me erg onwaarschijnlijk dat het met een RX6500XT gemeten is, bij de gemiddelde CPU-benchmark wordt de meest overkill GPU genomen die er voor handen is. Die geclaimde performance-winst is dan ook veel simpeler te verklaren: een overkill videokaart op een lage resolutie produceert met gemak honderden FPS en in de gemiddelde game houdt de CPU dat niet bij, dus zie je verschil tussen snelle en trage CPU's. En een CPU met meer cache levert gewoon betere prestaties wegens minder cache misses, dus meer FPS.
RX 6500 XT;2;0.39877989888191223;Nee er worden geen absolute resultaten gegeven. Alleen procentuele winst in performance tov 5900x (dus juist de snelste AM4 processor) en wordt gezegd dat ie veel sneller in games is als de 12900K (zie het artikel) Dat zou ook een goede verklaring kunnen zijn , maar sluit het andere niet uit. We zullen zien het als er meer bekend is.
RX 6500 XT;1;0.2823807895183563;Hmmm even gekeken op de preisvergeleich in Deutschland : Ook 350 euro daar pfff
RX 6500 XT;1;0.5450959205627441;Kadootje van The Underdog voor de trouwe Underdog-fanboys fans, die blijven geloven dat alles wat AMD aanraakt in goud verandert. 4 highend Cpu's Ryzen (5600x en hoger) , 2 higher end APU's (G5600 en G5700) en wel 1 aankomende CPU met Vcache (zonder GPU onboard welteverstaan) dat waarschijnlijk voor de meesten een papieren launch wordt. En maar teren op het feit dat er wel goedkope AM4-moederborden zijn. De toekomst van AMD blinkt! Oh ja... Next-Gen Zen 4 Ryzen komt ook ergens in de tweede helft van 2022! Betaalbaar en upgradable
RX 6500 XT;2;0.37271648645401;Rondom de verslaglegging en benchmarking van de 6500 XT hoor ik eigenlijk niemand praten over APU's, ofwel CPU's met integrated graphics. Misschien is dit geen populaire vraag onder tweakers en computer hardware liefhebbers, maar zal de low-end discrete GPU langzaam verdwijnen en door steeds sterkere APU's vervangen worden?
RX 6500 XT;2;0.47811374068260193;"Specifiek als het over deze kaart gaat: jazeker. Je ziet nu al dat een kaart van het kaliber GT1030 compleet nutteloos is omdat je net zo goed een PC kan bouwen met 5600G/5700G, die zijn even snel. De 6800U en varianten hebben 12 compute cores in plaats van de 16 van deze kaart, met dezelfde RDNA2-architectuur. Daarbij heeft zo'n low-end videokaart naastzetten dus echt nauwelijks zin. Nou is de RX6500XT wel een stuk hoger geklokt, maar dat zorgt wel even voor een (relatief) absurd hoog stroomverbruik met weinig winst dus dat kun je ook moeilijk toegevoegde waarde noemen ten opzichte van een 6800U. Zen4 komt aan het einde van het jaar ook met RDNA2-IGPs, en het lijkt me dat die dan ook wel minstens 12 compute cores krijgt. Natuurlijk krijgen we bij de komst van sterkere IGP's ook sterkere videokaarten, dus het concept ""low-end discrete videokaart"" schuift misschien natuurlijk gewoon op naar alles boven de IGP. Wat dat betreft is het interessanter of game-ontwikkelaars hun games licht genoeg maken om op een IGP te draaien."
RX 6500 XT;3;0.43978026509284973;Het is zeker waar dat discrete GPU's natuurlijk (gemiddeld gezien) ook sterker worden met elke generatie. Als ik kijk naar de PS5 en XBox series X|S, dan zitten daar volgens mij custom APU's in met 32-56 grafische compute units. Dat is een aardige stap boven de 12 compute units van de 6800U. Wellicht verwacht AMD dat er geen markt is voor APU's die zo sterk zijn, of lopen ze spaak met Sony/Xbox als ze losse APU's met dergelijke performance verkopen? Of misschien zie ik een technische limitatie over het hoofd waardoor het (nog) niet van de grond komt?
RX 6500 XT;2;0.4220949113368988;Die GT1030 is in sommige spellen nog steeds sneller dan de 5600G hoor en een processor als de 12400F is al 60 euro goedkoper dan de 5600G. Dus ja, nu is een 5600G nog aan te raden, maar er is nou ook weer niet zoveel nodig in prijsverandering om een GT1030 + Intel 12400F combo nu al 'compleet nutteloos' te gaan noemen. Sterker nog : als je nu een Ryzen 3x00G cpu zonder losse GPU hebt is het de goedkopere keuze voor een gamer upgrade dus ca 100 euro neertellen voor de GT1030. 245 euro is dan wel erg veel meer. Een ander voordeel is eventueel voor een woonkamer media/emulator pctje de hoeveelheid geluid de fans maken onder load. De GT1030 is er in een fanless versie. De iGPU zal extra hitte op de CPU veroorzaken en dus daardoor de CPU fan harder laten draaien.
RX 6500 XT;1;0.3003401756286621;Als dit het is wat je voor 350 euro kan krijgen ben ik wel klaar met pc gaming. Er is gewoon geen upgrade path dan van een 1060 rond die prijs.
RX 6500 XT;1;0.699974536895752;Belachelijk en te duur voor wat hij kan. Moet je voorstellen dat je vroeger voor deze bedragen al bijna een High-End videokaart had en nu een instapmodel krijg dat vroeger rond de €100,- zat. Bijvoorbeeld in 2013 kocht ik de Sapphire Radeon R9 290, 4096GB (4GB) voor €355,- een High-End videokaart dat bijna de Titan versloeg van nVidia van €999,-.. Dat waren nog eens tijden. Ik had ook ooit een High-End kaart van ASUS de Asus ENGTX480/2DI 1536MB, 2xDVI, mHDMI, PCIe voor €485,-. Ik wil er alleen maar mee aangeven dat het nu enorm duur is geworden en nu voor het zelfde bedrag een instapmodel krijgen.
RX 6500 XT;5;0.5339108109474182;Heb nu nog steeds 2 x 290x kaarten in gebruik. Verbaasd me nog wat ze kunnen. Forza 5 is gewoon te spelen al dan wel op medium 1080P.
RX 6500 XT;5;0.42269134521484375;Wat gaaf dat jullie de oude quotes van AMD er nog even bij hebben behaald. In dat soort dingen zie je het vakmanschap van een goede journalist! En het maakt een review ook een stukje sappiger :-)
RX 6500 XT;2;0.31895479559898376;Dat ze dit een laptopchip in desktopkaart durven noemen... Is meer GPU pyramid scheme in volle gang.
RX 6500 XT;5;0.36287274956703186;"Nog even en 2e hands 2 750ti's opsporen en die in SLI laten draaien wordt de beste optie ;-) (Nee ik heb niks in V&A staan) Ik zou niet weten hoe dat dan loopt met PCI-E power draw, maar daar zouden handige jongens vast ook wel iets op kunnen vinden? Vergeet niet : een single 750ti is ook sneller dan bv een GT1030 of 5600G. (Als je de textures onder de 2G houdt natuurlijk) Wel mis je bepaalde codecs ed en zal ie dus waarschijnlijk geen moderne digitale copy protectie van online streams aan kunnen, maar als zelfs de 730 een comeback maakt zou juist een van de meest succesvolle kaarten toch ook soelaas kunnen bieden voor de casual gamer?"
RX 6500 XT;3;0.2893105149269104;Ja zo werkt dat. Aanbod bepaald de prijs kun je aanbod maken op welke manier dan ook zal het geld op leveren.
RX 5700 XT;2;0.45877212285995483;Kan iemand mij uitleggen wat het nut van de duurtest is? Ik begrijp waarom het interessant is, als het zinvol gebracht wordt. Maar nu zien we de kloksnelheden na een uur. Ja joepie, en dus? Wat relevant zijn is de framerates na een uur. Er staat dat het elke seconde gelogd wordt, leuk, maar misschien ook die resultaten dan delen? Kunnen we ervan uitgaan dat in de benchmarks hij permanent op zijn boost clock draait? En gezien die een 8% hoger is dan de klok na een uur, dat we dus 8% van alle benchmark resultaten moeten aftrekken? En ik begrijp wel dat alles langer stressen een heel karwei is, maar ik heb zelf liever minder benchmark resultaten als die dan wel accurater zijn, een 8% verschil is nogal significant. Of worden de benchmarks bij jullie testopstelling zo snel achter elkaar gedraaid dat hij wel warm blijft? Want als ik de Super review erbij haal (reviews: Nvidia RTX 2060 en 2070 Super - Meer waar voor je geld dan begrijp ik uit de bewoordingen daar dat die wel stabiele klokfrequenties heeft. En als ze dus allemaal benchmarks krijgen nadat ze zijn afgekoeld, dan zijn de resultaten hier dus gewoon 8% te hoog vergeleken met de echte resultaten (waarbij ik de echte resultaten zie als wat je krijgt als temperatuur stabiel is geworden, immers maakt niemand het uit dat een kaart de eerste 5 minuten sneller draait). Oké eerlijk is eerlijk, als de AIBs komen met fatsoenlijke koelers dan is dit probleem er hopelijk niet meer, maar ik vind dat er wel meer aandacht aan besteed had mogen worden. Overigens andersom: Als benchmarks zo snel achter elkaar worden gedraaid dat de kaart warm blijft, dan zou een kaart met fatsoenlijke koeler dus een 8% sneller moeten zijn.
RX 5700 XT;2;0.519751787185669;Ik heb de tekst bij de duurtest nog wat aangevuld. De boostclocks die AMD opgeeft worden in deze praktijktest helemaal niet gehaald. AMD geeft ook zelf aan dat dit waarden zijn die alleen onder 'optimale omstandigheden' gehaald worden. Dat is ook de reden van de introductie van de Game Clock. Dat is de snelheid die je in de praktijk moet verwachten en dat blijkt ook te kloppen. Gedurende de hele duurtest zijn er weinig grote schommelingen. De eerste paar seconden liggen de kloksnelheden iets hoger, maar dat is geen 8 procent. De 5700XT begint op 1817MHz en zit na een paar seconden al rond de 1800MHz. We doen de duurtest om te kijken of er extreme verschillen in de kloksnelheden optreden bij langdurige belasting. Dat is hier dus niet het geval.
RX 5700 XT;3;0.48420390486717224;Nuttige toevoeging, bedankt daarvoor. Dan weten we iig dat de benchmark resultaten wel redelijk nauwkeurig zouden moeten zijn voor ook als je een uur aan het gamen bent. Dan toch wel een verzoek voor toekomstige reviews (of deze al ): Dat is natuurlijk niet heel interessant voor ons als lezer, als jullie vervolgens alleen de laatste (gemiddelde) waarde geven. Zou een grafiekje niet veel interessanter zijn? Van temperatuur, fanspeed en klokfrequentie? Vergelijkbaar met bijvoorbeeld:
RX 5700 XT;3;0.3876454830169678;Dat is relevant om te zien wat er gebeurd als de kaart goed wam loopt en of er dan throttling komt of niet, ik game meestal toch wel langer dan een uur en als een kaart na een uur (te) warm loopt heb je een probleem.
RX 5700 XT;2;0.4569360911846161;Uiteraard begrijp ik het idee erachter, maar de getallen zijn nietszeggend. We zien niet hoe het over de tijd gaat, we weten niet in welke situatie de benchmarks zijn gedraaid, en we weten niet welke invloed het op de framerate heeft.
RX 5700 XT;2;0.5455145835876465;Met wat software gooi je simpelweg gewoon de fan speed wat omhoog en de kaart zal nooit aan zijn max temp komen. Mijn 980ti draait rond de 80 graden als ik de fan speed van 23 naar 50% zet dan maakt de kaart weliswaar iets meer geluid maar de videokaart komt hierbij nooit meer boven de 60 graden ook niet na 4 uur spelen. Heb het overigens altijd raar gevonden dat videokaarten met stock settings altijd grijpen naar down throttle en nooit naar de fanspeed ophogen om koel te blijven. Voor constante max performance zal echter dus altijd handmatig aan de fanspeed gedraaid moeten worden
RX 5700 XT;3;0.4762724041938782;Toch jammer dat AMD wéér voor het middensegment gaat en dat ze steeds nét niet bij die top komen. Ze proberen nu met de 5700XT de 2070RTX te verslaan maar ondertussen heeft Mvidia nog 2 topmodellen daarboven zitten. Geen hardwarematige raytracing lijkt nu nog geen groot probleem maar een hoop toekomstige spellen krijgen daar wel ondersteuning voor. Cyberpunk 2077, bijv. en daar lijkt me het met al dat neonlicht een mooie toevoeging en dat je dan nog maar 60-75 FPS haalt.. zolang het maar sneller is dan de refreshrate van de monitor, prima toch? Het is het bij AMD vaak nét niet, erg jammer want ik ben opzich wel een AMD fanboy en ik zou graag zien dat ze de concurrent crushen, lukt ze nu met Intel oo redelijk mar Nvidia is kennelijk nog een stap te ver.
RX 5700 XT;2;0.37189579010009766;"Er wordt schijnbaar nog aan een 'grote' Navi variant gewerkt, die in het hogere segment de strijd moet aangaan. Maar het gros van de markt koopt gewoon geen 700-1200 Euro videokaarten (en terecht, wat een absurde prijs..). Begrijpelijk dat AMD zich daarop richt dan; dat is een veel groter deel van de markt. Buiten dat, heeft AMD hiermee zo te zien degelijke kaarten neergezet. Een betere value proposition dan de nieuwe Super kaarten - goed voor de consument. Éigenlijk zijn ze nog steeds te duur voor 'midrange' videokaarten, maar goed."
RX 5700 XT;3;0.38047659397125244;Alles is relatief. De prijsrange die je noemt is toevallig ongeveer ook die van high end telefoons. Dan heb ik meer en veel langer plezier van een videokaart(telefoon is 600 uiterste max voor mij). Goedkoper is altijd fijner natuurlijk, maar ik vind het het geld dubbel en dwars waard. Maar ieder heeft daar zijn eigen mening ovet natuurlijk en dat is prima.
RX 5700 XT;2;0.4562873840332031;Uiteraard, je hebt volledig gelijk. Het is alleen jammer om te zien dat de pricerange zo verschoven is, van €200-ish midrange met €500-ish high end naar €400 midrange en €700-1200 high-end. Dit is puur het resultaat geweest van het feit dat AMD geen competitie kon bieden, en nu het de nieuwe status quo is, gaat AMD helaas ook mee hierin.. @EMR77 : Klopt, je laat je stem horen als consument met je portemonnee. Aan de andere kant, NVidia's GPU-omzet is gekrompen met bijna 50% als gevolg van teleurstelling in de RTX kaarten. Er is dus wel iéts gebeurd, maar te weinig.
RX 5700 XT;2;0.44186681509017944;Ik wens AMD ook competitie op de high end gebied toe, dat is beter voor ons allen(nu heb ik helaas geen keuze). En ik hoop dat Intel volgend jaar ook met een knaller komt. En bij telefoons de prijs het net zo hard verschoven, daar was 1200 euro 5 jaar geleden ook ondenkbaar...
RX 5700 XT;2;0.506025493144989;Dat is het nu bij veel merken nog steeds hoor. Er is maar een handjevol uitschieters en je ziet dat die ook redelijk afgestraft worden. De verkopen bij Apple dalen niet voor niets. Er is genoeg betaalbaar alternatief en 200-300 toestellen zijn nog nooit zo snel en capabel geweest. De huidige 200-300 euro toestellen concurreren soms zelfs met de high-end toestellen van een jaar eerder. Op GPU vlak zijn er maar twee spelers, waarvan ze nu beide een flinke prijs vragen en de segmenten hebben lopen rekken. Er was voorheen troep, midrange en high-end, nu zijn er veel meer tussensegmenten te vinden die allemaal veel duurder zijn. Ooit kocht je voor 200-300 euro een high-endje met iets minder renderdingen (HD7950 bijv.) en nu zijn er bij nvidia alleen al 5 alternatieven voor de duurste kaart. Bij telefoons is er geen verschuiving, maar gewoon een paar merken die gek geworden zijn. Bij videokaartjes is er daadwerkelijk een verschuiving veel duurdere kaarten.
RX 5700 XT;1;0.6212844252586365;Maar de consumenten bleven massaal overpriced kaarten kopen van Nvidia Waarom zo AMD niet meedoen??? De consumenten hebben laten zien, dat het ze niet uitmaakt. Anders was het marktaandeel van Nvidia allang gekrompen.
RX 5700 XT;3;0.48075613379478455;Leuk dat je het met een telefoon vergelijkt maar dat is een compleet product, een videokaart is nogal saai zonder verdere hardware. Ondanks dat ben ik het wel met je eens hoor...
RX 5700 XT;3;0.35170549154281616;Succes dan met je telefoon zonder abo... Denk dat het prima te vergelijken is. De dure telefoon is net zo 'bovenop een simpele PC' als de videokaart dat is, toch? Die simpele PC of simpele telefoon heeft vrijwel iedereen, ga je duurder dan doe je dat heel bewust en is het een luxe artikel. De vraag is of je de meerwaarde ziet. Ik zie die totaal niet in een toestel van over de 300 euro. Anderen zien het niet in een GPU van over de 300 euro.
RX 5700 XT;2;0.37038177251815796;De 5700XT/2070RTX Super is geen mid-range. Dat is het begin vd high-end en zo'n kaart koop ik altijd, mits ze onder de €500 blijft. Dat doet de Super kaart niet maar de AMD wel. De AMD-kaart is wel iets trager maar veel scheelt het niet. Edit: ofwel heb ik een verkeerd beeld van wat mid-range en high-end is. Voor mij is mid-range Nvidia 2060 S/5700 en high-end begint van 5700XT/RTX 2070 S tot RTX 2080Ti. Als iemand het anders ziet zegt hij het maar.
RX 5700 XT;3;0.40200555324554443;Ik snap AMD wel, het grote geld zit nog altijd in het middensegment en als ik zou willen concurreren met Nvidia zou ik ook in dat segment beginnen. En het verschil tussen 60-70 FPS en bijv. 140 FPS is écht wel merkbaar, ook op een 60 Hz monitor. Ik vind het zelfs zo merkbaar dat ik liever op low speel met hoge FPS dan op high met 60 FPS.
RX 5700 XT;4;0.24137257039546967;Natuurlijk is er meer geld te verdienen in het middensegment maar qua als AMD meteen ook Nvidia van de troon weet te stoten qua snelste videokaart dan is dat wel erg goede reclame. Amd komt nu met een kaart die net zo snel is als de 2070 maar geen hardware raytracing heeft. Nu hoeft die raytracing je verder niet te boeien maar de mogelijkheid om het te gebruiken is er in elk geval. Zoals je nu ook bij de CPU's ziet, daar weet AMD Intel wel van de troon te stoten met een CPU die sneller is dan de 9900K en iedereen is gehyped over de Zen2 processor. Dat hadden ze ook met AMD moeten doen, meteen die 5800X en 5900X op de markt gooien, ook al kost die €700,- Als de snelheid beter is dan een 2080 (ti) dan is het dat geld ook waard en krijgt AMD een hoop gratis reclame.
RX 5700 XT;2;0.3208288848400116;Als de geruchten kloppen komen er nog 56 en 64 cu versies van NAVi (zoals vega) en als ze de problemen met hun GCN arch. hebben opgelost, vermoed ik dat de 64 zeker sneller is dan de 2080S.
RX 5700 XT;2;0.3743891417980194;Heb je hier wat meer info over? Ik was hiervan niet op de hoogte. En ben ook benieuwd welke problemen er zijn met de 64.
RX 5700 XT;2;0.5147448182106018;Bij de 64 konden ze blijkbaar niet al de Cores bezig houden waardoor je dus performance problemen hebt. Ook de overhead van de waves (meer clock tikken nodig dan nu met navi) geeft je nog maar eens een nadeel.
RX 5700 XT;2;0.4304177165031433;"Sorry, maar wat jij en een aantal anderen 'echt wel merkbaar' vinden, vind het merendeel van de wereld echt niet 'niet echt'. Wellicht dat het merkbaar onder bepaalde omstandigheden bij bepaalde games, net zoals met 1080p/4k video, maar dat zijn in mijn ervaring eerder uitzonderingen dan de regel. Wellicht dat super jonge ogen dat wel merken, maar mijn kennis/ervaring zegt eerder dat ze denken dat ze het wel merken. Aan de andere kant dat het wellicht minder opvalt bij mensen die zijn opgegroeid met een Atari 2600 (hout),,, ;-)"
RX 5700 XT;1;0.42759954929351807;Ook wij van atari hout zien het verschil, koop maar een 144 hz scherm, zal je zien dat je wel degelijk verschil ziet...ik was ook een unbeliever maar nu nie meer.
RX 5700 XT;4;0.4347721040248871;Precies. Ik heb mijn ouders een keer voor mijn Eizo gezet en sindsdien snappen zij het ook. En dan heb je het wel over twee totale digibeten.
RX 5700 XT;3;0.384215384721756;Ja jammer alleen dat ze er 3 jaar te laat mee zijn. We hadden al een GTX 1080, 1080 11Gbps, 1070ti, Vega 56, 64, RTX 2060 en RTX 2060 super. Serieus...als je nu nog aan het wachten bent... Dit prijspunt was allang bereikt.
RX 5700 XT;2;0.445181280374527;Zolang je niet onder de 60fps dipt maakt het geen verschil op een 60hz monitor. Op wat inputlag na, maar die winst is verwaarloosbaar met zo'n traag scherm.
RX 5700 XT;3;0.44577279686927795;Dit is niet helemaal waar.. verschillende testen (te laat om nu bronnen te vermelden) hebben laten zien dat een hogere FPS op een 60hz scherm weldegelijk een vloeiendere ervaring geeft die meetbaar en merkbaar is.
RX 5700 XT;2;0.395381361246109;Wat zou ik niet snappen? 60 hz monitor is maximaal 60 beeldjes per seconde. Meer beeldjes naar je monitor sturen kan niet. Enige voordeel is dat je input lag theoretisch gezien zou kunnen afnemen, maar dat is verwaarloosbaar omdat je al met bijna 17ms extra van je scherm zit.
RX 5700 XT;5;0.47241196036338806;Fast Sync, en de totale lag is meer dan een monitor refresh. Dat is wanneer jouw input zichtbaar wordt, niet wanneer je 'm registreert. Verder zit je met hogere interne FPS veel dichter tegen de refresh rate momenten van je 60hz paneel aan, waardoor tearing ook minder kan zijn. Dat is ook game specifiek. En het allerbelangrijkste: bij een framedrop is de kans onder de 60 te komen veel lager. Leuk, die minnetjes, maar het toont maar weer eens aan hoe diep Tweakers.net gezonken is. De lamme leidt de blinde hier, diep triest. Ik snap @BruceLee dan ook heel erg goed. Het is erg vermoeiend. Enjoy
RX 5700 XT;1;0.8519813418388367;"""Zolang je niet onder de 60fps dipt"", zegt @Anoniem: 159816 . Dan is die kans nul... En een -1 op een respect- en inhoudsloze reactie ""Als je er niks van snapt kan je beter niet reageren."" is volkomen terecht. Niks ""hoe diep Tweakers.net gezonken is""."
RX 5700 XT;1;0.4408296048641205;Zelfs in iets simpels als Overwatch dip je nog bij tijd en wijlen onder de 60, dus ik weet niet waar dat statement vandaan komt, maar veel praktijkervaring zal het niet zijn.... en als je een frame cap aanzet of Vsync dan is de kans helemaal levensgroot. Net zomin als het 'goed snappen' bevestigd zou worden door het statement dat je er geen profijt van hebt 'omdat je het niet kan zien'. Het is ook niet alleen input lag namelijk. Of omdat je toch al op 'slechts 17ms zit'... getuigt ook niet van veel ervaring.
RX 5700 XT;5;0.407906711101532;Wow, Koeitje heeft 100% gelijk, als je een 60Hz scherm heb is alles hoger gemiddeld weggegooi energy (zelfde voor 100 en 144Hz en hogere schermen waar de FPS hoger zijn dan de Hz van de scherm), omdat je GPU/CPU harder moet draaien, en het zorgt er voor dat je scheuren krijgt (of hoe dat heet) op je beeld, en is G-Sync en FreeSync erg handig om te hebben, of je capt hem in je spellen op 60fps, wat steeds meer spellen hebben.
RX 5700 XT;2;0.3637373745441437;nVidia heeft 'fast-sync' waarbij de GPU maximaal presteert om de input lag zo laag mogelijk te houden, maar tegelijkertijd de frames die naar de monitor gestuurd worden gecapt zijn op de refresh rate. AMD heeft naar ik aanneem een vergelijkbare techniek.
RX 5700 XT;1;0.30421099066734314;Heb je nog steeds 16.66667 ms input lag.
RX 5700 XT;1;0.287261426448822;Als een scherm maximaal 60fps kan weergeven, hoe kan je dan meer dan 60 fps weergeven? Ben benieuwd naar je uitleg Bruce.
RX 5700 XT;3;0.6466809511184692;Bij mijn geforce 960 is het wel fijn dat ik de settings zo zet dat ie wat meer kan produceren dan 60fps. Als de kaart dan gelocked is op 60fps belast je de kaart wat minder en heb je minder warmte en dus herrie. Mijn voorkeur is dus meestal om niet helemaal het maximale uit de videokaart halen
RX 5700 XT;3;0.38248100876808167;je kunt ze niet weer geven maar wel renderen, Vergeet niet dat heel veel game code draait per tick of te wel per frame, een terp die reageert op de tick van de framerate zorgt voor een veel smoother terp.
RX 5700 XT;2;0.4416724443435669;Games draaien ook een simulatie thread die minder ticks per seconden verwerkt dan de graphics threads. Graphics werken toch op interpolatie van de simulatie thread. Het blijft maar een representatie.
RX 5700 XT;2;0.3975280821323395;60hz = 16,6667 ms tussen frames. Als je framerate hoger is dan dat reageert de game inderdaad sneller op een muisklik, echt is deze muisklik gebaseerd op jouw input welke weer gebaseerd is op een traag scherm. Je reageert pas als je het op je scherm ziet en dan klik je. Dan maakt het niet meer uit, het effect zie je toch pas bij de volgende frame 1/60 sec later.
RX 5700 XT;2;0.464539498090744;"In theorie wel, maar dan ga je ervanuit dat je als gamer op dat éne cruciale beeldje zit te wachten waardoor je een bepaalde actie in gang zou zetten. Niet alles gaat om cijfertjes. Dit gaat over een gevoel van responsiveness - en met 120 fps op 60hz halveer je letterlijk de reactietijd per frame. En frames lopen constant door. Die optelsom maakt dat het geheel soepeler en beter aanvoelt. Wij mensen zijn maar traag; 100-200ms reactietijd is niet ongewoon, sterker, richting de 150-200ms is gemiddeld. Dus op dat grote getal lijken al die kleine beetjes winst sowieso al peanuts. Maar ze zijn het niet. Ons brein pakt elk beetje ondersteuning met beide handen (?) aan. Ik zal je zeggen, als ik nu 30 FPS op een console speel dan voelt het alsof ik dronken rondloop. Je went aan een bepaalde respons, en teruggaan is lastig. Ben je dan effectief beter en sneller geworden? Ja - dat heb je dan geconditioneerd namelijk. Het verschil van 60>120 is gevoelsmatig kleiner dan het verschil tussen 30>60 (terwijl er het dubbele verschil in FPS tussen zit!) maar het is wel degelijk aanwezig. Dat is lastig met een 'ms' uit te drukken. En visueel kan het dan alsnog achterlopen, het soepele gevoel blijft."
RX 5700 XT;1;0.4041692614555359;Euh ten eerste mijn scherm is 100Hz en laat me spellen op 100fps lopen als dat gaat, en ten tweede wat is er met je aan de hand dat je zo reageert op mensen, is een normaal gesprek hebben zo moeilijk voor jou?
RX 5700 XT;1;0.2698526680469513;Nah je moet geen vsync etc ingame aanzetten krijg je weer extra inputlag van..
RX 5700 XT;2;0.4305330216884613;Doe ik ook niet echt, ik gebruik als dat kan de ingame FPS cap, en steeds meer spellen hebben dat, waarom zou ik me grafische kaart 100% kontakt laten draaien als dat niet hoef, en heb geen last van inputlag, en anders gewoon geen cap als die er niet is, heb trouwens G-Sync op mijn monitor zitten.
RX 5700 XT;2;0.5300638675689697;Tearing is veel subtieler bij hoge fps , maar ook subtieler bij slow pace games. Wat mij meeste irriteerd en erg onrealistisch en storend vind in shooters is bunnyhop gedrag iets wat NPC nooit doen. En rusteloos inertialoos staminaloos bewegen als vlieg en dan zuiver kunnen schieten. Voor PC gamen is het eigenlijk ook Pay to win. Je koopt Ultrawide Gsync 144hz sherm een 2080ti en je hebt een voordeel tov meerderheid met een 60hz scherm en 1060 of 2060 . Voor online gamen op console is die variatie minder extreem en minder last van cheaters. Maar ik prefereer milsim games waar idioot bewegen je zuiverheid drastisch kelderd. Toch speel ik ook CoD en BF voor mij zijn ze gelijk kwa CQC het blijven bunnyhop games de ander is de game wereld wat groter en met tanks en vliegtuigen. Beide zijn arcade . Arma gaat mij te ver en Game als Operation flashpoint Red River is voor mij de juiste dosis realisme en met coop vs npc ook geen bunny hoppen.
RX 5700 XT;1;0.5707507729530334;Nee sorry ik geloof niet dat de verschil tussen 60 en 144Hz zo enorm is, het licht er nog steeds aan of je goed bent in shooters online, ken meerdere mensen die een 60Hz monitor hebben en gewoon vaak winnen in shooters online.
RX 5700 XT;5;0.24480095505714417;Het zijn niet alleen de frames op je scherm die belangrijk zijn. In FPS op het een 60Hz scherm wil je gewoon 120,180 of zelfs 240fps hebben ondanks dat je scherm maar 60 is. Dit alles heeft te maken met andere aspecten in een game engine dan alleen de weergave.
RX 5700 XT;2;0.39720526337623596;125fps was relevant in Quake III omdat de physics afhingen van je fps en verder kon je CRT bak naar 120hz. Ik ken niet één moderne game waarbij de physics afhangen van je framerate en de meeste LCD schermen gaan niet verder dan 60Hz (144Hz schermen zijn dik in de minderheid). 60Hz betekent 60 beelden per seconden. Meer dan dat wordt niet getoond. Het verschil in inputlag merk je ook niet bij moderne games al was het maar omdat ze retetraag zijn (Fortnite val je bij in slaap als je het vergelijkt met Painkiller of Q3), met mogelijke uitzondering van de lui die echt competitief spelen.
RX 5700 XT;2;0.38620099425315857;Er zijn wat console games waarbij de physics gelockt zijn aan de FPS, waardoor dat soort games op de PC hard locked zijn op 30 of 60 fps.
RX 5700 XT;3;0.3388555347919464;Bijvoorbeeld bij de Orginele Call of Duty was dat belangrijk (dit heb ik op hoog niveau gespeeld) daar had je een voordeel wanneer je het spel op 125fps of op 333fps zette. Op 333fps kon je sneller schieten en hoger springen. Je scherm kan dat niet aan maar de game zelf laat die FPS wel zien en zo krijg je voordeel ten opzichte van iemand die de hoge fps niet kan halen. Daarom werd later ook een config checker gebruikt die stelde dat je fps niet hoger ingesteld mocht zijn dan 250.
RX 5700 XT;2;0.3610457181930542;Wie gaat nu al een kaart aanschaffen voor een game die voorlopig nog niet uit is en je geen flauw idee heb hoe goed deze gaat lopen op de huidige kaarten. Voor zover ik kan zien moet je gaan kijken naar welke games je speelt en wat vervolgens je budget is voor een nieuwe kaart. De 5700xt is in een aantal gevallen sneller dan €100 duurdere RTX270 Super, leuk als dat je main games zijn... Maar het ligt geheel aan op wat voor resolutie/kwaliteit en welke game je speelt. Imho is dat de fans onder load veel luidruchtiger zijn een veel groter issue. Wellicht dat dit bij kaarten van andere fabrikanten beter wordt...
RX 5700 XT;2;0.4335896968841553;Van Cyberpunk zijn er al gameplay beelden met een RTX2080 en daar ziet dat raytracing er wel erg mooi uit. En volgens jaar dan heeft Nvidia wel al een kaart met hardware raytracing op de markt terwijl AMD daar in elk geval tot 2021 niet mee komt en als ze er mee komen dan komen ze met een hybride oplossing, niet volledig hardware. Ik heb nu een aantal reviews gezien en over het algemeen komt de 5700XT slechts in de buurt van de RTX2070 (veel tests hebben nog niet de super variant getest) en voor het extra geld krijg je er wel weer hardware raytracing bij.
RX 5700 XT;2;0.5320300459861755;Op 't moment is raytracing meer een gimmick dan een significante toevoeging. Visueel zijn de verschillen zeer minimaal, en het kost je grofweg de helft van de framerate. En Nvidia vindt dat je er extra voor moet betalen, no thank you!
RX 5700 XT;3;0.4991089403629303;Prestatieproblemen zijn er momenteel zeker, maar een gimmick zou ik het niet noemen. Raytracing maakt toch wel een groot verschil wat mij betreft.
RX 5700 XT;4;0.28869277238845825;Er zit nog meer aan te komen:
RX 5700 XT;2;0.40116021037101746;Voor deze prijzen is het geen midden segment meer. Meer hoog. Aangezien Nvidia nu alle rtx kaarten vanaf 2060 hoog noemt. En mid zijn de 1660(ti). Top zijn de 2080 en 2080ti. Midden segment is toch wel een prijs tussen 200 en 300. Dat word de 5600 wel. Rx5600xt word zeker vega 56 prestaties voorbij. Aangezien de rx5700 de vega64 voorbij gaat. En als we de prijzen mogen geloven word die dus 300 usd en de rx5600 250 usd. 5500xt = 200 en 5500 = 150. Betekent dat we met deze generatie alles 50 usd meer moeten gaan betalen. Hopelijk gaan ze nog flink oorlog voeren in prijs. Alleen Nvidia kennende vinden ze hun kaarten nu toch wel goed geprijsd en zal er niks van komen.
RX 5700 XT;2;0.5252195596694946;"AMD is vele malen kleiner dan Nvidia en zeker Intel en toch moet het concurreren met beiden. Dat ze het op dit moment Intel erg moeilijk maken is al erg knap en bijna een wonder te noemen. Ook Nvidia ""crushen"" is gewoonweg (jammergenoeg) niet realistisch. Uit noodzaak ""kiezen"" ze weer het middensegment, omdat daar simpelweg het meeste geld wordt gemaakt. Daarnaast kost het erg veel investeringen om een topkaart te maken als de RTX 2080Ti. En dan is het de vraag hoe veel mensen een flagship kaart willen kopen van AMD."
RX 5700 XT;3;0.46347159147262573;Je praat wel over raytracing alsof dat al een iets is wat de toekomst is, maar dan heb je het over 60 fps dat dat prima is want dat is je refresh rate ook. Meeste gamers zitten toch echt op 144 hz, dat is een groter pluspunt dan wel of geen raytracing, freesync of niet.
RX 5700 XT;2;0.46283838152885437;Meeste gamers zitten helemaal niet op 144Hz. De meeste serieuze gamers misschien wel, maar het gros zit gewoon naar een 60Hz 1080p TN scherm te kijken. En als ze wel een 144Hz scherm hebben dan draait het vaak op 60Hz omdat dat de standaard instelling is.
RX 5700 XT;2;0.39308962225914;We hebben het hier over videokaarten van 300 a 400 euro, dan ben je toch best een serieuze gamer? Dus je koopt een videokaart van 400 euro maar hebt een monitor van 100 euro, dat vind ik dan raar, en ik denk ook niet dat dat de norm is voor de meeste mensen die een kaart hebben in de prijs klasse.
RX 5700 XT;4;0.3550514280796051;"Als je verstand hebt van techniek, ja misschien. Maar genoeg vrienden van mij die ""het verschil toch niet zien"", maar wel een rtx2060 of beter kopen. Dikke fps en geen haperingen is wat zij belangrijk vinden ipv. snelle reactietijden. Daarnaast zijn 1440p en uwhd schermen op 120 of 144hz een stuk duurder dan hun 1080p varianten. En mensen gaan vaak toch ook liever voor een groter scherm (want dat zien ze wel)."
RX 5700 XT;2;0.3155272901058197;Lol, nee ik had eerst een 1080p 144hz scherm, nu een 1440p 144hz scherm, vrienden van mij hebben zelfs een 240 hz (cs:go) scherm, raytracing intereseert me niet maar hoog fps wel, en 60 hz omdat dat de standaard instelling is? Ben je aan t dromen ofzo, eerste wat je doet is t scherm op 144 zetten en in windows...
RX 5700 XT;2;0.5169315934181213;Doe mij maar een stabiele 60Hz met raytracing. Ik heb totaal geen behoefte aan 144Hz. Gaat ook niet gebeuren want ik game op een TV. Ik ben dan ook niet van de shooters moet ik eerlijk toegeven. Maar zelfs bij racesims heeft 144Hz een voordeel, maar toch, daar ligt mijn focus niet op. Moet ik ook eerlijk zeggen dat met mijn instellingen een 1080ti de 144fps over het algemeen bij lange na niet trekt, dus heeft het ook niet veel toegevoegde waarde.
RX 5700 XT;1;0.34021228551864624;Dat is wat anders, je gamed op je tv, als je op een monitor speelde zou het toch ook geen 60 hz monitor zijn van 100 euro? Je videokaart kost 500 euro, de meeste met een kaart van 500 euro hebben ook een aardige monitor lijkt mij?
RX 5700 XT;2;0.4572722911834717;Als een monitor een betere kleurweergave of 3D biedt(ja ik heb nog een Zalman ZM-M220W ergens ), full array local dimming, HDR, of noem maar op biedt, dan vind ik dat van groter belang. Ook in het geval van een TV zou een 120 of 144Hz mogelijkheid minder belangrijk zijn. Van bijna alle zaken waarop ik selecteer staat 60+ refresh onder aan mijn lijst. Ik vind het een aardige toevoeging, maar to-taal geen noodzaak. En ik game echt vrij veel op die TV, mijn PC heeft niet voor niks een plek in de woonkamer. Ik heb het zelfs zover dat ik mijn simrig er binnen 2 minuten voor kan hebben staan. En mijn kaart was duurder dan 500 euro, mijn racestuur ook. Andere prioriteiten. Dè gamer bestaat gewoon niet, iedereen heeft zijn voorkeur.
RX 5700 XT;4;0.42419490218162537;Dat is zeker waar, iedereen om mij heen heeft wel een 144 of 240 hz monitor, maar zal ook zijn omdat we ook veel shooters spelen competitief.
RX 5700 XT;2;0.4935897886753082;Dan heb je een goede reden. Maar ik speel eigenlijk nooit meer shooters en al helemaal niet in multiplayer. Er is zoveel meer in PC gaming dan online shooters... Ik speel zo nu en dan wel eens een off-line shooter(al is dat óók al heel lang geleden) met verhaal zoals iets uit de Far Cry serie of zo, maar daar is lag totaal niet interessant. Je vindt het ook vast eng dat ik bijna uitsluitend met mijn X360 controller game? En in alle andere gevallen met mijn racestuur of Vive controllers. En natuurlijk maakt het bij racen ook uit. Alleen ben ik zonder 144Hz scherm rap zat. Gaat me ook veel meer om de lol dan de competitie.
RX 5700 XT;3;0.26417815685272217;"Nee vind zeker niet eng dat je op je controller game, ik heb best veel Watchdogs gespeeld, ook met controller. Ik speel ook asetto corsa met een G25 stuurtje, en ik speel ook nog Command and Conquer; Red alert 2. Ik speel veel shooters, maar vind strategie games en racen ook heel leuk Shooters ben ik juist pas de laatste jaren geinteresseerd in geraakt, ik speelde voorheen alleen rts games."
RX 5700 XT;3;0.5211179256439209;Was ook een beetje met een knipoog. Voor shooters en strategie games ontkom je niet aan een(goede) muis(tenzij je concurrenten dezelfde input methode hebben natuurlijk). Ik neig tegenwoordig (naast race sims)meer richting actie-adventure achtige games. Zoals inderdaad Watch_Dogs of Assassin's Creed(ben met GF overigens net een Enslaved: Odyssey to the West playthrough aan het doen). Maar ook niet al te zware RPG's, liefst wel in first person view(zoals Skyrim dus). Qua racesims doen we meestal Project Cars 2(vooral vanwege de weersomstandigheden), rFactor 2, Automobilista, Assetto iets minder vaak(ben toevallig nu wel met Competizione bezig, erg tof met een DD1 ) en als het we eens wat stoom af willen blazen Wreckfest .
RX 5700 XT;1;0.40218842029571533;Tv's doen ook gewoon 120hz hoor .
RX 5700 XT;3;0.3559545576572418;Inmiddels wellicht. Maar over het algemeen zijn het 60Hz panelen die verder vooral een flinke zwik beeldmanipulatie doen en zo fictief het aantal beelden per seconde verhogen. Gelukkig zijn we nu eindelijk op het punt belandt dat er daadwerkelijk 120hz native panelen komen die eveneens adaptive vsync ondersteunen.
RX 5700 XT;2;0.26488107442855835;Ongeveer al 2+ jaar inderdaad.
RX 5700 XT;3;0.48338302969932556;Ja voor slow pace singleplayer exploring Sandbox adventure games. Waar je ook van scenery kan genieten is 1440p meer GFX features zo ook RTX wel meerwaarde . Games zoals Stalker CoP. Crysis original . Online doe wel op PSN iedereen wat gelijker. Als ze daar ook freesync komt krijg je meer ongelijkheid. Maar nog niet zo extreem als op PC. Een Red dead redemption had ik ook liever op release ook op PC gehad.
RX 5700 XT;2;0.38958582282066345;Waar denk je dat het meeste geld te halen is? 99% van de mensen kunnen of willen niet 1500 euro neerleggen voor de 2080ti Turbo Super AquaCooling SuperCharge Extra Plus Extreme Edition, en gaan voor de midrange kaarten, waar dus deze kaarten in zitten. Sterker nog, de XT is nog wel iets meer dan midrange. Je vergeet dat wij hier allemaal gear heads zijn, maar de meesten nog niet het geld uitgeven voor het hele systeem dat die ene videokaart kost. Dus afwachten of er iets high end komt, maar denk maar niet dat het een verkooptopper wordt, omdat de 2080ti dat ook niet is. Het is meer pochen dat je deze prestaties kunt halen met kaarten in je line-up, iets wat het gewone volk toch niet snapt.
RX 5700 XT;3;0.41321152448654175;"De Ti is misschien iets teveel een niche product maar de gewone 2080 moet te doen zijn. En natuurlijk spreekja daarmee niet het grootste deel van de markt aan maar de hele martk kijkt wel naar je. Als men kan zeggen ""dit is onze topkaart maar we hebben voor de gamer met een lager budget ook een wat goedkopere kaart"" dan is dat een goede marketing, bied je de klanten met een lager budget precies wat ze willen maar kunnen ondertussen ook tegen de concurrent zeggen ""kijk maar uit, we hijgen in je nek"" Maar nu is het weer zo dat als je de snelste kaart wil hebben je toch weer bij Nvidia uitkomt, heb je een kleiner budget dan kom je weer bij AMD uit."
RX 5700 XT;1;0.6518809795379639;"Pochen dus... En als iedereen met verstand ervan tegen de gewone man zegt: ""leuk die überschall dingen, maar als je waar voor je geld wilt, moet je die andere hebben"", kun je roepen wat je wilt, dan kopen alleen fanboys 'm, en daar kun je niet op leven. Of zoals mijn oma tegen mijn oom zei: ""je mag de grootste hebben, maar als je er niet mee om kunt gaan, kun je er nog niks mee"". (Ging over de zoveelste grote auto die mijn oom in elkaar had gereden)"
RX 5700 XT;3;0.2658596932888031;Dat geeft toch niet? AMD is nu qua watt/performace ongeveer gelijk aan Nvidia. Dat is heel erg goed te noemen. AMD komt later ook met topmodellen uit die tegen de 2080ti en dergelijke moeten strijden. Maar het zou dom zijn daar vol op in te zetten, om de kroon in handen te krijgen of te evenaren. Er is nu nog schaarste met de 7nm productie, en dan moeten ze keuzes maken, en dan is het verstandiger kleinere chips te produceren waar ze een mooie marge op kunnen pakken, dan grotere die moeilijker verkopen, en minder snel te produceren zijn. AMD heeft tevens voor de nieuwe PS5 en Xbox opdracht om gpu en cpu te maken. Dus in 2020 komt dat uit. Ik ben positief verrast iig! Goede prijs prestatie. Nvidia levert nu ook meer voor minder, en dat doen ze niet zomaar!
RX 5700 XT;2;0.41282209753990173;Nvidia wist dus precies waar AMD mee zou komen en hebben met de nieuwe Super kaarten ervoor gezorgd dat ze toch weer net iets sneller zijn en als reactie verlaagde AMD op het laatste moment de prijzen.. Goed voor de concurrentie in het midden segment wat hopelijk de prijzen ook bij Nvidia verder zal drukken.. Maar de vraag blijft wel staan waarom AMD al vele jaren niks in het top segment uitbrengt?!? De Radeon VII heeft weer akelig gefaald tegenover de 2080, laat staan de 2080 Super straks of de 2080Ti.. Kunnen ze niet of willen ze niet?!? Verder zou ik wachten op 3rd party custom kaarten met veel betere vooral stillere koelers.. Dat geldt zowel voor AMD als Nvidia.. De FE modellen alleen kopen als je echt niet kan en/of wil wachten en/of perse een blower koeler nodig hebt i.v.m. heel kleine kast ofzo..
RX 5700 XT;4;0.3938029706478119;Ik denk dat de focus vooral heeft gelegen op de ontwikkeling van een goede CPU. Daar zijn ze zeker geslaagd. Hopelijk wordt die winst gebruikt om de GPU kant weer wat R&D geld te geven. Binnen een paar jaar zullen ze zeker weer mee kunnen doen. Als ik kijk naar het energieverbruik, is er al een goede stap gemaakt.
RX 5700 XT;2;0.37132126092910767;Dat beter energieverbruik is vooral te danken aan 7 nm productie, met dank aan TSMC. Ik heb altijd gezegd dat Global Foundries een blok aan het been was voor AMD want ze zaten contractueel aan hun vast. Vroeger moest AMD hun kaarten altijd bij GF laten produceren maar die hadden altijd de slechtere(minder performante/meer energieverbruikende) node. Gelukkig zit AMD nu niet meer vast aan GF(ook al omdat ze hun 7 nm ontwikkeling stopgezet hebben en alleen nog 14 nm doen) en kunnen ze bij TSMC laten produceren en je ziet dat ze stilletjes aan beginnen terug te komen. Ook hun blunders met hun Fury(X) en Vega- kaarten met het dure HBM(2)-geheugen is AMD duur te staan gekomen. AMD heeft momenteel 7 nm node nodig tov 12 nm node van Nvidia om enigszins met hun te kunnen concurreren. Zo kunnen ze er meer transistors en dus hogere snelheid voor hetzelfde verbruik inproppen. Ze kunnen momenteel geen concurrent voor de 2080s/ti op de markt brengen omdat ze er de middelen niet voor hebben. Pas met rijping van het 7 nm-procede van TSMC volgend jaar kunnen ze een grotere chip en concurrent to deze Nvidia - modellen zetten. Maar helaas kanNvidia dan ook weeral met een nieuwe kaart op 7 nm komen.
RX 5700 XT;3;0.5649401545524597;"Ik ben het gedeeltelijk, maar niet helemaal met je eens. 7nm heeft geholpen, maar vergeet niet dat de Radeon VII ook op 7nm was gemaakt. Het was een chip met 60 compute units, en toch kon het niet eens de RTX 2080 bijhouden... Niet in stroomverbruik, en niet in beelden per seconde. Als we naar de 5700XT kijken in vergelijking tot de Radeon VII, het heeft 33% minder (40 ipv 60) compute units (en dus ook TFLOPS), 24% kleinere chip, en gebruikt 25% minder stroom zonder HBM. De kloksnelheden zijn redelijk gelijk, en tóch bereikt the 5700XT bijna de snelheden van de Radeon VII in spellen (de Radeon VII is maar ~5% sneller). Computerbase.de heeft een interessante test gedaan, waarbij ze verschillende chips met gelijkwaardige aantal 'kernen' op dezelfde kloksnelheid laten draaien. Daaruit blijkt dat per klok de nieuwe RDNA architectuur van AMD gemiddeld; 39% sneller is dan Polaris 28% sneller is dan Vega 13% sneller is dan Pascal van nVidia 1% sneller is dan Turing van nVidia 7nm heeft geholpen met stroomverbruik, maar het grootste deel van de efficientie komt toch vanuit de architectuur zelf."
RX 5700 XT;2;0.24828383326530457;Dat klopt. De Vega- chip en de van Vega afgeleide chip in de Radeon VII waren geen chips specifiek bedoeld voor gaming maar meer voor de zakelijke markt. In die tijd had AMD niet genoeg geld om twee chips te ontwerpen, een voor gaming en een voor zakelijk. Aangezien er meer winst te halen is per chip in de zakelijke markt heeft AMD toen gekozen om een chip te maken specifiek voor de zakelijke markt. De game-markt was bijzaak. De RDNA-architectuur is veel meer bedoeld voor gaming. AMD heeft vooral de single-threaded prestaties verbeterd en niet de parallelle. De 5700-chip is ook veel kleiner dan Vega. Voor de gamemarkt gaat AMD de RDNA-architectuur gebruiken en voor de zakelijke markt blijven ze vasthouden aan de GCN-architectuur.
RX 5700 XT;2;0.4914698004722595;grotendeels willen niet. ze konden het doen, er was zelfs een grote Polaris in ontwikkeling, maar het zou veel geld kosten om te ontwikkelen en de verkoop aantallen zouden beperkt zijn. aangezien AMD toen nog flink op de kosten moest letten zijn ze geschrapt. Inmiddels komt er veel meer geld binnen mede dankzij Ryzen en is het R&D budget verhoogt. Lisa heeft gezegd dat ze ook aan de top weer gaan concurreren. maar nog niet wanneer dat zou zijn.
RX 5700 XT;2;0.5962156057357788;"In het topsegment kan je allen geld verdienen als je de beste kaart hebt; iemand die 1000 euro aan een GPU uitgeeft koopt niet de op-één-na-snelste GPU, die koopt de snelste, en zolang AMD geen kaart kan produceren die sneller is dan de 2080 Ti, heeft het niet zoveel zin daar een gooi naar te doen. Als ze een kaart maken die 95% van de prestaties van een 2080 Ti heeft, koopt geen hond hem. Daarnaast zijn high-end kaarten moeilijker te ontwikkelen. Goed presteren in het middensegment levert voorspelbare marges en vergt minder investering dus is gewoon veel minder risicovol. Voordat ze Ryzen hadden speelden ze met de CPUs precies hetzelfde spelletje. Nu zie je dat door Ryzen ze wel mee kunnen doen in het topsegment en ze daar meteen ook keihard concurreren."
RX 5700 XT;1;0.26744014024734497;De games waar AMD duidelijk wat achter blijft zitten vol nVidia geoptimaliseerde code?
RX 5700 XT;3;0.6994236707687378;Beetje lastig he games optimaliseren voor een kaart die nog niet uit is.
RX 5700 XT;1;0.42205381393432617;Dat hoeft niet, je zou eigenlijk verwachten dat een AMD (en andere fabrikanten) al zeer vroeg samples leveren aan bijvoorbeeld game dev's en engine ontwikkelaars zodat juist bij launch de eerste optimalisaties al gedaan zijn.
RX 5700 XT;5;0.25490131974220276;Alle vorige generaties van amd gpus zagen altijd juist na de launch de performance met de tijd steeds iets verbeteren, bv de 1060 was bij launch sneller dan een 480, en een jaar later waren ze gelijk.
RX 5700 XT;2;0.44246146082878113;Dat klopt, dat is echter niet handig voor de reviews. Vandaar dat je zou hopen dat AND daar van zou leren en dus zorgen dat de lunch prestaties nu wel optimaal zijn.
RX 5700 XT;3;0.3215057849884033;Wellicht vieze spelletjes van Nvidia met hun machtspositie.
RX 5700 XT;2;0.4356883764266968;Nvidia heeft gewoon meer geld en meer mankracht aan werken dat het allemaal tijdens launch al word geoptimaliseerd. En Amd met minder mankracht dat tijd nodig heeft. Je ziet ook na driver updates dat Nvidia ook nog per game sneller word, alleen word het niet zoveel beter als Amd. Amd pakt veel meer percentage verbeteringen.
RX 5700 XT;3;0.4097703993320465;Niet persee. Wolfenstein was een game die het heel goed deed op AMD hardware bijvoorbeeld. Maar het is wel een low level API game (vulkan in dit geval), en dus moet de developer zelf een groot deel van de optimalisatie doen. Die zijn wel gegaan voor de al bestaande AMD en Nvidia GPU's maar Navi is een nieuwe architectuur (en meer dan alleen een nieuwe GCN iteratie) dus is de game/engine er nog niet voor geoptimaliseerd. In DX11 en lager kan AMD zelf veel doen in de drivers, maar in low level API's is hun invloed veel beperkter. De DX11 games tonen toch wel duidelijk aan dat Navi voldoende performance in huis heeft. dus als ze de engine nog zouden optimaliseren voor Navi denk ik dat er nog veel winst te halen valt. het is natuurlijk maar de vraag of zo'n performance patch er komst. maar alle toekomstige low level API games zullen wel ook geoptimaliseerd zijn voor Navi.
RX 5700 XT;3;0.31234923005104065;Er schijnt vandaag nog een 3e kaart verschenen te zijn: de Radeon RX 5700 XT 50th Anniversary met een hogere Boost Clock. Normale Boost Clock voor de 5700XT is 1905 MHz en van de 50th Anniversary is die 1980 MHz Hier alle specs op een rij (naar beneden scrollen svp)
RX 5700 XT;1;0.5666958689689636;Die is niet opeens verschenen, die is tegelijk aangekondigd met de andere 2 kaarten
RX 5700 XT;5;0.4315222501754761;En ook een hoger verbruik van 235W.
RX 5700 XT;2;0.5069873929023743;Ik twijfelde heel erg tussen de 2060 Super en de 5700 XT, nou de prestaties zijn wel duidelijk. Scheelt gewoon 10% voor dezelfde prijs. Alleen die koeler op de Radeon... nou weet ik het nog steeds niet.
RX 5700 XT;2;0.4742650091648102;Die fan kan je vast wel in een beter fancurve zetten dan wat hier geprutst is...binnen 2 weken zijn er vast betere drivers dan deze release drivers..
RX 5700 XT;1;0.3725142180919647;Wachten op een AIB-kaart die volgende maand uitkomt.
RX 5700 XT;4;0.36125025153160095;Klinkt goed ! Weet er iemand of er binnenkort ook nog een 5800 (XT) komt?
RX 5700 XT;2;0.37973538041114807;Hoogst waarschijnlijk duurt dat nog even. kan zo maar eind dit jaar of begin volgend jaar zijn.
RX 5700 XT;2;0.391522079706192;Ik vraag me af wat de GeForce RTX 2060s voor zin heeft, hij is ongeveer net zo snel als de GeForce RTX 2070, en vaaker zelfs sneller, snap niet de logica van Nvidia om hun Geforce RTX 2060 en 2070 van de markt te duwen met hun super modellen. Maak dan de Geforce RTX 2080 en 2080Ti goedkoper of en plaats daar een Geforce RTX 2080s tussen in.
RX 5700 XT;3;0.3819442391395569;tja, ze konden het goedkoop doen door gebruikt te maken van de verbeterde yields van 12nm waarschijnlijk. Door de betere yields hebben ze meer die's waarvan ze meer cores kunnen enableën(?), en op een hogere clocksnelheid laten lopen. het zijn verder nog steeds de zelfde die's en de zelfde PCB's. de 2060 bungled er wel een beetje bij nu, en de 2060s voegt idd niet veel toe zo boven de 2070.
RX 5700 XT;1;0.5032047629356384;"Zover ik weet is de RTX 2070 EOL, die wordt dus niet meer gemaakt.. Alleen de ""gewone"" 2060 blijft Nvidia leveren naast de nieuwe Super.. De 2080 is straks ook EOL zodra de 2080 Super er is.."
RX 5700 XT;3;0.2592073678970337;EOS(sales) bedoel je dan eerder, EOL zou ik kaarten noemen die geen nieuwe drivers meer krijgen.
RX 5700 XT;2;0.4413834512233734;Ja ach, alleen vervelend voor mensen die perongeluk een Geforce RTX 2060 of 2070 kopen inplaats van de Geforce RTX 2060s en 2070s, vooral mensen die er niet veel van weten. Update: En vond het jammer dat de Geforce GTX 1080 niet mee genomen werd in de benchmarks.
RX 5700 XT;2;0.391300231218338;Voor 1080 scores kan je een beetje uitgaan van tussen 1070Ti en 2070 FE ish. Ik heb zelf ook een 1080, destijds genoeg voor betaald maar zie nog steeds geen reden om te vervangen zonder serieus meer te moeten betalen.
RX 5700 XT;4;0.3481296896934509;Voor mij het zelfde, ook een van de rede dat ik wacht op de Geforce RTX 3080 of hele snelle Radeon kaart.
RX 5700 XT;1;0.3489474654197693;Goh is dat niet bij alles zo? Oneplus die T versies van zijn smartphones uitbrengt, auto's die facelifts krijgen,... Als iets high-tech is weet je toch dat het tegelijk rap weer obsolete is?
RX 5700 XT;3;0.42027485370635986;Ze moesten wel omdat anders de 5700 en 5700 XT te goed zouden afsteken tov die 2, nu komen ze met super versies maar helaas verlaagd amd nog ff de prijs, nu kunnen ze kiezen of meegaan en ook de prijs verlagen of amd meer sales gunnen. De 5700xt is basicly een 1080ti voor 400 euro, voor iedereen die geen raytracing wil een topper.
RX 5700 XT;3;0.44186508655548096;Ja maar breng dan een grafische kaart uit die tussen de Geforce RTX 2080 en 2080Ti in zit, en maak de Geforce RTX 2080 en 2080Ti goedkoper.
RX 5700 XT;1;0.6661142110824585;"Eigenlijk snap ik het ook niet. Ze hadden net zo goed prijs van de 2070 en 2080 kunnen verlagen. De enige wat ik kan bedenken is dat zij op deze manier als de goeierikken willen voorkomen voor hun idiote fans. Van kijk "" door 2070 super uit te brengen krijg je nu bijna 2080 prestaties voor dezelfde prijs. En omdat we 2gb extra op de 2060 super moeten zetten kost dat je 50 dollar meer en heeft bijna 2070 prestaties "". En alle Nvidia fans gaan uit hun dak! Die zeggen dan "" wtf! Nvidia is zo aardig voor ons om betere prestaties voor zelfde prijs te geven, Haleloeja Nvidia!"" En ondertussen denken de wat slimmere mensen "" tja, ik krijg voor hetzelfde geld wat Nvidia vraagt al 10% betere prestaties. Lijk ons veel beter "". Nog slimmere mensen denken van "" Dit hele generatie gpu's zijn zowiezo overpriced! Of het Nvidia is als Amd! Nvidia is niet te betalen en dat prijs is alleen voor gekken! Amd is nog steeds 50 teveel."" En als je denk dat we eigenlijk allemaal nog steeds wel zeker een jaartje vooruit kunnen met onze ouwe kaart. En volgende generatie dat ze weer op normale prijsniveau zitten en dan pas een nieuwe kopen."
RX 5700 XT;2;0.4325660169124603;Ja precies, maar ach ze hebben het al gedaan, maar ben veel meer benieuwd naar de Geforce RTX 3080, en hoop dat die wel normale prijzen krijgen, en dat AMD ook met een hele snelle kaarten komt waneer de Geforce RTX 3080 uit komt, anders ben ik bang dat de Geforce RTX 3080 ook heel duur woorden.
RX 5700 XT;5;0.29162901639938354;Nog iets is, zo hoeven ze de rtx2080ti niet in prijs te verlagen. Als ze de 2070 en de 2080 hadden verlaagd, dan moesten ze de 2080ti ook verlagen om in prijs verhouding goed te zitten, en nu hoef dat niet. En de 2080s komt toch nog niet in de buurt van de ti versie, wel dichterbij. Heel slim Nvidia. Ik zie je naam AmigaWolf. Ook Amiga 500 vroeger of had je de 1000? Ik had de 500. Prachtig ding. Erg veel plezier mee gehad.
RX 5700 XT;2;0.5660317540168762;"Ja maar echt niet leuk voor beginners van Game-Pc's niet meer duidelijk. Ja, me vader had eerst de Commodore 128D en toen paar jaar later de Amiga 1000 en 2000, waar ik veel mee gespeeld heb, toen kocht me vader de Amiga CD32 +Sx32 Pro voor me met 64MB ram en 500MB 2,5"" HDD, toen een paar jaar later kocht ik de Amiga 1200 met later de PPC 240MHz risk CPU en 68040 25MHz CPU Bvision en later Voodoo 3 en 96MB RAM, en scsi HDD en DVD spelen/writer, en zat met een goede vriend die ook een Amiga had op de Amiga Club Hoorn meerdere jaren ieder maand. Ja heel veel plezier gehad met Amiga computers, zelfs meer dan met Windows PC's, me vader had ook een 486 PC met Windows 3.11, maar ik was 100% alleen bezig op de Amiga, was in de jaren 80 en begin 90 veel leuker en beter dan de MS-DOS Pc's. Heb 3 a 4 haar terug een Amiga CD32 weer gekocht en zit nu een TF330 in wat de zelfde CPU heeft als de SX32-Pro, een Motorola 68030 50MHz CPU, en 64MB RAM en 8GB SD kaart, met 1000 spellen, en Workbench 3.1, moet nog de nieuwe Workbench 3.11 er op zetten, heb nu al 43 Amiga CD32 originele spellen, ben ze aan het sparen, en heb een Commodore 128D en laatste een Amiga 500 Plus gekocht."
RX 5700 XT;2;0.41081419587135315;Dacht dat 2070 ook EOL wat NV doet is soort rebranding maar dan zonder hogere type aanduiding. Een rebinning en zelfde chips anders in te zetten door meer units in te schakelen of chip shuffle.
RX 5700 XT;2;0.33382853865623474;Nee dacht niet een rebrand met zelfde chip, maar dacht nog meer kaarten met zelfde naam maar dan met Super achter de naam en wat sneller, had liever gezien dat ze de Geforce RTX 2070 en 2080 en 2080Ti goedkoper hadden gemaakt, en zo nodig een Geforce RTX 2080s tussen de 2080 en 2080Ti.
RX 5700 XT;5;0.6924387216567993;Ja en een 5900XT! AMD doet het super!
RX 5700 XT;1;0.2795320153236389;Las ergens september.
RX 5700 XT;2;0.3306180536746979;Zou mooi wezen, dan kunnen ze Nvidia echt het vuur aan de schenen leggen
RX 5700 XT;2;0.35012921690940857;Ik zou hem zo graag nu halen om ffxiv in 1440p/4k 60 te kunnen spelen, maar die fan jongens. Dat word wachten op een MSI of asus versie, hopen dat die niet een prijs van 500+ gaan vragen voor hun custom coolers erop, Raytracing gaming doe ik wel met mn PS5 later. Heb nu een R9 390 en een RTX2060super of RX5700XT zit ik tussen te twijfelen... maar geen founders/stock edities. ben bang voor de custom prijzen lol
RX 5700 XT;2;0.43475502729415894;Ligt eraan hoeveel geduld je hebt en of het aanbod hoog genoeg is om aan de vraag te voorzien.. De prijzen zullen in het begin wel hoger zijn maar hopelijk paar maanden daarna gaan zakken..
RX 5700 XT;1;0.41785404086112976;Hoe zit het tegenwoordig met de stabiliteit van de hardware en drivers enzo? Ik hoor veel gemixte verhalen en veel slechtere verhalen in het verleden. Zijn hier ook verbeteringen?
RX 5700 XT;2;0.4242883026599884;Als huidige AMD kaart bezitter heb ik qua drivers twee problemen gehad: 1. Freesync werkt bij mij vaker niet dan wel. En dan de handel herstarten en het werkt weer, start een bepaald spel op en het werkt weer niet. En soms ook wel. Goede nieuws is: Voor mij persoonlijk is de conclusie dat ik Freesync niet heel veel vind toevoegen, dus het maakt me niet veel uit . Voor degene die het wel duidelijk merken is het erg irritant als je zoiets hebt. 2. In BF5 hebben AMD drivers maandenlang een probleem gehad waardoor na een revive je silhouet van tegenstander/teamgenoot in beeld bleef, tot je weer een paar keer dood was. Hoe irritant is dat vraag je je af? Nou behoorlijk:
RX 5700 XT;3;0.396640807390213;Mooie kaarten, voor een mooie prijs, als je ze vergelijkt met de concurrentie, maar nog steeds 100 euro te duur voor midrange kaarten. Als je een budget hebt van rond de 400 euro voor een grafische kaart en naar 2060s of 2070s kijkt, lijkt het mij het meest verstandige om gewoon een 5700 of 5700xt te kopen. Veel betere prijs prestatie verhouding. Zoals zo vaak bij AMD moeten de drivers nog wat geoptimaliseerd worden, dus als we deze benchmarks over 1 jaar opnieuw draaien, zit AMD gewoon 10+% boven de concurrentie! Wel raar overigens dat er niets vermeld word over de betere image quality die AMD bied tegenover de concurrentie. Idem voor de anti-lag feature die deze kaarten zouden moeten hebben. Waarom is dit niet getest? In mijn ogen veel waardevollere techniek dan raytracing...
RX 5700 XT;1;0.4952828288078308;Dacht al dat ik de enige was die de nieuwe functies van deze kaarten miste in de review...idd een groot gemis
RX 5700 XT;1;0.6435110569000244;Bwah, een groot gemis weet ik niet. Als je RT aanzet op Nvidia kaarten zakken je fps in en is je spel eigenlijk niet meer speelbaar. Dus eigenlijk kan je RT niet gebruiken.
RX 5700 XT;5;0.45376768708229065;Eindelijk weer wat competitie in gpu-land
RX 5700 XT;3;0.30442821979522705;In het midden segment was AMD altijd goed, de polaris chips waren ook uitstekend. Dit is alleen maar beter voor de consument en ben benieuwd hoe de implementatie van raytracing gaat verlopen.
RX 5700 XT;3;0.2951124608516693;Weet iemand toevallig wanneer deze kaarten beschikbaar zijn in nederland en of er price drops komen bij andere radeon kaarten (zoals de vega kaarten)?
RX 5700 XT;4;0.4919327199459076;Mooie kaartjes. Uiteindelijk gaat het om de performance per watt en de prijspositionering en die lijkt dik in orde.
RX 5700 XT;5;0.3210243582725525;Hier vast de prijzen en modellen bij onze oosterburen 419 voor de 5700 XT en 369 voor de 5700:
RX 5700 XT;2;0.5261126160621643;"Voor mij is het wachten voorbij, ""Custom cooling"" Radeon VII wordt links ingehaald door NAVI. Achteraf gezien is marketing/presentatie Radeon VII heel vreemd."
RX 5700 XT;3;0.3894418478012085;Vega56 is een mooie kaart voor 250 euro, ik zag de pulse versie laatst voor dit bedrag, een van de betere versies. Als cpu zou ik zeker dan geen ryzen 1700 nemen, dan is de 3600 een betere keuze imo.
RX 5700 XT;5;0.5247285962104797;Interessant startschot van AMD met deze nieuwe serie kaarten die gelukkig wat zuiniger zijn. Nu ben ik helemaal nieuwsgierig naar de kaarten die hier onder gepositioneerd worden. Een Navi kaart, zeg een 5600, met een lager verbruik rond de 200 euro. Samen met een Ryzen 5 3600. Yes please!
RX 5700 XT;4;0.30519038438796997;hele mooie prestaties, de amd radeon rx 5700 doet 100 fps gemiddeld op ultra 1080p. daarmee is het natuurlijk wel meteen een kaart die voorbereid is op de next - gen, en qua snelheid op zijn minst kan concurreren met de next - gen consoles, terwijl die zeer waarschijnlijk verslagen worden. dan komen we wel meteen aan bij een heikel punt : gebrek aan ondersteuning voor hardware raytracing. waarschijnlijk hebben alle grotere next - gen multi - platform releases gewoon ondersteuning voor raytracing. waar qua snelheid de rx 5700 waarschijnlijk goed genoeg is voor de games die van eind 2020 tot eind 2023 voor de pc uitkomen ( ok, zolang je de settings ieder jaar een tandje bijsteld ), is het gebrek aan raytracing een serieus nadeel. veel mensen zouden sterk geneigd zijn een rtx super kaart te kopen, en de next - gen titels van eind 2020 tot eind 2023 met raytracing enabled settings te spelen. met nvidia ben je veel beter voorbereid op de toekomst. amd had dus geen andere keus dan de prijzen te verlagen. het gebrek aan raytracing is namelijk een grote tekortkoming, met het oog op de toekomst ( inclusief next - gen ). bovendien liepen de sales van de vorige generaties amd - kaarten onder gamers ook niet zo denderend, getuige de steam hardware survey. wat betreft de aanschaf van amd gpu ' s voor cryptomining, die markt is niet zo stabiel en is behoorlijk onvoorspelbaar. de meeste gamers zullen gewoon weer voor nvidia kiezen. en het feit dat de koeler op deze amd - kaarten zo belabberd is, helpt natuurlijk niet. de geluidsproductie onder load ligt veel te hoog, en de meeste mensen willen sowieso geen blower. de kaarten met betere koeling van asus, msi etc. laten nog maanden op zich wachten. en dan is het nog de vraag of de prijzen van die kaarten wel de nieuwe en verlaagde prijzen reflecteren, of zelfs nog een stukje boven de oude vraagprijs zitten. dat is namelijk wel al eens eerder voorgekomen, dat de beloofde prijzen niet waargemaakt konden worden.
RX 5700 XT;2;0.48177531361579895;Vind de noemer prijs geen factor, de geteste kaarten komen met aardige prut koelers waarbij de nvidia kaarten met echt deftige koelers komen. De meerpijs meteen al waard naar mijn mening. Het is maar afwachten waar board partners mee gaan komen en dan voor welk bedrag. Id software en cd projekt beweren met echt geoptimaliseerde raytracing games tegaan komen. Als deze titels wel echt deftig draaien dan begint amd wel achter te lopen. Dat het verbruik rond dat van Nvidia ligt, is op het eerste gezicht zeer goed. Maar aan de andere kan als je er dieper op in gaat weer niet. Dit zijn 7nm chips vs de 12nm van Nvidia wat theoretisch minder zuinig zou moeten zijn. Helemaal omdat er nu extra turning cores aanwezig maken die de chip die bij Nvidia flink groter maken. Dit laat toch weer zien dat AMD aardig de chips moet pushen om tot de gewenste prestaties te komen. Daarnaast is Turning zeer goed over te clocken. Dit is nu nog geen gegeven bij de nieuwe AMD GPU's. Dus ergens vind ik het wel een telleurstellende launch. Zoals vrijwel alle launches van de laatste 5 jaar bij AMD. Het is allemaal net wat meer op het randje, minder overlock mogelijkheden, wat warmer, wat meer verbruik, niet geschikt voor in laptops etc. Ik wil gewoon AMD weer GPU's neer zien zetten die gewoon beter zijn dan van Nvidia. Vaak was de launch cyclus vroeger gewoon om en om. Nvidia kwam met een GPU launch en een half jaar later kwam AMD met een set snellere GPU's uit. Dan weer 6 maanden erna een nieuwe Nvidia launch etc dat weer sneller was en zo ging de gehele cyclus door.
RX 5700 XT;5;0.479763001203537;Bizar om te zien hoe goed de 1080ti eigenlijk is. Dat was echt een briljante kaart.
RX 5700 XT;3;0.26661697030067444;Dit is best interessant. RTX is Nvidia's troef maar of je daar €100 extra voor wilt betalen? Ik niet, maar dat zal per persoon verschillen. Met deze prijsstelling wordt RTX uiteindelijk naar de PhysX-hoek gedrukt, al helemaal omdat de volgende generatie consoles het niet (of een alternatief) gaat hebben. Nvidia heeft echter nog veel meer mindshare dan AMD, wat dat aangaat had AMD gewoon een halo-product nodig of een veel agressievere prijsstelling. De performance van Navi valt in ieder geval niet tegen. De prijs is, vergeleken met Nvidia, prima. Het oude normaal van sub-€300 voor de RX 5700 en sub-€400 voor de XT gaan we niet snel terugzien ben ik bang, maar deze performance voor €370/425 is nog te doen. Maar even realistisch: Zelfs sinds vandaag heeft iedereen met een GTX970/R9 290 of beter nog steeds geen fatsoenlijke kaart om naartoe te upgraden. Die kaarten draaien ook alles nog op goede settings. De performancewinst per euro is nog steeds niet goed genoeg. Het is dat dit publiek ca. zes jaar heeft gehad om te sparen Edit: @D0gtag Ja goh, 4K. Je kon al die tijd geen fatsoenlijke videokaart kopen, dus heb je het geld maar in een betere scherm gestopt en nu heb je daadwerkelijk nog een reden om weer een smak geld uit te geven
RX 5700 XT;1;0.30265000462532043;"""Zelfs sinds vandaag heeft iedereen met een GTX970/R9 290 of beter nog steeds geen fatsoenlijke kaart om naartoe te upgraden."" Ik heb nog een 970 liggen maar volgens mij draait hij BF4 met moeite op 4k Als je alles op 1080p gaat draaien en je hebt geen hoge framerates nodig is een 970gtx nog prima, maar kom op, wil je 4k..."
RX 5700 XT;3;0.3694731295108795;Kijkend naar iemand die net zijn scherm van 1080p 60Hz heeft ge-upgrade naar 1440p 144Hz, en daar nog een nieuwe videokaart bij zoekt, krijg je met een 5700XT 8-10% betere prestaties t.o.v. 2060super, voor hetzelfde geld. Wil je echter meer frames en/of ondersteuning voor raytracing dan kom je weer bij Nvidia uit. Prijs/kwaliteit zit dus wel goed bij AMD (door de prijsdrop dan met name) al is het nog wel de vraag hoe ze precies in markt komen en dan met name de kaarten met andere/betere koeling. Zelf zoek ik ook een kaart die niet te warm mag worden, dus ik wacht nog even de reviews van andere modellen af voordat ik een keuze ga maken.
RX 5700 XT;5;0.5658463835716248;Kan een zeer interessante videokaart worden in die prijsklassen en doet het niet slecht op 4K. Ik ben zeer verrast door Navi
RX 5700 XT;3;0.45708951354026794;Navi is dus niet de revolutie die eerst voorspeld werd? Maar wel fijn dat … let op... met de nieuwe prijzen dat het wat concurrentie voor NVIDIA biedt. Maar de NVIDIA killer is het dus niet en laten we hopen dat de gebruikersreviews niet zo negatief worden als bij de Vega (Als je geen AMD fanboy bent zou ik dus nog een paar maandjes wachten?)
RX 5700 XT;2;0.3622393310070038;Ter aanvulling op deze review. Let even op dat deze kaarten nog steeds de multi monitor bug hebben met betrekking tot opgenomen vermogen. Bij de vrienden van Techpowerup stijgt het idle verbruik van 8W naar 36W. Zie Dit is iets dat helaas ook al voor kwam bij onder andere de RX 480 en is de reden geweest dat ik destijds niet voor die kaart ben gegaan. Ik gebruik namelijk twee monitoren. Zou mij al snel 20 euro per jaar aan extra stroom kosten terwijl daar geen enkele aanleiding toe is.
RX 5700 XT;5;0.26259368658065796;Oef, mijn twee jaar oude GTX 1080 Ti begint zo wel snel ‘oud’ aan te voelen, destijds een kaart van 750 euro, goed om te zien dat vergelijkbare prestaties inmiddels voor een aanzienlijk lagere prijs te krijgen zijn. Nu nog een betaalbare RTX 2080 Ti, een ‘Super’ variant van de 2080 en 2080 Ti zullen vast ook wel in de pijplijn zitten, maar Nvidia zal wachten op Navi 20.
RX 5700 XT;4;0.6757898926734924;Zeer nette resultaten van de Navi kaarten. Jammer alleen dat de AiB modellen pas ergens mid augustus uitkomen. Die zullen door de betere koeling waarschijnlijk betere boostclocks kunnen vasthouden en dan nog meer in de nek van de 2060/2070 Super hijgen. Goed gedaan AMD, kan niet wachten om de RX 5800(XT) en 5900(XT) te zien.
RX 5700 XT;5;0.3961949646472931;Mooi nieuws ik geef naast Ryzen 3600 ook Navi en X570 een serieuze kans en ben zeer benieuwd hoeveel winst er gehaald kan worden met PCI E4 en wacht een Zen 2 + X570 + RX 5700 Benchmark af Voor nu lijkt mij een Ryzen 2600 + B450 de meeste Bang voor Buck voor omstreeks de 200€ met een RX 5700 voor 370€ of een RTX 2060 voor 350€ komt op omstreeks de 550€ a 600€ Hopelijk geeft PCI E4 het excuus om voor een volledig AMD systeem te gaan want dat is voor nu het grote voordeel van X570 ten opzichte van B450 en X470 en Intel Moederborden
RX 5700 XT;1;0.4326013922691345;"Ben ik nou de enige die een grapje is opgevallen in de tekst van het artikel in de sectie Referentieontwerp met blower ? ""Het referentieontwerp van de RX 5700-kaarten gebruikt een koeler met een blowerontwerp. Daar is niet iedereen ""fan"" van..."". Ik heb misschien een weird sense of humor"
RX 5700 XT;4;0.41965433955192566;Hahaha, ja dat viel me ook onmiddellijk op. Leuke woordspeling.
RX 5700 XT;3;0.4874328374862671;Zie in t artikel niets staan over de nieuwe functies waar AMD zo mee adverteert voor deze kaarten (Kan dat ik eroverheen heb gelezen...t is nogal n lap tekst zo laat op de avond) Anti-lag (toch wel handig vooral in multiplayer games)...dit had best ook gebencht mogen worden...gamen is meer dan fps en zo n functie maakt te speelbaarheid toch net iets beter Image-sharpening & upscaling zonder performance verlies...ook dat had met wat side-by-side screenshotjes wel gebencht mogen worden. Zou iig een functie zijn die ik standaard aan zou zetten voor beter beeld zonder fps drops. Fidelity fx lijkt me wat moeilijk om te testen maar lijkt toch ook een leuke toevoeging te zijn. Beetje jammer...zou graag wat meer zien dan wat fps tabelletjes op n site zoals tweakers
RX 5700 XT;2;0.3743163049221039;Volgens mij zijn dit allemaal softwarematige dingen. Games moeten het dus ook implementeren. Uiteindelijke is dit nog steeds hun graphics core next met dubbele rops. Er zit dus niet echt iets nieuws in, maar je mag dit niet te luid zeggen ook al zegt amd dit zelf.
RX 5700 XT;5;0.42325881123542786;I see what you did there
RX 5700 XT;2;0.5390602946281433;AMD, ofwel voormalig ATI, is altijd al een zeer interessant alternatief geweest in het middensegment. Maar dit keer vind ik de prijsverschillen maar gering. Dan zou ik voor die paar tientjes meer toch voor de wat toekomstbestendiger (want raytracing) Nvidia gaan en dan zeker de super varianten van de 2060 en 2070. De 2080 serie is way overpriced en niet interessant tenzij je geld teveel hebt.
RX 5700 XT;5;0.6428641080856323;Amd wins again baby
RX 5700 XT;2;0.3363321125507355;Is het slim om nu een vega 56 met een ryzen 1700 te kopen voor een gamepc van ongeveer 800/900 euro? Of kan ik beter een van de nieuwe videokaarten kopen met bijvoorbeeld een ryzen 3600? Alvast bedankt voor je reactie!!
RX 5700 XT;2;0.47654813528060913;Zou dan sowieso voor een Ryzen 2000 serie gaan. De Vega kaarten zijn stroomvreters, zou ik persoonlijk ook links laten liggen. Aan de andere, heb je het nu nodig dan kan je misschien wel leuk zaken doen.
RX 5700 XT;2;0.3279159963130951;Wat voor gpu zou ik dan kopen? Ik zit te denken aan 2060 of 5700. En hoe duur worden de custom 5700 modellen onegeveer?
RX 5700 XT;1;0.6768648028373718;“...maar in alle gevallen levert het in 4k met Ultra-settings geen speelbare framerates op”. Serieus? Bijna 40fps is niet speelbaar? Wat voor 1337 tiener twitch gamer ben je dan wel niet? Wat een onzin, Tweakers onwaardig. Edit: Sony en MS belden net, zijn het met me eens.
RX 5700 XT;3;0.3805404007434845;Je hebt een Ryzen3 platform liggen en hebt een stel NAVI GPU`s liggen. Dus ga je die samen testen. NOPE. De afstand naar de andere testbench zal wel te ver lopen zijn.
RX 5700 XT;2;0.3482671082019806;"""Serieuze concurrent in het middensegment"" Gisteren nog zwaar gedownvote omdat ik zei dat de 5700XT geen high-end maar mid-range was."
RX 5700 XT;3;0.383082777261734;Beter cloud gamen met deze prijzen tegenwoordig voor single player games.
RX 5700 XT;5;0.784862220287323;Aangezien de GeForce kaarten niet werken onder Mojave in mijn Mac Pro’s, dan zijn dit dus prima upgrade-kaarten. Top AMD !
RX 5700 XT;1;0.5214317440986633;Wat een framerates zeg. Ik ben benieuwd hoe mijn nVidia 8800GT voor prestatie index haalt. Hehe, ik denk negatief. Ik moet echt nodig een keertje upgraden.
RX 5700 XT;5;0.5085221529006958;Waar is de echte Grote chip
RX 5700 XT;3;0.5184087157249451;"Leuk, deze kaarten zijn betaalbaar en snel, maar een ding missen ze wel ten opzichte van Nvidia, en dat is degelijke ""compute"" support. En van de meest gebruikte frameworks voor deep learning, TensorFlow, ondersteund deze GPUs of de vorige generatie zover ik weet nog niet. Alleen GPUs van Nvidia worden ondersteund. Dus als je hardware acceleratie wil met deep learning blijf je toch vast zitten aan Nvidia. Iemand enig idee of daar bij AMD focus op is? En zo ja, wanneer we daar wat van zien?"
RX 5700 XT;5;0.42379334568977356;Luid en heet. Kunnen die lui bij AMD nou nooit eens een fatsoenlijke koeler bouwen
RX 5700 XT;1;0.34868893027305603;Ik wacht nog met oordelen tot de 5800(XT) en de 5900(XT) kaarten gereleased worden. Dan is het nog wachten op de 5900X2 en dan koop ik er een.
RX 5700 XT;1;0.6245391964912415;Dat is exclusief btw. Als je Nederland kiest en checkout klikt wordt het 427 euro.
RX 5700 XT;1;0.40850943326950073;In Duitsland hebben ze 19% BTW wij 21% dus scheelt inderdaad 2%
RX 5700 XT;5;0.4415142834186554;9 vd 10 is www.amazon.de goedkoopste + voorraad.
RX 5700 XT;1;0.6107667088508606;En jammer genoeg leveren die niet in België. Begrijp ik niet.
RX 5700 XT;1;0.4107653498649597;klopt maar die prijzen zijn ex BTW 350 * 1.21 = 423.5
RX 5700 XT;1;0.6291405558586121;Hmmmm trollen maar zelf niet correct kunnen schrijven..... en lezen blijkbaar ook niet ......
RX 5700;2;0.45877212285995483;Kan iemand mij uitleggen wat het nut van de duurtest is? Ik begrijp waarom het interessant is, als het zinvol gebracht wordt. Maar nu zien we de kloksnelheden na een uur. Ja joepie, en dus? Wat relevant zijn is de framerates na een uur. Er staat dat het elke seconde gelogd wordt, leuk, maar misschien ook die resultaten dan delen? Kunnen we ervan uitgaan dat in de benchmarks hij permanent op zijn boost clock draait? En gezien die een 8% hoger is dan de klok na een uur, dat we dus 8% van alle benchmark resultaten moeten aftrekken? En ik begrijp wel dat alles langer stressen een heel karwei is, maar ik heb zelf liever minder benchmark resultaten als die dan wel accurater zijn, een 8% verschil is nogal significant. Of worden de benchmarks bij jullie testopstelling zo snel achter elkaar gedraaid dat hij wel warm blijft? Want als ik de Super review erbij haal (reviews: Nvidia RTX 2060 en 2070 Super - Meer waar voor je geld dan begrijp ik uit de bewoordingen daar dat die wel stabiele klokfrequenties heeft. En als ze dus allemaal benchmarks krijgen nadat ze zijn afgekoeld, dan zijn de resultaten hier dus gewoon 8% te hoog vergeleken met de echte resultaten (waarbij ik de echte resultaten zie als wat je krijgt als temperatuur stabiel is geworden, immers maakt niemand het uit dat een kaart de eerste 5 minuten sneller draait). Oké eerlijk is eerlijk, als de AIBs komen met fatsoenlijke koelers dan is dit probleem er hopelijk niet meer, maar ik vind dat er wel meer aandacht aan besteed had mogen worden. Overigens andersom: Als benchmarks zo snel achter elkaar worden gedraaid dat de kaart warm blijft, dan zou een kaart met fatsoenlijke koeler dus een 8% sneller moeten zijn.
RX 5700;2;0.519751787185669;Ik heb de tekst bij de duurtest nog wat aangevuld. De boostclocks die AMD opgeeft worden in deze praktijktest helemaal niet gehaald. AMD geeft ook zelf aan dat dit waarden zijn die alleen onder 'optimale omstandigheden' gehaald worden. Dat is ook de reden van de introductie van de Game Clock. Dat is de snelheid die je in de praktijk moet verwachten en dat blijkt ook te kloppen. Gedurende de hele duurtest zijn er weinig grote schommelingen. De eerste paar seconden liggen de kloksnelheden iets hoger, maar dat is geen 8 procent. De 5700XT begint op 1817MHz en zit na een paar seconden al rond de 1800MHz. We doen de duurtest om te kijken of er extreme verschillen in de kloksnelheden optreden bij langdurige belasting. Dat is hier dus niet het geval.
RX 5700;3;0.48420390486717224;Nuttige toevoeging, bedankt daarvoor. Dan weten we iig dat de benchmark resultaten wel redelijk nauwkeurig zouden moeten zijn voor ook als je een uur aan het gamen bent. Dan toch wel een verzoek voor toekomstige reviews (of deze al ): Dat is natuurlijk niet heel interessant voor ons als lezer, als jullie vervolgens alleen de laatste (gemiddelde) waarde geven. Zou een grafiekje niet veel interessanter zijn? Van temperatuur, fanspeed en klokfrequentie? Vergelijkbaar met bijvoorbeeld:
RX 5700;3;0.3876454830169678;Dat is relevant om te zien wat er gebeurd als de kaart goed wam loopt en of er dan throttling komt of niet, ik game meestal toch wel langer dan een uur en als een kaart na een uur (te) warm loopt heb je een probleem.
RX 5700;2;0.4569360911846161;Uiteraard begrijp ik het idee erachter, maar de getallen zijn nietszeggend. We zien niet hoe het over de tijd gaat, we weten niet in welke situatie de benchmarks zijn gedraaid, en we weten niet welke invloed het op de framerate heeft.
RX 5700;2;0.5455145835876465;Met wat software gooi je simpelweg gewoon de fan speed wat omhoog en de kaart zal nooit aan zijn max temp komen. Mijn 980ti draait rond de 80 graden als ik de fan speed van 23 naar 50% zet dan maakt de kaart weliswaar iets meer geluid maar de videokaart komt hierbij nooit meer boven de 60 graden ook niet na 4 uur spelen. Heb het overigens altijd raar gevonden dat videokaarten met stock settings altijd grijpen naar down throttle en nooit naar de fanspeed ophogen om koel te blijven. Voor constante max performance zal echter dus altijd handmatig aan de fanspeed gedraaid moeten worden
RX 5700;3;0.4762724041938782;Toch jammer dat AMD wéér voor het middensegment gaat en dat ze steeds nét niet bij die top komen. Ze proberen nu met de 5700XT de 2070RTX te verslaan maar ondertussen heeft Mvidia nog 2 topmodellen daarboven zitten. Geen hardwarematige raytracing lijkt nu nog geen groot probleem maar een hoop toekomstige spellen krijgen daar wel ondersteuning voor. Cyberpunk 2077, bijv. en daar lijkt me het met al dat neonlicht een mooie toevoeging en dat je dan nog maar 60-75 FPS haalt.. zolang het maar sneller is dan de refreshrate van de monitor, prima toch? Het is het bij AMD vaak nét niet, erg jammer want ik ben opzich wel een AMD fanboy en ik zou graag zien dat ze de concurrent crushen, lukt ze nu met Intel oo redelijk mar Nvidia is kennelijk nog een stap te ver.
RX 5700;2;0.37189579010009766;"Er wordt schijnbaar nog aan een 'grote' Navi variant gewerkt, die in het hogere segment de strijd moet aangaan. Maar het gros van de markt koopt gewoon geen 700-1200 Euro videokaarten (en terecht, wat een absurde prijs..). Begrijpelijk dat AMD zich daarop richt dan; dat is een veel groter deel van de markt. Buiten dat, heeft AMD hiermee zo te zien degelijke kaarten neergezet. Een betere value proposition dan de nieuwe Super kaarten - goed voor de consument. Éigenlijk zijn ze nog steeds te duur voor 'midrange' videokaarten, maar goed."
RX 5700;3;0.38047659397125244;Alles is relatief. De prijsrange die je noemt is toevallig ongeveer ook die van high end telefoons. Dan heb ik meer en veel langer plezier van een videokaart(telefoon is 600 uiterste max voor mij). Goedkoper is altijd fijner natuurlijk, maar ik vind het het geld dubbel en dwars waard. Maar ieder heeft daar zijn eigen mening ovet natuurlijk en dat is prima.
RX 5700;2;0.4562873840332031;Uiteraard, je hebt volledig gelijk. Het is alleen jammer om te zien dat de pricerange zo verschoven is, van €200-ish midrange met €500-ish high end naar €400 midrange en €700-1200 high-end. Dit is puur het resultaat geweest van het feit dat AMD geen competitie kon bieden, en nu het de nieuwe status quo is, gaat AMD helaas ook mee hierin.. @EMR77 : Klopt, je laat je stem horen als consument met je portemonnee. Aan de andere kant, NVidia's GPU-omzet is gekrompen met bijna 50% als gevolg van teleurstelling in de RTX kaarten. Er is dus wel iéts gebeurd, maar te weinig.
RX 5700;2;0.44186681509017944;Ik wens AMD ook competitie op de high end gebied toe, dat is beter voor ons allen(nu heb ik helaas geen keuze). En ik hoop dat Intel volgend jaar ook met een knaller komt. En bij telefoons de prijs het net zo hard verschoven, daar was 1200 euro 5 jaar geleden ook ondenkbaar...
RX 5700;2;0.506025493144989;Dat is het nu bij veel merken nog steeds hoor. Er is maar een handjevol uitschieters en je ziet dat die ook redelijk afgestraft worden. De verkopen bij Apple dalen niet voor niets. Er is genoeg betaalbaar alternatief en 200-300 toestellen zijn nog nooit zo snel en capabel geweest. De huidige 200-300 euro toestellen concurreren soms zelfs met de high-end toestellen van een jaar eerder. Op GPU vlak zijn er maar twee spelers, waarvan ze nu beide een flinke prijs vragen en de segmenten hebben lopen rekken. Er was voorheen troep, midrange en high-end, nu zijn er veel meer tussensegmenten te vinden die allemaal veel duurder zijn. Ooit kocht je voor 200-300 euro een high-endje met iets minder renderdingen (HD7950 bijv.) en nu zijn er bij nvidia alleen al 5 alternatieven voor de duurste kaart. Bij telefoons is er geen verschuiving, maar gewoon een paar merken die gek geworden zijn. Bij videokaartjes is er daadwerkelijk een verschuiving veel duurdere kaarten.
RX 5700;1;0.6212844252586365;Maar de consumenten bleven massaal overpriced kaarten kopen van Nvidia Waarom zo AMD niet meedoen??? De consumenten hebben laten zien, dat het ze niet uitmaakt. Anders was het marktaandeel van Nvidia allang gekrompen.
RX 5700;3;0.48075613379478455;Leuk dat je het met een telefoon vergelijkt maar dat is een compleet product, een videokaart is nogal saai zonder verdere hardware. Ondanks dat ben ik het wel met je eens hoor...
RX 5700;3;0.35170549154281616;Succes dan met je telefoon zonder abo... Denk dat het prima te vergelijken is. De dure telefoon is net zo 'bovenop een simpele PC' als de videokaart dat is, toch? Die simpele PC of simpele telefoon heeft vrijwel iedereen, ga je duurder dan doe je dat heel bewust en is het een luxe artikel. De vraag is of je de meerwaarde ziet. Ik zie die totaal niet in een toestel van over de 300 euro. Anderen zien het niet in een GPU van over de 300 euro.
RX 5700;2;0.37038177251815796;De 5700XT/2070RTX Super is geen mid-range. Dat is het begin vd high-end en zo'n kaart koop ik altijd, mits ze onder de €500 blijft. Dat doet de Super kaart niet maar de AMD wel. De AMD-kaart is wel iets trager maar veel scheelt het niet. Edit: ofwel heb ik een verkeerd beeld van wat mid-range en high-end is. Voor mij is mid-range Nvidia 2060 S/5700 en high-end begint van 5700XT/RTX 2070 S tot RTX 2080Ti. Als iemand het anders ziet zegt hij het maar.
RX 5700;3;0.40200555324554443;Ik snap AMD wel, het grote geld zit nog altijd in het middensegment en als ik zou willen concurreren met Nvidia zou ik ook in dat segment beginnen. En het verschil tussen 60-70 FPS en bijv. 140 FPS is écht wel merkbaar, ook op een 60 Hz monitor. Ik vind het zelfs zo merkbaar dat ik liever op low speel met hoge FPS dan op high met 60 FPS.
RX 5700;4;0.24137257039546967;Natuurlijk is er meer geld te verdienen in het middensegment maar qua als AMD meteen ook Nvidia van de troon weet te stoten qua snelste videokaart dan is dat wel erg goede reclame. Amd komt nu met een kaart die net zo snel is als de 2070 maar geen hardware raytracing heeft. Nu hoeft die raytracing je verder niet te boeien maar de mogelijkheid om het te gebruiken is er in elk geval. Zoals je nu ook bij de CPU's ziet, daar weet AMD Intel wel van de troon te stoten met een CPU die sneller is dan de 9900K en iedereen is gehyped over de Zen2 processor. Dat hadden ze ook met AMD moeten doen, meteen die 5800X en 5900X op de markt gooien, ook al kost die €700,- Als de snelheid beter is dan een 2080 (ti) dan is het dat geld ook waard en krijgt AMD een hoop gratis reclame.
RX 5700;2;0.3208288848400116;Als de geruchten kloppen komen er nog 56 en 64 cu versies van NAVi (zoals vega) en als ze de problemen met hun GCN arch. hebben opgelost, vermoed ik dat de 64 zeker sneller is dan de 2080S.
RX 5700;2;0.3743891417980194;Heb je hier wat meer info over? Ik was hiervan niet op de hoogte. En ben ook benieuwd welke problemen er zijn met de 64.
RX 5700;2;0.5147448182106018;Bij de 64 konden ze blijkbaar niet al de Cores bezig houden waardoor je dus performance problemen hebt. Ook de overhead van de waves (meer clock tikken nodig dan nu met navi) geeft je nog maar eens een nadeel.
RX 5700;2;0.4304177165031433;"Sorry, maar wat jij en een aantal anderen 'echt wel merkbaar' vinden, vind het merendeel van de wereld echt niet 'niet echt'. Wellicht dat het merkbaar onder bepaalde omstandigheden bij bepaalde games, net zoals met 1080p/4k video, maar dat zijn in mijn ervaring eerder uitzonderingen dan de regel. Wellicht dat super jonge ogen dat wel merken, maar mijn kennis/ervaring zegt eerder dat ze denken dat ze het wel merken. Aan de andere kant dat het wellicht minder opvalt bij mensen die zijn opgegroeid met een Atari 2600 (hout),,, ;-)"
RX 5700;1;0.42759954929351807;Ook wij van atari hout zien het verschil, koop maar een 144 hz scherm, zal je zien dat je wel degelijk verschil ziet...ik was ook een unbeliever maar nu nie meer.
RX 5700;4;0.4347721040248871;Precies. Ik heb mijn ouders een keer voor mijn Eizo gezet en sindsdien snappen zij het ook. En dan heb je het wel over twee totale digibeten.
RX 5700;3;0.384215384721756;Ja jammer alleen dat ze er 3 jaar te laat mee zijn. We hadden al een GTX 1080, 1080 11Gbps, 1070ti, Vega 56, 64, RTX 2060 en RTX 2060 super. Serieus...als je nu nog aan het wachten bent... Dit prijspunt was allang bereikt.
RX 5700;2;0.445181280374527;Zolang je niet onder de 60fps dipt maakt het geen verschil op een 60hz monitor. Op wat inputlag na, maar die winst is verwaarloosbaar met zo'n traag scherm.
RX 5700;3;0.44577279686927795;Dit is niet helemaal waar.. verschillende testen (te laat om nu bronnen te vermelden) hebben laten zien dat een hogere FPS op een 60hz scherm weldegelijk een vloeiendere ervaring geeft die meetbaar en merkbaar is.
RX 5700;2;0.395381361246109;Wat zou ik niet snappen? 60 hz monitor is maximaal 60 beeldjes per seconde. Meer beeldjes naar je monitor sturen kan niet. Enige voordeel is dat je input lag theoretisch gezien zou kunnen afnemen, maar dat is verwaarloosbaar omdat je al met bijna 17ms extra van je scherm zit.
RX 5700;5;0.47241196036338806;Fast Sync, en de totale lag is meer dan een monitor refresh. Dat is wanneer jouw input zichtbaar wordt, niet wanneer je 'm registreert. Verder zit je met hogere interne FPS veel dichter tegen de refresh rate momenten van je 60hz paneel aan, waardoor tearing ook minder kan zijn. Dat is ook game specifiek. En het allerbelangrijkste: bij een framedrop is de kans onder de 60 te komen veel lager. Leuk, die minnetjes, maar het toont maar weer eens aan hoe diep Tweakers.net gezonken is. De lamme leidt de blinde hier, diep triest. Ik snap @BruceLee dan ook heel erg goed. Het is erg vermoeiend. Enjoy
RX 5700;1;0.8519813418388367;"""Zolang je niet onder de 60fps dipt"", zegt @Anoniem: 159816 . Dan is die kans nul... En een -1 op een respect- en inhoudsloze reactie ""Als je er niks van snapt kan je beter niet reageren."" is volkomen terecht. Niks ""hoe diep Tweakers.net gezonken is""."
RX 5700;1;0.4408296048641205;Zelfs in iets simpels als Overwatch dip je nog bij tijd en wijlen onder de 60, dus ik weet niet waar dat statement vandaan komt, maar veel praktijkervaring zal het niet zijn.... en als je een frame cap aanzet of Vsync dan is de kans helemaal levensgroot. Net zomin als het 'goed snappen' bevestigd zou worden door het statement dat je er geen profijt van hebt 'omdat je het niet kan zien'. Het is ook niet alleen input lag namelijk. Of omdat je toch al op 'slechts 17ms zit'... getuigt ook niet van veel ervaring.
RX 5700;5;0.407906711101532;Wow, Koeitje heeft 100% gelijk, als je een 60Hz scherm heb is alles hoger gemiddeld weggegooi energy (zelfde voor 100 en 144Hz en hogere schermen waar de FPS hoger zijn dan de Hz van de scherm), omdat je GPU/CPU harder moet draaien, en het zorgt er voor dat je scheuren krijgt (of hoe dat heet) op je beeld, en is G-Sync en FreeSync erg handig om te hebben, of je capt hem in je spellen op 60fps, wat steeds meer spellen hebben.
RX 5700;2;0.3637373745441437;nVidia heeft 'fast-sync' waarbij de GPU maximaal presteert om de input lag zo laag mogelijk te houden, maar tegelijkertijd de frames die naar de monitor gestuurd worden gecapt zijn op de refresh rate. AMD heeft naar ik aanneem een vergelijkbare techniek.
RX 5700;1;0.30421099066734314;Heb je nog steeds 16.66667 ms input lag.
RX 5700;1;0.287261426448822;Als een scherm maximaal 60fps kan weergeven, hoe kan je dan meer dan 60 fps weergeven? Ben benieuwd naar je uitleg Bruce.
RX 5700;3;0.6466809511184692;Bij mijn geforce 960 is het wel fijn dat ik de settings zo zet dat ie wat meer kan produceren dan 60fps. Als de kaart dan gelocked is op 60fps belast je de kaart wat minder en heb je minder warmte en dus herrie. Mijn voorkeur is dus meestal om niet helemaal het maximale uit de videokaart halen
RX 5700;3;0.38248100876808167;je kunt ze niet weer geven maar wel renderen, Vergeet niet dat heel veel game code draait per tick of te wel per frame, een terp die reageert op de tick van de framerate zorgt voor een veel smoother terp.
RX 5700;2;0.4416724443435669;Games draaien ook een simulatie thread die minder ticks per seconden verwerkt dan de graphics threads. Graphics werken toch op interpolatie van de simulatie thread. Het blijft maar een representatie.
RX 5700;2;0.3975280821323395;60hz = 16,6667 ms tussen frames. Als je framerate hoger is dan dat reageert de game inderdaad sneller op een muisklik, echt is deze muisklik gebaseerd op jouw input welke weer gebaseerd is op een traag scherm. Je reageert pas als je het op je scherm ziet en dan klik je. Dan maakt het niet meer uit, het effect zie je toch pas bij de volgende frame 1/60 sec later.
RX 5700;2;0.464539498090744;"In theorie wel, maar dan ga je ervanuit dat je als gamer op dat éne cruciale beeldje zit te wachten waardoor je een bepaalde actie in gang zou zetten. Niet alles gaat om cijfertjes. Dit gaat over een gevoel van responsiveness - en met 120 fps op 60hz halveer je letterlijk de reactietijd per frame. En frames lopen constant door. Die optelsom maakt dat het geheel soepeler en beter aanvoelt. Wij mensen zijn maar traag; 100-200ms reactietijd is niet ongewoon, sterker, richting de 150-200ms is gemiddeld. Dus op dat grote getal lijken al die kleine beetjes winst sowieso al peanuts. Maar ze zijn het niet. Ons brein pakt elk beetje ondersteuning met beide handen (?) aan. Ik zal je zeggen, als ik nu 30 FPS op een console speel dan voelt het alsof ik dronken rondloop. Je went aan een bepaalde respons, en teruggaan is lastig. Ben je dan effectief beter en sneller geworden? Ja - dat heb je dan geconditioneerd namelijk. Het verschil van 60>120 is gevoelsmatig kleiner dan het verschil tussen 30>60 (terwijl er het dubbele verschil in FPS tussen zit!) maar het is wel degelijk aanwezig. Dat is lastig met een 'ms' uit te drukken. En visueel kan het dan alsnog achterlopen, het soepele gevoel blijft."
RX 5700;1;0.4041692614555359;Euh ten eerste mijn scherm is 100Hz en laat me spellen op 100fps lopen als dat gaat, en ten tweede wat is er met je aan de hand dat je zo reageert op mensen, is een normaal gesprek hebben zo moeilijk voor jou?
RX 5700;1;0.2698526680469513;Nah je moet geen vsync etc ingame aanzetten krijg je weer extra inputlag van..
RX 5700;2;0.4305330216884613;Doe ik ook niet echt, ik gebruik als dat kan de ingame FPS cap, en steeds meer spellen hebben dat, waarom zou ik me grafische kaart 100% kontakt laten draaien als dat niet hoef, en heb geen last van inputlag, en anders gewoon geen cap als die er niet is, heb trouwens G-Sync op mijn monitor zitten.
RX 5700;2;0.5300638675689697;Tearing is veel subtieler bij hoge fps , maar ook subtieler bij slow pace games. Wat mij meeste irriteerd en erg onrealistisch en storend vind in shooters is bunnyhop gedrag iets wat NPC nooit doen. En rusteloos inertialoos staminaloos bewegen als vlieg en dan zuiver kunnen schieten. Voor PC gamen is het eigenlijk ook Pay to win. Je koopt Ultrawide Gsync 144hz sherm een 2080ti en je hebt een voordeel tov meerderheid met een 60hz scherm en 1060 of 2060 . Voor online gamen op console is die variatie minder extreem en minder last van cheaters. Maar ik prefereer milsim games waar idioot bewegen je zuiverheid drastisch kelderd. Toch speel ik ook CoD en BF voor mij zijn ze gelijk kwa CQC het blijven bunnyhop games de ander is de game wereld wat groter en met tanks en vliegtuigen. Beide zijn arcade . Arma gaat mij te ver en Game als Operation flashpoint Red River is voor mij de juiste dosis realisme en met coop vs npc ook geen bunny hoppen.
RX 5700;1;0.5707507729530334;Nee sorry ik geloof niet dat de verschil tussen 60 en 144Hz zo enorm is, het licht er nog steeds aan of je goed bent in shooters online, ken meerdere mensen die een 60Hz monitor hebben en gewoon vaak winnen in shooters online.
RX 5700;5;0.24480095505714417;Het zijn niet alleen de frames op je scherm die belangrijk zijn. In FPS op het een 60Hz scherm wil je gewoon 120,180 of zelfs 240fps hebben ondanks dat je scherm maar 60 is. Dit alles heeft te maken met andere aspecten in een game engine dan alleen de weergave.
RX 5700;2;0.39720526337623596;125fps was relevant in Quake III omdat de physics afhingen van je fps en verder kon je CRT bak naar 120hz. Ik ken niet één moderne game waarbij de physics afhangen van je framerate en de meeste LCD schermen gaan niet verder dan 60Hz (144Hz schermen zijn dik in de minderheid). 60Hz betekent 60 beelden per seconden. Meer dan dat wordt niet getoond. Het verschil in inputlag merk je ook niet bij moderne games al was het maar omdat ze retetraag zijn (Fortnite val je bij in slaap als je het vergelijkt met Painkiller of Q3), met mogelijke uitzondering van de lui die echt competitief spelen.
RX 5700;2;0.38620099425315857;Er zijn wat console games waarbij de physics gelockt zijn aan de FPS, waardoor dat soort games op de PC hard locked zijn op 30 of 60 fps.
RX 5700;3;0.3388555347919464;Bijvoorbeeld bij de Orginele Call of Duty was dat belangrijk (dit heb ik op hoog niveau gespeeld) daar had je een voordeel wanneer je het spel op 125fps of op 333fps zette. Op 333fps kon je sneller schieten en hoger springen. Je scherm kan dat niet aan maar de game zelf laat die FPS wel zien en zo krijg je voordeel ten opzichte van iemand die de hoge fps niet kan halen. Daarom werd later ook een config checker gebruikt die stelde dat je fps niet hoger ingesteld mocht zijn dan 250.
RX 5700;2;0.3610457181930542;Wie gaat nu al een kaart aanschaffen voor een game die voorlopig nog niet uit is en je geen flauw idee heb hoe goed deze gaat lopen op de huidige kaarten. Voor zover ik kan zien moet je gaan kijken naar welke games je speelt en wat vervolgens je budget is voor een nieuwe kaart. De 5700xt is in een aantal gevallen sneller dan €100 duurdere RTX270 Super, leuk als dat je main games zijn... Maar het ligt geheel aan op wat voor resolutie/kwaliteit en welke game je speelt. Imho is dat de fans onder load veel luidruchtiger zijn een veel groter issue. Wellicht dat dit bij kaarten van andere fabrikanten beter wordt...
RX 5700;2;0.4335896968841553;Van Cyberpunk zijn er al gameplay beelden met een RTX2080 en daar ziet dat raytracing er wel erg mooi uit. En volgens jaar dan heeft Nvidia wel al een kaart met hardware raytracing op de markt terwijl AMD daar in elk geval tot 2021 niet mee komt en als ze er mee komen dan komen ze met een hybride oplossing, niet volledig hardware. Ik heb nu een aantal reviews gezien en over het algemeen komt de 5700XT slechts in de buurt van de RTX2070 (veel tests hebben nog niet de super variant getest) en voor het extra geld krijg je er wel weer hardware raytracing bij.
RX 5700;2;0.5320300459861755;Op 't moment is raytracing meer een gimmick dan een significante toevoeging. Visueel zijn de verschillen zeer minimaal, en het kost je grofweg de helft van de framerate. En Nvidia vindt dat je er extra voor moet betalen, no thank you!
RX 5700;3;0.4991089403629303;Prestatieproblemen zijn er momenteel zeker, maar een gimmick zou ik het niet noemen. Raytracing maakt toch wel een groot verschil wat mij betreft.
RX 5700;4;0.28869277238845825;Er zit nog meer aan te komen:
RX 5700;2;0.40116021037101746;Voor deze prijzen is het geen midden segment meer. Meer hoog. Aangezien Nvidia nu alle rtx kaarten vanaf 2060 hoog noemt. En mid zijn de 1660(ti). Top zijn de 2080 en 2080ti. Midden segment is toch wel een prijs tussen 200 en 300. Dat word de 5600 wel. Rx5600xt word zeker vega 56 prestaties voorbij. Aangezien de rx5700 de vega64 voorbij gaat. En als we de prijzen mogen geloven word die dus 300 usd en de rx5600 250 usd. 5500xt = 200 en 5500 = 150. Betekent dat we met deze generatie alles 50 usd meer moeten gaan betalen. Hopelijk gaan ze nog flink oorlog voeren in prijs. Alleen Nvidia kennende vinden ze hun kaarten nu toch wel goed geprijsd en zal er niks van komen.
RX 5700;2;0.5252195596694946;"AMD is vele malen kleiner dan Nvidia en zeker Intel en toch moet het concurreren met beiden. Dat ze het op dit moment Intel erg moeilijk maken is al erg knap en bijna een wonder te noemen. Ook Nvidia ""crushen"" is gewoonweg (jammergenoeg) niet realistisch. Uit noodzaak ""kiezen"" ze weer het middensegment, omdat daar simpelweg het meeste geld wordt gemaakt. Daarnaast kost het erg veel investeringen om een topkaart te maken als de RTX 2080Ti. En dan is het de vraag hoe veel mensen een flagship kaart willen kopen van AMD."
RX 5700;3;0.46347159147262573;Je praat wel over raytracing alsof dat al een iets is wat de toekomst is, maar dan heb je het over 60 fps dat dat prima is want dat is je refresh rate ook. Meeste gamers zitten toch echt op 144 hz, dat is een groter pluspunt dan wel of geen raytracing, freesync of niet.
RX 5700;2;0.46283838152885437;Meeste gamers zitten helemaal niet op 144Hz. De meeste serieuze gamers misschien wel, maar het gros zit gewoon naar een 60Hz 1080p TN scherm te kijken. En als ze wel een 144Hz scherm hebben dan draait het vaak op 60Hz omdat dat de standaard instelling is.
RX 5700;2;0.39308962225914;We hebben het hier over videokaarten van 300 a 400 euro, dan ben je toch best een serieuze gamer? Dus je koopt een videokaart van 400 euro maar hebt een monitor van 100 euro, dat vind ik dan raar, en ik denk ook niet dat dat de norm is voor de meeste mensen die een kaart hebben in de prijs klasse.
RX 5700;4;0.3550514280796051;"Als je verstand hebt van techniek, ja misschien. Maar genoeg vrienden van mij die ""het verschil toch niet zien"", maar wel een rtx2060 of beter kopen. Dikke fps en geen haperingen is wat zij belangrijk vinden ipv. snelle reactietijden. Daarnaast zijn 1440p en uwhd schermen op 120 of 144hz een stuk duurder dan hun 1080p varianten. En mensen gaan vaak toch ook liever voor een groter scherm (want dat zien ze wel)."
RX 5700;2;0.3155272901058197;Lol, nee ik had eerst een 1080p 144hz scherm, nu een 1440p 144hz scherm, vrienden van mij hebben zelfs een 240 hz (cs:go) scherm, raytracing intereseert me niet maar hoog fps wel, en 60 hz omdat dat de standaard instelling is? Ben je aan t dromen ofzo, eerste wat je doet is t scherm op 144 zetten en in windows...
RX 5700;2;0.5169315934181213;Doe mij maar een stabiele 60Hz met raytracing. Ik heb totaal geen behoefte aan 144Hz. Gaat ook niet gebeuren want ik game op een TV. Ik ben dan ook niet van de shooters moet ik eerlijk toegeven. Maar zelfs bij racesims heeft 144Hz een voordeel, maar toch, daar ligt mijn focus niet op. Moet ik ook eerlijk zeggen dat met mijn instellingen een 1080ti de 144fps over het algemeen bij lange na niet trekt, dus heeft het ook niet veel toegevoegde waarde.
RX 5700;1;0.34021228551864624;Dat is wat anders, je gamed op je tv, als je op een monitor speelde zou het toch ook geen 60 hz monitor zijn van 100 euro? Je videokaart kost 500 euro, de meeste met een kaart van 500 euro hebben ook een aardige monitor lijkt mij?
RX 5700;2;0.4572722911834717;Als een monitor een betere kleurweergave of 3D biedt(ja ik heb nog een Zalman ZM-M220W ergens ), full array local dimming, HDR, of noem maar op biedt, dan vind ik dat van groter belang. Ook in het geval van een TV zou een 120 of 144Hz mogelijkheid minder belangrijk zijn. Van bijna alle zaken waarop ik selecteer staat 60+ refresh onder aan mijn lijst. Ik vind het een aardige toevoeging, maar to-taal geen noodzaak. En ik game echt vrij veel op die TV, mijn PC heeft niet voor niks een plek in de woonkamer. Ik heb het zelfs zover dat ik mijn simrig er binnen 2 minuten voor kan hebben staan. En mijn kaart was duurder dan 500 euro, mijn racestuur ook. Andere prioriteiten. Dè gamer bestaat gewoon niet, iedereen heeft zijn voorkeur.
RX 5700;4;0.42419490218162537;Dat is zeker waar, iedereen om mij heen heeft wel een 144 of 240 hz monitor, maar zal ook zijn omdat we ook veel shooters spelen competitief.
RX 5700;2;0.4935897886753082;Dan heb je een goede reden. Maar ik speel eigenlijk nooit meer shooters en al helemaal niet in multiplayer. Er is zoveel meer in PC gaming dan online shooters... Ik speel zo nu en dan wel eens een off-line shooter(al is dat óók al heel lang geleden) met verhaal zoals iets uit de Far Cry serie of zo, maar daar is lag totaal niet interessant. Je vindt het ook vast eng dat ik bijna uitsluitend met mijn X360 controller game? En in alle andere gevallen met mijn racestuur of Vive controllers. En natuurlijk maakt het bij racen ook uit. Alleen ben ik zonder 144Hz scherm rap zat. Gaat me ook veel meer om de lol dan de competitie.
RX 5700;3;0.26417815685272217;"Nee vind zeker niet eng dat je op je controller game, ik heb best veel Watchdogs gespeeld, ook met controller. Ik speel ook asetto corsa met een G25 stuurtje, en ik speel ook nog Command and Conquer; Red alert 2. Ik speel veel shooters, maar vind strategie games en racen ook heel leuk Shooters ben ik juist pas de laatste jaren geinteresseerd in geraakt, ik speelde voorheen alleen rts games."
RX 5700;3;0.5211179256439209;Was ook een beetje met een knipoog. Voor shooters en strategie games ontkom je niet aan een(goede) muis(tenzij je concurrenten dezelfde input methode hebben natuurlijk). Ik neig tegenwoordig (naast race sims)meer richting actie-adventure achtige games. Zoals inderdaad Watch_Dogs of Assassin's Creed(ben met GF overigens net een Enslaved: Odyssey to the West playthrough aan het doen). Maar ook niet al te zware RPG's, liefst wel in first person view(zoals Skyrim dus). Qua racesims doen we meestal Project Cars 2(vooral vanwege de weersomstandigheden), rFactor 2, Automobilista, Assetto iets minder vaak(ben toevallig nu wel met Competizione bezig, erg tof met een DD1 ) en als het we eens wat stoom af willen blazen Wreckfest .
RX 5700;1;0.40218842029571533;Tv's doen ook gewoon 120hz hoor .
RX 5700;3;0.3559545576572418;Inmiddels wellicht. Maar over het algemeen zijn het 60Hz panelen die verder vooral een flinke zwik beeldmanipulatie doen en zo fictief het aantal beelden per seconde verhogen. Gelukkig zijn we nu eindelijk op het punt belandt dat er daadwerkelijk 120hz native panelen komen die eveneens adaptive vsync ondersteunen.
RX 5700;2;0.26488107442855835;Ongeveer al 2+ jaar inderdaad.
RX 5700;3;0.48338302969932556;Ja voor slow pace singleplayer exploring Sandbox adventure games. Waar je ook van scenery kan genieten is 1440p meer GFX features zo ook RTX wel meerwaarde . Games zoals Stalker CoP. Crysis original . Online doe wel op PSN iedereen wat gelijker. Als ze daar ook freesync komt krijg je meer ongelijkheid. Maar nog niet zo extreem als op PC. Een Red dead redemption had ik ook liever op release ook op PC gehad.
RX 5700;2;0.38958582282066345;Waar denk je dat het meeste geld te halen is? 99% van de mensen kunnen of willen niet 1500 euro neerleggen voor de 2080ti Turbo Super AquaCooling SuperCharge Extra Plus Extreme Edition, en gaan voor de midrange kaarten, waar dus deze kaarten in zitten. Sterker nog, de XT is nog wel iets meer dan midrange. Je vergeet dat wij hier allemaal gear heads zijn, maar de meesten nog niet het geld uitgeven voor het hele systeem dat die ene videokaart kost. Dus afwachten of er iets high end komt, maar denk maar niet dat het een verkooptopper wordt, omdat de 2080ti dat ook niet is. Het is meer pochen dat je deze prestaties kunt halen met kaarten in je line-up, iets wat het gewone volk toch niet snapt.
RX 5700;3;0.41321152448654175;"De Ti is misschien iets teveel een niche product maar de gewone 2080 moet te doen zijn. En natuurlijk spreekja daarmee niet het grootste deel van de markt aan maar de hele martk kijkt wel naar je. Als men kan zeggen ""dit is onze topkaart maar we hebben voor de gamer met een lager budget ook een wat goedkopere kaart"" dan is dat een goede marketing, bied je de klanten met een lager budget precies wat ze willen maar kunnen ondertussen ook tegen de concurrent zeggen ""kijk maar uit, we hijgen in je nek"" Maar nu is het weer zo dat als je de snelste kaart wil hebben je toch weer bij Nvidia uitkomt, heb je een kleiner budget dan kom je weer bij AMD uit."
RX 5700;1;0.6518809795379639;"Pochen dus... En als iedereen met verstand ervan tegen de gewone man zegt: ""leuk die überschall dingen, maar als je waar voor je geld wilt, moet je die andere hebben"", kun je roepen wat je wilt, dan kopen alleen fanboys 'm, en daar kun je niet op leven. Of zoals mijn oma tegen mijn oom zei: ""je mag de grootste hebben, maar als je er niet mee om kunt gaan, kun je er nog niks mee"". (Ging over de zoveelste grote auto die mijn oom in elkaar had gereden)"
RX 5700;3;0.2658596932888031;Dat geeft toch niet? AMD is nu qua watt/performace ongeveer gelijk aan Nvidia. Dat is heel erg goed te noemen. AMD komt later ook met topmodellen uit die tegen de 2080ti en dergelijke moeten strijden. Maar het zou dom zijn daar vol op in te zetten, om de kroon in handen te krijgen of te evenaren. Er is nu nog schaarste met de 7nm productie, en dan moeten ze keuzes maken, en dan is het verstandiger kleinere chips te produceren waar ze een mooie marge op kunnen pakken, dan grotere die moeilijker verkopen, en minder snel te produceren zijn. AMD heeft tevens voor de nieuwe PS5 en Xbox opdracht om gpu en cpu te maken. Dus in 2020 komt dat uit. Ik ben positief verrast iig! Goede prijs prestatie. Nvidia levert nu ook meer voor minder, en dat doen ze niet zomaar!
RX 5700;2;0.41282209753990173;Nvidia wist dus precies waar AMD mee zou komen en hebben met de nieuwe Super kaarten ervoor gezorgd dat ze toch weer net iets sneller zijn en als reactie verlaagde AMD op het laatste moment de prijzen.. Goed voor de concurrentie in het midden segment wat hopelijk de prijzen ook bij Nvidia verder zal drukken.. Maar de vraag blijft wel staan waarom AMD al vele jaren niks in het top segment uitbrengt?!? De Radeon VII heeft weer akelig gefaald tegenover de 2080, laat staan de 2080 Super straks of de 2080Ti.. Kunnen ze niet of willen ze niet?!? Verder zou ik wachten op 3rd party custom kaarten met veel betere vooral stillere koelers.. Dat geldt zowel voor AMD als Nvidia.. De FE modellen alleen kopen als je echt niet kan en/of wil wachten en/of perse een blower koeler nodig hebt i.v.m. heel kleine kast ofzo..
RX 5700;4;0.3938029706478119;Ik denk dat de focus vooral heeft gelegen op de ontwikkeling van een goede CPU. Daar zijn ze zeker geslaagd. Hopelijk wordt die winst gebruikt om de GPU kant weer wat R&D geld te geven. Binnen een paar jaar zullen ze zeker weer mee kunnen doen. Als ik kijk naar het energieverbruik, is er al een goede stap gemaakt.
RX 5700;2;0.37132126092910767;Dat beter energieverbruik is vooral te danken aan 7 nm productie, met dank aan TSMC. Ik heb altijd gezegd dat Global Foundries een blok aan het been was voor AMD want ze zaten contractueel aan hun vast. Vroeger moest AMD hun kaarten altijd bij GF laten produceren maar die hadden altijd de slechtere(minder performante/meer energieverbruikende) node. Gelukkig zit AMD nu niet meer vast aan GF(ook al omdat ze hun 7 nm ontwikkeling stopgezet hebben en alleen nog 14 nm doen) en kunnen ze bij TSMC laten produceren en je ziet dat ze stilletjes aan beginnen terug te komen. Ook hun blunders met hun Fury(X) en Vega- kaarten met het dure HBM(2)-geheugen is AMD duur te staan gekomen. AMD heeft momenteel 7 nm node nodig tov 12 nm node van Nvidia om enigszins met hun te kunnen concurreren. Zo kunnen ze er meer transistors en dus hogere snelheid voor hetzelfde verbruik inproppen. Ze kunnen momenteel geen concurrent voor de 2080s/ti op de markt brengen omdat ze er de middelen niet voor hebben. Pas met rijping van het 7 nm-procede van TSMC volgend jaar kunnen ze een grotere chip en concurrent to deze Nvidia - modellen zetten. Maar helaas kanNvidia dan ook weeral met een nieuwe kaart op 7 nm komen.
RX 5700;3;0.5649401545524597;"Ik ben het gedeeltelijk, maar niet helemaal met je eens. 7nm heeft geholpen, maar vergeet niet dat de Radeon VII ook op 7nm was gemaakt. Het was een chip met 60 compute units, en toch kon het niet eens de RTX 2080 bijhouden... Niet in stroomverbruik, en niet in beelden per seconde. Als we naar de 5700XT kijken in vergelijking tot de Radeon VII, het heeft 33% minder (40 ipv 60) compute units (en dus ook TFLOPS), 24% kleinere chip, en gebruikt 25% minder stroom zonder HBM. De kloksnelheden zijn redelijk gelijk, en tóch bereikt the 5700XT bijna de snelheden van de Radeon VII in spellen (de Radeon VII is maar ~5% sneller). Computerbase.de heeft een interessante test gedaan, waarbij ze verschillende chips met gelijkwaardige aantal 'kernen' op dezelfde kloksnelheid laten draaien. Daaruit blijkt dat per klok de nieuwe RDNA architectuur van AMD gemiddeld; 39% sneller is dan Polaris 28% sneller is dan Vega 13% sneller is dan Pascal van nVidia 1% sneller is dan Turing van nVidia 7nm heeft geholpen met stroomverbruik, maar het grootste deel van de efficientie komt toch vanuit de architectuur zelf."
RX 5700;2;0.24828383326530457;Dat klopt. De Vega- chip en de van Vega afgeleide chip in de Radeon VII waren geen chips specifiek bedoeld voor gaming maar meer voor de zakelijke markt. In die tijd had AMD niet genoeg geld om twee chips te ontwerpen, een voor gaming en een voor zakelijk. Aangezien er meer winst te halen is per chip in de zakelijke markt heeft AMD toen gekozen om een chip te maken specifiek voor de zakelijke markt. De game-markt was bijzaak. De RDNA-architectuur is veel meer bedoeld voor gaming. AMD heeft vooral de single-threaded prestaties verbeterd en niet de parallelle. De 5700-chip is ook veel kleiner dan Vega. Voor de gamemarkt gaat AMD de RDNA-architectuur gebruiken en voor de zakelijke markt blijven ze vasthouden aan de GCN-architectuur.
RX 5700;2;0.4914698004722595;grotendeels willen niet. ze konden het doen, er was zelfs een grote Polaris in ontwikkeling, maar het zou veel geld kosten om te ontwikkelen en de verkoop aantallen zouden beperkt zijn. aangezien AMD toen nog flink op de kosten moest letten zijn ze geschrapt. Inmiddels komt er veel meer geld binnen mede dankzij Ryzen en is het R&D budget verhoogt. Lisa heeft gezegd dat ze ook aan de top weer gaan concurreren. maar nog niet wanneer dat zou zijn.
RX 5700;2;0.5962156057357788;"In het topsegment kan je allen geld verdienen als je de beste kaart hebt; iemand die 1000 euro aan een GPU uitgeeft koopt niet de op-één-na-snelste GPU, die koopt de snelste, en zolang AMD geen kaart kan produceren die sneller is dan de 2080 Ti, heeft het niet zoveel zin daar een gooi naar te doen. Als ze een kaart maken die 95% van de prestaties van een 2080 Ti heeft, koopt geen hond hem. Daarnaast zijn high-end kaarten moeilijker te ontwikkelen. Goed presteren in het middensegment levert voorspelbare marges en vergt minder investering dus is gewoon veel minder risicovol. Voordat ze Ryzen hadden speelden ze met de CPUs precies hetzelfde spelletje. Nu zie je dat door Ryzen ze wel mee kunnen doen in het topsegment en ze daar meteen ook keihard concurreren."
RX 5700;1;0.26744014024734497;De games waar AMD duidelijk wat achter blijft zitten vol nVidia geoptimaliseerde code?
RX 5700;3;0.6994236707687378;Beetje lastig he games optimaliseren voor een kaart die nog niet uit is.
RX 5700;1;0.42205381393432617;Dat hoeft niet, je zou eigenlijk verwachten dat een AMD (en andere fabrikanten) al zeer vroeg samples leveren aan bijvoorbeeld game dev's en engine ontwikkelaars zodat juist bij launch de eerste optimalisaties al gedaan zijn.
RX 5700;5;0.25490131974220276;Alle vorige generaties van amd gpus zagen altijd juist na de launch de performance met de tijd steeds iets verbeteren, bv de 1060 was bij launch sneller dan een 480, en een jaar later waren ze gelijk.
RX 5700;2;0.44246146082878113;Dat klopt, dat is echter niet handig voor de reviews. Vandaar dat je zou hopen dat AND daar van zou leren en dus zorgen dat de lunch prestaties nu wel optimaal zijn.
RX 5700;3;0.3215057849884033;Wellicht vieze spelletjes van Nvidia met hun machtspositie.
RX 5700;2;0.4356883764266968;Nvidia heeft gewoon meer geld en meer mankracht aan werken dat het allemaal tijdens launch al word geoptimaliseerd. En Amd met minder mankracht dat tijd nodig heeft. Je ziet ook na driver updates dat Nvidia ook nog per game sneller word, alleen word het niet zoveel beter als Amd. Amd pakt veel meer percentage verbeteringen.
RX 5700;3;0.4097703993320465;Niet persee. Wolfenstein was een game die het heel goed deed op AMD hardware bijvoorbeeld. Maar het is wel een low level API game (vulkan in dit geval), en dus moet de developer zelf een groot deel van de optimalisatie doen. Die zijn wel gegaan voor de al bestaande AMD en Nvidia GPU's maar Navi is een nieuwe architectuur (en meer dan alleen een nieuwe GCN iteratie) dus is de game/engine er nog niet voor geoptimaliseerd. In DX11 en lager kan AMD zelf veel doen in de drivers, maar in low level API's is hun invloed veel beperkter. De DX11 games tonen toch wel duidelijk aan dat Navi voldoende performance in huis heeft. dus als ze de engine nog zouden optimaliseren voor Navi denk ik dat er nog veel winst te halen valt. het is natuurlijk maar de vraag of zo'n performance patch er komst. maar alle toekomstige low level API games zullen wel ook geoptimaliseerd zijn voor Navi.
RX 5700;3;0.31234923005104065;Er schijnt vandaag nog een 3e kaart verschenen te zijn: de Radeon RX 5700 XT 50th Anniversary met een hogere Boost Clock. Normale Boost Clock voor de 5700XT is 1905 MHz en van de 50th Anniversary is die 1980 MHz Hier alle specs op een rij (naar beneden scrollen svp)
RX 5700;1;0.5666958689689636;Die is niet opeens verschenen, die is tegelijk aangekondigd met de andere 2 kaarten
RX 5700;5;0.4315222501754761;En ook een hoger verbruik van 235W.
RX 5700;2;0.5069873929023743;Ik twijfelde heel erg tussen de 2060 Super en de 5700 XT, nou de prestaties zijn wel duidelijk. Scheelt gewoon 10% voor dezelfde prijs. Alleen die koeler op de Radeon... nou weet ik het nog steeds niet.
RX 5700;2;0.4742650091648102;Die fan kan je vast wel in een beter fancurve zetten dan wat hier geprutst is...binnen 2 weken zijn er vast betere drivers dan deze release drivers..
RX 5700;1;0.3725142180919647;Wachten op een AIB-kaart die volgende maand uitkomt.
RX 5700;4;0.36125025153160095;Klinkt goed ! Weet er iemand of er binnenkort ook nog een 5800 (XT) komt?
RX 5700;2;0.37973538041114807;Hoogst waarschijnlijk duurt dat nog even. kan zo maar eind dit jaar of begin volgend jaar zijn.
RX 5700;2;0.391522079706192;Ik vraag me af wat de GeForce RTX 2060s voor zin heeft, hij is ongeveer net zo snel als de GeForce RTX 2070, en vaaker zelfs sneller, snap niet de logica van Nvidia om hun Geforce RTX 2060 en 2070 van de markt te duwen met hun super modellen. Maak dan de Geforce RTX 2080 en 2080Ti goedkoper of en plaats daar een Geforce RTX 2080s tussen in.
RX 5700;3;0.3819442391395569;tja, ze konden het goedkoop doen door gebruikt te maken van de verbeterde yields van 12nm waarschijnlijk. Door de betere yields hebben ze meer die's waarvan ze meer cores kunnen enableën(?), en op een hogere clocksnelheid laten lopen. het zijn verder nog steeds de zelfde die's en de zelfde PCB's. de 2060 bungled er wel een beetje bij nu, en de 2060s voegt idd niet veel toe zo boven de 2070.
RX 5700;1;0.5032047629356384;"Zover ik weet is de RTX 2070 EOL, die wordt dus niet meer gemaakt.. Alleen de ""gewone"" 2060 blijft Nvidia leveren naast de nieuwe Super.. De 2080 is straks ook EOL zodra de 2080 Super er is.."
RX 5700;3;0.2592073678970337;EOS(sales) bedoel je dan eerder, EOL zou ik kaarten noemen die geen nieuwe drivers meer krijgen.
RX 5700;2;0.4413834512233734;Ja ach, alleen vervelend voor mensen die perongeluk een Geforce RTX 2060 of 2070 kopen inplaats van de Geforce RTX 2060s en 2070s, vooral mensen die er niet veel van weten. Update: En vond het jammer dat de Geforce GTX 1080 niet mee genomen werd in de benchmarks.
RX 5700;2;0.391300231218338;Voor 1080 scores kan je een beetje uitgaan van tussen 1070Ti en 2070 FE ish. Ik heb zelf ook een 1080, destijds genoeg voor betaald maar zie nog steeds geen reden om te vervangen zonder serieus meer te moeten betalen.
RX 5700;4;0.3481296896934509;Voor mij het zelfde, ook een van de rede dat ik wacht op de Geforce RTX 3080 of hele snelle Radeon kaart.
RX 5700;1;0.3489474654197693;Goh is dat niet bij alles zo? Oneplus die T versies van zijn smartphones uitbrengt, auto's die facelifts krijgen,... Als iets high-tech is weet je toch dat het tegelijk rap weer obsolete is?
RX 5700;3;0.42027485370635986;Ze moesten wel omdat anders de 5700 en 5700 XT te goed zouden afsteken tov die 2, nu komen ze met super versies maar helaas verlaagd amd nog ff de prijs, nu kunnen ze kiezen of meegaan en ook de prijs verlagen of amd meer sales gunnen. De 5700xt is basicly een 1080ti voor 400 euro, voor iedereen die geen raytracing wil een topper.
RX 5700;3;0.44186508655548096;Ja maar breng dan een grafische kaart uit die tussen de Geforce RTX 2080 en 2080Ti in zit, en maak de Geforce RTX 2080 en 2080Ti goedkoper.
RX 5700;1;0.6661142110824585;"Eigenlijk snap ik het ook niet. Ze hadden net zo goed prijs van de 2070 en 2080 kunnen verlagen. De enige wat ik kan bedenken is dat zij op deze manier als de goeierikken willen voorkomen voor hun idiote fans. Van kijk "" door 2070 super uit te brengen krijg je nu bijna 2080 prestaties voor dezelfde prijs. En omdat we 2gb extra op de 2060 super moeten zetten kost dat je 50 dollar meer en heeft bijna 2070 prestaties "". En alle Nvidia fans gaan uit hun dak! Die zeggen dan "" wtf! Nvidia is zo aardig voor ons om betere prestaties voor zelfde prijs te geven, Haleloeja Nvidia!"" En ondertussen denken de wat slimmere mensen "" tja, ik krijg voor hetzelfde geld wat Nvidia vraagt al 10% betere prestaties. Lijk ons veel beter "". Nog slimmere mensen denken van "" Dit hele generatie gpu's zijn zowiezo overpriced! Of het Nvidia is als Amd! Nvidia is niet te betalen en dat prijs is alleen voor gekken! Amd is nog steeds 50 teveel."" En als je denk dat we eigenlijk allemaal nog steeds wel zeker een jaartje vooruit kunnen met onze ouwe kaart. En volgende generatie dat ze weer op normale prijsniveau zitten en dan pas een nieuwe kopen."
RX 5700;2;0.4325660169124603;Ja precies, maar ach ze hebben het al gedaan, maar ben veel meer benieuwd naar de Geforce RTX 3080, en hoop dat die wel normale prijzen krijgen, en dat AMD ook met een hele snelle kaarten komt waneer de Geforce RTX 3080 uit komt, anders ben ik bang dat de Geforce RTX 3080 ook heel duur woorden.
RX 5700;5;0.29162901639938354;Nog iets is, zo hoeven ze de rtx2080ti niet in prijs te verlagen. Als ze de 2070 en de 2080 hadden verlaagd, dan moesten ze de 2080ti ook verlagen om in prijs verhouding goed te zitten, en nu hoef dat niet. En de 2080s komt toch nog niet in de buurt van de ti versie, wel dichterbij. Heel slim Nvidia. Ik zie je naam AmigaWolf. Ook Amiga 500 vroeger of had je de 1000? Ik had de 500. Prachtig ding. Erg veel plezier mee gehad.
RX 5700;2;0.5660317540168762;"Ja maar echt niet leuk voor beginners van Game-Pc's niet meer duidelijk. Ja, me vader had eerst de Commodore 128D en toen paar jaar later de Amiga 1000 en 2000, waar ik veel mee gespeeld heb, toen kocht me vader de Amiga CD32 +Sx32 Pro voor me met 64MB ram en 500MB 2,5"" HDD, toen een paar jaar later kocht ik de Amiga 1200 met later de PPC 240MHz risk CPU en 68040 25MHz CPU Bvision en later Voodoo 3 en 96MB RAM, en scsi HDD en DVD spelen/writer, en zat met een goede vriend die ook een Amiga had op de Amiga Club Hoorn meerdere jaren ieder maand. Ja heel veel plezier gehad met Amiga computers, zelfs meer dan met Windows PC's, me vader had ook een 486 PC met Windows 3.11, maar ik was 100% alleen bezig op de Amiga, was in de jaren 80 en begin 90 veel leuker en beter dan de MS-DOS Pc's. Heb 3 a 4 haar terug een Amiga CD32 weer gekocht en zit nu een TF330 in wat de zelfde CPU heeft als de SX32-Pro, een Motorola 68030 50MHz CPU, en 64MB RAM en 8GB SD kaart, met 1000 spellen, en Workbench 3.1, moet nog de nieuwe Workbench 3.11 er op zetten, heb nu al 43 Amiga CD32 originele spellen, ben ze aan het sparen, en heb een Commodore 128D en laatste een Amiga 500 Plus gekocht."
RX 5700;2;0.41081419587135315;Dacht dat 2070 ook EOL wat NV doet is soort rebranding maar dan zonder hogere type aanduiding. Een rebinning en zelfde chips anders in te zetten door meer units in te schakelen of chip shuffle.
RX 5700;2;0.33382853865623474;Nee dacht niet een rebrand met zelfde chip, maar dacht nog meer kaarten met zelfde naam maar dan met Super achter de naam en wat sneller, had liever gezien dat ze de Geforce RTX 2070 en 2080 en 2080Ti goedkoper hadden gemaakt, en zo nodig een Geforce RTX 2080s tussen de 2080 en 2080Ti.
RX 5700;5;0.6924387216567993;Ja en een 5900XT! AMD doet het super!
RX 5700;1;0.2795320153236389;Las ergens september.
RX 5700;2;0.3306180536746979;Zou mooi wezen, dan kunnen ze Nvidia echt het vuur aan de schenen leggen
RX 5700;2;0.35012921690940857;Ik zou hem zo graag nu halen om ffxiv in 1440p/4k 60 te kunnen spelen, maar die fan jongens. Dat word wachten op een MSI of asus versie, hopen dat die niet een prijs van 500+ gaan vragen voor hun custom coolers erop, Raytracing gaming doe ik wel met mn PS5 later. Heb nu een R9 390 en een RTX2060super of RX5700XT zit ik tussen te twijfelen... maar geen founders/stock edities. ben bang voor de custom prijzen lol
RX 5700;2;0.43475502729415894;Ligt eraan hoeveel geduld je hebt en of het aanbod hoog genoeg is om aan de vraag te voorzien.. De prijzen zullen in het begin wel hoger zijn maar hopelijk paar maanden daarna gaan zakken..
RX 5700;1;0.41785404086112976;Hoe zit het tegenwoordig met de stabiliteit van de hardware en drivers enzo? Ik hoor veel gemixte verhalen en veel slechtere verhalen in het verleden. Zijn hier ook verbeteringen?
RX 5700;2;0.4242883026599884;Als huidige AMD kaart bezitter heb ik qua drivers twee problemen gehad: 1. Freesync werkt bij mij vaker niet dan wel. En dan de handel herstarten en het werkt weer, start een bepaald spel op en het werkt weer niet. En soms ook wel. Goede nieuws is: Voor mij persoonlijk is de conclusie dat ik Freesync niet heel veel vind toevoegen, dus het maakt me niet veel uit . Voor degene die het wel duidelijk merken is het erg irritant als je zoiets hebt. 2. In BF5 hebben AMD drivers maandenlang een probleem gehad waardoor na een revive je silhouet van tegenstander/teamgenoot in beeld bleef, tot je weer een paar keer dood was. Hoe irritant is dat vraag je je af? Nou behoorlijk:
RX 5700;3;0.396640807390213;Mooie kaarten, voor een mooie prijs, als je ze vergelijkt met de concurrentie, maar nog steeds 100 euro te duur voor midrange kaarten. Als je een budget hebt van rond de 400 euro voor een grafische kaart en naar 2060s of 2070s kijkt, lijkt het mij het meest verstandige om gewoon een 5700 of 5700xt te kopen. Veel betere prijs prestatie verhouding. Zoals zo vaak bij AMD moeten de drivers nog wat geoptimaliseerd worden, dus als we deze benchmarks over 1 jaar opnieuw draaien, zit AMD gewoon 10+% boven de concurrentie! Wel raar overigens dat er niets vermeld word over de betere image quality die AMD bied tegenover de concurrentie. Idem voor de anti-lag feature die deze kaarten zouden moeten hebben. Waarom is dit niet getest? In mijn ogen veel waardevollere techniek dan raytracing...
RX 5700;1;0.4952828288078308;Dacht al dat ik de enige was die de nieuwe functies van deze kaarten miste in de review...idd een groot gemis
RX 5700;1;0.6435110569000244;Bwah, een groot gemis weet ik niet. Als je RT aanzet op Nvidia kaarten zakken je fps in en is je spel eigenlijk niet meer speelbaar. Dus eigenlijk kan je RT niet gebruiken.
RX 5700;5;0.45376768708229065;Eindelijk weer wat competitie in gpu-land
RX 5700;3;0.30442821979522705;In het midden segment was AMD altijd goed, de polaris chips waren ook uitstekend. Dit is alleen maar beter voor de consument en ben benieuwd hoe de implementatie van raytracing gaat verlopen.
RX 5700;3;0.2951124608516693;Weet iemand toevallig wanneer deze kaarten beschikbaar zijn in nederland en of er price drops komen bij andere radeon kaarten (zoals de vega kaarten)?
RX 5700;4;0.4919327199459076;Mooie kaartjes. Uiteindelijk gaat het om de performance per watt en de prijspositionering en die lijkt dik in orde.
RX 5700;5;0.3210243582725525;Hier vast de prijzen en modellen bij onze oosterburen 419 voor de 5700 XT en 369 voor de 5700:
RX 5700;2;0.5261126160621643;"Voor mij is het wachten voorbij, ""Custom cooling"" Radeon VII wordt links ingehaald door NAVI. Achteraf gezien is marketing/presentatie Radeon VII heel vreemd."
RX 5700;3;0.3894418478012085;Vega56 is een mooie kaart voor 250 euro, ik zag de pulse versie laatst voor dit bedrag, een van de betere versies. Als cpu zou ik zeker dan geen ryzen 1700 nemen, dan is de 3600 een betere keuze imo.
RX 5700;5;0.5247285962104797;Interessant startschot van AMD met deze nieuwe serie kaarten die gelukkig wat zuiniger zijn. Nu ben ik helemaal nieuwsgierig naar de kaarten die hier onder gepositioneerd worden. Een Navi kaart, zeg een 5600, met een lager verbruik rond de 200 euro. Samen met een Ryzen 5 3600. Yes please!
RX 5700;4;0.30519038438796997;hele mooie prestaties, de amd radeon rx 5700 doet 100 fps gemiddeld op ultra 1080p. daarmee is het natuurlijk wel meteen een kaart die voorbereid is op de next - gen, en qua snelheid op zijn minst kan concurreren met de next - gen consoles, terwijl die zeer waarschijnlijk verslagen worden. dan komen we wel meteen aan bij een heikel punt : gebrek aan ondersteuning voor hardware raytracing. waarschijnlijk hebben alle grotere next - gen multi - platform releases gewoon ondersteuning voor raytracing. waar qua snelheid de rx 5700 waarschijnlijk goed genoeg is voor de games die van eind 2020 tot eind 2023 voor de pc uitkomen ( ok, zolang je de settings ieder jaar een tandje bijsteld ), is het gebrek aan raytracing een serieus nadeel. veel mensen zouden sterk geneigd zijn een rtx super kaart te kopen, en de next - gen titels van eind 2020 tot eind 2023 met raytracing enabled settings te spelen. met nvidia ben je veel beter voorbereid op de toekomst. amd had dus geen andere keus dan de prijzen te verlagen. het gebrek aan raytracing is namelijk een grote tekortkoming, met het oog op de toekomst ( inclusief next - gen ). bovendien liepen de sales van de vorige generaties amd - kaarten onder gamers ook niet zo denderend, getuige de steam hardware survey. wat betreft de aanschaf van amd gpu ' s voor cryptomining, die markt is niet zo stabiel en is behoorlijk onvoorspelbaar. de meeste gamers zullen gewoon weer voor nvidia kiezen. en het feit dat de koeler op deze amd - kaarten zo belabberd is, helpt natuurlijk niet. de geluidsproductie onder load ligt veel te hoog, en de meeste mensen willen sowieso geen blower. de kaarten met betere koeling van asus, msi etc. laten nog maanden op zich wachten. en dan is het nog de vraag of de prijzen van die kaarten wel de nieuwe en verlaagde prijzen reflecteren, of zelfs nog een stukje boven de oude vraagprijs zitten. dat is namelijk wel al eens eerder voorgekomen, dat de beloofde prijzen niet waargemaakt konden worden.
RX 5700;2;0.48177531361579895;Vind de noemer prijs geen factor, de geteste kaarten komen met aardige prut koelers waarbij de nvidia kaarten met echt deftige koelers komen. De meerpijs meteen al waard naar mijn mening. Het is maar afwachten waar board partners mee gaan komen en dan voor welk bedrag. Id software en cd projekt beweren met echt geoptimaliseerde raytracing games tegaan komen. Als deze titels wel echt deftig draaien dan begint amd wel achter te lopen. Dat het verbruik rond dat van Nvidia ligt, is op het eerste gezicht zeer goed. Maar aan de andere kan als je er dieper op in gaat weer niet. Dit zijn 7nm chips vs de 12nm van Nvidia wat theoretisch minder zuinig zou moeten zijn. Helemaal omdat er nu extra turning cores aanwezig maken die de chip die bij Nvidia flink groter maken. Dit laat toch weer zien dat AMD aardig de chips moet pushen om tot de gewenste prestaties te komen. Daarnaast is Turning zeer goed over te clocken. Dit is nu nog geen gegeven bij de nieuwe AMD GPU's. Dus ergens vind ik het wel een telleurstellende launch. Zoals vrijwel alle launches van de laatste 5 jaar bij AMD. Het is allemaal net wat meer op het randje, minder overlock mogelijkheden, wat warmer, wat meer verbruik, niet geschikt voor in laptops etc. Ik wil gewoon AMD weer GPU's neer zien zetten die gewoon beter zijn dan van Nvidia. Vaak was de launch cyclus vroeger gewoon om en om. Nvidia kwam met een GPU launch en een half jaar later kwam AMD met een set snellere GPU's uit. Dan weer 6 maanden erna een nieuwe Nvidia launch etc dat weer sneller was en zo ging de gehele cyclus door.
RX 5700;5;0.479763001203537;Bizar om te zien hoe goed de 1080ti eigenlijk is. Dat was echt een briljante kaart.
RX 5700;3;0.26661697030067444;Dit is best interessant. RTX is Nvidia's troef maar of je daar €100 extra voor wilt betalen? Ik niet, maar dat zal per persoon verschillen. Met deze prijsstelling wordt RTX uiteindelijk naar de PhysX-hoek gedrukt, al helemaal omdat de volgende generatie consoles het niet (of een alternatief) gaat hebben. Nvidia heeft echter nog veel meer mindshare dan AMD, wat dat aangaat had AMD gewoon een halo-product nodig of een veel agressievere prijsstelling. De performance van Navi valt in ieder geval niet tegen. De prijs is, vergeleken met Nvidia, prima. Het oude normaal van sub-€300 voor de RX 5700 en sub-€400 voor de XT gaan we niet snel terugzien ben ik bang, maar deze performance voor €370/425 is nog te doen. Maar even realistisch: Zelfs sinds vandaag heeft iedereen met een GTX970/R9 290 of beter nog steeds geen fatsoenlijke kaart om naartoe te upgraden. Die kaarten draaien ook alles nog op goede settings. De performancewinst per euro is nog steeds niet goed genoeg. Het is dat dit publiek ca. zes jaar heeft gehad om te sparen Edit: @D0gtag Ja goh, 4K. Je kon al die tijd geen fatsoenlijke videokaart kopen, dus heb je het geld maar in een betere scherm gestopt en nu heb je daadwerkelijk nog een reden om weer een smak geld uit te geven
RX 5700;1;0.30265000462532043;"""Zelfs sinds vandaag heeft iedereen met een GTX970/R9 290 of beter nog steeds geen fatsoenlijke kaart om naartoe te upgraden."" Ik heb nog een 970 liggen maar volgens mij draait hij BF4 met moeite op 4k Als je alles op 1080p gaat draaien en je hebt geen hoge framerates nodig is een 970gtx nog prima, maar kom op, wil je 4k..."
RX 5700;3;0.3694731295108795;Kijkend naar iemand die net zijn scherm van 1080p 60Hz heeft ge-upgrade naar 1440p 144Hz, en daar nog een nieuwe videokaart bij zoekt, krijg je met een 5700XT 8-10% betere prestaties t.o.v. 2060super, voor hetzelfde geld. Wil je echter meer frames en/of ondersteuning voor raytracing dan kom je weer bij Nvidia uit. Prijs/kwaliteit zit dus wel goed bij AMD (door de prijsdrop dan met name) al is het nog wel de vraag hoe ze precies in markt komen en dan met name de kaarten met andere/betere koeling. Zelf zoek ik ook een kaart die niet te warm mag worden, dus ik wacht nog even de reviews van andere modellen af voordat ik een keuze ga maken.
RX 5700;5;0.5658463835716248;Kan een zeer interessante videokaart worden in die prijsklassen en doet het niet slecht op 4K. Ik ben zeer verrast door Navi
RX 5700;3;0.45708951354026794;Navi is dus niet de revolutie die eerst voorspeld werd? Maar wel fijn dat … let op... met de nieuwe prijzen dat het wat concurrentie voor NVIDIA biedt. Maar de NVIDIA killer is het dus niet en laten we hopen dat de gebruikersreviews niet zo negatief worden als bij de Vega (Als je geen AMD fanboy bent zou ik dus nog een paar maandjes wachten?)
RX 5700;2;0.3622393310070038;Ter aanvulling op deze review. Let even op dat deze kaarten nog steeds de multi monitor bug hebben met betrekking tot opgenomen vermogen. Bij de vrienden van Techpowerup stijgt het idle verbruik van 8W naar 36W. Zie Dit is iets dat helaas ook al voor kwam bij onder andere de RX 480 en is de reden geweest dat ik destijds niet voor die kaart ben gegaan. Ik gebruik namelijk twee monitoren. Zou mij al snel 20 euro per jaar aan extra stroom kosten terwijl daar geen enkele aanleiding toe is.
RX 5700;5;0.26259368658065796;Oef, mijn twee jaar oude GTX 1080 Ti begint zo wel snel ‘oud’ aan te voelen, destijds een kaart van 750 euro, goed om te zien dat vergelijkbare prestaties inmiddels voor een aanzienlijk lagere prijs te krijgen zijn. Nu nog een betaalbare RTX 2080 Ti, een ‘Super’ variant van de 2080 en 2080 Ti zullen vast ook wel in de pijplijn zitten, maar Nvidia zal wachten op Navi 20.
RX 5700;4;0.6757898926734924;Zeer nette resultaten van de Navi kaarten. Jammer alleen dat de AiB modellen pas ergens mid augustus uitkomen. Die zullen door de betere koeling waarschijnlijk betere boostclocks kunnen vasthouden en dan nog meer in de nek van de 2060/2070 Super hijgen. Goed gedaan AMD, kan niet wachten om de RX 5800(XT) en 5900(XT) te zien.
RX 5700;5;0.3961949646472931;Mooi nieuws ik geef naast Ryzen 3600 ook Navi en X570 een serieuze kans en ben zeer benieuwd hoeveel winst er gehaald kan worden met PCI E4 en wacht een Zen 2 + X570 + RX 5700 Benchmark af Voor nu lijkt mij een Ryzen 2600 + B450 de meeste Bang voor Buck voor omstreeks de 200€ met een RX 5700 voor 370€ of een RTX 2060 voor 350€ komt op omstreeks de 550€ a 600€ Hopelijk geeft PCI E4 het excuus om voor een volledig AMD systeem te gaan want dat is voor nu het grote voordeel van X570 ten opzichte van B450 en X470 en Intel Moederborden
RX 5700;1;0.4326013922691345;"Ben ik nou de enige die een grapje is opgevallen in de tekst van het artikel in de sectie Referentieontwerp met blower ? ""Het referentieontwerp van de RX 5700-kaarten gebruikt een koeler met een blowerontwerp. Daar is niet iedereen ""fan"" van..."". Ik heb misschien een weird sense of humor"
RX 5700;4;0.41965433955192566;Hahaha, ja dat viel me ook onmiddellijk op. Leuke woordspeling.
RX 5700;3;0.4874328374862671;Zie in t artikel niets staan over de nieuwe functies waar AMD zo mee adverteert voor deze kaarten (Kan dat ik eroverheen heb gelezen...t is nogal n lap tekst zo laat op de avond) Anti-lag (toch wel handig vooral in multiplayer games)...dit had best ook gebencht mogen worden...gamen is meer dan fps en zo n functie maakt te speelbaarheid toch net iets beter Image-sharpening & upscaling zonder performance verlies...ook dat had met wat side-by-side screenshotjes wel gebencht mogen worden. Zou iig een functie zijn die ik standaard aan zou zetten voor beter beeld zonder fps drops. Fidelity fx lijkt me wat moeilijk om te testen maar lijkt toch ook een leuke toevoeging te zijn. Beetje jammer...zou graag wat meer zien dan wat fps tabelletjes op n site zoals tweakers
RX 5700;2;0.3743163049221039;Volgens mij zijn dit allemaal softwarematige dingen. Games moeten het dus ook implementeren. Uiteindelijke is dit nog steeds hun graphics core next met dubbele rops. Er zit dus niet echt iets nieuws in, maar je mag dit niet te luid zeggen ook al zegt amd dit zelf.
RX 5700;5;0.42325881123542786;I see what you did there
RX 5700;2;0.5390602946281433;AMD, ofwel voormalig ATI, is altijd al een zeer interessant alternatief geweest in het middensegment. Maar dit keer vind ik de prijsverschillen maar gering. Dan zou ik voor die paar tientjes meer toch voor de wat toekomstbestendiger (want raytracing) Nvidia gaan en dan zeker de super varianten van de 2060 en 2070. De 2080 serie is way overpriced en niet interessant tenzij je geld teveel hebt.
RX 5700;5;0.6428641080856323;Amd wins again baby
RX 5700;2;0.3363321125507355;Is het slim om nu een vega 56 met een ryzen 1700 te kopen voor een gamepc van ongeveer 800/900 euro? Of kan ik beter een van de nieuwe videokaarten kopen met bijvoorbeeld een ryzen 3600? Alvast bedankt voor je reactie!!
RX 5700;2;0.47654813528060913;Zou dan sowieso voor een Ryzen 2000 serie gaan. De Vega kaarten zijn stroomvreters, zou ik persoonlijk ook links laten liggen. Aan de andere, heb je het nu nodig dan kan je misschien wel leuk zaken doen.
RX 5700;2;0.3279159963130951;Wat voor gpu zou ik dan kopen? Ik zit te denken aan 2060 of 5700. En hoe duur worden de custom 5700 modellen onegeveer?
RX 5700;1;0.6768648028373718;“...maar in alle gevallen levert het in 4k met Ultra-settings geen speelbare framerates op”. Serieus? Bijna 40fps is niet speelbaar? Wat voor 1337 tiener twitch gamer ben je dan wel niet? Wat een onzin, Tweakers onwaardig. Edit: Sony en MS belden net, zijn het met me eens.
RX 5700;3;0.3805404007434845;Je hebt een Ryzen3 platform liggen en hebt een stel NAVI GPU`s liggen. Dus ga je die samen testen. NOPE. De afstand naar de andere testbench zal wel te ver lopen zijn.
RX 5700;2;0.3482671082019806;"""Serieuze concurrent in het middensegment"" Gisteren nog zwaar gedownvote omdat ik zei dat de 5700XT geen high-end maar mid-range was."
RX 5700;3;0.383082777261734;Beter cloud gamen met deze prijzen tegenwoordig voor single player games.
RX 5700;5;0.784862220287323;Aangezien de GeForce kaarten niet werken onder Mojave in mijn Mac Pro’s, dan zijn dit dus prima upgrade-kaarten. Top AMD !
RX 5700;1;0.5214317440986633;Wat een framerates zeg. Ik ben benieuwd hoe mijn nVidia 8800GT voor prestatie index haalt. Hehe, ik denk negatief. Ik moet echt nodig een keertje upgraden.
RX 5700;5;0.5085221529006958;Waar is de echte Grote chip
RX 5700;3;0.5184087157249451;"Leuk, deze kaarten zijn betaalbaar en snel, maar een ding missen ze wel ten opzichte van Nvidia, en dat is degelijke ""compute"" support. En van de meest gebruikte frameworks voor deep learning, TensorFlow, ondersteund deze GPUs of de vorige generatie zover ik weet nog niet. Alleen GPUs van Nvidia worden ondersteund. Dus als je hardware acceleratie wil met deep learning blijf je toch vast zitten aan Nvidia. Iemand enig idee of daar bij AMD focus op is? En zo ja, wanneer we daar wat van zien?"
RX 5700;5;0.42379334568977356;Luid en heet. Kunnen die lui bij AMD nou nooit eens een fatsoenlijke koeler bouwen
RX 5700;1;0.34868893027305603;Ik wacht nog met oordelen tot de 5800(XT) en de 5900(XT) kaarten gereleased worden. Dan is het nog wachten op de 5900X2 en dan koop ik er een.
RX 5700;1;0.6245391964912415;Dat is exclusief btw. Als je Nederland kiest en checkout klikt wordt het 427 euro.
RX 5700;1;0.40850943326950073;In Duitsland hebben ze 19% BTW wij 21% dus scheelt inderdaad 2%
RX 5700;5;0.4415142834186554;9 vd 10 is www.amazon.de goedkoopste + voorraad.
RX 5700;1;0.6107667088508606;En jammer genoeg leveren die niet in België. Begrijp ik niet.
RX 5700;1;0.4107653498649597;klopt maar die prijzen zijn ex BTW 350 * 1.21 = 423.5
RX 5700;1;0.6291405558586121;Hmmmm trollen maar zelf niet correct kunnen schrijven..... en lezen blijkbaar ook niet ......
RX 5600 XT;5;0.3538467288017273;De buren geven trouwens aan dat partners hebben aangegeven de de snelheid verbeteringen ook zullen komen naar kaarten die op de adviesprijs zitten. Dus niet enkel de high end kaarten met custom cooling. We begrijpen van AMD-partners dat de 'nieuwe kloksnelheden' ook naar modellen zullen komen die zich rond AMD's adviesprijs van een dikke 300 euro begeven. Op dat prijspunt biedt de RX 5600 XT een zeer scherpe prijs-prestatieverhouding.
RX 5600 XT;3;0.5450650453567505;Dan moeten de koeling van de 'low end' kaarten wel voldoende zijn om die 22W extra ingame te behapstukken. Dat is dus 1/6e meer warmte ontwikkeling.
RX 5600 XT;2;0.38533544540405273;Of de goedkopere kaarten gaan eerder terugklokken om de warmteproductie binnen de perken te houden als de verdere koeling niet goed genoeg is.
RX 5600 XT;3;0.43919873237609863;Ja dat is daar meestal het gevolg van... Dan heb je dus mindere prestaties en kun je beter een andere kopen.
RX 5600 XT;2;0.39617791771888733;Dit klopt niet geheel, er komen meer types per merk en de snellere zullen duurder zijn dan adviesprijs nu. pricewatch: Asus ROG Strix RX 5600 XT 06G Gaming pricewatch: MSI Radeon RX 5600 XT GAMING X pricewatch: Powercolor Radeon RX 5600 XT Red Devil 6G de 'mindere' zijn wel dichter bij adviesprijs pricewatch: MSI Radeon RX 5600 XT MECH OC Hier nog meer info te lezen mbt
RX 5600 XT;1;0.49362361431121826;Kun je echter voor rond 300 euro een 5600 XT kopen waar een oc-bios op te flashen is, of die al vanuit de fabrikant zo'n bios heeft, dan krijg je prestaties die in de buurt van de 2060-kaart komen voor een lagere prijs. Je komt dan ook in het vaarwater van de RX 5700 die tegen de 350 euro kost. Gemiddeld sneller is natuurlijk ook 'in de buurt van', maar wat een slechte woordkeuze, in de buurt van word natuurlijk geïnterpreteerd als 'bijna net zo snel, maar net niet'. Dit valt me ontzettend vaak op in reviews op Tweakers, voor mij eigenlijk geen verrassing meer en dat het elke keer een klein foutje of toeval is geloof ik niet (meer).
RX 5600 XT;2;0.5274225473403931;Best vervelend dat je weer op zo'n detail moet gaan letten als de beschikbaarheid van een OC BIOS. Voor veel consumenten gaat dit niet duidelijk zijn, en dan krijg je weer sterk verschillende resultaten voor producten met grofweg dezelfde naam. Doet mij denken aan de GT 1030 van een tijd terug, waarbij er ook DDR3 en GDDR5 varianten op de markt waren voor dezelfde prijs, maar met wild verschillende prestaties. Konden ze nu maar allemaal ineens die OC BIOS hebben, of tenminste een 5600 (zonder OC) en 5600 XT (met OC) label krijgen ofzo.
RX 5600 XT;2;0.42504459619522095;"Naar wat ik begrijp van de review van GamersNexus, is dat de ""OC"" BIOS meer een panieksprong is nadat AMD de prijsverlaging van de 2060 door had (insert surprised Pikachu). Ze hebben dit pas laat doorgepusht, zodat de leveranciers nu kaarten leveren waarvan de BIOS nog niet is geupdate naar de OC variant. Waarschijnlijk zal het aanbod met de OC VBIOS in de nabije toekomst de standaard worden, maar ontzettend dom dat AMD dit zo heeft gedaan. Krijgt de consument weer te maken met een loterij. Tweakers kunnen nog wel een VBIOS flashen, maar je gemiddelde consument lukt het wat minder. Hopelijk geven webshops dit aan..."
RX 5600 XT;2;0.48710355162620544;Ik vind dat zelf te negatief verwoord. AMD had weinig anders kunnen doen. In de basis is deze $279 5600XT dezelfde GPU als de 5700 ($349)/5700XT ($399). Natuurlijk zit er minder geheugen op maar dat zijn kosten die zo ver ik weet de AIB (MSI, Saphire, enz) dragen dus daar heeft AMD weinig aan. Er is vast iemand die kan opzoeken hoeveel 2GB GDDR6 kost maar dat is in ieder geval geen 70 - 120 dollar. Nog lager inzetten met de prijs is voor AMD geen logische stap. Daar is deze chip te duur voor. Vervolgens komt Nvidia met een prijsverlaging van de RTX 2060 en als je dan kan AMD een paar dingen doen -niets doen en vervolgens afgeslacht worden in de reviews en weinig verkopen -de prijs verlagen mogelijk tot het punt dat er verlies wordt gemaakt -de prestaties opschroeven ook al kom je dan weer in de buurt van de 5700 De laatste optie kan tot kannibalisatie van de 5700 leiden, echter als AMD niets doet dan zal de goedkopere RTX 2060 dat ook wel doen. Dat ding gaat überhaupt al als warme broodjes over de toonbank. Dus ja dan is die derde optie zo gek nog niet. Het is nu onoverzichtelijk en AMD had ook later kunnen reageren maar in de praktijk passen veel websites hun reviews achteraf niet aan waardoor de conclusie van deze 5600XT door de prijsdaling van de RTX 2060 dan permanent negatief zou uitvallen. Dus dat ze nu bij launch nog met een BIOS komen is wat dat betreft te begrijpen. Wat AMD wel had kunnen doen is de prestaties van de 5600XT eerder lekken waardoor Nvidia mogelijk eerder had gereageerd met dezelfde prijsdaling en AMD het product nog een keer had kunnen bijstellen vóór het in de winkel lag.
RX 5600 XT;2;0.46114376187324524;De reden dat het geheugen minder is zal waarschijnlijk te maken met de breedte van de memory bus. Deze is in plaats van 256 bit 192 bit. Dan kun je dus automatisch minder geheugen gebruiken helaas. Anders krijg je taferelen zoals we zagen bij de GTX 970, waarbij een deel van het geheugen niet op volle snelheid benut kan worden.
RX 5600 XT;2;0.3845368027687073;AMD heeft gewoon mazzel dat er nog wat rek in de gpu zit, anders moesten ze een prijsverlaging doorvoeren. Een prijsverlaging was imo beter geweest want nu is de kaart niet meer te overclocken door de gebruiker. Nu is de kaart (in de praktijk) even duur als de RTX2060, maar heeft bijvoorbeeld geen raytracing. Ja hij is wat sneller als de RTX2060, maar dat komt door de overclock. Als je die Nvidia kaart ook overclockt, zijn ze weer gelijk. Ze vergelijken immers de tot de max overgeclockte RX5600XT met een stock RTX2060 met 1680mhz core, terwijl deze ook flink overgeclocked kan worden naar ~2100mhz. Dan blijft de vraag: ga je voor een Nvidia kaart met raytracing optie, of een AMD kaart? De prijs is gelijk.. Ik denk dat veel mensen voor Nvidia zullen kiezen.
RX 5600 XT;3;0.26902228593826294;inderdaad. De recente uitgeleverde RX5600XT's hebben nog geen nieuwe PE VBIOS waar de volgende batch wel mee geleverd gaat worden. dit om de concurrentie aan te gaan met Nvidia RTX2060(niet S) Nvidia heeft recent de prijzen ervan verlaagd, en EVGA komt gelijk met zgn RTX2060 KO @willemdemoor welke RX5600XT hebben jullie gebruikt, en welke BIOS versie ? Powercolor RX 5600 XT Red Dragon nieuw v oud bios 1675-1682 MHz v 1566-1570 MHz bandwidth: 336 v 288 power use: 176w v 152w
RX 5600 XT;3;0.3262362778186798;Voor elke GPU heb je een aangepaste bios, mining dev's doen dit al jarenlang (gratis) enja bedrijven willen hier geld voor. Aan jou de keuze hoe een waar je de bios vandaan haalt natuurlijk, tweakers zullen eerder voor de eerste optie kiezen lijkt mij. Als voorbeeld ging mijn GPU van 28Mh/s naar 35Mh/s door een aangepaste bios die ik door een mining dev aangeleverd heb gekregen, temps gingen omlaag en de gpu werd stabieler. @necessaryevil het is niet gewoon even overclocken want dit kan je met elke GPU met de standaard AMD drivers/software. Mijn clocks zijn namelijk UNDERCLOCKED, wel is mijn snelheid drastisch verbeterd. Het zijn dan ook niet de core/memoryclock waar hun aan prutsen (zoals elke amd gebruiker standaard kan). Dat ze het een OC-bios noemen zie ik dan eerder als een term voor de leek die wel van overclocken gehoord heeft (iets aanpassen qua settings). Want het klopt, er zijn bepaalde settings aangepast in de bios waardoor die sneller en stabieler loopt. Maar een GPU bios fine tunen is totaal anders dan overclocken waar de meeste van gehoord hebben Iets wat veel gamers volgens mij niet weten, maar AMD heeft standaard ook een hele leuke functie in de AMD driver/software zitten.. Er is een optie die staat standaard op kwaliteit, gooi die maar eens op berekenen (even systeem herstarten)
RX 5600 XT;1;0.4176468849182129;Waar prutsen ze wel aan? Zo spannend kan het niet zijn..
RX 5600 XT;2;0.34320756793022156;"Uhm jawel, zo spannend is het wel.. Hiervoor zal ik thuis op discord moeten om je de tool te laten zien. Hierbij heb ik dan een kant en klare bios gekregen die ik via die tool kon inspoelen (nee niet AMD / ATI Flash), in die tool had je wel 50+ opties die je kon aanpassen. Dus niet: memoryclock, coreclock, alles behalve deze 2 Dit zijn mensen die knetter veel verstand hebben van de hardware en vooral de onderliggende software/firmware die het aanstuurt. Deze OC bios is (bijna) 100% zeker zo'n bios zoals de mining devs ook ontwikkelen. GPU's worden allemaal met een standaard bios geleverd, die gewoon draait en waarbij het opvalt dat een 1080 net wat minder snel is dan een 1080Ti. Standaard in de zin van; het werkt. Is zo'n standaard bios geoptimaliseerd voor de hardware die eraan zit? Natuurlijk niet.... En het optimaliseren bereik je niet door je memory/core clocks wat omhoog te klikken (eerder het tegenover gestelde zoals een onstabiel systeem)... Het echte fine tunen vind plaats in de bios van de GPU Ik krijg al gelijk zin om te zien hoe een juist ingestelde 1080 bios presteert vergeleken een standaard 1080Ti"
RX 5600 XT;1;0.7832878828048706;Wat een onzin zeg. Je beseft toch dat gamen en minen twee heel verschillende dingen zijn? Een mining geoptimaliseerd bios zal de gpu underclocken en het geheugen overclocken. Beiden worden ook meestal zwaar undervolt. Daarnaast worden de timings en bootstrap aangepast. Een mining bios zorgt ervoor dat je een behoorlijke klets fps kwijt zult zijn, als je ze al aan de praat krijgt in Windows...
RX 5600 XT;2;0.409035861492157;"Zoals ik aangaf, ik game niet met de pc dus dat zou ik eens moeten testen. Wat ik wel ermee wilde zeggen; indien je de bios voor mining kan optimaliseren, dan kan dat vast ook wel voor computer spelletjes... Of het zo is weet ik niet, nooit naar omgekeken. Dusja sorry dat jij het onzin vind dat ik geen spelletjes op de computer doe of wat vind jij onzin? Mijn mem en core is niet overclocked, wel een klein beetje ""underclocked""."
RX 5600 XT;4;0.38932567834854126;Blijkbaar memory straps, timings etc.
RX 5600 XT;2;0.3849097788333893;"Ze zullen vast inderdaad ook andere settings zoals timings, mogelijk wat voltage e.d. tweaken, echter is de tekst vrij duidelijk over wat de primaire veranderingen van deze ""OC bios"" zijn: Of te wel heel ordinair de core en memory clocks verhogen tegenover een hoger TDP (en dus ook zeer waarschijnlijk een hoger verbruik). Ik zou er eigenlijk maar niet vanuit gaan dat deze biossen verder echt heer erg geoptimaliseerd zijn. AMD staat er namelijk best bekend om dat ze GPU's juist niet geoptimaliseerd uitleveren, vandaar ook juist dat geoptimaliseerde biossen in het miningwereldje zo populair zijn, je kan daarmee namelijk vaak de prestaties flink verhogen ofwel het verbruik flink verlagen t.o.v. stock (of een middenweg tussen beide)."
RX 5600 XT;2;0.3930167555809021;Dat weet je niet... Wat wordt er in de bios gedaan om deze clocks überhaupt (echt) stabiel te krijgen? Ik weet natuurlijk ook niet wat ze bij deze RX5600 XT en etc precies gaan doen in de bios hoor. Vandaar dat ik ook gelijk met de devs uit t mining wereldje kwam die dit al jarenlang doen waarbij je inderdaad echt een leuke boost kan krijgen zonder problemen met de stabiliteit van je hardware. Er zullen vast ook wel andere soort devs zijn die dit kunnen, ik ken er toevallig enkele al jarenlang die juist in het mining wereldje zitten omdat daar de echte performance telt en niet zoals bij gamers die het voor wat fps doen
RX 5600 XT;3;0.44095495343208313;Dit is wel heel wat anders dan die gt1030 waar veel langzamer geheugen in zat. Het is altijd al geweest dat kaarten met dezelfde chip een andere bios hebben, vooral bij verschillende fabrikanten.
RX 5600 XT;3;0.5579255819320679;Voor wie wat minder op de hoogte is over het gt1030 verhaal : de prestaties van DDR4 versies zijn bijna de helft van de GDDR5 versies.
RX 5600 XT;2;0.41130030155181885;ik denk dat AMD zich bewust richtte op tweakers.net met deze flash optie (niet serieus bedoeld) was die minder performende GT 1030 niet met DDR4 IPV DDR3 (misschien typfout)? betreft OC labels, persoonlijk ben ik hierop negatief, ik zie dat het AMD is die dat last minute heeft gedaan, waarom zouden AIB fabrikanten (asus,asrock/msi/etc..) er een OC label voor moeten verbergen voor AMD's fout? een betere oplossing zou zijn: laat AMD die check en flash doen met een driver install met driver installatie, die dan geplukt wordt van de AIB website. (zelfde als een UEFI update vanuit windows met GPU driver install. (checkbox) enige echte problemen wat ik zie is een schone OS/driver installatie of driver update die nodig zal zijn (en evt internet als de VGA bios niet ingebakken in driver package zit).
RX 5600 XT;1;0.4026516377925873;Stiekem even gechecked in de pricewatch en het is idd DDR4 ipv DDR3 . En minder dus als in 'half' (Ze hadden het dus beter de DDR4 versie de GT1010 ofzo kunnen noemen)
RX 5600 XT;3;0.5568597912788391;"Kan je met het gewone bios niet overklokken of moeilijker? Ik zou trouwens voor een algemene conclusie gewoon uitgaan van de kaart op stock snelheden, natuurlijk is het voor tweakers juist leuk om wel te weten wat de invloed van overklokken is. Ja het is niet heel duidelijk dat er versies zijn met en zonder OC bios, maar in mijn visie is overklokken toch nog steeds een beetje een ""hack"". Ik vind dat geintje dat de GT 1030 met DDR3 en GDDR5 te krijgen is verwarrender."
RX 5600 XT;3;0.4623221158981323;Je kan beter even wat doorsparen en de 5700XT nemen. Ik heb zelf die gekocht voor 390,- dus het is te doen. En levert prima everaringen, heb op dit moment wel wat issues, maar hopelijk ligt dat aan het scherm en niet aan de GPU.
RX 5600 XT;3;0.25296899676322937;5700XT is bijna hetzelfde als een 5700 en bij de meeste kaarten kan je gewoon een xt bios erop flashen en prestaties krijgen die nagenoeg gelijk zijn aan de 5700xt. Dus ik zou zeker geen 5700XT nemen, tenzij je veel geeft om de garantie.
RX 5600 XT;3;0.44805213809013367;De 5700 heeft een kleiner aantal cores en je haalt er dus zelfs met een 5700 XT-BIOS nooit dezelfde prestaties uit. Verder zijn 5700 XT chips, naast de hogere core count, ook beter gebind en dus kun je d'r met hetzelfde voltage hogere kloksnelheden uit halen. Het verschil is niet zo heel groot, maar wel significant.
RX 5600 XT;2;0.45882466435432434;Het verschil tussen een 5700 met XT bios en een 5700XT is verwaarloosbaar, daarnaast is er nauwelijks over te clocken op de 5700XT. Het verschil is juist niet significant.
RX 5600 XT;3;0.3939662575721741;Dat is het wél. BIOS flash op een 5700, en alle limieten verhogen brengt je meestal tussen de 2-5 FPS van een stock 5700 XT. Meerdere FPS verschil is misschien niet dramatisch, maar is zeker significant. Op basis van een selectie aan benchmarks van het eerste het beste artikel dat ik erover tegen kwam: 4%, vs. een stock 5700 XT. Laat staan een aftermarket variant die een maximum boost klok van 2050 ipv. 1900 mhz heeft. Een bodemwaarde van 4% an sich is al statistisch significant.
RX 5600 XT;4;0.42256903648376465;...Wat een leuke optie is als je bereid bent om een 5700 te kopen en de bios te flashen met alle mogelijke risico's van dien. Zelf heb ik een 5700 XT gekocht ipv een 5700 omdat ik gewoon de kaart wilde kopen 'as is'. Betekent niet dat de 5700 met bios flash een mindere optie is in mijn ogen, maar de echte 5700 XT halen heeft wel zo zijn voordelen.
RX 5600 XT;2;0.442863792181015;'alle risico's van dien', je doet feitelijk hetzelfde als wat de third-party leveranciers je nu laten doen met de 5600 met hun bios-oc. Tuurlijk is het vanuit een garantie oogpunt een probleem, maar buiten de garantie en het kleine flash risico zie ik weinig nadelen.
RX 5600 XT;3;0.31383001804351807;buiten de garantie van twee jaar tja dan ben je ook niet snugger bezig. Daarom is het een prima kaart de 5700XT geen gezeik etc, als je kaart stuk gaat ben je niet fucked up. Alles is in rekening genomen bij mij.
RX 5600 XT;3;0.4433290362358093;Je kan beter even doorsparen voor 4x een Titan RTX met NVLink brug. Anders lukt 4K 140fps op Ultra echt niet met 99.999% frametimes echt niet. Wat je nodig hebt hangt volledig af van a) het doel waarvoor je gaat gebruiken, b) de prestatie/prijs-verhouding, c) hoeveel je bereid bent bij te betalen voor hogere prestaties. Als je kijkt naar prestatie/prijs-verhouding (fps per euro) is de RX 5600 extreem goed, en zeker aantrekkelijker dan de RX 5700 of 5700 XT. Voor 1080p 120fps of 1440p 60fps lijkt deze extreem capabel. Als je kijkt naar de prestatiescores is de RX 5600 XT ongeveer 5% langzamer voor, terwijl het prijs verschil veel groter is: 305 i.p.v. 379 euro, ruim 20%. Daarmee scoort de RX 5600 dus veel beter qua fps per euro.
RX 5600 XT;1;0.5897765755653381;Maar kijk je dan naar de kosten van alleen de videokaart of naar de totale kosten van de hele computer? Want met alleen een videokaart kun je niks. Als je de meeste fps/euro wilt, game je op de IGP.
RX 5600 XT;4;0.3850390613079071;Wat ik kan zien is dat je het beste de AMD Radeon RX 5600 XT kan kopen in plaats van een AMD Radeon RX 5700, en hem lekker OC, en dan is hij in de meeste spellen bijna net zo snel als de AMD Radeon RX 5700, als hij aardig goedkoper is dan de AMD Radeon RX 5700. Best leuk om te hebben voor 1080P.
RX 5600 XT;1;0.4473794400691986;Koop ik liever de 5700 non xt.
RX 5600 XT;1;0.6743958592414856;Die van mij was nog geen 300 euro in september. Frappant dat die prijzen omhoog zijn gegaan ipv naar beneden.
RX 5600 XT;1;0.4529876410961151;Je doelt op de Navi black screen bug die AMD probeert op te lossen?
RX 5600 XT;1;0.626975953578949;Klopt, mijn scherm flikkert helaas. Ik weet niet of ik dit terug moet sturen de kaart, of moet behouden en op een driver moet wachten. Heb nog garantie vooral 14 dagen garantie.
RX 5600 XT;1;0.32928499579429626;"Tja buiten die 14 (soms 30) dagen retour recht is dat vooral wat je moet gaan doen; wachten. Als je daar geen zin in hebt dan kun je beter ruilen voor een vergelijkbaar model uit het groene kamp."
RX 5600 XT;3;0.3239164352416992;Ik kan wel wachten dat niet. Het is meer de vraag komt er uberhaupt iets wat het zal fixen. Bij sommige is het iets hardwarematigs. Ik had mijn nieuwe Build op een tv scherm gekoppeld van Samsung. Vandaag mijn monitor binnen gekregen, dus ga kijken of het aan dat ligt. Enige wat ik dan bij de groene markt zou halen is de 2070ti op dit moment maar kost 580,- tov de 390van mijn XT scheelt toch wel veel geld. Ga trouwens direct dit proberen, zag dat AMD een driver update heeft van vandaag Radeon Software Adrenalin 2020 Edition 20.1.3 Highlights Support For Radeon RX 5600 XT Fixed Issues An intermittent black screen or loss of display may occur when performing parallel actions such as web browsing, gaming or watching video. A limited number of games such as Nioh™, Dragon Quest Builders 2™, WWE™2K20, Dead or Alive 6™ and Atelier Ryza™ may crash or fail to launch. Wolfenstein™ 2: The New Colossus is not detected in Radeon Software games manager. Text overflow in some UI boxes or toast messages may be experienced in some language localizations. Fan Tuning may change back to the default state when switching between available GPUs. Copy text options are not available in the display specs table for Radeon Software. An intermittent black screen or loss of display may occur when the system is left idle at desktop. Factory Reset install may keep previously configured Radeon Software game profiles. This can cause mismatch between global graphics settings and per profile settings. Eerste punt is meteen ook de beste.
RX 5600 XT;3;0.3803432881832123;Meest logische alternatief is de RTX 2060 Super die gemiddeld 5% trager is dan een 5700XT. Wellicht dat de driver update het oplost .
RX 5600 XT;1;0.41968977451324463;Eerder 10% langzamer dan 5% wat best veel is, helemaal voor die prijs. Kijk hier maar eens
RX 5600 XT;1;0.3294231593608856;nog eerder 15! tenzij je echt iemand bent die niets kunt met hoe dingen stock uit t fabriek komen. gooi nog s undervolten erbij en 20% sneller, ja NEE undervolte doet Geen wonderen bij nvidia
RX 5600 XT;3;0.4274722635746002;Jammer dat de Vega56 & 64 niet in de vergelijking zijn opgenomen.
RX 5600 XT;3;0.32211247086524963;Idd, dat hebben ze bij de buren wel meegenomen net als de GTX1070 en meer...:
RX 5600 XT;4;0.28214231133461;Ik blijf nog even bij mijn Vega56.. zeker met mijn klein oud schermpje is er geen reden tot upgrade.
RX 5600 XT;2;0.4364306330680847;Weet niet of ik nu blij moet worden met de wildgroei van bios-varianten, OC wel, OC niet, quiet, stock, even buiten natuurlijk de namen die ze nu de kaarten al gaan geven (zoals een Gaming OC, maar blijkbaar geen 1750 boostclock), was het wel te verwachten dat hij gewoon tussen de 5500 en 5700 (en secondair tussen vega 56 en 64) zit. Maar mijn vega was zo onstabiel als het kon, het blijft bij bepaalde 5700s nog steeds gezeur rond drivers en crashes (en bios), snap dat elke fabrikant natuurlijk z'n eigen kaarten wil uitbrengen, maar waar het vroeger nog simpel AMD/Nvidia was op med/high/ultra, heb je nu uit 12+ kaarten te kiezen per segment. Nu beginnen de prijzen van de 5700's te zakken, zijn marginaal sneller, de 5600 OC's zullen wel een 'OC' prijs krijgen, dat het nu denk ik beter is te kijken naar een gevestigde 5700 (hoewel bv een Red Dragon nog steeds ver over de 400 ligt), dan weer het water in de duiken opstartdrama van bios, drivers en stabiliteit.
RX 5600 XT;2;0.5439983010292053;Ik kan mij eigenlijk niet anders herinneren dan dat er tientallen kaarten per type uitkwamen. Als ik me niet vergis had alleen EVGA al 8 varianten van mijn GTX 1070. Dat heeft verder weinig met de stabiliteit te maken. Beste advies lijkt mij dat wanneer je merkt dat je kaart niet stabiel is, stuur hem dan gewoon lekker terug binnen de 14/30 dagen die daar voor staan. Dan hoef je je van te voren ook niet zo druk te maken.
RX 5600 XT;3;0.5652592778205872;Ik vind het eigenlijk wel opvallend dat, zelfs op 1440p en 4k, de 5600XT OC weinig last lijkt te hebben van de kleinere memory bus ivm de 5700.
RX 5600 XT;3;0.32913315296173096;ik begrijp die nvidia keuze niet. ik heb een tweedehands 1070ti die het makkelijk beter doet in deze spellen, zeker op 1080p. Dat blijft een prima kaartje voor 250 euro
RX 5600 XT;2;0.41196367144584656;Ja als je deze kaart voor 250 euro zou kunnen kopen, maar als je op pricewatch kijkt dan is deze kaart een stuk duurder. Je hebt het namelijk nu over een kaartje die nieuw vanaf 450 euro te koop is. en de meeste beginnen zelfs nog veel hoger, gemiddeld rond de 650 euro. Niet slecht dat je hem voor 250 euro hebt kunnen kopen.
RX 5600 XT;3;0.8347430229187012;okay, maar hij wasvheel populair in de vorige generatie, goedkoper en minevengoed als de 1080 die erin staat, beter dan de 2060. dus ik vind het wel een relevante vergelijking. mijn prijs tweedehands, nu kun je hem wel voor 250 krijgen ik denk dat ik iets meer betaald heb.
RX 5600 XT;5;0.4758946895599365;waar hebbe jullie t over? 2060 heeft die kaart mooi vervange!
RX 5600 XT;2;0.4449128210544586;zegmaar niet makkelijk, en sommigen willen risicos van een 2e hands kaart niet lopen beter paar 10tjes meer en altijd een nieuw kaart met zeker goed garantie, (vooral toch als je zeker een extreem overclocker als mij bent)(ookal niks meer aan garantie, maar daarom)
RX 5600 XT;1;0.5252007246017456;Ah OC is hier wat straks het standaard wordt voor 5600XT. Dus mijn reactie weggehaald
RX 5600 XT;3;0.4055551290512085;Een mooie review. Mis eigenlijk alleen een vergelijk met de Gtx 1070. Of moet ik hier de 1660 voor aanhouden?
RX 5600 XT;3;0.38091325759887695;1070 gemiddeld 5-20% (elk game gebruikt elk kaart anders) sneller dan 1600series behalve 1660 zelf, dus je kan wel gemiddeld overal zelf wel zon 10% bij de 60ti and de 60s plakken toch ?
RX 5600 XT;1;0.562766969203949;Valt me op dat niemand hier valt over het feit dat deze kaarten dus undertuned gereleased zouden gaan worden als er niks tussen was gekomen. Als Nvidia dit gedaan had stond menigeen hier te schuimbekken en spreken van schande en bedrog dat ze een product launchen wat gelimiteerd is. Zo zie je maar, team rood is ook gewoon een bedrijf dat geld wil verdienen en echt niet het beste voor heeft met de consument.
RX 5600 XT;2;0.29171761870384216;"testmethode: Intel Core i9 9900K @ 5GHz (8 cores, 8 threads) Een pci-e 3.0 moederbord dus. Terwijl de kaart PCI-e 4.0 ondersteund. Waarom niet met PCI-e 4.0 testen??? Maakt niet uit? Ok..... hieronder een testje met de rx5500 xt op 4.0: ""What pcgameshardware.de showed was that during memory reads and writes, the transfer rate was effectively cut in half. Whereas PCIe 4.0 x16 would reach 12.5 GBps, x8 only reaches around 6.5-6.7 GBps -- half the throughput. Memory copy speeds are not affected, as this is the transfer rate from the memory itself. So what does this mean for performance? The improvements varied by title (and settings), but pcgameshardware.de tested Assassin’s Creed Odyssey, Battlefield V, Far Cry: New Dawn, Wolfenstein Youngblood and Shadow of the Tomb Raider. In each test, there were improvements in the 4GB card when it was running on a PCIe 4.0 system. Some were significant, others not so much."""
RX 5600 XT;2;0.47345343232154846;Komt door maar 4GB VRAM en omdat de kaart maar PCIe x8 ondersteund, wat erg vreemd is voor een gaming kaart. De enige kaarten die dat hebben gedaan waren van die low-end Nvidia GT kaarten van een paar tientjes. Daar heb je met 6GB dus veel minder last van en deze kaart heeft wel gewoon PCIe x16 ondersteuning. Niks aan de hand dus . Rustig aan. In the end, it's a curious choice for AMD to wire these cards in an x8 configuration, particularly with the 4GB card, which has a greater chance of running into VRAM limits. While many titles will not run into these issues, those that do can take a severe performance hit.
RX 5600 XT;3;0.47991180419921875;Testsysteem : Tuurlijk mooi en snel maare dit is toch niet wat een gemiddelde thuis heeft staan: Moederbord Gigabyte Z390 Aorus Master Processor Intel Core i9 9900K @ 5GHz (8 cores, 8 threads) Geheugen G.Skill Trident Z 2x 8GB DDR4-3200 CL14 Ssd 2x Samsung 970 EVO 1TB Voeding Seasonic Prime Titanium 1200W b.v. Testsysteem Moederbord Gigabyte Z390 Aorus Master Processor Intel Core i9 9600K Niet overclocked. Geheugen G.Skill Trident Z 2x 8GB DDR4-3200 CL16 Ssd 1x Samsung 970 EVO 1TB en ter vergelijk een AMD setupje wat is het verschil. Nu zit ik weer naar waardes te kijken die voor de gemiddelde persoon dus niet uitkomen in 1080p. Met ander woorden als je nu kijkt naar deze waardes en zo'n kaartje koopt dat je b.v. altijd 100FPS hebt en je hebt niet zoals het test systeem, dan .. jammer joh.. wat minder FPS.. Leuk en tof dat er getest wordt, alleen jammer dat het niet echt realistisch is. my2cents..
RX 5600 XT;2;0.37572982907295227;Kom op zeg. Ze gebruiken dat systeem alleen om uit te sluiten dat er een bottleneck zit buiten de GPU om om zo de prestaties van de GPU te kunnen meten. Voor thuisgebruik: Kijk van elk onderdeel GPU/CPU wat de individuale FPS zijn en kies ze zo uit dat die dicht bij elkaar liggen. De laagste van de 2 is dan de FPS die je gaat halen. Dat kan je alleen doen als de tests zo uitgevoerd zijn als hier. Als je bij een midrange kaart een midrange CPU pakt zijn de hier genoemde waardes prima te halen voor normale personen. Alleen de mid settings 1080p ga je misschien niet halen maar zo hoog gaat je scherm toch niet.
RX 5600 XT;3;0.35569486021995544;Je wil niet dat je GPU test een bottleneck heeft in de CPU, dus daarvoor neem je een CPU die zeker snel genoeg is. En je wil hetzelfde testsysteem gebruiken voor alle GPU's, dus ook voor de high-end GPU's. Het is dus logisch dat je voor die high-end testen niet op een gemiddeld systeem uitkomt.
RX 5600 XT;3;0.3653692901134491;Aan de andere kant deze kaart valt nu precies in het segment van de mainstream game pc build guide van januarie, Het zou eigenlijk wel leuk zijn geweest als ze deze test ook met die pc dan hadden gedaan ter vergelijking. in deze pc raden ze aan om de AMD Ryzen 5 2600 Wraith Boxed te nemen.
RX 5600 XT;2;0.3851241171360016;ik game al 8jaar op uhd, veel verschil op hogere resoluties zou nieteens tussen een ryzen3 en een 7 uitmaken
RX 5600 XT;1;0.41146066784858704;Een i5 met 6 cores is in moderne games zoals RDR2 en Assassin's Creed: Origins een bottleneck met alles boven een RX 580, al helemaal als het om framedrops gaat, dus dat is helemaal geen logische keus. Je moet juist met een i9 9900K of een r9 3900X testen om er zeker van te zijn dat je de prestaties van de grafische kaart meet, ipv. die van de CPU. Overigens zullen relatief weinig gamers die zelf een PC bouwen een 9600K hebben gekocht, aangezien je voor dezelfde prijs een 6c/12t Ryzen 5 2600 kon kopen die misschien wel iets lagere doorsnee framerates levert maar wel weinig tot geen framedrops, en verkoopcijfers al sinds pakweg begin 2018 uitwijzen dat AMD veruit de populairste CPU-leverancier voor zelfbouwers is.
RX 5600 XT;3;0.26772916316986084;Past deze in een Mini-Itx behuizing zoals Phanteks Evolv Shift? Tot nu toe heb ik alleen maar snelle nVidia kaarten gezien die korter kunnen zijn dan 23 cm...
RX 5600 XT;5;0.2929731607437134;Die kast ondersteund “gewoon” kaarten tot 35cm dacht ik.
RX 5600 XT;1;0.39093291759490967;mITX zegt iets over wat voor moederbord erin past, niks over wat voor videokaart erin past.
RX 5600 XT;1;0.30047550797462463;Op reddit gelezen dat veel mensen met succes deze besteld hebben op amazon.jp
RX 5600 XT;3;0.40854117274284363;Dus je kan het beste een 5700 nemen waar een XT bios op te flashen valt als je een 5700XT wilt en je kan het beste een 5600XT nemen met een OC-bios als je een 5700 wilt. AMD is wel een beetje aan het rommelen op deze manier.
RX 5600 XT;4;0.22157427668571472;Dat OC verhaal is eindelijk weer een goede reden, naast een betere koeler en PCB, om 3/4/5 tientjes meer te betalen voor een Strix /Gaming X /Aorus premium kaart. Slim gedaan van AMD.
RX 5600 XT;1;0.8907373547554016;Instap kaart van > 300 euro Instap .... Ook altijd pijnlijk om die dollar prijzen te vermelden. Thanks EU. Echt heel concurrerend is het niet qua positionering. Geknepen 5700.
RX 5600 XT;2;0.3591044545173645;maar het is bijna een 700 voor de prijs van een 600 (en nog niet blij)
RX 5600 XT;1;0.590664267539978;waarom niet, ze geven toch bijna de 700 voor 50(als je tijdje wacht 100)euro minder!
RX 5600 XT;1;0.275706946849823;"en wie weet komt er binnenkort opeen ook nog een 5700bios voor die kaarten uit ;p"
RX 5600 XT;3;0.6752943396568298;Allemaal leuk en aardig maar ik mis de meerwaarde bij zulke reviews icm welke monitors er gebruikt kunnen worden? Gsync/Freesync maken veel uit in de gaming experience. Dus wat is verstandig voor de doorsnee gamer?
RX 5600 XT;5;0.4433501660823822;alles 60hz freesync (logisch want t s AMD) zou geen moeite moeten zijn,, ookal s t uhd, dan gooi je nogsteeds maar alles op fulhd, is nog steeds beter en mooier dan fulhd op een fullhd monitor. veel mensen die 144hz hebben en 100fps niet halen moeten instellen op 60hz, anders te veel tearing!
RX 5600 XT;3;0.5301592946052551;blijf het toch lastig vinden, gebruik zelf een ultra wide om te gamen.. moet je dan toch kijken naar 1080p of naar 1440p kaarten (aangezien de resolutie vaak 2x1080p is oid?)
RX 5600 XT;2;0.36803290247917175;geen veel hoofdpijn bezorgen voor maar 50-100euro, ultieme 1080p kaarten zijn 5500 5600 xt ultieme qhd kaarten zijn 5700 -xt
RX 5600 XT;1;0.40303289890289307;wete niet genoeg, hoop voor dit jaar genoeg informatie
RX 5600 XT;1;0.5937915444374084;Misschien tijd voor Tweakers om de review aan te passen? Of iig aandacht aan het volgende te besteden : Gamers Nexus heeft behoorlijk slechte signalen gekregen van kaartfabrikanten als oa MSI die waarschijnlijk bij een deel de geheugen OC helemaal niet gaan ondersteunen. Ook is een groot deel van de huidige 5600XT kaarten helemaal niet getest met de OC settings en zal dus tot niet werkende kaarten kunnen leiden. Ook zijn er indicaties dat de kaarten die wel de OC aankunnen meer gaan kosten dan de adviesprijs... Waarbij dus de 5700 al gauw de betere en dus zelfs goedkopere optie is. AMD mag zich doodschamen : om de marktpositie te redden van de 5600XT hebben ze hun klanten weer eens het enge bos ingestuurd met onduidelijke specificaties en daarbij bewust van de mogelijke problemen de OC varianten toch naar de reviewers gestuurd. Lijkt me een goede reden om hun GPU's helemaal maar links te laten liggen.
RX 5600 XT;3;0.23909339308738708;1080p........................ u mean 720p
RX 5600 XT;4;0.37972164154052734;De 5600XT klinkt als een prima instap kaart voor gaming op 1440p medium/high 48-75Hz adaptive sync. Wel ben ik benieuwd hoe deze kaart het gaat volhouden in zeg eind volgend jaar met games die zwaarder worden, en consoles die zwaarder uitgerust worden in compute kracht. Offtopic handop denken vanuit eigen perspectief: De performance target klinkt voor mij als waardige toekomst upgrade over mijn RX 570 met vergelijkbaar power budget mocht ik a) meer GPU rekenkracht nodig hebben én b) als mijn RX 570 overlijdt.
RX 5600 XT;3;0.3494764268398285;voor mezelf ben blij dat ik ook de grafische fidelity naar beneden kan zetten, en dat ik zobiezo geen AAA gamer ben (meest zware game die ik heb draait al prima op een Radeon RX 550 2GB met wat setting naar beneden), en dat ik liever zelf experimenteer met maken van games zelf als leerproces vooralsnog. ben zelf ook nooit echt een gamer geweest, mijn workloads zijn meer cpu&ram based (VM, DAW, CAD) dan GPU based. maar wel mooi dit soort grafische accelerator vooruitgang nog te zien nu dat nodes moelijker worden te ontwikkelen.
RX 5600 XT;1;0.5486112833023071;Waar zijn de videokaarten van 600 euro die alles op 100 fps draaien op ultra bij 2k en 4k.. Zzzzzzzzz
RX 5600 XT;2;0.409595251083374;Hoezo? De Rx5600 kost 10% minder dan de RTX 2060 beide stock hij presteerd ook 10% minder stock. Omdat de kaart meer potentieel heeft met de RX5700 die kan hij evt. met een dual bios richting de RTX 2060 super komen. Die gewoon 100 euro meer kost. Lijkt me een prima product dan. Zeker als je een Vsync monitor hebt. Al zou ik zelf liever die paar tientjes extra neerleggen voor de normale 5700
RX 5600 XT;2;0.3575781285762787;Harder dan dus in eerste instantie leek... Nog nooit zo de stoom bij de Gamers Nexus over AMD uit de oren zien komen na deze wanhoopsactie met een dubieuze OC. Na wat gesprekken met fabrikanten blijkt het een zootje te zijn welke kaarten het eventueel aankunnen en wat de adviesprijs gaat zijn.
RX 590;5;0.484871506690979;Zelf undervolt ik altijd, dus eigenlijk wel benieuwd naar het stroomverbruik en performance van deze kaart als je hem gelijk klokt aan de RX580. Geeft een beter zicht op wat de stap naar 12 nm nu precies brengt.
RX 590;1;0.5580331683158875;Dan gaat er zo 50W af!
RX 590;2;0.5626336932182312;Jammer genoeg ligt het heel erg aan de die of deze ver under-gevolt kan worden, en je weet nooit of het 100% stabiel is, ook al test je urenlang. Dit is dan ook waarom AMD de voltage veel hoger doet dan nodig is, en zo'n beetje elke chipfabrikant dit doet.
RX 590;4;0.47442102432250977;Met een stress-test van een aantal uren kun je al vrij goed beoordelen of iets stabiel draait hoor. Ben je een beetje bang, dan geef je hem net een beetje extra nadat je een stabiele stress test hebt kunnen draaien. Als stabiliteit zo belangrijk is, dan kun je net zo goed op fabrieksinstellingen blijven.
RX 590;1;0.4516166150569916;Wat ik altijd aan houdt bij het OC'en van mijn hardware is een stress test van 24 uur, als ik 24 uur lang op de beoogde OC kan draaien terwijl ik mijn CPU/GPU stress test (nadat hij het al een paar uur vol heeft gehouden), dan laat ik het daarop staan, als het faalt core multiplier/OC/voltage verhogen/verlagen
RX 590;1;0.589530885219574;Oh ok? Nou als ik kombuster uren draai dan kan het zo zijn dat mijn kaart stabiel draait. Ik krijg m met 1 game na 15 min aan het crashen. Dus zelfs zo’n duurtest zegt niet alles.
RX 590;1;0.4749075770378113;Ik heb hetzelfde gehad. Bij het overclocken van mn RAM (Ryzen 1700, RAM naar 3200mhz) kon ik Unigine Heaven GPU compleet cappen en keihard mn GPU erbij overclocken, maar bij het spelen van Overwatch daarna bleef ik om het half uur crashen. Na lang niet begrepen te hebben wat er aan de hand was (gebeurde alleen bij Overwatch) updatete ik mijn Bios ooit (wat CPU en RAM overclock reset), en plots crashde Overwatch niet meer. Toen ik alleen de CPU overclockte daarna gebeurde het niet meer. Benchmarks gebruiken vaak niet dezelfde resources als games en soms kan een game gigantisch hard een type resource hoggen. Ik moet wel zeggen dat mijn geval een uitzondering is, want dat is de enige keer geweest dat ik niet stabiel liep ondanks de benchmark stress tests.
RX 590;2;0.36954888701438904;"Inderdaad. Als een ""burn in"" test de kaart over alle componenten tegen de limiet belast dan kan een game een bepaalde hotspot creëren waarbij de kaart wel ineens vatbaar wordt voor een hang-up bij een bepaalde overclock. Ik heb dat met name dan met Company of Heroes 2 in 4K / Ultra."
RX 590;2;0.45132923126220703;Dat je Unigine Heaven stabiel kunt draaien terwijl een game als Overwatch crashed door een instabiele RAM overclock is niet geheel onlogisch. Heaven veroorzaakt namelijk slechts een relatief minimale load op de memory controller, en veroorzaakt een relatief minimaal geheugenverbruik. Heaven is dan ook totaal niet geschikt om RAM stabiliteit te bepalen. Heaven is een GPU benchmark, die niet meer dan een indicatie kan geven van de stabiliteit van de GPU. Of het nuttig is als stabiliteits test van de GPU is afhankelijk van hoe je Heaven gebruikt, en in hoeverre Heaven in staat is alle functionaliteit van je GPU te belasten op de manier waarop je het gebruikt. En dan nog geldt dat Heaven zeer beperkt is in de functionaliteit die het aanspreekt op de GPU waardoor het nut van heaven als stabiliteits test behoorlijk beperkt is.
RX 590;2;0.48144692182540894;Waar bij mijn RX580 1080mv prima werkt in games laat op Radeon Relive bij het opnemen van ingame video direct het zaakje crashen. Undervolten is leuk maar het werkt niet voor alle mogelijke scenario's. Ik moet dan gewoon inperken met een 1090mv wat ansich niet verkeerd is ten opzichte van de stock 1150mv. Ik denk dat AMD ook met het vrij lage fanprofiel bewust kiest voor een hoger voltage. Als een GPU immers 'heet' loopt is er een hoger voltage nodig om goed te blijven werken dan wanneer je een koelere GPU hebt. Kijk maar naar LN2. Daar is het benodigde voltage vaak veel lager op LN2 / stock dan wanneer je op luchtkoeling zit ofzo.
RX 590;3;0.415558397769928;Vrij goed, totdat je een game tegenkomt die meer delen van de GPU hoger kan belasten en dan valt het allemaal in duigen en kun je opnieuw beginnen. Hoe dan ook, de RX590 valt eigenlijk precies tussen de 1060 en de 1070, wat uitschieters daargelaten, en dat is helemaal niet verkeerd. Alleen jammer dat het DUBBEL zoveel energie kost als een 1060 nodig heeft. Dat is echt een enorm gat, zelfs na een undervolt... Een GTX 1080 is nog zuiniger. 220W is een high-end TDP budget. Verder vwb de shrink, dat stelt weinig voor. 12nm is geen geweldige node, dat liet Turing al zien.
RX 590;1;0.6505926847457886;Niet noodzakelijk! Het heeft ook te maken met dat je die extreme applicaties hebt waarbij een kaart net niet stabiel kan zijn en het is prima mogelijk dat AMD veel conservatiever is dan Intel (op meerdere manieren). Een hoop speculatie, zeker is dat je gemiddeld op ongeveer 0,1 V uitkomt dat je de spanning kan verlagen. Serieus, wat maakt het uit als het niet 100% stabiel zou zijn? Je computer ontploft niet. Je krijgt 1 crash, in het ergste geval moet je je computer opnieuw opstarten en je verhoogt de spanning 5 mV of zo. Big deal!
RX 590;3;0.37087830901145935;Ik ben ook benieuwd naar hoe het verbruik zal zijn wanneer je gewoon een beetje browst (wat ik toch het grootste gedeelte van de tijd doe), en idle. Zo te zien is deze 590 idle wel zuiniger dan de 580.
RX 590;2;0.3520646393299103;Deze 50W is dan wel een undervolted 590 vs een stock 580, ik betwijfel ten zeerste of dit verschil nog significant is als de 580 ook undervolted was.
RX 590;1;0.3511083424091339;Waarom? Waar baseer je dat op? Aangezien de 590 op een ander procedé gebakken is, kan je dat niet zomaar vergelijken.
RX 590;2;0.47467100620269775;Tegen de 80W zelfs. Je ziet ook dat de scores consequent iets lager zijn dan bij de RX580. Of dat nu komt door het undervolten of door verschillen in de gebruikte hardware / videokaart is niet duidelijk. Er is geen test waarbij de kloks en het voltage gelijk zijn aan de RX580.
RX 590;1;0.6710401773452759;Geen idee wat undervolten inhield dus ben ff gaan googlen en kwam op Wikipedia dit artikel tegen maar de laatste update is alweer ruim 5,5 jaar geleden. Iemand die deze pagina aan zou willen passen? bvd
RX 590;2;0.3398453891277313;Eigenlijk zegt de naam het al, een lager voltage. Om hitte/energie te besparen. Daar hoef je de Wiki niet voor te updaten
RX 590;2;0.34406694769859314;Ik blijf het bij de Battlefield benchmarks het maar vreemd vinden dat DirectX 12 gebruikt wordt om mee te benchmarken. Het is sinds Battlefield 1 (die gebruikt is om te benchmarken) alombekend dat DirectX 12 ruk presteert (en het grafisch geen verschil maakt!) en dat daarom 90% van de spelers op DirectX 11 speelt. Waarom dan toch met DirectX 12 benchmarken? @willemdemoor Dit viel mij bij de RTX roundup ook heel erg op.
RX 590;2;0.39308297634124756;Het is een test van de GPU, niet van de game. De benchmark dient om prestaties met andere GPU's te vergelijken, niet louter om een zo representatief mogelijk beeld van de gameprestaties te bieden. Als die met Dx11 altijd pakweg 10% hoger liggen, is het simpel zat om dat als lezer mee te wegen - als dat prestatieverschil inderdaad in de game/API zit, kan dat nooit op een specifieke videokaart opeens anders uitpakken. Als er dan een keuze is tussen een oudere en een nieuwere API, begrijp ik de keuze voor de modernere, die immers ook degene is waarvoor de videokaart primair is ontwikkeld en die in de toekomst vermoedelijk meer en meer wordt toegepast. Er zijn andere games met DirectX 11 getest om daar een beeld van te krijgen.
RX 590;1;0.4887315034866333;Wat betreft DX12, dat is tot nu toe slecht, maar tot nu toe gaat het vooral om halfslachtige DX12-patches, niet om spellen die vanaf het begin en enkel DX12 gebruiken. Ik vrees dat dat nog lang zo zal blijven want MS gebruikt DX12 als pressiemiddel om gamers te dwingen om W10 te gebruiken. Als de developers nu eens Vulkan zouden gebruiken, daarvan is gebleken dat het uitstekend werkt en het is platform-agnostisch.
RX 590;2;0.4624350666999817;Vraag is natuurlijk wel hoe interessant benchmarks zijn van een scenario dat in de praktijk niet gebruikt zal worden. Het lijkt mij juist van belang, als klant zijnde die mogelijke zijn aankopen zal baseren op reviews, dat reviews zoveel mogelijk praktijk situaties testen. Ook een reden waarom ik synthetische benchmarks eigenlijk vaak volledig oninteressant vind. Leuk dat een kaart A in 3Dmark versie x 10000 punten haalt, terwijl de concurrent kaart B 9000 punten haalt. Mij als consument zegt dat weinig, ik kan er bijv niet uithalen dat Game Y op kaart A dan 100FPS heelft en op kaart B 90 FPS. Want het kan best dat wanneer kaart A meer punten haalt in 3DMark, in Spel Y kaart B juist meer FPS haalt en dus de betere koop al zijn als je als consument voornemens bent voornamelijk game Y te gaan spelen. Ik ben het zeker met je eens dat benchmarks er voor moeten dienen om de consument een leidraad te geven om prestaties van verschillende GPU's te vergelijken, echter zal het dan ook belangrijk zijn dat de benchmark suite aansluit op wat de consument wil gaan doen en dus realistische scenario's test. Als dan voor Battlefield blijkt dat bijna iedereen op DX11 speelt omdat de DX12 schijnbaar te wensen over laat, zou ik het juist niet logisch vinden om DX12 te testen over DX11. Een leuke zou dan wel worden Total War: Warhammer 2. In mijn ervaring speelt namelijk iedereen met een Nvidia kaart die game op DX11 (want betere prestaties dan op DX12) en terwijl je met AMD kaarten in ieder geval soms beter uit bent op DX12. @willemdemoor misschien is bovenstaande ook iets om mee te nemen in het overleg met je collega's
RX 590;3;0.30593791604042053;Ik zal het met m'n collega's bespreken... Kijken of dat veranderd moet worden...
RX 590;4;0.3889429569244385;DirectX 11 vs 12 heeft toch net een andere workload voor zowel CPU als GPU. Dus als je in het verleden altijd DX12 hebt getest, dan kun je dat het beste nu ook blijven doen. DX12 met DX11 vergelijken, zelfs als de performance in de buurt ligt, geeft een vertekend beeld. Het verschil tussen DX11 en DX12 is irrelevant als je op zoek bent naar het verschil tussen meerdere kaarten. Natuurlijk zijn beide interessant om te testen, maar niet onderling op dezelfde GPU met elkaar te vergelijken.
RX 590;3;0.4113146960735321;De vraag is welke combi van games en settings een goede mix vormt. Je wil dus verschillende soorten games, verschillende engineer en een mix van DX11, DX11 en wat Vulkan. Het moet representatief zijn voor de games die zoal gespeeld worden, niet voor een individuele game.
RX 590;4;0.2604987919330597;daarom hebben we (zie lijstje hier) een mix van 4* DX12, 6* DX11 en 1* Vulkan, met allemaal verschillende game-engines (en dito genres)
RX 590;4;0.4116244316101074;Precies. En dat is belangrijker dat per game de settings kiezen die het populairst zijn voor die game
RX 590;3;0.45261695981025696;Eens. Aan de andere kant heb je nu de DXR componenten en als je die naast niet-DXR content wil leggen is DX12 wel handig. Anders is er geen pijl meer op te trekken.
RX 590;4;0.2777365446090698;Ook @willemdemoor Het beste is natuurlijk om met beide te benchmarken maar dan komt de review later en ze zijn bang daardoor minder clicks (=inkomsten) te krijgen. Gezien hoe verschillend de micro-architecturen van Ryzen en Intel en Nvidia en AMD zijn moet je altijd alle 4 de combinaties testen en gezien hoe verschillend ze omgaan met diverse features van Vulkan en DX12 moet dat beide worden getest. Enkel dan krijg je een genuanceerd beeld over hoe goed de hardware presteert.
RX 590;1;0.43858200311660767;Dus de XFX kaart die eigenlijk een overklokte 590 heeft, hebben jullie teruggeklokt tot reference waarden?
RX 590;5;0.5412187576293945;225 Watt voor deze performance mag je ook best milieuvervuiling noemen.
RX 590;3;0.4763088822364807;Valt mee. Als je toch niet hoger dan de refreshrate aan frames wilt halen dan zet je Radeon Chill aan. Dan presteert de GPU ook niet meer dan dat het toveren moet in frames op beeld en dan bespaar je een hoop aan (onnodige) GPU cycles. Zo blijft de GPU namelijk ook lekker koel (hier ruim 20 graden verschil) en je merkt er niets van qua Frametimes.
RX 590;5;0.5280358791351318;Jip! Zelf gebruik ik op dit moment RX 570. Radeon WattMan en Chill maakt het zo makkelijk in paar muisklikken. GPU Overclock op 1320mHz, Undervolt op 1.010v, Memory Overclock op 1950mHz, Underclock op 0.920v = 150W naar 95W. = Minder Watt verbruik maar veel meer FPS.
RX 590;5;0.5081148147583008;Naast wat @Jism zegt, een heel erg belangrijk punt, je kan ook de spanning met pak hem beet 0,1 V verlagen en het vermogen is prima. Vraag mij niet waarom AMD standaard die spanning te hoog zet, ze doen het en je kan het gemakkelijk zelf verlagen. Dat gezegd hebbende, ik zou op dit moment een Vega56-kaart bij Caseking (altijd lagere prijzen dan in Nederland voor AMD-kaarten, je kan met IDEAL betalen als je dat wil) of Overclockers (lage koers Pond, momenteel kan je voor €340 een Sapphire Pulse Vega56 kopen). Die kaart haalt met hetzelfde vermogen als de GTX 1070 een FPS die dichter bij die van de 1070 Ti dan bij de 1070 ligt als je wat speelt met de spanning en zo.
RX 590;4;0.4917624592781067;Een prima reactie!
RX 590;2;0.3763313889503479;Wie kan mij uitleggen dat een GTX1070 met: - 58% minder compute units - 17% minder steamprocessors/cuda cores - 2,5% meer kloksnelheid - 8,5% minder rekenkracht - 17% minder texture units - 33% minder energieverbruik Gehakt maakt van een RX590? Natuurlijk zeggen specs niet alles maar het is wel een enorm verschil.
RX 590;2;0.40850481390953064;Dat komt omdat je als consument niet merkt dat Nvidia al jaren lang een enorme voorsprong op Radeon heeft opgebouwd. Dit doet Nvidia door nieuwe kaarten fysiek te simuleren (kamer van 10 bij 10 meter) en zo kunnen ze voor elk prijspunt de prestatie doelen behalen en daarna de chip zo klein mogelijk maken. (Is ergens een video van op YouTube, als iemand hem kan vinden graag!). TLDR: Hoe beter de architectuur, des te 'simpeler' de chip en des te groter de winst van Nvidia. Het verschil is zo groot dat op reddit al twee jaar geleden werd gespeculeerd dat een RX 480 van $199 dollar even duur is om te maken als een GTX 1080 van $499. Dat klinkt absurd maar waar Nvidia vroeger op Geforce nauwelijks winst maakte en al het geld van Quadro kwam halen ze tegenwoordig honderden miljoenen uit Geforce. Sterker nog de netto winst was het afgelopen kwartaal 90%(!) hoger dan het jaar ervoor. We praten over 1210 miljoen per kwartaal. Dit ná het mining tijdperk. Voor de Geforce 6 serie was Nvidia blij met 500 miljoen in een jaar. Als je naar de TFLOPS kijkt dan zie je dat Nvidia niet alleen meer prestatie per euro uit de hardware haalt, maar ook minder ruwe kracht nodig heeft om dat prestatie niveau te bereiken. Als consument betalen we nu simpelweg veel te veel voor zo'n GTX 1070. Helaas zijn we voorlopig volledig afhankelijk van AMD Radeon om dit probleem op te lossen. Binnenkort zegt Intel echter ook met een videokaart te komen, dus wie weet schudden die de markt op.
RX 590;1;0.3229531943798065;Maar in principe halen ze dus die winst er uit omdat hun software beter is? Want hoe zit dat? Heeft iemand ooit hierover een goed review over gegeven?
RX 590;3;0.4236477315425873;Nou nee, aan de software kant heeft Nvidia overigens wel de naam om in DX11 minder cpu overhead te vereisen. Maar dat is niet de verklaring voor de hierboven besproken verschillen. Dat volgt uit de architectuur. Wat ze verbeteren kun je bij elke architectuur lezen. In de bron staan de meeste de architecturale veranderingen. De prestaties per Mhz, per core, enz kunnen toenemen. Verder neemt ook vaak de 'lossless memory compression' toe waarmee de effectieve bandbreedte stijgt.
RX 590;5;0.5859379768371582;dank je, ik zal er even rustig door heen kijken
RX 590;3;0.2618527114391327;Hier is de link naar de video van de door jou genoemde simulatie kamer:
RX 590;3;0.5071350336074829;Bedankt, maar ik had een nieuwere ruimte gezien waarbij het allemaal wat slordiger was .
RX 590;2;0.449566125869751;wat ik raar vind is... frame tearing met Nvidia in vergelijking met AMD Radeon. GTX 1060 kaartje > CS:GO (simpel spel toch? 110 fps ~ 300 fps.. en springt behoorlijk heen en weer. heb ik de R9 390, heb ik 270~300 fps, en daar springt het wat tussen heen en weer. (klopt beetje off topic) maar daarin vind ik dat de Radeon kaart een stabielere kaart is.
RX 590;2;0.43275609612464905;Hoe kom je daar bij? Eeen GTX 1070 heeft een veel hogere boostclock (makkelijk 1900mhz met GPU boost 3.0). Daarnaast kan je CUDA cores en streamprocessors niet 1:1 vergelijken.
RX 590;1;0.5313489437103271;Specs zeggen wel alles, maar je vergelijkt nu appels met peren en dat werkt niet.
RX 590;3;0.5666025280952454;Ok groenteboer leg het uit dan{ de specs zijn ? }
RX 590;3;0.5599914193153381;De compute units/cores kan je vergeten, die zijn niet 1 op 1 te vergelijken. Ik ben het wel met je eens dat AMD altijd wat grover is, vaak zie je dat over de jaren de AMD kaarten steeds beter worden omdat drivers steeds beter worden.
RX 590;3;0.40429481863975525;Misschien omdat de RX 590 NIET concurreert met de GTX 1070? Maar positioneert tussen RX 580 en GTX 1070? 8.5% minder rekenkracht? GTX 1070 = 6.5 TFLOPs RX 590 = 7.1 TFLOPs En sowieso kun je CUDA cores en Streamprocessors niet 1:1 vergelijken. Beide engines hebben voor en nadelen. Met DirectX11 is GTX 1070 zeer efficiënt. Met DirectX12/Vulkan komen de Streamprocessors weer meer tot z'n recht.
RX 590;3;0.3028774559497833;De rx580(90) maakt weer gehakt van Nvidia kaarten in SPECviewperf bench bijvoorbeeld. Cad/cam/simulatie gericht. De rx580 presteert nagenoeg gelijk aan een 1080ti in dat geval. Lijkt erop dat AMD hun kaarten iets te breed in wil zetten, hetzelfde verhaal bij vega. Beter dat ze een 'pro' serie en een 'gamer' serie in de markt zetten. Of er iig daadwerkelijk werk van maken. Focus AMD!
RX 590;5;0.5711557865142822;De rx590 is niet de 1070 tegenstander, maar de rx vega 56, die maakt gehakt van een 1070. Heb je een freesync scherm maak je helemaal winst.
RX 590;4;0.2971312999725342;Nvidia heeft efficientie op de eerste plaats gezet in hun architectuur vanaf Kepler. De doorontwikkeling vanaf dat moment had een focus op: - zo smal mogelijke VRAM bus (liever sneller geheugen dan een bredere bus, zie Kepler Refresh 7xx) - hogere kloksnelheden (liever snellere shaders dan méér shaders, eerste grote sprong met Maxwell, en de tweede met Pascal + het uitstekende 16nm TSMC procede) - efficient geheugengebruik (delta compressie met Maxwell, dat had AMD pas met Polaris voor elkaar. Pascal deed er nog een schepje bovenop) - specialisatie. Nvidia heeft alles uit Geforce gesneden dat niet puur voor Geforce van belang is. De gaming GPUs doen ook echt alleen dat, andere workloads zijn zwaar gehandicapped. AMD had lange tijd veel beter GPGPU prestaties bijvoorbeeld, en Vega is primair een pro GPU, geen gaming GPU. Het klopt overigens niet helemaal wat je zegt, Nvidia heeft namelijk wel zo'n 20-30% hogere clocks vanaf Pascal. GPU Boost 3.0 brengt de meeste GPUs naar 1800+ mhz en met goede koeling kom je over de 1950mhz. Bij AMD mag je in je handjes knijpen als je er 1600 uit trekt met het volume op 11. AMD gebruikt één versie van GCN en verbeterde die mondjesmaat sinds 2012, en moest blijven opschalen (meer cores, bredere bus, meer shaders) tot het niet meer ging. Er zijn verkeerde keuzes gemaakt, waaronder het toepassen van HBM vóórdat ze met delta compressie aan de gang gingen (Polaris bewijst dat het prima kon op GDDR5, Fury X was helemaal niet nodig geweest). Ze hebben GCN onvoldoende doorontwikkeld en daar betalen we nu de prijs voor.
RX 590;1;0.2920476794242859;Ha, was net hetzelfde aan het denken
RX 590;3;0.4059644341468811;Is idd vreemd. Nvidia maakt efficiëntere kaarten blijkbaar. beetje zoals het verschil in IPC tussen AMD en Intel.
RX 590;3;0.6347042322158813;Het is moeilijk te vergelijken, maar de chip van de 1070 is wel 35% groter.
RX 590;3;0.40897586941719055;"Zoals ik het begrijp hebben AMD kaarten een veel simpelere opbouw, waardoor ze meer cores op een chip kwijt kunnen maar de cores zelf wat simpeler zijn uitgevoerd. Nvidia heeft gekozen voor minder cores die per stuk net wat sneller complexe berekeningen kunnen doen, waarschijnlijk met een instructieset die geoptimaliseerd is voor grafische berekeningen. Het resultaat is dat Nvidia momenteel koploper is op grafisch gebied terwijl AMD juist in bepaalde andere toepassingen meer rekenkracht uit hun kaarten haalt. Het is niet puur een kwestie dat één van de twee oplossingen altijd sneller is dan de andere; het hangt erg van de workload af."
RX 590;2;0.43033212423324585;Als de conclusie, die bij deze kaart staat, ook bij elke processor gebruikt zou worden, was praktische elke upgrade van Intel in de afgelopen jaren (behalve het laatste jaar) amper interessant. Ook de 2xxx serie van AMD kun je dan onder het kopje interessant? plaatsen. Deze kaart is 7 a 8% sneller, fijn toch? Het is dus een snellere kaart dan de 580 en daardoor interessant. Dat de 590 met de 580 concurreert is niet erg, die gaat er waarschijnlijk alleen maar goedkoper van worden. Verder lijkt er een stuk van het idle verbruik afgesnoept te zijn. Ook niet erg, want dat is een staat waar een kaart zich toch redelijk vaak bevindt. Jammer alleen van het meerverbruik bij volle snelheid.
RX 590;2;0.44879433512687683;De 580 is al enigzins outdated aan het raken en wordt gemeten met kaarten die al ruim 2 jaar op de markt zijn. Dus je kan wel stellen dat dit een telleurstellende relese is weer. De prijs is opzich wel okay maar dat is ook niet gek voor performance wat al 2 jaar oud is en toen was het al midrange performance.
RX 590;2;0.5472344160079956;Ik denk toch dat je de doorsnee gamer overschat. Het overgrote deel van alle gamers heeft echt nog geen 1440p monitor thuis staan en dus ook weinig/geen behoefte aan een dure kaart die 1440p aankan. De markt voor pakweg een 1070 en hoger is nog steeds relatief klein, dat zijn al kaarten voor het iets meer hardcore publiek.
RX 590;2;0.3876973092556;1070GTX is nu nog net acceptabel voor 1440P maar bij bijvoorbeeld Hitman 2 zou deze ook al naar 1080P gezet meoten worden om ten allertijden 60fps minimaal vast te kunnen houden. Games worden zwaarder, normaal groeien midrange kaarten hier in mee.Met hardcore publiek denk ik aan kaarten vanaf de 1080TI tot de 2080TI nu.
RX 590;2;0.37879806756973267;Hardcore zal in ieders ogen wel iets anders zijn natuurlijk, bij AMD mag je er vanuit gaan dat ze een heel duidelijke blik hebben op welk prijspunt ideaal is binnen de markt, daar ontwikkelen ze natuurlijk naar. High end kaarten is mooi maar het moet uiteindelijk geld opbrengen. 300 Euro is voor mij persoonlijk de grens tussen mainstream gamen<> hardcore gamen, zelf geef ik meer uit dan dat maar ik kan me gerust voorstellen dat heel veel mensen 300 euro al heel veel geld vinden voor één component, je moet ook denken dat een ps4 maar iets meer kost dan dat en kant en klaar is.
RX 590;3;0.47946274280548096;Mwa, ik neem aan dat je dan wel als eis hebt alles op ultra te zetten? Ik heb Hitman 2 niet gespeeld, maar ik kan met mijn 1060 tot nu toe nog goed meekomen op QHD (1440p). Persoonlijk zie ik tijdens het spelen het verschil vaak niet tussen high en ultra, soms ziet zelfs medium er nog bijna hetzelfde uit. De resolutie maakt voor mij veel meer uit.
RX 590;2;0.3899122178554535;Ik zie genoeg games al moeite heb om 60fps op 1440P met de 1070GTX. Dus denk niet dat de 1060 ook maar 80% van dit presteerd gezien de benchmarks. Hitman 2 heb ik op high staan niet op ultra. Ultra duikt regelmatig onder de 40. FF15, Far Cry 5, Assassins Creed Origins & odyssey, Hitman 2, BF5 etc laten allemaal zien dat het voor de mainstream kaarten wel hoog tijd wordt om ook wat upgrades te ontvangen. Dit staat nu een paar jaar stil.
RX 590;3;0.44183826446533203;Tja, inderdaad. Voor 1440 had ik aanvankelijk m'n 1080 gekocht. Dat ging wel ok. Als ik nu kijk bij m'n neefje, dan is eigenlijk een GTX1080 soms al niet echt meer afdoende. Wel rekenend, dat ik die kaart alweer 3 jaar geleden had gekocht. Toen voor 3440x1440 100hz een 1080Ti erbij gehaald. En dat ging prima, maar toch niet altijd meer soepel. En nu daarvoor dus een 2080Ti erin gezet. En dan zakt Hitman 2 in sommige maps / settings nog steeds naar de 40fps. Maar goed, daar hebben alle resoluties / kaarten last van, het is waarschijnlijk eerder iets in de renderpipeline / engine wat niet helemaal wil dan dat het aan de hardware ligt. Tegelijkertijd zijn een aantal van de andere game die je noemt ook gewoon heel erg zwaar. En ik zou persoonlijk onder een GTX1080 kaart niet weten wat je hiervoor zou moeten kopen. Hoog tijd dat Nvidia met een GTX2050 / 2060 komt, maar dan hopelijk met een beetje normale prijzen. Maar goed, even wachten tot Feb dus nog, ze gaan em pas volgende maand aankondigen. AMD mag eigenlijk wel zakken met z'n Vega prijzen, dan zijn ze opeens wel wat competetief. Maar ze zijn erg duur om te fabriceren, dus tja... En dat je er een enorme voeding voor nodig hebt omdat ze behoorlijk stroom lusten, helpt ook voor een kleiner budget niet mee...
RX 590;2;0.41783472895622253;Dit is dan ook een midrange kaart. Waarom zou die niet dan niet als midrange voldoen? Je kunt de meeste spellen meer dan prima op 1080p spelen, en dat voor een prijs van ~€285,- Voor heel veel mensen zal dit een leuke instapkaart zijn. Snel genoeg en betaalbaar. Het is volgens mij ook niet zo dat er in DX12/Vulkan heel veel dingen bijgekomen zijn waardoor deze kaart niet meer voldoet. Zolang de prijsstelling prima is, is er mijn inziens weinig reden tot klagen.
RX 590;2;0.41555774211883545;Omdat dit stilstand is. Dit is maar een marginaal sneller kaart 2 jaar na dato toen dit al de benchmark voor midrange prestaties was. Moderne games BF5, Hitman 2 etc draaien niet meer op 60fps in 2080P op deze kaart hoor. Helpt ook niet mee dat Nvidia qua upgrades ook meteen fors in de prijs omhoog is gegaan. Maar normaal gesproken had nu de Vega 56 midrange moeten zijn qua prestaties.
RX 590;3;0.4854475259780884;Als je zo denkt dan kunnen we de RX 550, 560, 570, GTX 1030 GTX 1050 en 1060 ook wel van de markt halen. De RX590 is geen stilstand, geen innovatie, maar gewoon een evolutie. Niet indrukwekkend, maar verder prima, zeker voor de aangegeven prijs.
RX 590;2;0.39615920186042786;Nee ze verschuiven gewoon naar het lagere segment. Voor veel zal de prijs even goedkoper lijken maar dan vergeet je denk ik dat de mining boom even de prijzen flink hoger liet uitvallen maar de 480 was oorspronkelijk voor die prijs verkocht 2 jaar geleden en is maar iets langzamer. DIt is gewoon een iets efficientere versie van diezelfde GPU weer.
RX 590;3;0.35616496205329895;Dus, sneller voor hetzelfde geld is verkeerd? Niets mis mee toch? Positioneert zich met zijn prijs prima tussen de tragere en snellere kaarten. Precies waar deze kaart hoort te zijn.
RX 590;2;0.4231356680393219;Het is een practisch overgelockte 580 en jij bent daar na 2 jaar tevreden mee. Normaal gesproken mocht je na 2 jaar een generational leap verwachten van 25% tot 30%.
RX 590;2;0.5788530707359314;"Nee, eigenlijk niet. Het is alleen wel ""weinig"" sneller. Maar dat is een gegeven paard in de bek kijken. De timing van deze refresh vind ik eigenlijk heel raar. Een jaar geleden had ik er meer van gesnapt. Waar dit een mainstream kaart was, net als de 1060 dat was, komt het nu amper nog mee in recente 2018 titels. Als je deze kaart nu als budget mainstream kaart koopt denk ik dat je volgend jaar al de drang te upgraden gaat voelen. Terwijl als je de 580 kocht twee jaar terug, je er jaren perfecte 1080P gaming voor terug kreeg. De kaart kan bijna 3 jaar oude 1060 en 1070 kaarten niet significant overstijgen relatief tot het vorige model. Een 580 valt tussen beide Nvidia modellen in, en de 590 ook, maar dan iets dichter bij de 1070. Hoe moet je dat nou weer uitleggen in de retail. Wat er dus mis mee is, is dat het wel een heel kleine boost is, die ook nog meer stroom benodigd, het product niet anders concurrerend is met de competitie en voor een refresh wel erg laat komt. Ik kan deze kaart eigenlijk moeilijk aan iemand aanraden die er langer dan ter overbrugging mee wil gaan doen en wel moderne games wil gaan spelen. Maar datzelfde heb ik met de 1060 ook eigenlijk al."
RX 590;2;0.3664172291755676;Maar dat is onzin. Ik kan met mijn RX480 prima moderne games spelen, waarom zou dat op een RX590 dan ineens niet meer kunnen?
RX 590;1;0.27334147691726685;AMD zou gewoon nu een mid-range kaart moeten hebben die net zo snel is als minimaal een Fury X, dit is natuurlijk gewoon bs. Een HD 7850 was sneller dan een HD 6970. Een RX 580/90 is gewoon duurder dan een een R9 290 uit 2013, die 3 jaar geleden €250 was en marginaal sneller.
RX 590;2;0.429633229970932;Dat waren ze toch ook? Ik heb weleens het idee dat AMD aanhangers last hebben van calimero syndroom. Laat ik bijvoorbeeld eens quoten van de i7-7700 review: reviews: Kaby Lake voor de desktop: de kleine stap van 6700K naar 7700K Als je een RX580 hebt is de upgrade naar een RX590 totaal niet interessant. Zit je nog een stuk daaronder, en wil je gaan upgraden naar iets in deze richting, dan kan het interessant worden afhankelijk van het prijspunt.
RX 590;2;0.4260892868041992;"""Ik heb weleens het idee dat AMD aanhangers last hebben van calimero syndroom""? Wat de kaby lake review betreft. Er staat: ""De beloofde prestatieverbereringen van maximaal twaalf procent zien we in de benchmarks echter niet terug, noch op de desktop, noch op laptops."" Hier hebben we het over 7 a 8%. Dat is zeker niet niks (maar ook niet heel bijzonder). En iedereen die zijn RX580 (480) voor een RX590 om gaat ruilen is mesjogge. Maar als upgrade van iets dat daaronder zit zeker niet verkeerd."
RX 590;4;0.30961528420448303;TIP: Upgrade gratis je RX 580 naar een RX 590 door hem simpleweg over te klokken. De enige kanttekening is dat je niet kan garanderen dat je dezelfde kloksnelheden haalt als de RX 590 en dat hij ook meer zal verbruiken. En voor 6 à 7 % betere prestaties mag de winkelprijs ook niet meer dan 6 à 7 % hoger liggen dan de RX 580. Ze hadden deze kaart eerder een RX 585 moeten heten zodat iedereen weet dat het gewoon een efficiëntere versie is van de RX 580, zoals ze ook deden met de HD 285. Consumentenbedrog? Imho toch een beetje...
RX 590;4;0.34971684217453003;"Een RX580 kun je overklokken en hem (hopelijk) naar stock RX590 snelheid krijgen. Een RX590 kun je overklokken en hem sneller dan de stock RX590 snelheid krijgen. Maar bedankt voor de tip ;-)"
RX 590;1;0.4508589804172516;Een hoop RX580's halen de 1500Mhz mark al niet zonder waterkoeling. En dan nog is het de vraag of je zo'n OC wel wilt met een TDP van 220W. Dan kan je net zogoed een Vega 56 halen, undervolten en 40% meer performance eruit halen.
RX 590;2;0.34334561228752136;Alleen heeft deze kaart een hogere MSRP dan de 580 8GB, in plaats van als vervanger voor hetzelfde bedrag.
RX 590;1;0.5330164432525635;Probleem is in dit geval dat het geen vervanger is van de RX580, het is een 50 dollar duurdere uitvoering. Aldus de advies prijs van $279 vs $229. Was dit eens 680 geweest van $229 dan was die beperkte stijging mooi meegenomen, nu betaal je hem uit eigen zak.
RX 590;2;0.48188960552215576;Wat ik niet goed snap is waarom ze niet andere componenten hebben verbeterd buiten de clock snelheid. Denk bij voorbeeld aan een 384 bit geheugen bus of meer geheugen?
RX 590;5;0.3520924746990204;Omdat ze dan een nieuwe chip hadden moeten ontwikkelen. Dit is dezelfde chip (zelfs als de 480 als ik me niet vergis) maar op kleiner procedé gemaakt waardoor de kloksnelheden wat omhoog kunnen. Extra brede geheugenbus maken is vergelijkbaar met van je 4 cilinder auto een 6 cilinder maken, nogal een klus. Deze 590 kan vooral meer toeren maken.
RX 590;3;0.7272317409515381;Oke duidelijk, misschien een leek vraag maar is de bus interface niet gewoon een interface en dit de weg naar de gpu is dus we hebben nu een 4baans weg maar we maken er een 6 baans weg van. Maar de opbouw van de chip blijft het zelfde. bedoel mijn oude r9 280x heeft al een 384bit interface.
RX 590;1;0.4582994282245636;Het is heel simpel. AMD heeft een contract met globalfoundries en die hebben op hun beurt de procede van 14nm verbeterd naar nu 12nm. Die bieden gewoon een gratis upgrade aan naar 12nm voor AMD en AMD neemt dat gewoon op hun beurt weer af.
RX 590;5;0.5837466716766357;Als je met de gtx 1060 of 1070 vergelijkt moet je wel freesync meenemen. Een gsync monitor kost zo 150 a 200 euro meer. Games worden niet alleen vloeiender met meer frames per seconde maar ook met de manier waarop die frames worden weergegeven.
RX 590;1;0.6359030604362488;Waarom moet je dat mee nemen? Wat als je al een G-Sync of FreeSync monitor hebt? Dan is de keuze waarschijnlijk toch al snel gemaakt? En wat als je niet van plan bent een nieuwe monitor te kopen? Of je wilt een 4K monitor kopen? Allemaal irrelevant voor deze review.
RX 590;3;0.420655757188797;Eens hoor maar het is wel interessante informatie voor mensen die gaan kiezen tussen AMD of NVidea die bijvoorbeeld een nieuwe build maken en ook een scherm gaan kopen.. Ik heb mijn broertje AMD geadviseerd omdat gsync monitoren een stuk duurder zijn en een freesync 144hz scherm behoort tot de wenzen :-)
RX 590;1;0.41403597593307495;Je kan de monitoren niet 1 op 1 vergelijken. Want 9 van de 10 keer zijn de Freesync monitoren in het algemeen goedkoper uitgevoerd. Bij Asus bijvoorbeeld bij de Gsync monitoren een deftige metalen stand en bij freesync zijn deze stands plastic etc en zo zijn er meer verschillen.
RX 590;2;0.4826866686344147;Het prijsverschil zit m vooral in het feit dat bij Gsync veel moet worden betaald voor licentie, en bij Freesync niet. Bij budget monitors scheelt dat zomaar de helft vd prijs.
RX 590;3;0.48509639501571655;Dat Gsync meer kost is een feit, mensen vergeten alleen te kijken naar het feit dat goede Freesync monitoren een vrij kleine range hebben waarin Freesync daadwerkelijk zijn werk doet. Bijvoorbeeld 50 tot 75hz bij de aller goedkoopste Freesync monitoren. Waarbij alle Gsync monitoren direct in het hoge segment zitten met een bereik van 45hz tot en met 165hz waarbij syncing wordt toegepast. Mooie is dat Freesync dan wel toegankelijk is op goedkopere monitoren, maar de experience is niet gelijk aan Freesync op dure monitoren en dat ligt bij Gsync een heel stuk dichter bij elkaar. Daarbij betreft het hie reen hardware scaler die een stuk mooiere resultaten geeft wanneer je reolution scaling toepast. De helft boeit het niet maar het zit er wel in. Dat en dan wat ik eerder noemde de vaak iets meer luze uitvoeringen van de behuizing en monitor stand rechtvaardigd toch echt wel het prijsverschil. Het is alleen jammer dat Nvidia kaarten bijvoorbeeld niet beiden ondersteunen of andersom.
RX 590;2;0.33692917227745056;Ik snap je punt maar juist bij Asus is dit niet zo (XG258Q vs PG258Q).
RX 590;3;0.5474334955215454;Ook daarbij zie je kwaliteit verschillen in de afwerking, vooral de achterkant van het scherm (minder belangrijk, maar toch).
RX 590;3;0.4774230718612671;Wat is precies het verschil aan de achterkant? De PG heeft een metalen ring en de XG een plastic ring met daarin RGB verlichting. Het is anders, maar niet per se een mindere afwerking.
RX 590;3;0.3117698132991791;Ik zie dat ze 'm aangepast hebben. De Gsync variant had eerst een zilvere gladde coating op de achterkant en op de metalen voet. Nu niet meer
RX 590;5;0.492531418800354;Daar ben ik het zeker mee eens! Net zoals ze ook naar de prijs van het moederbord/CPU combi moeten kijken náást de individuele onderdelen. Maar goed. Iedereen weet inmiddels dat je de beste prijs/prestatieverhouding bij AMD krijgt en als geld je niet uit maakt, je voor de beste prestaties een Gsync, Intel en nivea kaart moet komen.
RX 590;2;0.3815920054912567;Ik heb geen van beide en ben niet van plan om een nieuwe monitor voorlopig aan te schaffen, dus waarom zou ik dan freesync meenemen?
RX 590;3;0.4303823709487915;Ik ben meer benieuwd of ze ooit nog met een kaart komen die de performance van de RTX2080Ti kan evenaren (minus de hoge prijs)
RX 590;3;0.27042698860168457;Komen ze ook, maar pas in 2019 (Q2 dacht ik), namelijk Navi.
RX 590;1;0.33349236845970154;De eerste Navi-gpu voor 2019 werd meer een mainstream-concurrent dacht ik... Pas in 2020 ofzo een echte high-end Navi, volgens de geruchtenmolen allemaal...
RX 590;3;0.4085279703140259;Maar de 2060/2070's van AMD (AKA Navi) komen dus wel in 2019, toch?
RX 590;1;0.4414868652820587;via AMD niks gehoord, alleen geruchtencircuit, maar ja, 1ste helft zou dat moeten gebeuren...
RX 590;2;0.37709495425224304;Volgens UDF Tech zal de Navi-architectuur tegenvallen voor de enthusiast market, omdat ze denken dat Navi vooral voor de Playstation 5 en Xbox 2(?) wordt ontworpen. Ikzelf verwacht daarom een Navi GPU voor de pc die niet beter presteert dan wat in 2019 zal worden aangenomen als mainstream. Bron:
RX 590;3;0.3692033588886261;high end navi komt pas later. de eerste 7nm navi gaat weer een mid range kaart zijn, maar wel een mid range voor 7nm, dus gtx1080-gtx2080 performance ergens, voor een 200-300 euro prijs.
RX 590;3;0.27499163150787354;Zo, gewaagde aanname. RTX 2080 performance voor €200-300,-?
RX 590;2;0.407196044921875;Als we RTX 2080 performance krijgen voor 200-300 euro dan is de GPU market volledig omgegooid. en dat gaat dus nooit gebeuren. AMD variant zal dan rond 600 euro zitten tegenover de RTX 2080 800-900 variant. dat zou nog wel eens kunnen. En dan zal het wel spannend zijn voor de mid-high end gamers.
RX 590;3;0.2987512946128845;Dat klinkt inderdaad iets geloofwaardiger maar alsnog ga ik daar niet vanuit. Dat is €300,- goedkoper dan de RTX 2080. Als we er voor het gemak vanuit gaan dat de aankomende Navi kaart deze performance daadwerkelijk gaat halen.
RX 590;5;0.37650591135025024;hij gaat zeer zeker geen 600 euro kosten want zeg 6-9 maanden later moeten er nog high end navi's boven geplaatst worden. en dat soort dingen gebeuren elke keer als we een nieuwe node gaan gebruiker. zeker als we (opnieuw) 2 nodes tegelijk krijgen. de rx480 was ook gelijk aan de 390x.
RX 590;3;0.4644307792186737;Naja zo werd dat ook verteld over Vega dat het high end moet zijn. bleek toch wat anders te zijn:p. We zullen zien zou ik zeggen.
RX 590;2;0.47853949666023254;vega is high end, 1080 is ook high end. Maar Vega heeft het probleem dat het zowel een professionele als een gamer GPU moest zijn (door tijd en geld gebrek doordat de prio bij Navi lag). Daarom heeft Vega 1/2 FP64 performance (dus FP64 Tflops is de helft van de FP32 Tflops) wat enorme hoeveelheden transistors en stroom kost, maar voor gaming totaal niet nuttig is. In vergelijking nvidia's verhouding bij de 1000 serie is 1/32. Navi heeft dat probleem niet. dat is een GPU puur voor gaming, en zal dus 1/16 (net als Polaris) of misschien ook wel 1/32 FP64 verhouding hebben.
RX 590;2;0.4241052269935608;HOHO, ik zeg ergens tussen de 1080 en 2080. wel goed lezen, en niet gelijk de helft overboord gooien.
RX 590;3;0.5762165784835815;Nou okey, ergens volgend jaar GTX 1080 performance voor 200-300 euro is geen hele gekke gedachte. RTX 2080 voor 200-300 natuurlijk wel
RX 590;3;0.5820662975311279;Dat is zeer nice (maar die prijs is wel erg laag ). Die moet ik hebben. Maar ja, eerst maar eens zien of die 144hz/fps kan halen..
RX 590;2;0.5245361924171448;Ik had graag een vergelijking gezien met de RX 480. Ik zit na 2,5 jaar nog altijd op het reference model en wil graag wat meer performance en een stillere koeler. Overclocken is haast onmogelijk met het reference model en Vega blijft gewoon te duur. Is deze dan de moeite waard?
RX 590;3;0.44057467579841614;toch genoeg vergelijkingen met de 1060? dus dan kun je daar nu ook je conclusies mee trekken..
RX 590;3;0.5312312245368958;Ik denk dat mijn RX 480 nog een tijdje moet draven, jammer dat die niet in de test is meegenomen. Het verschil zal niet al te groot zijn.
RX 590;3;0.45993921160697937;Ook jammer inderdaad, maar het zal zeker niet groot zijn. Mijn RX 480 doet 1400 gpu - 2200 vram
RX 590;1;0.26031649112701416;M'n vram is maar 1750, ik heb een 4 Gb versie. Nog niet geprobeerd over te klokken.
RX 590;1;0.8278688788414001;Zwaar bezopen 225W TDP kan gewoon niet meer in deze tijd sorry
RX 590;1;0.8895623683929443;"Echt kansloos die marktstrategie van AMD. brengen ze een ""nieuwe"" kaart uit die niet eens kan tippen aan een mid-end kaart van Nvidia uit de vorige generatie. Op deze manier zal er nooit goede concurrentie zijn op de videokaartenmarkt."
RX 590;1;0.5763555765151978;Verdorie. Dan heeft switchen van mijn Sapphire Nitro R9 390 8 GB bijna geen zin. Nog maar ff afwachten dan
RX 590;1;0.6287354230880737;2x zoveel stroom nodig dan een 1060.....
RX 590;1;0.4866622984409332;Ze zitten ruim onder de 300 euro:
RX 590;3;0.6043434143066406;Als je kijkt naar directe performance in games ben ik toch een beetje teleurgesteld, een 1070 voor 300 euro is geen slechte deal.
RX 590;2;0.42118433117866516;Ik ben het niet eens met de conclusie dat het een opgevoerde RX580 is want dat is het niet. De RX580 kan dit soort clocksnelheden met een overclock simpelweg niet halen. Het gaat hier wel om een andere Gpu. Wel dezelfde opzet maar op een nieuw productie proces gebakken. Dus ik snap waar de conclusie vandaan komt maar ik vind het ook een beetje suggereren dat een RX580 een betere keuze is want als je die opvoert zou je dus hetzelfde kunnen krijgen als een 590.maar daar dat haal je dan niet. Polaris 20 gaat geen 1690 MHz op lucht koeling doen.
RX 590;1;0.33776962757110596;Ik denk dat AMD met deze chip heel duidelijk laat zien dat een nieuwere produktie techniek op een kleinere schaal geen enkele zin heeft zonder de daar bij behorende innovaties. Het geeft ook aan dat veel Nm ridders hier die zich verkneukelen op de volgende shrink er volledig naast zitten. AMD heeft hier maar wat zitten bakken en een 4 jaar oude architectuur gerekt tot het uiterste. De 590 is voor mij de koning van de rebrands.
RX 590;3;0.42182761430740356;Is er een prestatie index of iets dergelijks? Ik begrijp dat er wordt vergeleken met directe concurrenten, maar ik mis vaak de vergelijking met oudere kaarten. Natuurlijk behoren 1060, 1070, 580 e.d. al niet tot de nieuwste, maar sommige gebruikers willen mogelijk overstappen naar 590 en komen van een 9x of R2 29x o.i.d. Ik kan natuurlijk vast we een review vinden van een 580 die wordt vergeleken met bv. een 970 en wat fps bij verzinnen. In mijn htpc zit een 970 die wil ik vervangen. Waarschijnlijk geen 590 maar een mini 1070. Dus ik ben mij aan het oriënteren.
RX 590;2;0.28961119055747986;"@ de testers bij hardware info. Nu ben ik een linux gamer, ja ze bestaan echt en zou het leuk vinden om te zien hoe de grafische kaarten daar presteren. snap heus wel dat jullie niet alle gazillion distro's af kunnen lopen. maar een ""dit is wat we ervoeren met deze (insert populair distro here)"" als baseline zou enorm gewaardeerd worden. no offence als dit wat te veel van de testdrukte is *edit* emm, zit net te bedenken dat een door steam als standaard geaccepteerde distro wellicht een handreiking zou kunnen zijn"
RX 590;2;0.4285145103931427;Wel vreemd, op papier krachtiger zijn maar in de praktijk een stuk langzamer.. Zo zie je maar weer dat specs op papier lang niet altijd alles zeggen.. Dan heb ik het over cudacores, compute units en de welbekende teraflops (waar consolebouwers ook altijd graag mee schermen), alles heeft de 590 daarvan meer dan de 1070, maar in de praktijk komt het er dus niet uit.. Waar ligt dat aan?!?
RX 590;1;0.4378531575202942;Het is niet geoptimaliseerd voor games. Die specs zeggen helemaal niets, aangezien de chip nog met software moet worden aangestuurd. Dit gaat in verschillende lagen en de driver zorgt daarvoor. AMD bakt er op dat vlak gewoon niet veel van.
RX 590;1;0.4665775001049042;Eerder onder de 250 euro wordt t pas interessant, de 580 8gb stond laatst nog voor 220,- euro en hij vreet minder energie zelfs?
RX 590;1;0.2480095624923706;Ben benieuwd of veel mensen deze RX590 kaart gaan kopen en tegen welke prijs, in Duitsland heb je op dit moment al een nieuwe RX 570 8GB voor 159 euro. Ik verwacht binnenkort met de black friday en cyber monday een armada van goedkope aanbiedingen met de RX 570's en 580's.
RX 590;5;0.3915901780128479;die van 159 klinkt nu al als een goede aanbieding t.o.v. tweakers pricewatch. Kan je delen waar je die precies gevonden hebt? google.de
RX 590;5;0.5121538043022156;Nice een upgrade voor mijn mining rig.
RX 590;1;0.4092637896537781;Zie die 300 euro investering maar eens terug te verdienen
RX 590;3;0.32423335313796997;Fixed
RX 590;1;0.4345763027667999;c/p error, fixed now, tnx
RX 590;1;0.39385610818862915;Zo te zien is de test samen met HWI uitgevoerd?
RX 590;1;0.6355133056640625;Al tijden gebruiken ze inderdaad dezelfde testgegevens. Beide hebben dan ook dezelfde problemen waardoor ook deze review weer belabberd is. Door de foutieve testmethode, waarmee beide sites maar blijven doormodderen, terwijl ze weten dat de tests niet overeenkomen met de praktijk.
RX 590;3;0.3929341733455658;Kan je aangeven wat er precies mis is met de testmethode? Daar ben ik wel benieuwd naar!
RX 590;3;0.5316084623336792;Dat is een beetje te veel om hier uit te leggen. Maar aangezien het meermaals aan HWI en Tweakers is doorgegeven moeten zij wel een waslijst hebben waar het op staat.
RX 590;5;0.5403780341148376;Als je het toevallig een keer uiteengezet hebt zie ik de waslijst graag, ik ben erg benieuwd!
RX 590;3;0.49438947439193726;The 90's called, they want their GPU designs back (aib) De grote vraag is of de in NL grote namen de moeite gaan nemen om de markt te voorzien van een beetje leuke modellen voor een redelijke prijs. Harstikke interessante chip die RX 580 super-OC, maar zie liever toch een iets luxere uitvoering van die dingen, danwel een lekker scherp geprijsde.
RX 590;2;0.3872125446796417;Je kunt wel nagaan dat er echt niet veel zal gaan komen. Het blijft wat het is ook al hoop je op wat beters. Ik begrijp je niet. Waarom moet het nog weer beter voor nog minder?? Ik denk dat het gewoon niet kan.
RX 590;3;0.3131081759929657;Voor mensen die net zoals ik nog geen X-sync monitor hebben en dat voorlopig ook nog niet aanschaffen is deze kaart natuurlijk wel een optie. Als ie op 300 euro komt dan zit ie qua prestaties precies goed ten opzichte van nvidia. En als ik dan in de toekomst wel een nieuwe monitor koop hoeft het geen dure G-sync variant te zijn.
RX 590;2;0.4888295829296112;Is dit niet gewoon het resultaat van het niet willen uitbrengen van verschillende versies van de 580 en een makkelijk cash grab want 590 klinkt sneller dan 580 + meer nieuws exposure. Hadden ze nvidia gevolgd dan hadden we nu een 14nm en 12nm 580 versie en weet je niet wat je in huis haalt.
RX 590;3;0.43374672532081604;Een mooi mid-range kaart voor Full-HD tot WQHD. Waarom geen geluidsproductie test? 4K in sommige benchmarks goed en andere benchmarks weer niet speelbaar
RX 590;3;0.3983752727508545;Is het dan bijvoorbeeld beter om te kijken naar een 980 of een 980ti?
RX 590;5;0.36867740750312805;Over een paar jaar is hij even snel als de gtx 1070.. fine wine
RX 590;1;0.38513830304145813;Haha testen met 398.11 driver, de standaard windows driver is zo oud. Dit is zo vertekent beeld en je kan dan wel zeggen dat al die Benchmarks weinig voorstellen. Die van Amd zal vast ook oud zijn, maar deze benchmarks zegt letter niks. Ja stront betekent nog meer. Ook Nvidia verlaagt game performance der loop der tijd, met kleine stapjes en bij nieuwe release big driver optimalisaties. Waardoor de Gpu nog meer der uitspringt.
RX 590;3;0.44876304268836975;Even los van dat ik het grootste deel van je punten inhoudelijk niet sterk vind (fine wine, really?). Zie hier voor een recente objectieve vergelijking tussen de RX580 en de GTX 1060: aldus
RX 590;5;0.31941935420036316;Zelfde verhaal toen ik had over de GTX 970 vs RX 480. Ik werd toen voor gek verklaard, want de GTX 970 was onverslaanbaar en FineWine RX 480? Ben je gek? Kijk waar GTX 970 nu staat. Over een half jaar maken we de balans nog een keer op met de games Battlefield 5, Forza Horizon 4, Forza 7, The division 2, Shadow of the Tomb Raider en de aankomende echte DirectX 12 en Vulkan games.
RX 590;2;0.3886193633079529;"Rare vergelijking. De GTX 970 stamt uit 2014 en heeft daarna ongeveer drie jaar lang boven de 300 euro gestaan, tot eind 2016. De eerste custom RX 480 kwam augustus 2016 uit (€300 8 gb). Op dat moment lagen de 1060 (€210 4gb en €300 8 gb) en de 1070 (€465) al in de winkel. De RX 480 is in de markt gezet als GTX 970 'killer'. AMD lijkt, net als jij, vergeten te zijn dat Nvidia min of meer gelijktijdig met een volledig nieuwe line-up kwam. Los daarvan: Met het 'fine wine' argument zijn per definitie twee dingen mis. In de eerste plaats is het helemaal geen compliment voor AMD dat dezelfde kaart een jaar later pas een gat slaat met de concurrentie. Op basis daarvan kun je beter AMD gaan driver shamen in de hoop dat ze er ooit serieus werk van gaan maken (zoals beloofd met Crimson). Tweede probleem is dat aan het eind van de dag 45 of 49 fps met een oudere videokaart er eigenlijk niet meer toe doet. Het is beiden suboptimaal en in beide gevallen is het handig de details te verlagen. Je kunt wat mij betreft dan de eerste 1-2 jaar beter de performance hebben en daarna via V&A je videokaart vervangen voor een nieuwer model. Maar dan het punt waar fine wine over zou moeten gaan; hoe lang je daadwerkelijk op een normale manier games kunt spelen. Pak eens een game als The Division uitgekomen in 2016. Wie hem wilde spelen met een GTX 460 uit 2010 had geen problemen, wie hem wilde spelen met een 5870/6850/6870 kreeg zwarte vlakken. AMD had die kaarten al laten vallen in de driver support en the division kon je dus vergeten. Mijn E-350 APU is helaas ook vrij snel laten vallen, Windows10 heeft geen hardware acceleratie. Niemand kan in de toekomst kijken, maar Radeon heeft momenteel juist iets te bewijzen wat betreft long term support."
RX 590;2;0.45747214555740356;Volgens mij overdrijf je nu een beetje . Volgens mij was de RX 480 gemaakt om op te boksen tegen GTX 1060. En het was niet €300 zoals jij je suggereert maar €270 (8GB) en €220 (4GB) Maar de Nvidia mensen die voor een dikke introductieprijs van de GTX 970 betaalde lachde de RX 480 hard weg, en daarom werd er direct veel met de GTX 970 vergeleken, ook de reviewers. Introductieprijs RX 480 (4GB / 8GB): Introductieprijs GTX 970 (4GB / 3.5GB): Maar hoe jij het bekijkt en opschrijft heb je natuurlijk gelijk. En dit is al tig keer aangehaald. Ik kan ook tegen argumenten geven hoe de markt is gegroeid, en dan komt het voor een groot gedeelte ook hier op neer: AMD moet dus eigenlijk ook net als Nvidia dog/streetfight/achterkamer-anti-consumer-tactics toepassen. Terugslaan met gelijke munt. Dan is de consument helemaal screwed! en BTW, als je toch over oude koeien uit de sloot haalt betreft drivers. Ik kan mij niet herinneren dat Radeon drivers letterlijk je GPU in de vernieling brachten. Volgens mij heeft Nvidia daarin wel een naam in. En de driver launch van de RTX2000 is natuurlijk ook om in te lijsten! Maar nogmaals, hoe jij het verwoord met de belichting van die ene kant van de medaille heb je gewoon gelijk!
RX 590;3;0.4887586236000061;"Best interessant. Zou je mij de link van jouw ""recente objectieve vergelijking"" gehele test kunnen geven van Techpowerup? Ik zie nu alleen een plaatje met selectief aantal games zonder verdere onderbouwingen en inzichten. Je vindt het grootste deel van mijn punten inhoudelijk niet sterk. Kun je aangeven welke en onderbouwen? Want misschien heb je wel gelijk en kan ik er lering uit trekken."
RX 590;1;0.5311274528503418;Ben je nu zo overstuur van mijn reactie dat je twee keer gaat reageren op dezelfde post? Logo van het plaatje is van techspot, niet van techpowerup. Ik heb dus twee bronnen gebruikt in mijn post (waarvan de uitkomst overigens was dat de RX 580 voor de TS een betere keuze was dan de GTX 1060). Hier is de directe vergelijking gemaakt:
RX 590;4;0.3599536120891571;"Haha, nee ik ben niet overstuurt, bedankt voor je bezorgdheid . Ik doe liever zo min mogelijk edits, maar juist liever nieuwe reactie plaatsen. Maakt het voor iedereen overzichtelijker en eerlijker. Ik zie soms mensen wel 3,4,5 keer hun eigen reacties editten met nieuwe onderwerpen. Dan is het bijna onmogelijk om daar nog op juiste manier te reageren. Ontopic: Bedankt voor je nieuwe link! Hier zie ik ook de datum van de test, op May 5, 2017 + selectief aantal games. Om dat te generaliseren als ""recente objectieve vergelijking"" gaat voor mij helaas niet op. Maar interessant is het zeker."
RX 590;1;0.45377224683761597;De games waren toch al bekend? Die stonden al in het overzicht. In de context videokaart review kun je 27 games echt niet selectief gaan noemen... Er is overigens online ook nog een test van eurogamer te vinden van oktober 2018 met dezelfde uitkomst (9 vs 7 voor de RX 580 / GTX 1060).
RX 590;2;0.3718302249908447;"Eurogamer aka Digital Foundry? Dezelfde mensen die heel lang buggy GimpWorkstm AC Untiy game in de benchmark meenamen? Dezelfde mensen die graag iedereen te vriend wilt houden? Dezelfde mensen met hardware/software connecties waar men een mooie deal uit haalt (Zelfde als Tweakers.net, maar Eurogamer is daar wel iets eerlijker over. Tweakers.net heeft het weggemoffeld onder sub-sub-menu zodat vrijwel niemand het weet. Laat mij aub zelf bepalen of 27 games afgelopen 6 jaar genoeg is om als ""recente objectieve vergelijking"" door te laten gaan. Volgens mij zijn er ook testen over ""review-testen"" geweest. Je kunt namelijk 27 andere games pakken waardoor de eindresultaat compleet maar dan ook compleet anders wordt. oa. AdoredTV - Benchmarks - what to trust?: Juist de reviews van Willem de Moor met de daarop volgende reacties van de lezers, gaat het vooral over de ongenoegen van zijn dubieuze testmethodes. Niet zo zeer over het product zelf. Om maar aan te geven hoe belangrijk de manier van testen is en de open inzage ter controle."
RX 590;2;0.41392478346824646;In ieder geval is Eurogamer nogal controversieel, vandaar de naamsverandering naar DigitalFoundry. Voor goede benchmarks raad ik Techspot/Hardware Unboxed aan. Zijn staafdiagrammen zijn verhelderend: RX 580 vs. GTX 1060 en dan per spel een staafje vertikaal gesorteerd van de grootste winst naar het grootste verlies. Je ziet direct hoe verschrikkelijk groot de invloed van selectie is op het bepalen welke kaart beter zou zijn. Voor GTAV, Assassin's Creed (eender welke, de engine is het probleem) en alle spellen die de Unreal4 engine (PUBG bijvoorbeeld) of Source engine gebruiken haalt de GTX 1060 een hogere FPS, voor de meeste recente spellen die niet een van die twee engines gebruiken AMD.
RX 580;3;0.44227179884910583;Had graag enkele oudere GPU's in de benches gezien. Natuurlijk legt bv. een R9 290 het keihard af op energieverbruik, maar heeft het zin om vanaf een GTX970 of andere populaire oude midrangekaart te upgraden? Dat de kaarten van nu alles wel draaien (mede dankzij optimalisaties vanuit devs) is wel duidelijk. Maar hoe groot is het gat met de huidige drivers? Dat is best interessant om te weten, vind ik zelf.
RX 580;3;0.2744161784648895;Tom's Hardware bijv. heeft de 970 meegenomen in de vergelijking. Dat is amper een race, helemaal bij games met een goede dx12/vulkan implementatie.
RX 580;2;0.37070292234420776;"Upgraden vanaf de 970 is niet de moeite, van 48 naar 52 fps in the witcher om er maar één uit te pakken. Persoonlijk zou ik niet upgraden voor minder dan 20%. Beste punt voor uit die review; Dat was voor mij, naast de hoge prijs na de eerste batch, de reden de 480 over te slaan."
RX 580;4;0.6034577488899231;Lekker selectief op dx11, en dan nog een nvidia titel ook. Witcher is al bijna 2 jaar oud. Als je een nieuwe kaart koopt kijk je enigzins wat de toekomst gaat brengen, en dan zijn de dx12 en vulkan benches veel relevanter. Daar kom je wel degelijk aan de 20%
RX 580;1;0.4871930480003357;"TW3 is juist een prima bench omdat het een unieke engine is, omdat het rete stabiel is en omdat er geen updates, API en driver changes meer voor zijn. Verder is TW3 juist ook een bench waar de RX 580 heel goed uit de verf komt, waar die meestal een GTX 1060 voor zich moet dulden staat AMD hier bovenaan. Kortom wat je roept (en waarvoor massaal +3 wordt uitgedeeld...???) is pertinent onjuist. Een Vulkan of DX12 bench kan maar zo over een maand volledig anders uitvallen, en mag echt nog geen maatstaf zijn voor het wel of niet kopen van je volgende GPU. De kans is dan levensgroot dat de werkelijke prestaties per game enorm verschillen. De lijst met DX12 games tot nu toe is ook te droevig voor woorden. Anno 2017 deze nieuwe APIs als uitgangspunt nemen, is niet slim. Ook qua prestaties in-game of qua grafische kwaliteit is een DX12 titel op dit moment nog helemaal niet per definitie beter. Slechts 6 DX12 exclusive games - en allemaal met support van Microsoft zelf. Dat spreekt boekdelen. Tot slot; RX 5xx is precies zoals voorspeld een lichte clockbump. Jammer, want dit is eigenlijk gewoon geen enkele vooruitgang terwijl we wel een jaar verder zijn. Ik snap niet dat AMD dit uitbrengt. Ze kannibaliseren volledig hun oude naamgeving, want een x80 was vroeger geen midrange maar een Nvidia x70 equivalent (poor man's high end zeg maar), en nu is het degradeerd tot een Nvidia x60 equivalent (en eigenlijk zelfs dat nét niet). AMD had de hele stack een cijfertje lager moeten geven om deze indruk niet te wekken. Domme actie weer."
RX 580;2;0.5147611498832703;"Als je de performance summary van Techpowerup erbij pakt dan komt daar , over alle benchmarks bij elkaar, hetzelfde beeld uit; 15% relatief. Ik zou daar persoonlijk geen 260 euro voor neerleggen. Dan kun je beter voor een GTX 1070 gaan met 60% relatieve performance voor 460 euro. Performance per euro ben je dan flink beter af (7,6 vs 17,3) Los daarvan betwijfel ik sterk of Vulkan ooit een succes gaat worden, er zijn tot nu toe erg weinig games die het ondersteunen. DirectX12 is ook nog twijfelachtig. Als we terug kijken naar DX10 dan was dat eigenlijk ook geen succes. Voor DX10 hebben we erg weinig games gezien. Ondanks het potentieel (far cry 2 dx10 draaide veel beter dan dx9). Het was DX11 dat echt doorbrak, en dat was een aantal jaar later. Nu voor de NVidia kaarten DX12 niets toevoegt, en nvidia een flink groter marktaandeel heeft, is de kans groot dat DX12 ook geen succesverhaal wordt. Met andere woorden; ik zou je vooral richten op de benchmarks van de games die er nu zijn en zoals je ze nu speelt en niet gaan gokken op vulkan en dx12. Tegen de tijd dat we daar echt rekening mee moeten gaan houden zitten we alweer met de volgende generatie."
RX 580;2;0.4972940683364868;"Het falen van dx10 had voornamelijk te maken met het falen van Vista. Dx11 brak door omdat 7 door brak. De adoptie van 10 is veel groter en sneller. Ook draait de Xbox een versie van 10 met dx12, waarbij MS steeds meer de lijnen van console/desktop aan het vervagen is. Vulkan is wat afwachten, zolang windows de desktop markt domineert is Vulkan eigenlijk ""dubbel"". En zal het voornamelijk moeten hebben van de Multi-OS titels. Het is overigens afwachten of Sony vast blijft houden aan het eigen API of dat ze op termijn overstappen naar Vulkan. Zal liggen aan de fabrikant die de volgende tender wint, denk ik zo. Dat Nvidia op dit moment een groot marktaandeel heeft lijkt me niet relevant, die zal op een bepaald moment moeten pompen. Kan niet zo zijn dat technische vooruitgang tegengehouden wordt omdat een debiel bedrijf angstvallig blijft vasthouden aan een achterhaalde arch. (al zullen ze ongetwijfeld de nodig geldzakken blijven strooien om dat uit te rekken)"
RX 580;2;0.3505571484565735;ms vervaagt de lijnen tussen desktop en console niet, dat gebeurt nu vanzelf omdat de consoles gewoon pc's zijn
RX 580;2;0.5356655120849609;De techniek van Nvidia is inderdaad beter, maar daar betaal je dan ook een stuk meer voor (1070/1080). Alleen is performance per watt in feite redelijk oninteressant, in de zin dat het vrijwel alleen uitmaakt door de invloed op temperatuur en geluid. Over het energieverbruik zelf hoef je je niet druk te maken tenzij je echt superveel gamet. En afwachten is idd altijd een goed idee. Waarom zou je haast maken? Er zijn een paar aardige verbeteringen doorgevoerd met deze rebrands, maar daar is het ook wel mee gezegd. Geen enkele rede tot kopen, als je al een 'recente' kaart hebt.
RX 580;3;0.4696020185947418;"Niet zozeer beter, maar anders. Wezelijk gezien is de architectuur van AMD beter, echter werd/wordt die niet optimaal geïmplementeerd of ondersteund. Het stroomverbruik ligt inderdaad hoger van de oudere AMD kaarten. Voor mij persoonlijk speelt dat geen rol, maar zoals je aangeeft; voor mensen die echt veel gamen of bij zakelijke toepassingen(servers-workstations) kan ik me voorstellen dat energie verbruik een breekpunt kan zijn. Je betaalt niet meer voor een nvdia kaart omdat ze ""beter"" zijn, maar gewoon omdat ze min of meer kunnen vragen wat ze willen omdat ze marktleider zijn. Marketing speelt ook een belangrijke rol. Hetzelfde zie je bij Intel. Intel is per definitie ook niet beter, maar wel marktleider door betere marketing, implementatie en ondersteuning vanuit devs. AMD heeft gewoon te vroeg ingezet wat betreft vernieuwing in multithread en asynchronische computing(GCN). Intel en Nvidia hebben gewoon beter naar de markt gekeken. In dat opzicht zijn of waren beide beter idd. Toch gaan we nu wel de kant op die AMD jaren geleden al heeft willen inzetten, of ingezet had zonder succes, meer op de hardware programmeren met een efficiënte architectuur. Hoe het ook loopt, het is voor de consument altijd beter om keuze te hebben uit min of meer gelijkwaardige hardware van verschillende fabrikanten :-) Of ze nu rebranden of niet, zolang de hardware er beter op wordt met nagenoeg dezelfde prijs, why not :-) Iedereen zeikt altijd over dat rebranden, maar doe er dan gewoon niets mee als het je niet aanstaat. :-) in het geval van de review: Heb je een RX480, gtx1060, of gelijkwaardig uit de vorige generatie(s), is het geen noemenswaardige upgrade. Voor een ieder die nu wil upgraden, doe er je voordeel mee, wat de keuze ook mag zijn."
RX 580;2;0.6199077367782593;Helaas betreft het lagere verbruik met een multi-monitor setup alleen situaties waarin twee identieke resoluties worden gedraaid. Als je de review van Techpowerup er bij pakt zie je dat wanneer er verschillende resoluties worden gebruikt er nog altijd een significant hoger verbruik is. Sowieso vind ik de kaart qua verbruik niks opzienbarends doen. Netto gezien is hij sneller dan de 480, maar hij verbruikt ook meer. Voor mij zijn de reviews vooral een bevestiging dat ik een jaar geleden de juiste keuze heb gemaakt door voor net een paar euro meer een R9 Fury te kopen in plaats van een 480. Het verbruik met een multi-monitor setup was voor mij een heikel punt, en daar is de Fury de helft zuiniger dan een 480 of 580, en loopt passief gekoeld niet warmer dan 40 graden. En in games komt ook 580 nog altijd niet aan de performance van de Fury.
RX 580;1;0.46853628754615784;Goed dat je het zegt, dat was mij nog niet opgevallen. Als ik me niet vergis zat de Fury ergens tussen de 600 (voor pascal) en 325 (na pascal). Value for money had je dan alsnog beter de GTX 1070 kunnen aanschaffen voor 450. Of heb ik een aanbieding gemist? Overigens blijft het zonde van de potentie die de fury had. Jammer dat AMD destijds niet een hele line-up gebaseerd op dat ontwerp heeft gelanceerd.
RX 580;2;0.5728442668914795;Helaas snijdt het mes aan twee kanten. Kiezen voor een 1070 betekende bovenop de meerprijs voor de GPU tevens een meerprijs te moeten betalen voor de GSync toeslag op m'n nieuwe monitor. Daar was ik niet toe bereid, temeer omdat het prestatieniveau van een 480 / R9 Fury mij hoog genoeg lag. Rationeel gezien was een 4GB 480 of 470 de beste price/performance keuze voor me geweest, maar een R9 Fury is natuurlijk wel wat specialer. Plus het feit dat ik mijn systeem zo stil mogelijk wilde hebben, en stil is dit apparaat zeker.
RX 580;2;0.4698832631111145;nou zover ontlopt het elkaar niet... een aantal benchmarks is er een redelijk verschil maar bij ghost recon is het bijv gewoon nek aan nek. en ik zou zeker niet gaan upgraden voor 10-15% performance... anyways kleine update, zoals verwacht, had gehoopt op iets meer (zeg 15% boost ipv 8-9%) maargoed alles is meegenomen gezien het dezelfde prijs is. nu zullen de 480's mogelijk wat goedkoper worden, en dat kan dan nog wel interessant zijn iig
RX 580;2;0.37942662835121155;Amper een race? Dat is wel een heel ruime interpretatie. Uit de test kan ik opmaken dat het maar 5% scheelt.
RX 580;1;0.3280283510684967;Je vergeet hierbij een 1, niet onbelangrijk punt.. De OC factor van een GTX 970 (easy 15-20% performance erbij). Ter info.. de GTX970 zit ook binnekort aan zijn 3e jaargang..
RX 580;3;0.4484747052192688;Ze hebben leuke kaarten gemaakt voor het midden segment. Die 1070 van nvidia is nog een stuk sneller laat staan als ze een vergelijking gaan maken van een 1070 ti of 1080, maar eigenlijk moet je met die kaarten niet vergeleken worden. Nu maar wachten op die nieuwe chips van amd hopelijk halen deze kaarten wel het topsegment. Het zou leuk zijn als die amd kaarten gewoonweg een 1080 qua specs eruit knikkert zodat amd het voortouw heeft. Maar dat is in het verleden niet gebeurt ik zie het niet snel gebeuren maar zou wel leuk zijn.
RX 580;1;0.585470974445343;1070 ti bestaat niet
RX 580;1;0.5034930109977722;"Er staat een fout in het artikel. In het ""verbruik"" gedeelte staat de volgende zin: Het referentieontwerp van de RX 480 heeft enkel een zespinsaansluiting en kan dus maximaal 150 watt verbruiken. Dat klopt niet. De 6 pins aansluiting is (over)gedimensioneerd op 75W maar dat zegt niets over hoeveel watt het apparaat maximaal kan verbruiken. Het is niet zo dat de aansluiting stroom levert, het apparaat trekt gewoon zoveel stroom door de aansluiting als het nodig heeft. Als de aansluiting te zwak is (te hoge weerstand heeft) wordt het te heet en heb je een probleem. Dat is waarom er meer en/of grotere aansluitingen op meer verbruikende kaarten zitten. Overigens kan de standaard 6 pins aansluiting ruim meer dan 75Watt aan voordat het fout gaat. Herinner je niet de (enigszins overdreven) commotie toen de RX480's net uit waren? Er werd te veel stroom door de moederbord aansluiting getrokken waardoor er evt goedkope moederborden beschadigd konden raken. AMD paste de verhouding tussen de twee aansluitingen zodanig aan dat er meer dan 75Watt over de zespins PSU aansluiting ging, over de standaard dus."
RX 580;4;0.3387502133846283;Aangepast
RX 580;4;0.4201565086841583;Inderdaad een 6p connector op een goede high end voeding kan prima 150 watt aan. Zolang de kabels maar dik genoeg zijn is het geen enkel probleem.
RX 580;3;0.37883123755455017;Ik zie persoonlijk weinig problemen met die statement. Bij benadering kun je rekenen met 75W voor mobo + 75W voor elke 6pins en +150W voor elke 8 pins connecor. In dit geval kom je dan inderdaad op 150W voor de stock RX 480. Uiteindelijk bleek de RX 480 te pieken op 166W, geen man overboord toch? Gevalletje 'close enough'. Wellicht is het woord gedimensioneerd beter, maar ik kan er persoonlijk niet over vallen. In ieder geval piekt de Sapphire RX 580 Nito piekt op 250W. Welke een 8+ 6 pin gebruikt, dus gedimensioneerd op 300W. Daarvoor krijg je dan wel 6% relatieve performance erbij.
RX 580;3;0.7409694790840149;Dan worden de RX480's wel ontzettend interessant om aan te schaffen. De prijzen gaan wel dalen en ook tweedehands zullen er wel wat aanbiedingen langs komen.
RX 580;3;0.5026522278785706;Ik hoop het ook, maar ik ben er bang voor. Deze 'opvolger' komt vrij vlot na de 400-serie, zouden die niet naast elkaar verkocht gaan worden? Verder denk ik dat de 2e-hands-markt ook niet overspoeld zal raken, wie een beetje verstand heeft doet zijn 470/480 niet de deur uit voor een 570/580.
RX 580;3;0.46088218688964844;Maar misschien wel een 470 de deur uit voor een 580. Ik verwacht weinig tot geen 480's, maar 470's of 460's juist wel.
RX 580;2;0.3572242558002472;Moet zeggen dat de tweedehands prijzen voor RX480's sinds launch heel stabiel zijn gebleven, het zijn eerder de RX470's die hier op V&A goede koopjes maken. (€135 voor een MSI TF model, nieuw in seal Ben benieuwd wat verkopers gaan doen, maar gezien de geringe prestatiewinst verwacht ik geen wonderen...
RX 580;1;0.4983878433704376;Ik verwacht eerder dat het gaat gebeuren met Vega. Hoop mensen die een 480 kochten wouden eigenlijk een Vega kaart. Maar die was er nog niet. En willen geen Nvidia kaart.
RX 580;2;0.4589970111846924;Jammer doe het dan niet . Intel 6600K/6700K naar 7600K/7700K was ook zo iets Maar ja, zijn er toch veel die het kopen enkel om het hogere getal.
RX 580;3;0.31063514947891235;Of omdat ze de voorgaande versie nog niet hadden en het nu tijd is voor een upgrade.
RX 580;2;0.36333075165748596;"idd wss zoals mijn situatie toen ik m'n 390x gekocht had, rebrand van de 290x maar ja m'n kaart was toen kapot en had dus een nieuwe nodig de 400 reeks was er nog lang niet. en het prijs verschil was niet zo super. Dus ja waarom geen 390 dan. ""een jaar"" later zit ik nu toch ook ""2 generaties"" achter. had ik toen een 290 gekocht zat ik er wel 3 achter. ach puur psychologisch"
RX 580;3;0.3103565573692322;de 380 is toch van de 290 ?
RX 580;3;0.26413843035697937;nog even gespiekt op reviews: AMD Radeon 300-serie Review - R9 390X, 380 en R7 370 van MSI getest > De topmodellen in de Radeon 300-serie zijn de R9 390 en de R9 390X. Die gpu's krijgen de codenaam Grenada mee, al staan ze beter bekend als Hawaii. ... In de 390 en 390X klokt AMD de gpu's 50MHz hoger dan in de 290 en 290X. Bovendien is er nu standaard 8GB werkgeheugen aanwezig, terwijl dat bij de 290 en 290X nog 4GB was. verder wordt er in het hele artikel ook maar over een kleine upgrade gesproken.
RX 580;5;0.4449448883533478;Weer wat geleerd
RX 580;2;0.38910651206970215;Als ze even duur zijn zou ik inderdaad degene met het hogere getal kopen, waarom niet?
RX 580;3;0.2957737445831299;Levertermijn ?
RX 580;2;0.4172406792640686;De sprong van intel 6xxx naar 7xxx is zelfs nog groter. Zou zeggen eerder vergelijkbaar met van i3-4160 naar i3-4170. Zelfde architectuur & productieproces maar na verloop van tijd wordt het mogelijk de klok een paar procent te verhogen. Wat op zich niet verkeerd is. Wat het stroomverbruik betreft zien we een gemengd beeld: Wat wel helpt is dat drivers ook een stukje volwassener zijn geworden. Maar voor de koopjesjagers: mocht je straks een flink afgeprijsde RX480 zien, profiteer er van.
RX 580;5;0.25341475009918213;Waarom niet? Als consument X met een R9 280 of GTX760 o.i.d. in zijn game PCtje zit dan is deze nieuwe 500 serie een leuke optie voor zo goed als dezelfde prijs als de 400 serie. Zelfde met Kaby Lake processors uit jouw voorbeeld: Als consument X met een Core i5 750 of Core i7 920 o.i.d. in zijn PCtje zit dan is Kaby Lake met dezelfde MSRP een leukere optie.
RX 580;3;0.5022309422492981;AMD biedt met een refresh voor hetzelfde geld kaarten aan met hogere (stock) kloksnelheden, ik zie geen reden tot klagen. Er zijn ook genoeg AIB designs die een OC van 5% krijgen, dat is toch ook reden genoeg om die te pakken in plaats van reference? Wat betreft de naamgeving ben ik het echter wel eens: ze wekken nu de illusie dat dit een volgende generatie is, ze hadden er beter RX485 etc. van kunnen maken, dit maakt het wel verwarrend voor non-Tweakers Zolang het bij deze enkele refresh blijft vind ik het prima, zeker gezien de korte tijd tussen de twee. Bij de volgende serie (6xx) verwacht ik dan wel een sprong voorwaarts.
RX 580;2;0.4648096263408661;Hogere clocks maar de chip gebruikt ook meer dan een 480. Is het dan eigenlijk niets meer dan een out of the box overclock? Ik had gehoopt op een wat verbeterd productieproces waardoor je iets hogere clocks krijgt bij dezelfde hoeveelheid vermogen.
RX 580;3;0.604829728603363;Idle iets zuiniger.. daar houdt het ook wel bij op. Had eigenlijk wel verwacht dat de RX 580 de 1500mhz kon aantikken
RX 580;4;0.6233349442481995;Een goede chip tikt de 1500MHz ook aan, een iets mindere blijft steken op 1475MHz. Sommige reviewers zijn te lui om het voltage te verhogen. Hardware.info tikt 1530MHz aan.
RX 580;4;0.42980754375457764;Een goede 480 tikt ook wel de 1500mhz aan
RX 580;4;0.46411287784576416;Tja, vooral leuk voor wie het afgelopen jaar geen nieuwe kaart heeft gekocht en dit jaar wel aan wat nieuws toe is, maar als upgrade van de vorige serie vrij doelloos.
RX 580;1;0.27332475781440735;En zelfs voor die groep geldt: blijf zuinig, koop alsnog een 4x0 model. Ik denk dat deze serie eigenlijk alleen een rol heeft als de budget range die onder Vega moet komen, omdat het gat tussen 4x0 en Vega anders te groot zou worden.
RX 580;3;0.34082767367362976;GPU prijzen gaan opzich best goed nu, door ryzen is de cpu markt ook weer een stukje beter, nu ram nog haha
RX 580;2;0.5087487101554871;RAM en SDD's zijn helaas alsmaar aan het stijgen i.v.m. de gigantische vraag voor de mobiele industrie op het moment. Helaas.
RX 580;5;0.33153626322746277;Gelukkig heb ik destijds op black friday een mx300 550gb gekocht voor 93 euro en een tridentz 3200 16gb kitje voor 113 op afuture voor het faillisement.. Maar het moet echt ff weer aantrekken, al helemaal voor budgetpc's waarbij ram nu een van de duurste componenten is terwijl het daarvoor juist een van de goedkopere was....
RX 580;1;0.8049780130386353;Klopt, erg spijtig. Ik was ook helemaal in shock van de gigantische prijs verhoging. Ook ik zit hier met een SSDtje van 110 euro en 16GB ram (Kingston hyperX savage nog wel) voor 100 euro. Gek dat het opeens zó duur is nu. Erg spijtig om te horen dat Afuture failliet is. Ik was altijd erg tevreden over deze gasten
RX 580;1;0.6872174739837646;"Ik keek volgens mij 10 keer per dag op de ""afgeprijsd"" tab bij afuture.... en elke keer als ik voor iemand iets moest maken, bij het afgeprijsde product ook nog een paar andere onderdelen kocht... zo jammer... geen enkele andere verkoper heeft zulke mooie deals als ""open verpakking"".. waar gewoon niks mis mee was alleen verpakking was open. Andere verkopers plakken verpakking gewoon weer dicht of verkopen het alleen als open verpakking als er echt schade is."
RX 580;3;0.39881041646003723;Ik zou wel een in-dept strijd zien tussen de RX 570/580 vs GTX 1060, de 570 lijkt erg dicht in de buurt te komen namelijk!
RX 580;2;0.521600604057312;Nou nee. Het is eerder zo dat de 580 nét in de buurt komt van de 1060 6GB. De 570 is de 1060 3GB equivalent. Maar door de bank genomen is het allemaal een beetje lood om oud ijzer. Zeker als je kijkt naar de min. FPS.
RX 580;2;0.45118436217308044;Waarom wordt hier enkel vergeleken met de vorige generatie en de recentste nvidia generatie ? Interessanter zou een vergelijking zijn over meerdere generaties, bijvoorbeeld RX580 - RX480 R9 380 - R9 290 ... (idem voor Nvidia) Dan kunnen mensen die niet elk jaar of elke 2 jaar een nieuwe videokaart kopen tenminste zien welke invloed een nieuwe kaart heeft op hun performantie. Vergelijken met de RX 400 heeft weinig nut, vermits de meeste van die eigenaars toch geen RX 500 zal kopen.
RX 580;3;0.45679739117622375;"goed punt, maar je merkt persoonlijk wel wanneer je kaart niet meer mee komt qua performance. er is altijd wel wat te vinden via zoeken in forums hoe een oudere kaart performd met nieuwere games. google of duckduckgo oid jouw kaart en nieuwe game + performance en je krijgt best wat resultaten. dan heb je ook nog zaken als wat heb je voor "" oude "" cpu en ram. sowieso is dat verschil in performance subjectief, mijn tri - xx 290 draait icm i5 6400 bv. bf1 op 1080p op ultra settings in mp met 64 spelers gemiddeld met 70fps, stil en koel. vind ik super prima. als een 1060 of rx x80 dat doen op ik noem maar wat, 80fps, moet ik dan upgraden? ik vind van niet. ze zijn zuiniger, dat zou een reden kunnen zijn. zelfs niet al halen ze 100fps. zoals ik deze review lees gaat het meer om het verschil tussen de rx 5xx, rx 4xx en gtxen in datzelfde segment. is het wel of niet de moeite waard om te upgraden mocht je een rx 4xx kaart hebben. vind ik prima. voor de ander is die 80 of 100 fps een must have. maar ook dan blijft de vraag, wat heb je voor "" oude "" cpu en mem in je pc. een gtx 970 op een oudere amd processor uit 2012 zorgt bij voorbaat al voor een cpu bottleneck. een 1060 heeft dan totaal geen nut. al schijnen de laatste fx processoren ineens beter te presteren sinds ryzen er is, vanwege optimalisatie in windows en drivers. dat kan misschien net die paar fps zijn die je nodig hebt om voorlopig jouw game beter of voldoende te kunnen spelen op je huidige kaart. er zijn dus meer factoren die meespelen dan alleen het verschil in fps tussen nieuwe en vorige gen gpu ' s met huidige games. benchmark en reviews van techsites is leuk leesvoer, maar ik prefereer altijd de uitgebreide gebruikers reviews, die geven vaak een realistischer en beter beeld van een specifiek product in de werkelijkheid. juist van oude hardware icm nieuwe games / software en vice versa."
RX 580;3;0.42095980048179626;Leuke prestatiewinst voor een refresh voor rx400 prijs. Snap alleen niet waarom tweakers in tegenstelling tot veel andere reviewers zulke slechte overkloks halen op grafische kaarten. Wat is hun methode dan?
RX 580;3;0.42427924275398254;Licht dit nu aan mij, of is er tussen de RX480 en RX580 geen verschil. Ik bedoel alleen de kloksnelheid is wat verhoogt ten opzichte van de RX480. En wat krijg je daar voor terug, een hoger verbruik wanneer i eenmaal vol aan de bak moet. En nu snap ik dat er nauwelijks verschil tussen de oude en de nieuwe kaarten zitten, dat wordt immers keurig in het artikel vermeld, maar voor de 137MHz extra waar de RX580 over beschikt, in vergelijking met de RX480 verbruikt i 35 Watt aan vermogen meer. (standaard clocks, van beide kaarten, dus zonder boost functie) De RX480: 150 Watt op 1120MHz De RX580: 185 Watt op 1257MHz Net even vlug zitten rekenen: Dat is 4,28 deel aan vermogen extra wat de RX580 nodig is, in vergelijking met verbruik van de RX480. En dat voor 137MHz extra. Ik zal dan ook bijna zeggen, ja, zo kan ik het ook wel. Maar goed, alle kleine beetjes helpen natuurlijk
RX 580;3;0.4200984537601471;Hoogstens 10% extra performance voor 20% meer verbruik? Zo kan ik het ook wel. Gewoon de Power Limit van een RX480 wat hoger zetten en je hebt je eigen RX580 gemaakt. Houd het lekker bij de RX480 zolang die goedkoper blijft.
RX 580;3;0.4578792452812195;Ik zou ook graag benchmarks zien op medium / high settings. Dat maakt grafisch vaak amper verschil met ultra maar je hebt veel meer frames. Als ik dus de witcher 3 wil spelen op 4k medium, welke kaart moet ik dan kopen?
RX 580;1;0.5097554922103882;Op 4k? Een 1070 of een 1080. In ieder geval geen AMD kaart.
RX 580;5;0.7481599450111389;Bij de keuze van mijn RX480 8GB was de keuze tussen de verschillende merken duidelijk. MSI. ( Gaming X 8 GB ) Gewoon de stilste en beste koeling door alle reviews heen. Net zoals in enkele reviews heb ik ze direct na het inbouwen op 1350 Core gezet en 2250 Memory ( Core Voltage & Powerlimit met MSI afterburner op de standaard instellingen naar het maximum geschoven ). Werkte direct , heb zelf nog niet geprobeerd hoger te gaan. Dat met 1x 6 pin. Kost nu 270 euro en ze is nog steeds te koop. ( TIP van de week )
RX 580;5;0.5685020089149475;Ik ben erg blij met mijn Asus RX 480 8Gb en die voldoet prima.
RX 580;2;0.4047483205795288;Ik ben er blij met m'n R290x, intussen al 5 jaar oud ofzo maar nog altijd exact de performance van de RX480/580, zal heel misschien een marginaal verschil inzitten maar veel zal het niet zijn. Snap sowieso niet waarom mensen de 480/580 kopen terwijl de R290 tussen de 110-140 euro in de V en A wordt verkocht...
RX 580;3;0.3311208486557007;Je voeding en je kast moeten natuurlijk wel voorzien zijn op een gpu die groter is, meer hitte produceert en meer verbruikt. En dan moet je nog eens met de extra decibels kunnen leven. Argumenten genoeg voor beide keuzes dus. Prijzen vergelijken van tweedehands en nieuw is sowieso appelen en peren.
RX 580;3;0.3787509500980377;De 290 / 290X waren inderdaad fantastische kaarten en nog steeds interessant als 2e hands (mits je voeding het aan kan). Maar bij Mindfactory.de kon je een RX480 4GB in luxe MSI Gaming variant voor 199 euro kopen, nieuwe kaart, 3 jaar garantie, duidelijk lager energieverbruik, ik snap wel waarom deze goed verkoopt.
RX 580;3;0.24561436474323273;Bij computeruniverse was vorige week de ASUS STRIX RX 480 8GB voor €199. Ook een leuke deal (die ik net miste )
RX 580;1;0.28976351022720337;De R290X is van eind 2013. Nog geen 3,5 jaar dus.
RX 580;5;0.5182884335517883;En ik ben erg blij met m'n HD6970, kan er nog vanalles mee spelen op prima kwaliteit.
RX 580;5;0.6862837076187134;Ik ben nog erg blij met mijn VIC-II in mijn Commodore 64. Het speelt al mijn commodore games op uitstekende kwaliteit en hoef nog niet te upgraden .
RX 580;1;0.5673933029174805;Van die afgeragde miner afdankertjes die nog heter worden dan de kachel? Nee dank je.
RX 580;2;0.3821605145931244;Scheelt energie in de winter hoor en dat is niet eens een grapje. Ik kan het weten, vorig jaar had ik een Crossfire setup van twee 290's in m'n case hangen
RX 580;3;0.39631104469299316;Kost dat niet juist energie omdat de airco aan moet?
RX 580;3;0.34055209159851074;In de zomer wel... maar dat is gelijk weer een excuus om meer naar buiten te gaan: dat geneuzel
RX 580;5;0.35504502058029175;"""Scheelt energie in de winter hoor en dat is niet eens een grapje."" Ik ben dit volledig met je eens. Ikzelf zit met twee GTX 760 kaarten tezamen met een AMD FX-8350. En kan je vertellen dat ik de hele winter lang geen verwarming/radiator ben nodig geweest op mijn slaapkamer/werkkamer. Ook wel leuk om te vermelden, aangezien ik wel één radiator aan heb, één die mijn AMD FX koelt namelijk, dus net even een ander soort radiator zeg maar. En aangezien deze eigenlijk wel iets groter mag dan het 140 bij 140mm exemplaar dat ik nu bezit, kan i dus soms wel eens goed warm worden wanneer mijn AMD FX goed aan het werk gezet wordt. Reken daar de twee videokaarten nog eens bij op, en je hebt wel enigszins een idee wat dat met een niet al te grote slaapkamer/werkkamer kan doen. (Ik werk grotendeels vanuit huis, net als mijn baas, vandaar dat mijn slaapkamer ook regelmatig als werkkamer dient)"
RX 580;1;0.4793236255645752;Of je nou je ruimte elektrisch verwarmt of met de kachel... 2e optie is vele malen goeiekoper. Dus denken dat het energie scheelt, nee. Als je een zuinigere kaart kan kopen scheelt dat veel meer dan 4 maanden in het jaar een klein beetje gas kunnen besparen.
RX 580;3;0.4032086730003357;Begrijp me niet verkeerd, maar de rediator die mijn slaapkamer zwinters verwarmd is de radiator die mijn CPU koelt. Deze om precies te zijn En dat op gas stoken goedkoper is snap ik ook wel, maar ben ze niet beiden tegelijkertijd nodig. En aangezien er zwinters ook nog gewerkt wordt, staat i dus toch aan. De videokaarten alleen, wanneer er op de betreffende PC flink gegamed wordt. In idle worden deze kaarten niet heel warm, zo rond de 35 graden ongeveer. En dat laatste (in idle) is tijdens werktijd dan ook het geval
RX 580;4;0.33496516942977905;Jij kan je kamer met 30-100 watt verwarmen? Netjes, goede isolatie heb je dan. Vast ook geen ramen?
RX 580;5;0.33788031339645386;Euh, klik Plus twee GTX 760 videokaarten grotendeels in idle, plus speakers en monitor, en twee actieve speakers. En to top it off, een toetsenbord en muis. Ook leuk, Ik heb achter de stekker die uiteindelijk het stopcontact in gaat, een Hema wattage meter zitten. Deze kwam goed uit de test bij de consumentenbond 15euro. . Gehele opstelling in Watt: Idl: +/- 140 Youtube, internet: +/- 180 Games: +/- 550 Tweakers.net reviews Bijna allemaal benoemen ze de buitensporige warmte productie van deze CPU.
RX 580;2;0.3150561451911926;Dan nog, met 150 watt red je het ook niet. En die 30-100 is gemiddeld, of staat hij de hele dag aan? Jouw radiator heeft een vermogen uitgedrukt in kilowatt, dat is wat anders dan jouw pc. Zet hem anders van de winter uit en zet je pc niet te grazen, ik hoor je bij de eerste dag vorst wel
RX 580;5;0.3433853089809418;Check mijn eerste reactie even. Werk vanuit huis, snap je. Ben ik aan het werk dan staat i altijd aan
RX 580;1;0.34792545437812805;afgeragde miner afdankertjes? Ik ben bij een miner geweest om mijn kaart op te halen. En ik kan je zeggen hij was vere van afgeragd. Super schoon. En was er praktisch kouder als mijn koelkast. 9/10 miners onderhouden juist hun kaarten super goed omdat elke dag dat hij op een RMA afdeling ligt geen geld oplevert.
RX 580;4;0.4102027118206024;R9 290X: schoolvoorbeeld van goede driverondersteuning van AMD. Zag laatst ook een vergelijking GTX770 en 7950 anno 2017, waarbij de GTX770 werd ingemaakt. Bij launch was deze juist vaak de meerdere van de 7970... Laat toch even zien dat AMD wat meer future proof is ten opzichte van Nvidia, of Team Green probeert zijn oudere gens te gimpen Misschien leuk voor de redactie om eens te kijken hoe oude kaarten anno 2017 presteren?
RX 580;3;0.44776591658592224;Grappig, het lijkt mij eigenlijk juist andersom. nVidia heeft vaak gewoon erg goede drivers bij launch en AMD is langzamer en doen er een aantal maanden/jaren over. Kwa hardware specs lijkt het vaak dat AMD wat meer zou moeten hebben dan de vergelijkbare nVidia kaart, (vergelijk de 1060 met deze 480/580). Je ziet ook dat de AMD kaarten het over het algemeen ook beter doen op DX12, daar waar de nVidia kaarten je het best op DX11 kunt draaien. DX11 legt nadruk op optimalisaties van gfx-kaart fabrikant, DX12 leunt op optimalisaties door gameontwikkelaar. Maar goed mijn 7970 draait nog steeds prima hoor
RX 580;3;0.34832632541656494;Want ze willen altijd het beste en nieuwste...R290 werkt nog prima als je niet zijkt over herrie en stroomverbruik. Ik heb ook een R290 werkt prima en voor die 6 tot 8 uurtjes gamen in de week boeit mij het stroomverbruik niet zo veel. Persoonlijk zou ik een adviesprijs van 100€ geven aangezien het bijna zelfde presteerd als een oude videokaart zoals R290...R290 is groter en duurder om te produceren dus zou de 580 voordeliger moeten zijn. En vooral omdat het jaren later op de markt verschijnt.
RX 580;2;0.41454416513442993;"Ja de 290X heeft een boel brute kracht en is daarmee nog steeds relevant en de 2 kaarten liggen qua prestaties dicht bij elkaar, maar de 290X is een kaart die nieuw 600+ euro kostte, waar de 480X nog geen 300 euro kost. Beetje appels met peren vergelijken als je het mij vraagt. Daarnaast kun je onder vrijwel hetzelfde stroomverbruik (opvallend genoeg 290W voor de 290X) 2x480X in Crossfire draaien. Ook kan ik het mij goed voorstellen dat met driverupdates e.d. de ondersteuning voor nieuwe technologiën (nieuwere DirectX versies of Vulcan bijvoorbeeld) breder zal zijn bij een nieuwere GPU, die daarmee wat toekomstbestendiger is dan een verouderde. Kortom; met een 480/580X ben je iets beter gericht op toekomstige ontwikkelingen en kun je wat centen besparen op een voeding die niet noodzakelijk 1000W hoeft te leveren En die ene mafkees met een 8K scherm heeft hoedanook een 480/580X nodig, als we binnen het AMD kamp blijven"
RX 580;1;0.38914644718170166;Ven benieuwd naar de prijzen. Gister wou ik al een RX 480 kopen. Toen hoorde ik dat er vandaag een refresh uitkomt. Al hoewel ik nog niks in de pricewatch zie staan. Edit: er staat er al wel één voor maal liefst €386,-
RX 580;3;0.37358590960502625;Dan mag je nog zeker een maand geduld hebben. De voorraden zal dan eerst op orde moeten zijn voordat de prijs gaat dalen.
RX 580;1;0.3337630331516266;Dat zijn de gebruikelijke prijzen die shops inlezen. Hoef je je niks van aan te trekken Als je kan wachten, geef het dan nog een weekje of twee. De kans is groot dat er dan concurrentie ontstaat waardoor je kaart onder de adviesprijs beland. Of nog beter: een RX480 die voor weinig uit de schappen moet
RX 580;5;0.28436216711997986;Azerty heeft er ook 1 te koop voor 239 dus dat valt reuzen mee.
RX 580;4;0.4598575234413147;Leuke review. Dit zijn gewoon leuke kaartjes voor weinig. Dus gewoon een hoop performance voor weinig. Ik ben benieuwd of de 560 de 1050ti gaat verslaan. Stiekem hoop ik dat omdat ik dat segment veel interessanter vind.
RX 580;4;0.354233980178833;De gewone 1050 is al 32% sneller dan de RX460: De 560 zal nog steeds iets langzamer zijn en op prijs moeten concurreren.
RX 580;2;0.41807812452316284;Ik betwijfel het. De 460 was zwakker dan de 1050. Als die wat beter wordt komt die waarschijnlijk rond 1050 niveau te liggen. De 1050Ti is dan nog altijd beter.
RX 580;3;0.3358110189437866;De 1050 Ti is dan ook weer een stukje duurder. Dan kan je ook een 4GB RX470 RX570 overwegen...
RX 580;3;0.6210551857948303;Prestatiegewijs is dat interessant (+30 à 40%). Qua stroomverbruik is het dat allerminst (en hier spreken we niet over 30 of 40%, maar eerder richting het dubbele) en dat zou voor mij reden genoeg zijn om hem niet te overwegen.
RX 580;1;0.8298805952072144;"Wat natuurlijk onzin is heb je een review die dat stroomverbruik onderschrijft? AMD zit inmiddels ook gewoon op 14nm; we leven niet meer in 2015. Laat staan dat het stroomverbruik, zelfs op 10 uur per dag, 7 dagen per week, ook maar enige impact heeft op je jaarrekening..."
RX 580;5;0.6765862107276917;Ik heb nu een unlocked 460 en dit is echt een geweldige videokaart. Lekker zuinig en speelt de meeste spellen gewoon netjes. Ik lees net dat de 560 gewoon een unlockte 460 is daar zit ik dus al op
RX 580;1;0.5211965441703796;Hoezo onzin? In deze review verbruikt de 1050Ti ongeveer 60W tijdens het gamen. De RX470 160W. Dat is dus zelfs meer dan het dubbele. En dan kijk ik nog niet eens naar de RX570... Daar komt nog bij dat dit kaarten zijn die veel verkocht zullen worden. Dat een grafische kaart van € 1000 iets meer verbruikt, kan ik nog door de vingers zien, maar één van € 200 zou net zo weinig mogelijk moeten verbruiken... Trouwens, de impact op je jaarrekening bedraagt in je eigen voorbeeld minstens € 73 (1 kWh x 365 x € 0,20 per kWh), iets normaler gerekend (2 uur per dag) kom je nog altijd uit op zo'n € 15.
RX 580;3;0.42622631788253784;Wellicht een beetje gekke opmerking, maar ik zou nu wel eens willen weten wat een verschil zo'n videokaart nu maakt ten opzichte van de geintegreerde graphics. Ik speel eigenlijk alleen StarCraft2 en Diablo3 (en wat retro via DosBox ), maar ik lees nooit of deze inmiddels oude spellen veel profijt hebben van zo'n (instap) kaart of dat dit niet uitmaakt. Ongetwijfeld is een aparte videokaart een verbetering, maar de geintegreerde graphics worden ook steeds beter...
RX 580;1;0.36659303307533264;Verbetering? 200 fps of 300fps maakt niet meer uit, dat is geen verbetering te noemen. SC2 draait vloeiend op elke nieuwe AMD of Intel CPU met ingebouwde graphics.
RX 580;1;0.7023618221282959;Mmm, net een MSI RX 480 4GB gekocht voor 230,- euro. Annuleren of toch kopen?
RX 580;1;0.3219541609287262;Je kunt nog 2 weken na aankoop je bedenken, dat is volkomen legitiem. Wordt pas een beetje dubieus als je'm gaat overklokken. Dat is per definitie buiten de specificaties.
RX 580;3;0.489817351102829;Ok, nou ja erg veel verschil zit er ook niet in.
RX 580;1;0.41929200291633606;"""De RX 580 met 8GB geheugen krijgt een adviesprijs van 260 euro"" Dus dat wordt 360 euro hier in Nederland Aangezien de rx480 hier na meer dan een half jaar nog steeds ver boven de AMD prijs zit,zal het met de 580 wel net zo zijn."
RX 580;1;0.6177349090576172;Inderdaad, veel te duur, prijzen liggen zelfs hoger dan de begin prijzen van de R400 serie, terwijl de advies prijzen van de R500 serie lager liggen. Laat ze eerst maar eens flink in prijzen zakken, de prijzen zijn echt asociaal te te noemen. Mijn advies is dan ook de R500 serie eerst niet te kopen. Kijk even goed rond of je mooie prijsjes kan vinden van de R400 serie.
RX 580;1;0.3786759674549103;Ik wil graag een 480 of 580 kopen. Zou ik nu een 480 kopen of even wachten tot wat de prijzen gaan doen?
RX 580;4;0.42424359917640686;Ik heb een leuke leerzame video van een sapphire 580X breakdown. ben zelf nog even aan het wachten op wat de prijzen doen leuke upgrade voor een r9 290x die bij mij kapot gegaan is.
RX 580;1;0.5359671115875244;NOFI : Welke component is wat en draait op hoeveel volt, eerst schrijven , dan direct wissen , veel blabla ... mjah , ik zie er de toegevoegde waarde niet van die video. Zo kan je elk rx480 pcb reviewen zonder er echt wijzer uit te worden welk je moet kopen...
RX 580;3;0.3815014064311981;14538 is niet slecht voor de MSI RX 480. 14857 MSI RX 480 28956 2X RX 480. 4455 1x MSI RX 480 (Timespy)
RX 580;3;0.4255765676498413;Jammer dat het een rebrand is en niet een refresh . Als je de R290 met de R390 vergelijkt had je wel wat verbeteringen dat is wat ik noem een refresh. Had meer gehoopt van de 500 series.
RX 580;1;0.4637952446937561;Deze kaarten zijn niet voor gebruikers bedoeld die al een rx4 serie heeft meer voor de gebruikers die nu wil upgraden samen met de Ryzen cpu
RX 580;1;0.4346732199192047;Kansloze kaarten van AMD. Bijna geen verbetering qua prestaties. Bovendien snijden ze zichzelf in de vingers met de RX4xx kaarten Even kijken wat Vega gaat doen, maar ik vrees het ergste.
RX 580;1;0.5305013656616211;gewoon een overgeclockte 480 dus met een hoger verbruik
RX 580;1;0.29196327924728394;Zou ook graag ryzen benchmarks met deze kaart zien ipv intel only
RX 580;1;0.6646876335144043;Dit is geen CPU review, en de RX 580 is bij lange na niet snel genoeg om een invloed van de CPU uberhaupt te kunnen meten op de geteste resoluties. Kortom, vrij zinloos.
RX 580;2;0.3184277415275574;Het gaat mij niet om complot theorieën hoor, ben gewoon benieuwd naar wat uitgebreidere benchmarks icm Ryzen omdat ik zo veel verschillende resultaten zag langskomen icm verschillende kaarten, Ryzen en diverse games.
RX 580;5;0.43396347761154175;Heb nu al een tijdje (met veel plezier overigens, wat ik niet had gedacht als Nvidia persoon) een RX480 8GB, met reference cooler. Deze kun je sowieso al +2% overclocken zonder problemen (de software van AMD is daar nu echt heeel makkelijk in). Neem aan dat de kaarten met speciale cooling nog hoger kunnen (misschien wel die 5% die de 580 extra geeft?) dit maakt voor veel mensen de rx480 echt een super kaart als deze *nog* goedkoper gaat worden door de introductie van de 580
RX 570;3;0.44227179884910583;Had graag enkele oudere GPU's in de benches gezien. Natuurlijk legt bv. een R9 290 het keihard af op energieverbruik, maar heeft het zin om vanaf een GTX970 of andere populaire oude midrangekaart te upgraden? Dat de kaarten van nu alles wel draaien (mede dankzij optimalisaties vanuit devs) is wel duidelijk. Maar hoe groot is het gat met de huidige drivers? Dat is best interessant om te weten, vind ik zelf.
RX 570;3;0.2744161784648895;Tom's Hardware bijv. heeft de 970 meegenomen in de vergelijking. Dat is amper een race, helemaal bij games met een goede dx12/vulkan implementatie.
RX 570;2;0.37070292234420776;"Upgraden vanaf de 970 is niet de moeite, van 48 naar 52 fps in the witcher om er maar één uit te pakken. Persoonlijk zou ik niet upgraden voor minder dan 20%. Beste punt voor uit die review; Dat was voor mij, naast de hoge prijs na de eerste batch, de reden de 480 over te slaan."
RX 570;4;0.6034577488899231;Lekker selectief op dx11, en dan nog een nvidia titel ook. Witcher is al bijna 2 jaar oud. Als je een nieuwe kaart koopt kijk je enigzins wat de toekomst gaat brengen, en dan zijn de dx12 en vulkan benches veel relevanter. Daar kom je wel degelijk aan de 20%
RX 570;1;0.4871930480003357;"TW3 is juist een prima bench omdat het een unieke engine is, omdat het rete stabiel is en omdat er geen updates, API en driver changes meer voor zijn. Verder is TW3 juist ook een bench waar de RX 580 heel goed uit de verf komt, waar die meestal een GTX 1060 voor zich moet dulden staat AMD hier bovenaan. Kortom wat je roept (en waarvoor massaal +3 wordt uitgedeeld...???) is pertinent onjuist. Een Vulkan of DX12 bench kan maar zo over een maand volledig anders uitvallen, en mag echt nog geen maatstaf zijn voor het wel of niet kopen van je volgende GPU. De kans is dan levensgroot dat de werkelijke prestaties per game enorm verschillen. De lijst met DX12 games tot nu toe is ook te droevig voor woorden. Anno 2017 deze nieuwe APIs als uitgangspunt nemen, is niet slim. Ook qua prestaties in-game of qua grafische kwaliteit is een DX12 titel op dit moment nog helemaal niet per definitie beter. Slechts 6 DX12 exclusive games - en allemaal met support van Microsoft zelf. Dat spreekt boekdelen. Tot slot; RX 5xx is precies zoals voorspeld een lichte clockbump. Jammer, want dit is eigenlijk gewoon geen enkele vooruitgang terwijl we wel een jaar verder zijn. Ik snap niet dat AMD dit uitbrengt. Ze kannibaliseren volledig hun oude naamgeving, want een x80 was vroeger geen midrange maar een Nvidia x70 equivalent (poor man's high end zeg maar), en nu is het degradeerd tot een Nvidia x60 equivalent (en eigenlijk zelfs dat nét niet). AMD had de hele stack een cijfertje lager moeten geven om deze indruk niet te wekken. Domme actie weer."
RX 570;2;0.5147611498832703;"Als je de performance summary van Techpowerup erbij pakt dan komt daar , over alle benchmarks bij elkaar, hetzelfde beeld uit; 15% relatief. Ik zou daar persoonlijk geen 260 euro voor neerleggen. Dan kun je beter voor een GTX 1070 gaan met 60% relatieve performance voor 460 euro. Performance per euro ben je dan flink beter af (7,6 vs 17,3) Los daarvan betwijfel ik sterk of Vulkan ooit een succes gaat worden, er zijn tot nu toe erg weinig games die het ondersteunen. DirectX12 is ook nog twijfelachtig. Als we terug kijken naar DX10 dan was dat eigenlijk ook geen succes. Voor DX10 hebben we erg weinig games gezien. Ondanks het potentieel (far cry 2 dx10 draaide veel beter dan dx9). Het was DX11 dat echt doorbrak, en dat was een aantal jaar later. Nu voor de NVidia kaarten DX12 niets toevoegt, en nvidia een flink groter marktaandeel heeft, is de kans groot dat DX12 ook geen succesverhaal wordt. Met andere woorden; ik zou je vooral richten op de benchmarks van de games die er nu zijn en zoals je ze nu speelt en niet gaan gokken op vulkan en dx12. Tegen de tijd dat we daar echt rekening mee moeten gaan houden zitten we alweer met de volgende generatie."
RX 570;2;0.4972940683364868;"Het falen van dx10 had voornamelijk te maken met het falen van Vista. Dx11 brak door omdat 7 door brak. De adoptie van 10 is veel groter en sneller. Ook draait de Xbox een versie van 10 met dx12, waarbij MS steeds meer de lijnen van console/desktop aan het vervagen is. Vulkan is wat afwachten, zolang windows de desktop markt domineert is Vulkan eigenlijk ""dubbel"". En zal het voornamelijk moeten hebben van de Multi-OS titels. Het is overigens afwachten of Sony vast blijft houden aan het eigen API of dat ze op termijn overstappen naar Vulkan. Zal liggen aan de fabrikant die de volgende tender wint, denk ik zo. Dat Nvidia op dit moment een groot marktaandeel heeft lijkt me niet relevant, die zal op een bepaald moment moeten pompen. Kan niet zo zijn dat technische vooruitgang tegengehouden wordt omdat een debiel bedrijf angstvallig blijft vasthouden aan een achterhaalde arch. (al zullen ze ongetwijfeld de nodig geldzakken blijven strooien om dat uit te rekken)"
RX 570;2;0.3505571484565735;ms vervaagt de lijnen tussen desktop en console niet, dat gebeurt nu vanzelf omdat de consoles gewoon pc's zijn
RX 570;2;0.5356655120849609;De techniek van Nvidia is inderdaad beter, maar daar betaal je dan ook een stuk meer voor (1070/1080). Alleen is performance per watt in feite redelijk oninteressant, in de zin dat het vrijwel alleen uitmaakt door de invloed op temperatuur en geluid. Over het energieverbruik zelf hoef je je niet druk te maken tenzij je echt superveel gamet. En afwachten is idd altijd een goed idee. Waarom zou je haast maken? Er zijn een paar aardige verbeteringen doorgevoerd met deze rebrands, maar daar is het ook wel mee gezegd. Geen enkele rede tot kopen, als je al een 'recente' kaart hebt.
RX 570;3;0.4696020185947418;"Niet zozeer beter, maar anders. Wezelijk gezien is de architectuur van AMD beter, echter werd/wordt die niet optimaal geïmplementeerd of ondersteund. Het stroomverbruik ligt inderdaad hoger van de oudere AMD kaarten. Voor mij persoonlijk speelt dat geen rol, maar zoals je aangeeft; voor mensen die echt veel gamen of bij zakelijke toepassingen(servers-workstations) kan ik me voorstellen dat energie verbruik een breekpunt kan zijn. Je betaalt niet meer voor een nvdia kaart omdat ze ""beter"" zijn, maar gewoon omdat ze min of meer kunnen vragen wat ze willen omdat ze marktleider zijn. Marketing speelt ook een belangrijke rol. Hetzelfde zie je bij Intel. Intel is per definitie ook niet beter, maar wel marktleider door betere marketing, implementatie en ondersteuning vanuit devs. AMD heeft gewoon te vroeg ingezet wat betreft vernieuwing in multithread en asynchronische computing(GCN). Intel en Nvidia hebben gewoon beter naar de markt gekeken. In dat opzicht zijn of waren beide beter idd. Toch gaan we nu wel de kant op die AMD jaren geleden al heeft willen inzetten, of ingezet had zonder succes, meer op de hardware programmeren met een efficiënte architectuur. Hoe het ook loopt, het is voor de consument altijd beter om keuze te hebben uit min of meer gelijkwaardige hardware van verschillende fabrikanten :-) Of ze nu rebranden of niet, zolang de hardware er beter op wordt met nagenoeg dezelfde prijs, why not :-) Iedereen zeikt altijd over dat rebranden, maar doe er dan gewoon niets mee als het je niet aanstaat. :-) in het geval van de review: Heb je een RX480, gtx1060, of gelijkwaardig uit de vorige generatie(s), is het geen noemenswaardige upgrade. Voor een ieder die nu wil upgraden, doe er je voordeel mee, wat de keuze ook mag zijn."
RX 570;2;0.6199077367782593;Helaas betreft het lagere verbruik met een multi-monitor setup alleen situaties waarin twee identieke resoluties worden gedraaid. Als je de review van Techpowerup er bij pakt zie je dat wanneer er verschillende resoluties worden gebruikt er nog altijd een significant hoger verbruik is. Sowieso vind ik de kaart qua verbruik niks opzienbarends doen. Netto gezien is hij sneller dan de 480, maar hij verbruikt ook meer. Voor mij zijn de reviews vooral een bevestiging dat ik een jaar geleden de juiste keuze heb gemaakt door voor net een paar euro meer een R9 Fury te kopen in plaats van een 480. Het verbruik met een multi-monitor setup was voor mij een heikel punt, en daar is de Fury de helft zuiniger dan een 480 of 580, en loopt passief gekoeld niet warmer dan 40 graden. En in games komt ook 580 nog altijd niet aan de performance van de Fury.
RX 570;1;0.46853628754615784;Goed dat je het zegt, dat was mij nog niet opgevallen. Als ik me niet vergis zat de Fury ergens tussen de 600 (voor pascal) en 325 (na pascal). Value for money had je dan alsnog beter de GTX 1070 kunnen aanschaffen voor 450. Of heb ik een aanbieding gemist? Overigens blijft het zonde van de potentie die de fury had. Jammer dat AMD destijds niet een hele line-up gebaseerd op dat ontwerp heeft gelanceerd.
RX 570;2;0.5728442668914795;Helaas snijdt het mes aan twee kanten. Kiezen voor een 1070 betekende bovenop de meerprijs voor de GPU tevens een meerprijs te moeten betalen voor de GSync toeslag op m'n nieuwe monitor. Daar was ik niet toe bereid, temeer omdat het prestatieniveau van een 480 / R9 Fury mij hoog genoeg lag. Rationeel gezien was een 4GB 480 of 470 de beste price/performance keuze voor me geweest, maar een R9 Fury is natuurlijk wel wat specialer. Plus het feit dat ik mijn systeem zo stil mogelijk wilde hebben, en stil is dit apparaat zeker.
RX 570;2;0.4698832631111145;nou zover ontlopt het elkaar niet... een aantal benchmarks is er een redelijk verschil maar bij ghost recon is het bijv gewoon nek aan nek. en ik zou zeker niet gaan upgraden voor 10-15% performance... anyways kleine update, zoals verwacht, had gehoopt op iets meer (zeg 15% boost ipv 8-9%) maargoed alles is meegenomen gezien het dezelfde prijs is. nu zullen de 480's mogelijk wat goedkoper worden, en dat kan dan nog wel interessant zijn iig
RX 570;2;0.37942662835121155;Amper een race? Dat is wel een heel ruime interpretatie. Uit de test kan ik opmaken dat het maar 5% scheelt.
RX 570;1;0.3280283510684967;Je vergeet hierbij een 1, niet onbelangrijk punt.. De OC factor van een GTX 970 (easy 15-20% performance erbij). Ter info.. de GTX970 zit ook binnekort aan zijn 3e jaargang..
RX 570;3;0.4484747052192688;Ze hebben leuke kaarten gemaakt voor het midden segment. Die 1070 van nvidia is nog een stuk sneller laat staan als ze een vergelijking gaan maken van een 1070 ti of 1080, maar eigenlijk moet je met die kaarten niet vergeleken worden. Nu maar wachten op die nieuwe chips van amd hopelijk halen deze kaarten wel het topsegment. Het zou leuk zijn als die amd kaarten gewoonweg een 1080 qua specs eruit knikkert zodat amd het voortouw heeft. Maar dat is in het verleden niet gebeurt ik zie het niet snel gebeuren maar zou wel leuk zijn.
RX 570;1;0.585470974445343;1070 ti bestaat niet
RX 570;1;0.5034930109977722;"Er staat een fout in het artikel. In het ""verbruik"" gedeelte staat de volgende zin: Het referentieontwerp van de RX 480 heeft enkel een zespinsaansluiting en kan dus maximaal 150 watt verbruiken. Dat klopt niet. De 6 pins aansluiting is (over)gedimensioneerd op 75W maar dat zegt niets over hoeveel watt het apparaat maximaal kan verbruiken. Het is niet zo dat de aansluiting stroom levert, het apparaat trekt gewoon zoveel stroom door de aansluiting als het nodig heeft. Als de aansluiting te zwak is (te hoge weerstand heeft) wordt het te heet en heb je een probleem. Dat is waarom er meer en/of grotere aansluitingen op meer verbruikende kaarten zitten. Overigens kan de standaard 6 pins aansluiting ruim meer dan 75Watt aan voordat het fout gaat. Herinner je niet de (enigszins overdreven) commotie toen de RX480's net uit waren? Er werd te veel stroom door de moederbord aansluiting getrokken waardoor er evt goedkope moederborden beschadigd konden raken. AMD paste de verhouding tussen de twee aansluitingen zodanig aan dat er meer dan 75Watt over de zespins PSU aansluiting ging, over de standaard dus."
RX 570;4;0.3387502133846283;Aangepast
RX 570;4;0.4201565086841583;Inderdaad een 6p connector op een goede high end voeding kan prima 150 watt aan. Zolang de kabels maar dik genoeg zijn is het geen enkel probleem.
RX 570;3;0.37883123755455017;Ik zie persoonlijk weinig problemen met die statement. Bij benadering kun je rekenen met 75W voor mobo + 75W voor elke 6pins en +150W voor elke 8 pins connecor. In dit geval kom je dan inderdaad op 150W voor de stock RX 480. Uiteindelijk bleek de RX 480 te pieken op 166W, geen man overboord toch? Gevalletje 'close enough'. Wellicht is het woord gedimensioneerd beter, maar ik kan er persoonlijk niet over vallen. In ieder geval piekt de Sapphire RX 580 Nito piekt op 250W. Welke een 8+ 6 pin gebruikt, dus gedimensioneerd op 300W. Daarvoor krijg je dan wel 6% relatieve performance erbij.
RX 570;3;0.7409694790840149;Dan worden de RX480's wel ontzettend interessant om aan te schaffen. De prijzen gaan wel dalen en ook tweedehands zullen er wel wat aanbiedingen langs komen.
RX 570;3;0.5026522278785706;Ik hoop het ook, maar ik ben er bang voor. Deze 'opvolger' komt vrij vlot na de 400-serie, zouden die niet naast elkaar verkocht gaan worden? Verder denk ik dat de 2e-hands-markt ook niet overspoeld zal raken, wie een beetje verstand heeft doet zijn 470/480 niet de deur uit voor een 570/580.
RX 570;3;0.46088218688964844;Maar misschien wel een 470 de deur uit voor een 580. Ik verwacht weinig tot geen 480's, maar 470's of 460's juist wel.
RX 570;2;0.3572242558002472;Moet zeggen dat de tweedehands prijzen voor RX480's sinds launch heel stabiel zijn gebleven, het zijn eerder de RX470's die hier op V&A goede koopjes maken. (€135 voor een MSI TF model, nieuw in seal Ben benieuwd wat verkopers gaan doen, maar gezien de geringe prestatiewinst verwacht ik geen wonderen...
RX 570;1;0.4983878433704376;Ik verwacht eerder dat het gaat gebeuren met Vega. Hoop mensen die een 480 kochten wouden eigenlijk een Vega kaart. Maar die was er nog niet. En willen geen Nvidia kaart.
RX 570;2;0.4589970111846924;Jammer doe het dan niet . Intel 6600K/6700K naar 7600K/7700K was ook zo iets Maar ja, zijn er toch veel die het kopen enkel om het hogere getal.
RX 570;3;0.31063514947891235;Of omdat ze de voorgaande versie nog niet hadden en het nu tijd is voor een upgrade.
RX 570;2;0.36333075165748596;"idd wss zoals mijn situatie toen ik m'n 390x gekocht had, rebrand van de 290x maar ja m'n kaart was toen kapot en had dus een nieuwe nodig de 400 reeks was er nog lang niet. en het prijs verschil was niet zo super. Dus ja waarom geen 390 dan. ""een jaar"" later zit ik nu toch ook ""2 generaties"" achter. had ik toen een 290 gekocht zat ik er wel 3 achter. ach puur psychologisch"
RX 570;3;0.3103565573692322;de 380 is toch van de 290 ?
RX 570;3;0.26413843035697937;nog even gespiekt op reviews: AMD Radeon 300-serie Review - R9 390X, 380 en R7 370 van MSI getest > De topmodellen in de Radeon 300-serie zijn de R9 390 en de R9 390X. Die gpu's krijgen de codenaam Grenada mee, al staan ze beter bekend als Hawaii. ... In de 390 en 390X klokt AMD de gpu's 50MHz hoger dan in de 290 en 290X. Bovendien is er nu standaard 8GB werkgeheugen aanwezig, terwijl dat bij de 290 en 290X nog 4GB was. verder wordt er in het hele artikel ook maar over een kleine upgrade gesproken.
RX 570;5;0.4449448883533478;Weer wat geleerd
RX 570;2;0.38910651206970215;Als ze even duur zijn zou ik inderdaad degene met het hogere getal kopen, waarom niet?
RX 570;3;0.2957737445831299;Levertermijn ?
RX 570;2;0.4172406792640686;De sprong van intel 6xxx naar 7xxx is zelfs nog groter. Zou zeggen eerder vergelijkbaar met van i3-4160 naar i3-4170. Zelfde architectuur & productieproces maar na verloop van tijd wordt het mogelijk de klok een paar procent te verhogen. Wat op zich niet verkeerd is. Wat het stroomverbruik betreft zien we een gemengd beeld: Wat wel helpt is dat drivers ook een stukje volwassener zijn geworden. Maar voor de koopjesjagers: mocht je straks een flink afgeprijsde RX480 zien, profiteer er van.
RX 570;5;0.25341475009918213;Waarom niet? Als consument X met een R9 280 of GTX760 o.i.d. in zijn game PCtje zit dan is deze nieuwe 500 serie een leuke optie voor zo goed als dezelfde prijs als de 400 serie. Zelfde met Kaby Lake processors uit jouw voorbeeld: Als consument X met een Core i5 750 of Core i7 920 o.i.d. in zijn PCtje zit dan is Kaby Lake met dezelfde MSRP een leukere optie.
RX 570;3;0.5022309422492981;AMD biedt met een refresh voor hetzelfde geld kaarten aan met hogere (stock) kloksnelheden, ik zie geen reden tot klagen. Er zijn ook genoeg AIB designs die een OC van 5% krijgen, dat is toch ook reden genoeg om die te pakken in plaats van reference? Wat betreft de naamgeving ben ik het echter wel eens: ze wekken nu de illusie dat dit een volgende generatie is, ze hadden er beter RX485 etc. van kunnen maken, dit maakt het wel verwarrend voor non-Tweakers Zolang het bij deze enkele refresh blijft vind ik het prima, zeker gezien de korte tijd tussen de twee. Bij de volgende serie (6xx) verwacht ik dan wel een sprong voorwaarts.
RX 570;2;0.4648096263408661;Hogere clocks maar de chip gebruikt ook meer dan een 480. Is het dan eigenlijk niets meer dan een out of the box overclock? Ik had gehoopt op een wat verbeterd productieproces waardoor je iets hogere clocks krijgt bij dezelfde hoeveelheid vermogen.
RX 570;3;0.604829728603363;Idle iets zuiniger.. daar houdt het ook wel bij op. Had eigenlijk wel verwacht dat de RX 580 de 1500mhz kon aantikken
RX 570;4;0.6233349442481995;Een goede chip tikt de 1500MHz ook aan, een iets mindere blijft steken op 1475MHz. Sommige reviewers zijn te lui om het voltage te verhogen. Hardware.info tikt 1530MHz aan.
RX 570;4;0.42980754375457764;Een goede 480 tikt ook wel de 1500mhz aan
RX 570;4;0.46411287784576416;Tja, vooral leuk voor wie het afgelopen jaar geen nieuwe kaart heeft gekocht en dit jaar wel aan wat nieuws toe is, maar als upgrade van de vorige serie vrij doelloos.
RX 570;1;0.27332475781440735;En zelfs voor die groep geldt: blijf zuinig, koop alsnog een 4x0 model. Ik denk dat deze serie eigenlijk alleen een rol heeft als de budget range die onder Vega moet komen, omdat het gat tussen 4x0 en Vega anders te groot zou worden.
RX 570;3;0.34082767367362976;GPU prijzen gaan opzich best goed nu, door ryzen is de cpu markt ook weer een stukje beter, nu ram nog haha
RX 570;2;0.5087487101554871;RAM en SDD's zijn helaas alsmaar aan het stijgen i.v.m. de gigantische vraag voor de mobiele industrie op het moment. Helaas.
RX 570;5;0.33153626322746277;Gelukkig heb ik destijds op black friday een mx300 550gb gekocht voor 93 euro en een tridentz 3200 16gb kitje voor 113 op afuture voor het faillisement.. Maar het moet echt ff weer aantrekken, al helemaal voor budgetpc's waarbij ram nu een van de duurste componenten is terwijl het daarvoor juist een van de goedkopere was....
RX 570;1;0.8049780130386353;Klopt, erg spijtig. Ik was ook helemaal in shock van de gigantische prijs verhoging. Ook ik zit hier met een SSDtje van 110 euro en 16GB ram (Kingston hyperX savage nog wel) voor 100 euro. Gek dat het opeens zó duur is nu. Erg spijtig om te horen dat Afuture failliet is. Ik was altijd erg tevreden over deze gasten
RX 570;1;0.6872174739837646;"Ik keek volgens mij 10 keer per dag op de ""afgeprijsd"" tab bij afuture.... en elke keer als ik voor iemand iets moest maken, bij het afgeprijsde product ook nog een paar andere onderdelen kocht... zo jammer... geen enkele andere verkoper heeft zulke mooie deals als ""open verpakking"".. waar gewoon niks mis mee was alleen verpakking was open. Andere verkopers plakken verpakking gewoon weer dicht of verkopen het alleen als open verpakking als er echt schade is."
RX 570;3;0.39881041646003723;Ik zou wel een in-dept strijd zien tussen de RX 570/580 vs GTX 1060, de 570 lijkt erg dicht in de buurt te komen namelijk!
RX 570;2;0.521600604057312;Nou nee. Het is eerder zo dat de 580 nét in de buurt komt van de 1060 6GB. De 570 is de 1060 3GB equivalent. Maar door de bank genomen is het allemaal een beetje lood om oud ijzer. Zeker als je kijkt naar de min. FPS.
RX 570;2;0.45118436217308044;Waarom wordt hier enkel vergeleken met de vorige generatie en de recentste nvidia generatie ? Interessanter zou een vergelijking zijn over meerdere generaties, bijvoorbeeld RX580 - RX480 R9 380 - R9 290 ... (idem voor Nvidia) Dan kunnen mensen die niet elk jaar of elke 2 jaar een nieuwe videokaart kopen tenminste zien welke invloed een nieuwe kaart heeft op hun performantie. Vergelijken met de RX 400 heeft weinig nut, vermits de meeste van die eigenaars toch geen RX 500 zal kopen.
RX 570;3;0.45679739117622375;"goed punt, maar je merkt persoonlijk wel wanneer je kaart niet meer mee komt qua performance. er is altijd wel wat te vinden via zoeken in forums hoe een oudere kaart performd met nieuwere games. google of duckduckgo oid jouw kaart en nieuwe game + performance en je krijgt best wat resultaten. dan heb je ook nog zaken als wat heb je voor "" oude "" cpu en ram. sowieso is dat verschil in performance subjectief, mijn tri - xx 290 draait icm i5 6400 bv. bf1 op 1080p op ultra settings in mp met 64 spelers gemiddeld met 70fps, stil en koel. vind ik super prima. als een 1060 of rx x80 dat doen op ik noem maar wat, 80fps, moet ik dan upgraden? ik vind van niet. ze zijn zuiniger, dat zou een reden kunnen zijn. zelfs niet al halen ze 100fps. zoals ik deze review lees gaat het meer om het verschil tussen de rx 5xx, rx 4xx en gtxen in datzelfde segment. is het wel of niet de moeite waard om te upgraden mocht je een rx 4xx kaart hebben. vind ik prima. voor de ander is die 80 of 100 fps een must have. maar ook dan blijft de vraag, wat heb je voor "" oude "" cpu en mem in je pc. een gtx 970 op een oudere amd processor uit 2012 zorgt bij voorbaat al voor een cpu bottleneck. een 1060 heeft dan totaal geen nut. al schijnen de laatste fx processoren ineens beter te presteren sinds ryzen er is, vanwege optimalisatie in windows en drivers. dat kan misschien net die paar fps zijn die je nodig hebt om voorlopig jouw game beter of voldoende te kunnen spelen op je huidige kaart. er zijn dus meer factoren die meespelen dan alleen het verschil in fps tussen nieuwe en vorige gen gpu ' s met huidige games. benchmark en reviews van techsites is leuk leesvoer, maar ik prefereer altijd de uitgebreide gebruikers reviews, die geven vaak een realistischer en beter beeld van een specifiek product in de werkelijkheid. juist van oude hardware icm nieuwe games / software en vice versa."
RX 570;3;0.42095980048179626;Leuke prestatiewinst voor een refresh voor rx400 prijs. Snap alleen niet waarom tweakers in tegenstelling tot veel andere reviewers zulke slechte overkloks halen op grafische kaarten. Wat is hun methode dan?
RX 570;3;0.42427924275398254;Licht dit nu aan mij, of is er tussen de RX480 en RX580 geen verschil. Ik bedoel alleen de kloksnelheid is wat verhoogt ten opzichte van de RX480. En wat krijg je daar voor terug, een hoger verbruik wanneer i eenmaal vol aan de bak moet. En nu snap ik dat er nauwelijks verschil tussen de oude en de nieuwe kaarten zitten, dat wordt immers keurig in het artikel vermeld, maar voor de 137MHz extra waar de RX580 over beschikt, in vergelijking met de RX480 verbruikt i 35 Watt aan vermogen meer. (standaard clocks, van beide kaarten, dus zonder boost functie) De RX480: 150 Watt op 1120MHz De RX580: 185 Watt op 1257MHz Net even vlug zitten rekenen: Dat is 4,28 deel aan vermogen extra wat de RX580 nodig is, in vergelijking met verbruik van de RX480. En dat voor 137MHz extra. Ik zal dan ook bijna zeggen, ja, zo kan ik het ook wel. Maar goed, alle kleine beetjes helpen natuurlijk
RX 570;3;0.4200984537601471;Hoogstens 10% extra performance voor 20% meer verbruik? Zo kan ik het ook wel. Gewoon de Power Limit van een RX480 wat hoger zetten en je hebt je eigen RX580 gemaakt. Houd het lekker bij de RX480 zolang die goedkoper blijft.
RX 570;3;0.4578792452812195;Ik zou ook graag benchmarks zien op medium / high settings. Dat maakt grafisch vaak amper verschil met ultra maar je hebt veel meer frames. Als ik dus de witcher 3 wil spelen op 4k medium, welke kaart moet ik dan kopen?
RX 570;1;0.5097554922103882;Op 4k? Een 1070 of een 1080. In ieder geval geen AMD kaart.
RX 570;5;0.7481599450111389;Bij de keuze van mijn RX480 8GB was de keuze tussen de verschillende merken duidelijk. MSI. ( Gaming X 8 GB ) Gewoon de stilste en beste koeling door alle reviews heen. Net zoals in enkele reviews heb ik ze direct na het inbouwen op 1350 Core gezet en 2250 Memory ( Core Voltage & Powerlimit met MSI afterburner op de standaard instellingen naar het maximum geschoven ). Werkte direct , heb zelf nog niet geprobeerd hoger te gaan. Dat met 1x 6 pin. Kost nu 270 euro en ze is nog steeds te koop. ( TIP van de week )
RX 570;5;0.5685020089149475;Ik ben erg blij met mijn Asus RX 480 8Gb en die voldoet prima.
RX 570;2;0.4047483205795288;Ik ben er blij met m'n R290x, intussen al 5 jaar oud ofzo maar nog altijd exact de performance van de RX480/580, zal heel misschien een marginaal verschil inzitten maar veel zal het niet zijn. Snap sowieso niet waarom mensen de 480/580 kopen terwijl de R290 tussen de 110-140 euro in de V en A wordt verkocht...
RX 570;3;0.3311208486557007;Je voeding en je kast moeten natuurlijk wel voorzien zijn op een gpu die groter is, meer hitte produceert en meer verbruikt. En dan moet je nog eens met de extra decibels kunnen leven. Argumenten genoeg voor beide keuzes dus. Prijzen vergelijken van tweedehands en nieuw is sowieso appelen en peren.
RX 570;3;0.3787509500980377;De 290 / 290X waren inderdaad fantastische kaarten en nog steeds interessant als 2e hands (mits je voeding het aan kan). Maar bij Mindfactory.de kon je een RX480 4GB in luxe MSI Gaming variant voor 199 euro kopen, nieuwe kaart, 3 jaar garantie, duidelijk lager energieverbruik, ik snap wel waarom deze goed verkoopt.
RX 570;3;0.24561436474323273;Bij computeruniverse was vorige week de ASUS STRIX RX 480 8GB voor €199. Ook een leuke deal (die ik net miste )
RX 570;1;0.28976351022720337;De R290X is van eind 2013. Nog geen 3,5 jaar dus.
RX 570;5;0.5182884335517883;En ik ben erg blij met m'n HD6970, kan er nog vanalles mee spelen op prima kwaliteit.
RX 570;5;0.6862837076187134;Ik ben nog erg blij met mijn VIC-II in mijn Commodore 64. Het speelt al mijn commodore games op uitstekende kwaliteit en hoef nog niet te upgraden .
RX 570;1;0.5673933029174805;Van die afgeragde miner afdankertjes die nog heter worden dan de kachel? Nee dank je.
RX 570;2;0.3821605145931244;Scheelt energie in de winter hoor en dat is niet eens een grapje. Ik kan het weten, vorig jaar had ik een Crossfire setup van twee 290's in m'n case hangen
RX 570;3;0.39631104469299316;Kost dat niet juist energie omdat de airco aan moet?
RX 570;3;0.34055209159851074;In de zomer wel... maar dat is gelijk weer een excuus om meer naar buiten te gaan: dat geneuzel
RX 570;5;0.35504502058029175;"""Scheelt energie in de winter hoor en dat is niet eens een grapje."" Ik ben dit volledig met je eens. Ikzelf zit met twee GTX 760 kaarten tezamen met een AMD FX-8350. En kan je vertellen dat ik de hele winter lang geen verwarming/radiator ben nodig geweest op mijn slaapkamer/werkkamer. Ook wel leuk om te vermelden, aangezien ik wel één radiator aan heb, één die mijn AMD FX koelt namelijk, dus net even een ander soort radiator zeg maar. En aangezien deze eigenlijk wel iets groter mag dan het 140 bij 140mm exemplaar dat ik nu bezit, kan i dus soms wel eens goed warm worden wanneer mijn AMD FX goed aan het werk gezet wordt. Reken daar de twee videokaarten nog eens bij op, en je hebt wel enigszins een idee wat dat met een niet al te grote slaapkamer/werkkamer kan doen. (Ik werk grotendeels vanuit huis, net als mijn baas, vandaar dat mijn slaapkamer ook regelmatig als werkkamer dient)"
RX 570;1;0.4793236255645752;Of je nou je ruimte elektrisch verwarmt of met de kachel... 2e optie is vele malen goeiekoper. Dus denken dat het energie scheelt, nee. Als je een zuinigere kaart kan kopen scheelt dat veel meer dan 4 maanden in het jaar een klein beetje gas kunnen besparen.
RX 570;3;0.4032086730003357;Begrijp me niet verkeerd, maar de rediator die mijn slaapkamer zwinters verwarmd is de radiator die mijn CPU koelt. Deze om precies te zijn En dat op gas stoken goedkoper is snap ik ook wel, maar ben ze niet beiden tegelijkertijd nodig. En aangezien er zwinters ook nog gewerkt wordt, staat i dus toch aan. De videokaarten alleen, wanneer er op de betreffende PC flink gegamed wordt. In idle worden deze kaarten niet heel warm, zo rond de 35 graden ongeveer. En dat laatste (in idle) is tijdens werktijd dan ook het geval
RX 570;4;0.33496516942977905;Jij kan je kamer met 30-100 watt verwarmen? Netjes, goede isolatie heb je dan. Vast ook geen ramen?
RX 570;5;0.33788031339645386;Euh, klik Plus twee GTX 760 videokaarten grotendeels in idle, plus speakers en monitor, en twee actieve speakers. En to top it off, een toetsenbord en muis. Ook leuk, Ik heb achter de stekker die uiteindelijk het stopcontact in gaat, een Hema wattage meter zitten. Deze kwam goed uit de test bij de consumentenbond 15euro. . Gehele opstelling in Watt: Idl: +/- 140 Youtube, internet: +/- 180 Games: +/- 550 Tweakers.net reviews Bijna allemaal benoemen ze de buitensporige warmte productie van deze CPU.
RX 570;2;0.3150561451911926;Dan nog, met 150 watt red je het ook niet. En die 30-100 is gemiddeld, of staat hij de hele dag aan? Jouw radiator heeft een vermogen uitgedrukt in kilowatt, dat is wat anders dan jouw pc. Zet hem anders van de winter uit en zet je pc niet te grazen, ik hoor je bij de eerste dag vorst wel
RX 570;5;0.3433853089809418;Check mijn eerste reactie even. Werk vanuit huis, snap je. Ben ik aan het werk dan staat i altijd aan
RX 570;1;0.34792545437812805;afgeragde miner afdankertjes? Ik ben bij een miner geweest om mijn kaart op te halen. En ik kan je zeggen hij was vere van afgeragd. Super schoon. En was er praktisch kouder als mijn koelkast. 9/10 miners onderhouden juist hun kaarten super goed omdat elke dag dat hij op een RMA afdeling ligt geen geld oplevert.
RX 570;4;0.4102027118206024;R9 290X: schoolvoorbeeld van goede driverondersteuning van AMD. Zag laatst ook een vergelijking GTX770 en 7950 anno 2017, waarbij de GTX770 werd ingemaakt. Bij launch was deze juist vaak de meerdere van de 7970... Laat toch even zien dat AMD wat meer future proof is ten opzichte van Nvidia, of Team Green probeert zijn oudere gens te gimpen Misschien leuk voor de redactie om eens te kijken hoe oude kaarten anno 2017 presteren?
RX 570;3;0.44776591658592224;Grappig, het lijkt mij eigenlijk juist andersom. nVidia heeft vaak gewoon erg goede drivers bij launch en AMD is langzamer en doen er een aantal maanden/jaren over. Kwa hardware specs lijkt het vaak dat AMD wat meer zou moeten hebben dan de vergelijkbare nVidia kaart, (vergelijk de 1060 met deze 480/580). Je ziet ook dat de AMD kaarten het over het algemeen ook beter doen op DX12, daar waar de nVidia kaarten je het best op DX11 kunt draaien. DX11 legt nadruk op optimalisaties van gfx-kaart fabrikant, DX12 leunt op optimalisaties door gameontwikkelaar. Maar goed mijn 7970 draait nog steeds prima hoor
RX 570;3;0.34832632541656494;Want ze willen altijd het beste en nieuwste...R290 werkt nog prima als je niet zijkt over herrie en stroomverbruik. Ik heb ook een R290 werkt prima en voor die 6 tot 8 uurtjes gamen in de week boeit mij het stroomverbruik niet zo veel. Persoonlijk zou ik een adviesprijs van 100€ geven aangezien het bijna zelfde presteerd als een oude videokaart zoals R290...R290 is groter en duurder om te produceren dus zou de 580 voordeliger moeten zijn. En vooral omdat het jaren later op de markt verschijnt.
RX 570;2;0.41454416513442993;"Ja de 290X heeft een boel brute kracht en is daarmee nog steeds relevant en de 2 kaarten liggen qua prestaties dicht bij elkaar, maar de 290X is een kaart die nieuw 600+ euro kostte, waar de 480X nog geen 300 euro kost. Beetje appels met peren vergelijken als je het mij vraagt. Daarnaast kun je onder vrijwel hetzelfde stroomverbruik (opvallend genoeg 290W voor de 290X) 2x480X in Crossfire draaien. Ook kan ik het mij goed voorstellen dat met driverupdates e.d. de ondersteuning voor nieuwe technologiën (nieuwere DirectX versies of Vulcan bijvoorbeeld) breder zal zijn bij een nieuwere GPU, die daarmee wat toekomstbestendiger is dan een verouderde. Kortom; met een 480/580X ben je iets beter gericht op toekomstige ontwikkelingen en kun je wat centen besparen op een voeding die niet noodzakelijk 1000W hoeft te leveren En die ene mafkees met een 8K scherm heeft hoedanook een 480/580X nodig, als we binnen het AMD kamp blijven"
RX 570;1;0.38914644718170166;Ven benieuwd naar de prijzen. Gister wou ik al een RX 480 kopen. Toen hoorde ik dat er vandaag een refresh uitkomt. Al hoewel ik nog niks in de pricewatch zie staan. Edit: er staat er al wel één voor maal liefst €386,-
RX 570;3;0.37358590960502625;Dan mag je nog zeker een maand geduld hebben. De voorraden zal dan eerst op orde moeten zijn voordat de prijs gaat dalen.
RX 570;1;0.3337630331516266;Dat zijn de gebruikelijke prijzen die shops inlezen. Hoef je je niks van aan te trekken Als je kan wachten, geef het dan nog een weekje of twee. De kans is groot dat er dan concurrentie ontstaat waardoor je kaart onder de adviesprijs beland. Of nog beter: een RX480 die voor weinig uit de schappen moet
RX 570;5;0.28436216711997986;Azerty heeft er ook 1 te koop voor 239 dus dat valt reuzen mee.
RX 570;4;0.4598575234413147;Leuke review. Dit zijn gewoon leuke kaartjes voor weinig. Dus gewoon een hoop performance voor weinig. Ik ben benieuwd of de 560 de 1050ti gaat verslaan. Stiekem hoop ik dat omdat ik dat segment veel interessanter vind.
RX 570;4;0.354233980178833;De gewone 1050 is al 32% sneller dan de RX460: De 560 zal nog steeds iets langzamer zijn en op prijs moeten concurreren.
RX 570;2;0.41807812452316284;Ik betwijfel het. De 460 was zwakker dan de 1050. Als die wat beter wordt komt die waarschijnlijk rond 1050 niveau te liggen. De 1050Ti is dan nog altijd beter.
RX 570;3;0.3358110189437866;De 1050 Ti is dan ook weer een stukje duurder. Dan kan je ook een 4GB RX470 RX570 overwegen...
RX 570;3;0.6210551857948303;Prestatiegewijs is dat interessant (+30 à 40%). Qua stroomverbruik is het dat allerminst (en hier spreken we niet over 30 of 40%, maar eerder richting het dubbele) en dat zou voor mij reden genoeg zijn om hem niet te overwegen.
RX 570;1;0.8298805952072144;"Wat natuurlijk onzin is heb je een review die dat stroomverbruik onderschrijft? AMD zit inmiddels ook gewoon op 14nm; we leven niet meer in 2015. Laat staan dat het stroomverbruik, zelfs op 10 uur per dag, 7 dagen per week, ook maar enige impact heeft op je jaarrekening..."
RX 570;5;0.6765862107276917;Ik heb nu een unlocked 460 en dit is echt een geweldige videokaart. Lekker zuinig en speelt de meeste spellen gewoon netjes. Ik lees net dat de 560 gewoon een unlockte 460 is daar zit ik dus al op
RX 570;1;0.5211965441703796;Hoezo onzin? In deze review verbruikt de 1050Ti ongeveer 60W tijdens het gamen. De RX470 160W. Dat is dus zelfs meer dan het dubbele. En dan kijk ik nog niet eens naar de RX570... Daar komt nog bij dat dit kaarten zijn die veel verkocht zullen worden. Dat een grafische kaart van € 1000 iets meer verbruikt, kan ik nog door de vingers zien, maar één van € 200 zou net zo weinig mogelijk moeten verbruiken... Trouwens, de impact op je jaarrekening bedraagt in je eigen voorbeeld minstens € 73 (1 kWh x 365 x € 0,20 per kWh), iets normaler gerekend (2 uur per dag) kom je nog altijd uit op zo'n € 15.
RX 570;3;0.42622631788253784;Wellicht een beetje gekke opmerking, maar ik zou nu wel eens willen weten wat een verschil zo'n videokaart nu maakt ten opzichte van de geintegreerde graphics. Ik speel eigenlijk alleen StarCraft2 en Diablo3 (en wat retro via DosBox ), maar ik lees nooit of deze inmiddels oude spellen veel profijt hebben van zo'n (instap) kaart of dat dit niet uitmaakt. Ongetwijfeld is een aparte videokaart een verbetering, maar de geintegreerde graphics worden ook steeds beter...
RX 570;1;0.36659303307533264;Verbetering? 200 fps of 300fps maakt niet meer uit, dat is geen verbetering te noemen. SC2 draait vloeiend op elke nieuwe AMD of Intel CPU met ingebouwde graphics.
RX 570;1;0.7023618221282959;Mmm, net een MSI RX 480 4GB gekocht voor 230,- euro. Annuleren of toch kopen?
RX 570;1;0.3219541609287262;Je kunt nog 2 weken na aankoop je bedenken, dat is volkomen legitiem. Wordt pas een beetje dubieus als je'm gaat overklokken. Dat is per definitie buiten de specificaties.
RX 570;3;0.489817351102829;Ok, nou ja erg veel verschil zit er ook niet in.
RX 570;1;0.41929200291633606;"""De RX 580 met 8GB geheugen krijgt een adviesprijs van 260 euro"" Dus dat wordt 360 euro hier in Nederland Aangezien de rx480 hier na meer dan een half jaar nog steeds ver boven de AMD prijs zit,zal het met de 580 wel net zo zijn."
RX 570;1;0.6177349090576172;Inderdaad, veel te duur, prijzen liggen zelfs hoger dan de begin prijzen van de R400 serie, terwijl de advies prijzen van de R500 serie lager liggen. Laat ze eerst maar eens flink in prijzen zakken, de prijzen zijn echt asociaal te te noemen. Mijn advies is dan ook de R500 serie eerst niet te kopen. Kijk even goed rond of je mooie prijsjes kan vinden van de R400 serie.
RX 570;1;0.3786759674549103;Ik wil graag een 480 of 580 kopen. Zou ik nu een 480 kopen of even wachten tot wat de prijzen gaan doen?
RX 570;4;0.42424359917640686;Ik heb een leuke leerzame video van een sapphire 580X breakdown. ben zelf nog even aan het wachten op wat de prijzen doen leuke upgrade voor een r9 290x die bij mij kapot gegaan is.
RX 570;1;0.5359671115875244;NOFI : Welke component is wat en draait op hoeveel volt, eerst schrijven , dan direct wissen , veel blabla ... mjah , ik zie er de toegevoegde waarde niet van die video. Zo kan je elk rx480 pcb reviewen zonder er echt wijzer uit te worden welk je moet kopen...
RX 570;3;0.3815014064311981;14538 is niet slecht voor de MSI RX 480. 14857 MSI RX 480 28956 2X RX 480. 4455 1x MSI RX 480 (Timespy)
RX 570;3;0.4255765676498413;Jammer dat het een rebrand is en niet een refresh . Als je de R290 met de R390 vergelijkt had je wel wat verbeteringen dat is wat ik noem een refresh. Had meer gehoopt van de 500 series.
RX 570;1;0.4637952446937561;Deze kaarten zijn niet voor gebruikers bedoeld die al een rx4 serie heeft meer voor de gebruikers die nu wil upgraden samen met de Ryzen cpu
RX 570;1;0.4346732199192047;Kansloze kaarten van AMD. Bijna geen verbetering qua prestaties. Bovendien snijden ze zichzelf in de vingers met de RX4xx kaarten Even kijken wat Vega gaat doen, maar ik vrees het ergste.
RX 570;1;0.5305013656616211;gewoon een overgeclockte 480 dus met een hoger verbruik
RX 570;1;0.29196327924728394;Zou ook graag ryzen benchmarks met deze kaart zien ipv intel only
RX 570;1;0.6646876335144043;Dit is geen CPU review, en de RX 580 is bij lange na niet snel genoeg om een invloed van de CPU uberhaupt te kunnen meten op de geteste resoluties. Kortom, vrij zinloos.
RX 570;2;0.3184277415275574;Het gaat mij niet om complot theorieën hoor, ben gewoon benieuwd naar wat uitgebreidere benchmarks icm Ryzen omdat ik zo veel verschillende resultaten zag langskomen icm verschillende kaarten, Ryzen en diverse games.
RX 570;5;0.43396347761154175;Heb nu al een tijdje (met veel plezier overigens, wat ik niet had gedacht als Nvidia persoon) een RX480 8GB, met reference cooler. Deze kun je sowieso al +2% overclocken zonder problemen (de software van AMD is daar nu echt heeel makkelijk in). Neem aan dat de kaarten met speciale cooling nog hoger kunnen (misschien wel die 5% die de 580 extra geeft?) dit maakt voor veel mensen de rx480 echt een super kaart als deze *nog* goedkoper gaat worden door de introductie van de 580
RX 480;2;0.4659394919872284;Dus om even kort samen te vatten, deze gloednieuwe 14nm AMD kaart kan zich qua prijs prestatie niveau net meten met een Nvidia kaart van 2 jaar oud die op 28nm geproduceerd wordt. En aan de comments te zien vinden veel mensen dat een geweldige prestatie? Daar komt nog een bij dat de advies prijzen voor de REFERENTIE kaarten op 220 en 260 euro liggen, normaal gezien worden de custom kaarten dus nog een paar tientjes duurder. Nvidia komt binnenkort ook nog met 1060, hier zou de 480 straks mee vergeleken worden. Aangezien Pascal een efficiëntie heeft die z'n 70-80% beter is dan Polaris kan je dus verwachten dat dit een lastig verhaal wordt voor AMD en dan hebben we het nog niet eens over de dramatisch slechte koeloplossing van AMD gehad. Tijdens de E3 persconferentie is door AMD gezegd dat de 480 de prestaties moet leveren van kaarten die $500 dollar kosten, ik denk dat ze daarmee twee jaar te laat zijn. Alles bij elkaar vind ik het dus een grote teleurstelling, en helaas voor ons als consument betekend dit nogmaals dat er bijna tot geen concurrentie is in videokaart land. Zowel high end als mid range zijn er betere opties als je alles in beschouwing neemt.
RX 480;2;0.41486382484436035;"Toch wel gek hoor. Toen de GTX 960 uitkwam was er helemaal niemand die een +2 wist te krijgen voor de opmerking ""Dus om even kort samen te vatten, deze gloednieuwe Maxwell videokaart kan zich qua prijs/prestatieniveau meten met een AMD kaart van 3 jaar oud?"" Het hele idee, de hele doelgroep van deze kaart (zoals ook aangekondigd op E3), is de huidige afzetmarkt van de GTX970. Ja, daar presteert hij inderdaad gelijk mee. Vorige week kostte een GTX970 nog ~€330. Een RX480 met eveneens 4GB videogeheugen, die dus vaak net iets sneller is, maar wel ondersteuning biedt voor HDR, h.265, Displayport 1.4 en Freesync, kost nu maar 219. Ja, Nvidia heeft daarop de prijzen laten dalen, maar het is duidelijk dat AMD op dit moment een minimaal gelijk presterende kaart met modernere features voor €50 minder verkoopt. Dat lijkt mij vrij fair. Het energieverbruik is inderdaad wel een dingetje. Hardware.info trekt hier de cijfers die AMD gebruikte in zijn presentatie goed onderuit, imo: de vergelijkingen waren niet volledig eerlijk Wat de 1060 gaat worden is sowieso nog maar de vraag. Ik geloof zeker dat de 1060 sneller gaat worden, maar Nvidia kan niet die kaart op $200 zetten en 'de volgende stap' 1070 op $380 zetten wat op dit moment nog geen enkele fabrikant weet te halen. Voor ons als consument breekt er een nieuwe tijd aan. Laten we afwachten tot Nvidia zijn antwoord op deze GPU's heeft uitgebracht, en afwachten tot AMD zijn antwoord op de 1070/1080 heeft uitgebracht. De concurrentie komt vanzelf wel Admin-edit:Opmerkingen over moderaties horen thuis in Frontpagemoderatie."
RX 480;2;0.33261922001838684;Op dat afwachten speculeert Nvidia door de GTX 1060 eerder dan oorspronkelijk gepland op de markt te brengen. Pure strategie om AMD het vuur aan de schenen te leggen. Dat de prijs van de 1070 nog zal dalen is zeker, maar of Nvidia dat doet na de RX490 (Q4 2016) of vlak ervoor is de vraag.
RX 480;1;0.31630977988243103;"Ik heb begrepen dat die 1060 launch daadwerkelijk midden juli gaat plaatsvinden; die geruchten kloppen. Al is het nog maar de vraag of dat net zo'n paper launch gaat worden als de huidige 1070/1080."
RX 480;2;0.5483177304267883;Het lijkt er in deze aanloop naar 14/16nm GPU's vooral te gaan om wie er nu een aardige stock heeft. Je zegt het goed dat de 1070/1080 paper launches zijn, dat is ook een reden dat de prijs nog zo hoog ligt. AMD daarentegen heeft aangegeven dat ze de RX480 al in grote getale hebben geproduceerd. Mogelijk proberen ze daarmee het momentum te pakken, en gezien de aankondiging van de 1060 zo snel, lijkt dat een aardige strategie. Niet in de laatste plaats omdat Nvidia's aanbod nu zo enorm duur is/lijkt en dat van AMD totaal tegenovergesteld aandoet. Tegelijkertijd vind ik de RX480 en met name dan 'Polaris' als GPU niet zo interessant. Er is geen echte vooruitgang, er is wat bijgeschaafd en op een kleinere node gebakken. Eigenlijk net als bij Nvidia, maar Nvidia haalt er veel hogere clocks uit en betere efficientie op die hogere clocks. De perf/watt winst van Maxwell schaalt gewoon heel mooi mee, en die van GCN niet. Jammer.
RX 480;2;0.5044715404510498;"Het verschil zit hem in het feit dat de 1070 en 1080 geen mainstream gamer kaarten zijn.. (1070 zit misschien nog op het randje wanneer de prijzen normal zijn, lees ongeveer 300 euro) AMD heeft het laatste jaar duidelijk aangegeven dat dit een mainstream kaart word. Daarnaast denk ik dat je met de opmerking ""maar Nvidia haalt er veel hogere clocks uit"" doelt op het feit dat Nvidia's nieuwe kaarten draaien op 1.8Ghz (1080)? Dit is een beetje een nutteloze vergelijking aangezien je core clocks niet kan vergelijken over verschillende architecturen.. En de efficientie van de Nvidia kaarten is ook een beetje overdreven, een GTX 980ti overclocked naar ongeveer 1500/1550Mhz is evensnel als een stock GTX 1070 die op 1683Mhz draait . De grootste reden dat de nieuwe Nvidia kaarten sneller zijn is dus dat ze een hogere core clock kunnen bereiken, de rest heeft er zo goed als weinig invloed op. (Er waren ook een hele poos een hoop geruchten dat Pascal is wat Maxwell eigenlijk had moeten zijn, maar door dat ze problemen hadden met de procede verkleining (niet alleen Nvidia maar ook AMD) hebben ze dus stukken Maxwell weghaalt om het op 28nm te laten passen. In hoeverre dit waar is weet ik niet.)"
RX 480;3;0.38071438670158386;"Vwb die verschillen in architectuur; tot op zekere hoogte waar, maar zo groot zijn de verschillen niet meer. Beiden passen ze delta compressie toe, hebben grotendeels dezelfde technieken, alleen de opbouw van de shaders en pipeline is iets anders. Maar ze doen hetzelfde, pixels pushen, en het is wel zo dat een 'streaming processor' icm clockspeed direct te linken is aan performance. Je zegt het zelf; Pascal kan meer teweeg brengen vanwege hogere clocks, en dat zeg ik dus ook. Het resultaat is dat AMD zoals ook op het oude GCN en zoals ook al met Kepler het geval was (2012!) meer transistors nodig heeft om gelijke prestaties te bereiken."
RX 480;3;0.5018982887268066;Ik zie het ja, denk dat ik er te snel over heen gelezen heb. Excuus. Haalt niet weg dat een rx 480 een redelijke sprong is van een r9 380 (die hij vervangt), rx 480 met een 1070 vergelijken is naar mijn idee niet helemaal correct. Maar goed, ik denk dat we het over het algemeen eens zijn haha.
RX 480;1;0.49424925446510315;Ik heb gehoord dat de 1060 7 Juli wordt gelaunched. Weet ik niet zeker hoor.
RX 480;2;0.4445003867149353;Ik sta niet bekend om mijn lovende woorden over het bedrijf AMD maar als deze kaart er straks voor 220 euro is en net wat beter is dan de GTX 970, die tot voorkort 350 euro moest kosten, dan vindt ik dat AMD geen slechte prestatie heeft geleverd. Dit alles wel onder voorbehoud van de jaarcijfers. Als AMD straks nauwelijks winst op de gpu tak draait, en dus te weinig voor deze kaart vraagt, dan neem ik mijn woorden terug. Juist die GTX 970 vond ik overigens veel te duur voor hoe oud hij was (2014). Dit komt natuurlijk deels door de euro problematiek, maar AMD zegt nu toch voor 220 euro op dat prestatie niveau mee te kunnen dingen. Ook de GTX 960 was niet bijster interessant voor prijs/prestatie. In ieder geval niet voor iemand die nog een GTX 660 Ti/GTX 670, GTX 680 of iets vergelijkbaars heeft. Mijn hoop was gevestigd op een GTX 1080 voor 500 euro. Echter voorlopig is dat met de paper launch van nvidia nog lang niet aan de orde. Het is nu vooral de vraag hoe (snel) Nvidia gaat reageren. Voorraden zijn er niet dus de advies prijs van de 1070 verlagen heeft geen nut. Het zal dus vooral spannend voor AMD worden hoe lang Nvidia er over doet om hun GTX 1060 te introduceren.
RX 480;3;0.3563253879547119;"Prima geschreven Richh zou ik zo zeggen. Ik wacht op verdere prijsdalingen, mijn R7 360 met 2Gb doet het goed genoeg, evenals mijn 7970 3Gb die ik voor renderen gebruik en een FirePro die ik op ebay heb gekocht voor €27,50, die goed werkt en nog steeds snel is. Ik heb een FirePro W 5000 gehad die net zo snel is als een Tesla T10 4Gb, voor een fractie van de prijs van de W5000. . En om nu zo'n 200 euro bij te moeten lappen voor 20 frames extra per seconde terwijl het nu ook vloeiend gaat...Tja hoe gek kun je zijn. Een nieuwe kaart ,mocht ik mijn ""oude"" verkopen is me het geld niet waard. Ik was eerder een nVidia fan maar sinds ze marktleider zijn geworden zijn de prijzen omhoog geschoten. Ik wil niet als melkkoe worden gezien door de computerindustrie, een paar vernieuwingen, en het is weer geld tanken bij de consument. De productie kosten zijn aanzienlijk lager geworden, maar dat vertaald zich niet naar de prijs. Ik werk nu met tweedehandskaarten, daardoor hou ik aanzienlijk meer geld over voor echt leuke dingen met vrienden. Dat is voor mij de ware schat van het leven."
RX 480;2;0.34277650713920593;'Voor ons als consument breekt er een nieuwe tijd aan. Laten we afwachten tot Nvidia zijn antwoord op deze GPU's heeft uitgebracht, en afwachten tot AMD zijn antwoord op de 1070/1080 heeft uitgebracht. De concurrentie komt vanzelf wel :)' dat is nogmaar te hopen... zoals ik in de andere AMD RX480 threat heb gezegd: en nu maar afwachten wat dit gaat doen.. dat kunnen 2 dingen zijn 1. Nvidia gaat zijn prijzen verlagen om 'enigzins' gelijk te komen met AMD's prijs (reken 350 voor Nvidia vs 275 voor AMD) 2. Nvidia haalt zijn schouders op 'want het is toch maar een 970 OC' dus we hebben niks te vrezen van AMD (alweer niet) dus die prijs blijft mooi staan. ik vrees zelf voor het 2e punt. want ze kunnen wel zeggen (zoals ze dus ook met de hype rondom de 480 hebben gedaan) 'wij hebben de betaalbare stap naar VR etc. etc. etc. maar als straks blijkt dat OOK de 490 'net aan' een 1070 zou inhalen met een beetje tweaken.. dan is het tochwel gedaan voor mij wat betreft AMD.. heb ze in deze nieuwe chip GEN een beetje het voordeel van de twijfel gegeven.. maar die lijkt misplaats te zijn. AMD zal nooit een snellere kaart uitbrengen als Nvidia op hetzelfde tijdvlak. en dat heeft vooral te maken met 1 ding.. keiharde pegels voor RND die ze niet hebben in kamp rood. in de schaal vergelijking met kamp groen.
RX 480;3;0.4013240933418274;Eneregieverbruik is wel echt een issue, daaruit kun jij eigenlijk enigszins opmaken dat de architectuur van Nvidia op dit moment superieur is. Een gtx 1070 is 2x zo snel, en verbruikt hetzelfde. Dat impliceert dat Nivida bij downscaling een veel snellere gpu met een lager stroomverbruik dan de 480 kan maken. Of die dan ook goedkoper kan worden weet ik niet, maar het lijkt me een logische stap.
RX 480;3;0.4811966121196747;"Laten we het wel even in het juiste perspectief zetten: de teleurstelling wordt natuurlijk voornamelijk veroorzaakt door de (Nederlandse) prijsstelling. Wanneer de RX480 prijs dichter bij de € 200,- dan bij de € 300,- had gelegen, had het wel een mooie prestatie geweest. Op een of andere manier liggen (hardware)prijzen bij de introductie veel hoger dan in andere, omringende landen. Dat is niet alleen bij hardware zo, maar geldt voor meerdere produkten, zo heb ik in Duitsland een koffiemachine gekocht voor € 860,- terwijl de goedkoopste in Nederland € 1340,- kostte. Ook gereedschap e.d. zijn goedkoper bij onze oosterburen. Klaarblijkelijk geldt in Nederland nog steeds het principe: ""Greed is good"". Het is nog even afwachten, want introductieprijzen liggen vaak hoger dan een paar weken/maanden later, dus je conclusie vind ik net iets te voorbarig. Het is idd wel wachten op de GTX 1060, die zal in ieder geval de prijzen weer een beetje sneller drukken."
RX 480;3;0.5521934628486633;Dit prijzen zullen in korte tijd nog wel wat dalen. De eerste kopers komen toch wel, dus kunnen ze ook een goede prijs vragen. Denk dat als er straks after-market kaarten komen dan zal dat nog sneller gaan. Winkels gaan over het algemeen nog wel een stuk lager dan de adviesprijs.
RX 480;2;0.3014363944530487;"Ik reageerde op jandd0's post, die wel erg negatief was. Zelf zie ik het positief in; als ik het zo inschat zal een OC RX480 rond de € 200,- uitkomen, wat het een héél aantrekkelijke optie maakt, zeker voor 1080p gaming 's het een no brainer. Tel daarbij de DX12 implementatie, Freesync etc bij op en de keuze tussen de RX480 en de GTX970 is snel gemaakt. GTX 970 heeft dus in feite afgedaan, maar wat gaat de GTX1060 brengen? Als die onder de €200,- komt, zal het een flinke strijd worden tussen het groene en rode kamp."
RX 480;2;0.44211089611053467;Van wat ik van iemand begrepen heb is dat AMD met de RX-Serie op een kleinere procedé meer chips uit een wafer kan halen en dus goedkoper kan produceren dan Nvidia dat kan. Maar misschien vergis ik me met de volgende aankomende series. Daarnaast heeft Nvidia volgensmij behoorlijk problemen met het leveren van de 1070. Volgensmij is er geen webshop in Nederland die hem op voorraad heeft. Ik verwacht daar voorlopig nog geen concurrentie van.
RX 480;1;0.41856226325035095;Nvidia is ook overgestapt op een kleiner procedé, dus dat argument gaat niet helemaal op. 1070 en zelfs 1080 zijn nu al bijna een maand uit, maar leverbaar in Nederland? Ho maar. Het was voor ons in ieder geval een paperlaunch.
RX 480;2;0.30859318375587463;Dat zou idd mooi zijn, dat ze beide rond de 200 euro uitkomen, dat is ook wat ik verwacht, eerst maar eens zien of de 1060 ook geen paperlaunch wordt. Daar mag amd op hopen kunnen ze mooi wat kaartjes verkopen.
RX 480;2;0.3999737501144409;dat werd ook over Nvidia gezegt... als je een week of 3 terug het forum heb gezien.. introductie van de 1080 en daarna de 1070 'ja de prijzen van de after-markets worden zoveel % minder want....' nou mooi niet dus he.. MSI armor 1070 8 GB kaart.. pre-order was eerst 460.. nu staat hij ook gewoon heel netjes op 512 euro.. dus waar 52 euro 'meer' vandaan komt, afgezien van 'kleine voorraad' is mij een raadsel.. en met mij niet de enige op dit forum lijkt me.
RX 480;1;0.4325883388519287;"no offence naar de review maar : van tweakers had ik betere benchmark en games selectie verwacht. het lijkt wel of tweakers ook al de koorts van nvidia "" inception "" te pakken heeft. gamers en gpu - compute gebruikers, houdt deze punten in je achterhoofd : gameworks en directx12 / vulkan zijn heikel punten van nvidia. over gameworks is al genoeg geschreven en we weten allemaal waar het voor dient : ongelijke speelveld creeren zodat nvidia kaarten beter uit de verf komen en de sterke kant van amd - architectuur onbenut laten. amd heeft daarop met mantle api terug geslagen om onder andere gameworks te omzeilen. directx12 / vulkan zijn de geadopteerde dochters van mantle. dit heeft alleen meer tijd nodig om in te burgeren. laat nu net nvidia juist directx 12 / mantle vertragen want gameworks heeft dan nauwelijks effect meer [UNK] heck, nvidia heeft gameworks sinds kort openbaar voor iedereen gemaakt omdat het bijna is uitgespeeld door de next - gen api ' s. en de nvidia directx 12 _ 1 bestaat officieel niet eens! ook weer om zand bij de consumenten te strooien. en waar is "" total war : warhammer directx 12 "" benchmark? tweakers test 8 games waarvan 5 (! ) nvidia gameworks titels. houdt dit in je achterhoofd mensen voordat je blind conclusies trekt. en vergeet nogmaals, de directx12 / vulkan niet. ik lees zelfs als de rx 480 paar tientjes meer kost dan een gtx 970 oc dat je beter voor laatste kunt gaan... amd rx 480 is juist een zegen voor de gamers voor zowel amd en nvidia lovers! betaalbaar, energie zuinig, op dit moment tussen het niveau van een gtx 970 en gtx 980. en met de vooruitstrevende techniek al ingebakken voor directx12 en vulkan gaat het binnen een jaar de gtx 980 voorbij. en de multi - gpu toekomst ziet goed uit als alle nvidia gameworks proprietary games verouderd en vergeten is. een goede youtube kanaal over gameworks en directx 12 / vulkan : adoredtv"
RX 480;3;0.41736477613449097;Zoals ik in een andere reactie ook al opmerkte, zijn de grote problemen van Project Cars met AMD-kaarten die er bij launch waren opgelost. Zeker in het midrange-segment waarin de RX 480 zich bevindt zijn verhoudingen tussen de AMD en Nvidia-kaarten in deze game niet veel anders dan in andere games. Wat betreft Tomb Raider: het geeft goed aan dat er DirectX12-games zijn die heel goed gebruik maken van nieuwe features en het goed doen op AMD-kaarten (zoals Hitman, een game die we ook testen en waarin de RX 480 bijzonder goed naar voren komt) en dat er DX12-games zijn met een wat 'luie' implementatie. Wat mij betreft goed om beide kanten van het verhaal te laten zien en dat is precies wat we doen. We zijn bekend met het effect van HairWorks bij The Witcher 3 en hebben dat voor onze benchmarks dan ook uitstaan. Jouw post insinueert dat Nvidia-kaarten bij games die we getest hebben in het voordeel zijn, maar als je puur naar de resultaten kijkt is dat simpelweg niet zo. Het verschil tussen bijvoorbeeld de RX480 en de GTX 970 is behoorlijk constant in alle games met als hoogste uitschieter Hitman, waarin het juist de RX 480 is die het veel beter doet en uiteindelijk is de RX480 in 6 van de 8 games sneller op 1920x1080. Afgaande op de resultaten hebben we een nette selectie van games waarbij geen enorme uitschieters te zien zijn richting het 'groene' of 'rode' kamp. Net als jij kan ik overigens niet wachten tot we veel meer games zien met een goede DX12- of Vulkan-implementatie. Ik ben erg benieuwd of verschillen dan groter zullen worden.
RX 480;1;0.5024054050445557;De laatste keer dat een review mij zoveel hoofdpijn gaf als met de RX 480 kan ik mij bijna niet herinneren. Niet vanwege de kaart, maar enkel vanwege DX12 op zowel Nvidia als AMD kaarten. Dan gaan de prestaties 20% omhoog, dan gaan ze 5% omlaag, dan start een benchmark niet eens op op een specifieke kaart, vervolgens zijn de prestaties in een andere benchmark -exact- gelijk ongeacht de API. DX12 toont enorme potentie, maar een kritische en afwachtende notitie lijkt mij niet meer dan vanzelfsprekend. AMD wil zo veel mogelijk DX12 zien, Nvidia knalt daar gameworks titels tegen aan, maar uiteindelijk kan je weinig anders doen dan de boel nuanceren zoals nu ook gebeurd.
RX 480;3;0.6424915194511414;Fair enough. Maar laten we niet over het woord insinueren hebben want dan schiet je (ook) in je eigen voet daarmee. Wat belangrijk is om beide kanten van de medaille te laten zien, juist op het punt waar een omslag plaats kan vinden naar de nieuwe API’s en optimalisaties. Ja ik maak met mijn kritiek niet populair mee met +80% Nvidia marketshare ten opzichte van AMD. Maar het is wel belangrijk om de kopers van nu duidelijk te informeren waar Abraham de mosterd haalt maar vooral waar het heen gaat.
RX 480;1;0.3981372117996216;Ashes of the singularity geen AMD love? Ten eerste is het spel is een AMD featured game: Ten tweede is het spel gesponsored door AMD en oorspronkelijk bedoeld als Mantle tech demo. Sinds maart 2014 werken de makers van het spel Dan Baker en Tim Kipp van oxide games voor AMD. Als dat geen AMD love is dan weet ik het niet meer. Zie 2de slide:
RX 480;2;0.2632114887237549;Verschil tussen Nvidia en AMD geoptimaliseerde games is dat AMD ook Nvidia kans geeft op in de code te kijken voor driveroptimalisaties. Nvidia’s proprietary juist niet. En DirectX 12 code was voor iedereen bekend en toegankelijk, ook Nvidia.
RX 480;1;0.7330424785614014;Als ik je betoog lees denk ik: waarom nog überhaupt een AMD kaart, geen game komt ermee uit de verf.
RX 480;2;0.5224997401237488;Het ligt behoorlijk ingewikkeld in elkaar. Iemand die veel beter dan mij kan uitleggen is onder andere AdoredTV. Soms moet je een muur of 2 slopen wil je het complete verhaal zien omdat je soms AMD/Nvida mindshare infectie kunt hebben en hoe sommige (agressieve) marketing technieken werken. Maar hoe het er nu uit ziet is de AMD RX 480 een pareltje voor de grote gros PC-gamers. Ook voor Nvidia minded mensen! Kijk maar wat er met de prijzen van GTX 970/980 aan het gebeuren is. En de AMD RX 480 voor de betaalbare schappelijke prijs wordt alleen maar beter en beter door de nieuwe API's ontwikkelingen. Daarom mijn eerdere betoog om hier meer op te letten als tegengeluid, en ook omdat Nvidia mindshare sterk is. Mijn mening: GTX 970 én GTX 980 zijn sinds vandaag dood verklaart. En ook de aanstaande GTX 1060 gaat op den duur afleggen omdat het gewoon de oude DX11 gebaseerde architectuur gaat hebben. Terwijl de game-industrie allemaal naar DX12/Vulkan marcheren.
RX 480;2;0.49908584356307983;Toch wel straf dat AMD net op Computex ook Ashes of singularity had gekozen om te vergelijken met die ene competitieve net gelanceerde kaart van de groene zijde... Terzijde, als de kaarten effectief gaan kosten wat toen beloofd was is dit een heel strafef bang for the buck kaart. Helaas als de prijzen voor de 8GB versie rond de huidige 300 euro blijven plakken heeft de PR machine van AMD nogmaals gefaald. Ik ben eerlijk gezegd na de lovende woorden vorige maand een beetje teleurgesteld.... De twijfel over ZEN neemt nu ook toe
RX 480;3;0.28541865944862366;kan je door de mods niet meer waarderen maar +2 is dit zeker waard. staat een boel in jou comment wat menig game dev. bij kamp groen slapeloze nachten geeft denk ik zomaar.
RX 480;2;0.3355391025543213;En opmerkelijk het verschil in positie tussen de andere kaarten. In VR tussen de beide 980-ers, en bij de monitor versie tussen de 970 en de 290X. Waar komt dat verschil vandaan?
RX 480;1;0.5191366672515869;Welke betere opties dan? Een Gtx960 is 20 euro goedkoper maar bijna twee maal zo traag. Een 970 is 60 euro duurder en even snel. In crossfire komen ze dichtbij de gtx1080 voor honderden euro's minder.
RX 480;2;0.26006144285202026;"Zie de review van Techpowerup daarvoor; Zelfs als je de niet schaalde games eruit haalt(!!) komt hij niet aan de GTX 1080. Doe je dat niet vliegt ook een GTX1070 er ruim aan voorbij. Ook heb je daarvoor een zwaardere voeding nodig, heb je meer stroomverbruik en lawaai, geen opties tot een wat kleinere behuizing etc.. Zo aantrekkelijk vind ik Crossfire 480 voor een vergelijkbare prijs als een GTX1070 dan eigenlijk niet tbh"
RX 480;3;0.4677010774612427;Eens er zitten zeker de bekende nadelen aan een twee kaarten opstelling en ik prefereer ook een enkele. Maar jandd0 stelt dat de kaart niet concurrerend is Terwijl als crossfire werkt hij ook bij jou link sneller is dan de 1070. Lijkt me prima concurrerend. In de tweakers review is hij in Alien, Dragon Age, Far Cry en The Witcher erg dichtbij de gtx1080. Als je de 4GB versie neemt kan je dan voor 440 euro 1070+ prestaties halen in de spellen waarbij crossfire werkt. Dat is echt een prima product. Als enkele kaart zeker. Als die 8GB erg belangrijk is (niet vaak) of je speelt spellen zonder CF ondersteuning dan is de 1070 de handigere optie inderdaad.
RX 480;2;0.4467853605747223;"""Waar crossfire werkt"" Laat dat nou juist het hele probleem zijn met 2 (of meer) kaart opstellingen. Het lijkt wel of er steeds minder games, bij launch, een goede SLI / Crossfire ondersteuning hebben."
RX 480;5;0.30592015385627747;Je zal net de games spelen waar cross fire wel goed schaalt, spaar je mooi 400 euro uit tov 1080..
RX 480;2;0.40708109736442566;Nee dat crossfire ding is onzin. Het is net iets minder dan 970. Dus crossfire zijn deze kaarten ongeveer iets beter dan een 980ti. Maar 1080 is verreweg heg best
RX 480;3;0.25282126665115356;En dan hebben we het nog niet over het energieverbruik gehad wat nu pas op het niveau van de vorige generatie NVIDIA kaarten ligt. Stukje van Tom:
RX 480;3;0.5517503619194031;komt wel meer voor bij eerste PCB designs, effe bios bijsturen en opgelost. typische tom's fuss.
RX 480;4;0.3231610059738159;Niet alleen typische Tom's fuss, ook bij andere reviewers is dit waargenomen. Bijv. ook op de retail kaart van Saphire, waar het eerst ook gegooid werd op de reviewkaarten met een bepaalde bios. Betekent vooralsnog dat AMD 3 maanden heeft om bij te stellen om niet de validatie van PCI-SIG te verliezen. Blijft dan de keuze tussen reguleren van opgenomen wattage (=achteruit in performance) of alsnog ande re verdeling voeding kiezen naast de 75w uit het slot, als ze binnen de spelregels voor validatie willen blijven met een 6-pins aansluiting (hoewel een beetje voeding hier buiten de speciale makkelijk veel meer kan leveren, maar dat valt dan volgens mij buiten de specs van PCI-SIG). Zie ook deze thread. Een risico is wel dat op mobo's met een zwakkere energievoorziening je bij reguliere operatie in een wat zwaardere game al problemen kan ondervinden, laat staan als je een OC doet of Crossfire gaat draaien. Extra warmteontwikkeling en invloed op andere geïntegreerde pci devices zoals audio en netwerk bijvoorbeeld. Eens zien hoe dit zich ontvouwt, hoop voor AMD van harte dat het geen of een heel klein issue is.
RX 480;1;0.44167932868003845;Dus jij vind 199 dollar ten opzichte van 330 dollar wat de 970 koste bij release teleurstellend? In dx12 gamens komt hij ook bij dan wel niet de 980 voorbij. En je zecht toch ook niet bij de 1070. Ook eigenlijk teleurstellend. een kaart uitbrengen die zich kan meten met een kaart die 1 jaar oud is? dan laten we prijs even buiten beschouwing want dat is totaal niet doorslaggevend.. Misschien voor jouw niet. Maar 90% de consumenten kijkt wel naar de prijs. en dan naar de performance. En laten we eerlijk wezen. Deze presteert beter als een 970. En wat de 1060 gaat doen weten we niet. Maar die is er momenteel niet. Dus is AMD king of the prijs per FPS king. Zoals ze 9/10 keer zijn.
RX 480;5;0.5504783987998962;ik heb al jaren een gtx 780Ti deze staat nog steeds 7e als snelste kaart in de wereld. de RX 480 (22e plaats) is 1/3 langzamer dan de 780Ti. dus koop altijd een Ti wacht op de 1080Ti kan je weer 4 jaar verder mee.
RX 480;5;0.6920689344406128;Je vergeet wel 1 heel belangrijk punt. Deze kaart is nieuw, ondersteund de nieuwste tech en zal met DX12 en vulkan beter presteren. Iets wat de kaarten van 2 jaar terug van Nvidia nooit zullen doen. Het is gewoon de beste kaart in de 100-300 doel groep. Je bent eigenlijk gek als je een GTX 980 of 970 koopt in plaats van deze.
RX 480;2;0.4369610548019409;Als men de enige concurrent afkraakt, wanneer zij de prestaties van het high-end segment naar het midrange segment brengen. Terwijl Nvidia die prestatie juist in het hoge segment wil houden, dan moeten wij als consumenten toch wel afvragen of wij goed bezig zijn. Wat mij verder uitermate stoort aan de review is dat de reviewer zijn oordeel baseerd de prijs op het moment van schrijven, maar op dat moment is er nog schaarste en zullen de prijzen altijd hoger zijn. Gezien zijn ervaring in het veld zou hij dit moeten weten. Al een dag na de review was de 8gb versie al 269,- euro, dus op de advies prijs van AMD en is de kaart wel de prijs prestatie kampioen zoals AMD 'm in markt zette. Echt objectief vind ik het niet, maar ja de reclame inkomsten zijn ook nodig.
RX 480;3;0.2791992127895355;euhm met een GTX 970, die inderdaad 2 jaar zo oud is maar waar je nog altijd 50 tot 100 euro meer voor neertelt dan deze kaart is het op zijn minst een economische verbetering
RX 480;1;0.4117368757724762;Arstechnica zei dat er meer dan 90 watt via de pcie bus wordt gevraagd wat meer is dan de 75 die toegestaan zijn. Is daar iets over bekend ?
RX 480;1;0.2955508232116699;Op een Reddit topic reageerde AMD dat ze het gaan onderzoeken en de situatie van de testers gaan nabootsen. Er zijn ook modellen dus waar dat niet zo is. De kaart heeft officieel wel de specificaties voor pci-e. (vrij vertaald) Hier in dit topic word je vraag door AMD beantwoord.
RX 480;4;0.307089626789093;"Ik vind dat de kaart mooie prestaties neerzet voor de prijs! Blijkt over het algemeen sneller dan een GTX 970 OC, en dat voor 220 EUR. Gezien dat de vorige generatie DE kaart was qua prijs/performance, die gemiddeld gemakkelijk 330 - 350 EUR kostte voor de aftermarket kaarten, krijg je dezelfde prestatie voor VEEL minder geld (62% van de prijs van een GTX 970). Ik denk dat velen de prestaties van een GTX 970 willen, om alles vlot op 1080p 60fps te kunnen draaien, maar voor velen was deze kaart toch een tikje te duur. Nu is deze prestatie dus bereikbaar voor de massa. Niet te vergeten dat deze kaart ook de ""minimale VR"" kaart was, dus nu is dit bereikbaar voor 220 EUR."
RX 480;3;0.4617288112640381;Volgens mij is het nog even afwachten voor welke prijzen ze precies in de schappen komen te liggen: de genoemde 8GB modellen van MSI en Gigabyte (en ook Sapphire, als ik de prijzen op Afuture mag geloven - 325 euro) wijken in ieder geval flink af ten opzichte van de prijzen waar zo velen hier zich met flinke stelligheid in vast hadden gebeten, dus de kans lijkt me ook zeker aanwezig dat - op zijn minst bij sommige merken - de 4GB versie duurder uitvalt dan de 220 Euro adviesprijs. Wat me trouwens nogal opviel de laatste tijd is een tendens dat velen hier zich vrij halsstarrig vastklampen aan die info die gunstig uitvalt, en datgene dat ze minder bevalt een beetje onder het tapijt moffelen onder het mom 'ik moet het nog zien' (vrij sterk aanwezig bij al die berichten over de komende AMD generatie, maar komt eigenlijk overal wel voor waar je sterke 'kamp-vorming' hebt, van consoles tot smartphones en wat dan ook). Maar goed, maakt verder ook niet echt uit: de enige die de dupe is van cognitieve dissonantie/confirmation bias ben je uiteindelijk zelf...
RX 480;2;0.4181639552116394;En als iedereen hier genoegen mee zou nemen komt er volgende generate 150€ bovenop de advies prijs Dus laat ze maar lekker verontwaardigd zijn denk ik dan. De priizen zullen sowieso snel stabiliseren, velen kunnen ze voor deze prijs helaas niet laten liggen dus kunnen ze het ervoor vragen de eerste dagen.
RX 480;1;0.300556480884552;Idd. even wachten op prijzen. Zou ook goed kunnen dat de prijsverlaging van de 970 is om stock leeg te maken. Voor de oude prijs zullen ze de 970 (dus rond de 330) niet makkelijk verkopen denk ik. Sowieso weinig ruimte voor meer prijsverlaging denk ik. Nvidia kan ze niet veel goedkoper produceren dan amd ze kan toch? Zoiets heb ik begrepen. In iedergeval mensen die een (nieuwe) videokaart 'bang for buck' willen kopen hebben nu na vandaag aan beide merken een mooie optie naar keuze.
RX 480;1;0.805428683757782;Het gaat mij niet om gelijk willen hebben, salty gedrag of Schadenfreude, meer om verbazing waarom velen zichzelf zo kritiekloos in de luren laten leggen zodra een nieuwtje in hun straatje past. Ik kan me prima voorstellen dat iemand een persoonlijke voorkeur voor een bepaald merk heeft en blij kan zijn met de aankoop/bezit van een bepaald product (dat heb ik ook), maar ik snap oprecht niet waarom dat zo vaak gepaard moet gaan met zulke oogkleppen, of met kwaad of zelfs agressief worden als iemand wat lelijks zegt over je favoriete product/merk? Heb je dan zo'n pathetisch leven dat je je zo expliciet moet identificeren met een bepaald merk (en dan hebben we het niet eens over sexy dingen maar feitelijk gewoon om domme 'dingen' als een videokaart, een spelletjesconsole, een mobieltje...)
RX 480;4;0.3110526204109192;"helemaal mee eens, maar ik vraag me nu wel af waarom (""en dan hebben we het niet eens over sexy dingen"") een stel prachtige tieten dit wel zouden rechtvaardigen volgens jouw"
RX 480;5;0.22887274622917175;"Ooit gehoord van de termen als ""Arm Candy"" en ""Trophy Wife""? Bovendien, vergeleken met een videokaart of een mobieltje kosten die dingen een godsvermogen (alhoewel dat eerder onderhoud betreft) - kan me heel goed voorstellen dat je er dan ook wel een beetje mee te koop wil lopen </offtopic>"
RX 480;1;0.44380566477775574;Azerty verkoopt de 8GB versie vanaf €269,-
RX 480;1;0.4256431758403778;Ik zag dat bij Afuture een kwartier later de prijs ook al met een tiietntje was gedaald. Even afwachten lijkt me dan ook het devies: kan me voorstellen dat veel mensen deze kaart willen aanschaffen (het is imo. ook gewoon een goede deal), maar even wachten tot het stof is neergedaald kan volgens mij heel lucratief zijn.
RX 480;2;0.3515879213809967;Geef je groot gelijk, plus(wat ik denk) is dat fabrikanten zoals Msi etc.. Niet zomaar de kaart voor de adviesprijs van Amd de deur uit zullen doen, wanneer ze dit zouden doen gok ik dat zij een omzet verlies van andere modellen zullen krikgen en dit willen ze natuurlijk zoveel mogelijk voorkomen, want als die kaart voor zo goedkoop aangeboden zal worden, zullen voorgaande modellen of niet of nauwlijks nog gekocht worden of ze zullen de prijzen ervan drastisch omlaag moeten gooien.. We kunnen na mijn idee voorlopig nog wel even wachten tot de kaarten echt interessant worden, ik heb zo het gevoel dat elke fabrikant bij elkaar in de buurt qua prijs zal gaan zitten en dat er maat 1á2, 10 á 20 euro goedkoper zullen zijn. Vind de prestaties zeer mooi maar aangezien als ik zo de overclock prestaties zie.. Zal er na mijn idee weinig verschil in zitten of je er nou een reference of non-reference koeler op hebt zitten..
RX 480;2;0.42978349328041077;Geluidsdruk is wel belachelijk hoog imo. Hopelijk heeft die niet ook nog coil-whine
RX 480;5;0.27230843901634216;wachten dan even op de aftermarket coolers. kan je er gelijk een flinke OC op gooien.
RX 480;2;0.4077504277229309;Is nog maar de vraag. Kaart beschikt over een 6-pins aansluiting en kan dus slechts max 150 W verbruiken. Of kunnen fabrikanten meer 6-of 8-pins connectoren toevoegen? Ik heb de indruk dat de RX480 aan alle kanten wordt toegeknepen. De kaart zou gemakkelijk 30 W meer mogen verbruiken want verbruikt in-game dan nog altijd 10 W minder dan de 1070 gtx. Het 14 nm procedee laat dit toe. De performance/watt verhouding bij nVidia is veel beter. nVidia heeft gekozen voor hogere prestaties(gpu veel hoger geklokt dan AMD) bij hoger verbruik, AMD voor lagere prestaties bij lager verbruik. Ik verwacht dus dat fabrikanten veel hoger geklokte kaarten gaan uitbrengen waardoor de prestatie drastisch zal stijgen en meer in lijn met nVidia komt te liggen. Tweakers zelf heeft er bij het overklokken al 10 % prestatieverbetering uitgekregen zonder spanningsverhoging. Fabrikanten gaan de spanning wel verhogen dus ik verwacht >10% prestatiewinst.
RX 480;5;0.5399535298347473;ze starten altijd met basis kaarten, dan smijten ze er een custom cooler op, dan bouwen ze eigen pcb. altijd in die volgorde om snelle go to market te hebben. beste designs krijg je dus laatst.... het is ook nog eerste gpu batch zonder process optimalistaie. dus latere gpu zullen minder verbruiken.
RX 480;3;0.2857353985309601;Hopelijk brengt AMD ook een RX485 revisie uit voor de RX480 zoals ze recent aangegeven hebben om beter te concurreren met nVidia aangezien de 1060 op het niveau van de 980 zit maar de RX480 net daaronder.
RX 480;1;0.3518145680427551;en die 1060 zal dan ook wel weer 100eur meerkosten...
RX 480;1;0.38505375385284424;Dat is de prijs geen 220 euro doe mer maar meer dan 100 bij
RX 480;3;0.5328847169876099;Dat is een beetje overdeven, 50 euro bijleggen is voldoende:
RX 480;2;0.45509570837020874;Ik ben bang dat deze prijzen omhoog gaan. Afuture is de eerste in de pricewatch maar helaas ook weer boven de 300 euro Dus voor mensen die er 1tje willen hebben voor een goede prijs Bestellen bij azerty zolang die prijs nog laag is. ze kunnen helaas niet uit voorraad leveren. maar je zult ze vast krijgen voor die prijs.
RX 480;1;0.8904107809066772;Raalte 0 stuks voorradig Online ruim voorradig Althans wat hun site zegt
RX 480;1;0.24940644204616547;Netjes van Azerty. Hoop dat niemand zo gek is om 60 meer neer te tellen bij Alternate, zal dat stel uitbuiters leren.
RX 480;2;0.41743776202201843;40 euro meer voor de 8gb variant vind ik vrij fors. Ik zie ook niet direct een toepassing voor die 8gb behalve CAD modellen renderen oid.
RX 480;3;0.5566055774688721;Ik zeker wel. Ik merk dat mijn 4GB op mijn 290 al te weinig is. Ik ga zeker voor de 480 8GB alleen al voor de exra V-Ram
RX 480;1;0.3115844130516052;Met wat voor toepassingen merk je het dan? Ik zit persoonlijk nog op 1 gb, en heb dus geen idee.
RX 480;3;0.5273909568786621;Games als GTA V vinden het wel fijn om meer VRAM te hebben, vooral op alles vanaf 1080p.
RX 480;5;0.38330480456352234;Assetto Corsa met wat graphic mods ga je gauw over de 4GB heen. Tevens is GTA V ook behoorlijk geheugen vretend.
RX 480;5;0.37871232628822327;Op spellen als Ashes of the Singularity raden ze al sterk een 4GB VRAM kaart aan voor de recommended/medium settings en 8GB VRAM voor de maximum settings. Als ik in de map editor (niet VRAM) bezig ben in Ashes of the Singularity (Ben een modder voor maps/scenarios/campaigns) dan gebruikt ie de volle 16 GB RAM, voor grote maps heb je 32 GB RAM nodig in de editor (Niet in game godszijdank anders zou bijna niemand ze kunnen spelen). En ze willen graag NOG grotere maps gaan maken (makkelijk 4x de grootte) dus die devs hebben 64-128 GB ram in hun systemen. Gekkenwerk, maar het is dus vooral een stukje toekomstbestendigheid als je voor 8GB ipv 4GB gaat.
RX 480;2;0.4361971318721771;Geheugen gebruik staat niet in verhouding tot benodigd geheugen. Het kan zo zijn dat je Vram counter 4GB aan geeft, maar de kaart prima met 3GB overweg kan zonder in fps te zaken. Ik ben met 2x 980ti 6GB in SLI nog geen geheugen te kort tegen gekomen in GTA V, lijkt me sterk dat een 290 te weinig aan 4GB heeft.
RX 480;3;0.5222197771072388;ik merk het ook met dirt rally, project cars en asetto corsa dat 4gb eigenlijk net te weinig is. ik heb nu een gigabyte g1 gaming gtx 960 met 4gb vram. maar af en toe loopt die 4gb toch aardig vol.
RX 480;4;0.3670443296432495;Misschien op het moment niet, maar games gaan steeds meer V-ram gebruiken, dus 'better safe than sorry', en dus is die 4GB extra het echt wel waard imo.
RX 480;5;0.3141248822212219;Je krijgt niet alleen meer VRAM, het geheugen is ook iets sneller geklokt (7Gbs vs 8Gbs).
RX 480;3;0.3788609802722931;Medetweakers, blijf alstublieft weg van Crossfire (of SLI) als alternatief van bijvoorbeeld de GTX 1070. Want dit heeft enkele grote nadelen, zoalsSlechte support door games (zie Hitman en Tomb Raider). Ik vind die Prestatie-index DX11 dan ook erg onrepresentatiefOnstabiele games (bv.: geluidsproductie (je produceert letterlijk twee keer zoveel geluid (dBa schaal is logaritmisch) en de warmte kan nog slechter je kast uit waarmee de fans ook nog harder moet draaien)Wat wel een redelijke optie is om te investeren in de mogelijkheid om SLI/Crossfire in de toekomst mogelijk te maken (door een grote voeding en een moederbord met twee PCi-e aansluitingen). Zodat je later een goedkope tweedehands vidoekaart op de kop kan tikken.
RX 480;2;0.3656449615955353;Daar wil ik graag een toevoegen dat dit nog niet 2 keer zo hard klinkt voor het menselijk oor Voor een mens klinkt een toename van 10db twee keer zo hard. Bron: Nog een bron: Neemt niet weg dat SLI/CrossFire wel degelijk nadelen heeft maar deze specifieke ligt iets genuanceerder dan wellicht op eerste gezicht lijkt
RX 480;3;0.324951171875;"Ik had er daarom juist ook tussen haakjes bijgezet dat de ""(dBa schaal is logaritmisch)"". Omdat ik weet dat hetgeen wat je zegt klopt ."
RX 480;2;0.3999887704849243;Toch eigenlijk om te janken, dat er met de aankondiging van DX12 werd gesproken over verbeterde Multi-GPU support, met switch van AFR naar SFR, shared memory pooling en meer. Bron: Kijken we naar de enige DX12 titels in deze test, presteert de CF opstelling slechter dan de enkele kaart - Of DX12 is nog niet volwassen en het werkt gewoon nog niet. - Of de developers hebben gewoon geen support voor CF/SLI ingebouwd. Ik hoop dat de optimalisatie-clusterf*ck niet de standaard gaat worden, aangezien nu een nieuwe kaart kopen, en later een tweede erbij zetten flink kan besparen.
RX 480;1;0.7295828461647034;"ROTR en Hitman zijn niet geheel toevallig allebei Square Enix spellen, die altijd al redelijk belabberde multi-gpu support heeft ingebouwd in hun spellen. En in allebei de titels is er net genoeg dx12 features toegepast zodat ze er het stickertje op kunnen plakken. In het devblog van TOTR wordt er heel veel gepraat maar weinig gezegd. Als je concreet kijkt zijn er alleen CPU features van de API toegevoegd, en is bijvoorbeeld async compute alleen voor de XBone beschikbaar gemaakt (!) Dit zal voor Hitman niet veel anders zijn. Compleet waardeloze dx12 benchmarks. Spellen die dx12 een stuk completer hebben toegepast; een Total War of Ashes. MS heeft recentelijk ook een aantal dx12 titels uitgebracht via de store (Quantum of GoW). Worden gewoon genegeerd door heren Tweakers."
RX 480;2;0.4754992127418518;een verbubbeling in geluid klopt niet helemaal... bij een verdubbeling van de geluidsbron (2 kaarten ipv 1) stijgt het geluidsniveau met ong 3 dba. dus voor het menselijk gehoor stijgt het geluidsniveau met 3 dba met 2 kaarten ipv 1 kaart.
RX 480;4;0.3261716961860657;Zie mijn reactie hierboven.
RX 480;5;0.5345973372459412;220 euro voor 290x of gtx970 prestaties. En ook nog super zuinig (zeker tov 290x) Dat is toch gewoon prima werk? Snap de negatieve berichten hier niet. Erg mooi kaartje. Doet me qua prijs/prestatie aan de oude 4800 series denken die ook erg fijn waren.
RX 480;5;0.5155418515205383;Super zuinig? Even zuinig als de NVIDIA kaarten van een generatie eerder is al super zuining?
RX 480;3;0.44861477613449097;Je hebt gelijk, super zuinig is niet het juiste woord. Ik bedoelde het meer ten opzichte van de energieslurpende 290x die hij qua prestaties evenaart.
RX 480;4;0.3322356045246124;En daarbij komt kijken dan kaarten van een voorgaande generatie op een groter process gebaseerd waren 28nm. De vergelijking met nvidias huidige kaarten is dus eerlijker. En dan loopt AMD toch wel enorm achter. De 1070 verstookt vrijwel hetzelfde maar is veel krachtiger.
RX 480;2;0.4377674162387848;Probleem is dat de verwachtingen hoger waren...
RX 480;3;0.4746291935443878;In welk opzicht? Zou een relatief zuinige midrange kaart worden met flinke prestatie winst t.o.v. de oudere generatie. Dat is toch ook gelukt? Je hebt nu high-end prestaties van de 290x voor midrange prijs en minder verbruik. In crossfire zit hij dicht tegen de 1080 aan en boven de 1070. Ik snap niet zo goed wat er nog meer verwacht was dan. Enige gekke is de winkelprijs die veel hoger is dan adviesprijs, maar dat zal zich snel rechttrekken lijkt me
RX 480;2;0.38863450288772583;Ik denk dat veel mensen hem dicht tegen de 980 aan hadden verwacht.
RX 480;2;0.5184482336044312;Probleem is voornamelijk dat de 8GB versies eerst zijn uitgekomen die een adviesprijs hadden van 260 euro en nu op veel plekken ruim 300 euro kosten en dat mensen zich daardoor bedrogen voelen. Als ze kijken bij Azerty zie ik bijvoorbeeld de MSI staan voor 279 euro wat ik op zich nog niet zo'n gekke prijs vind. Ik zit al even te wachten op deze kaarten aangezien ik ook na mijn HD4870's toe was aan een nieuwe kaart / nieuwe kaarten. Nu vraag ik me alleen af hoe goed dit gaat werken om drie schermen aan te sturen en of het met een losse kaart gaat lukken of dat het handig is om straks voor een CFX opstelling te gaan.
RX 480;5;0.3303685188293457;Eens.
RX 480;2;0.37825244665145874;Duurt het even of zijn er geen grafieken van de benchmarks?
RX 480;3;0.3280094265937805;productreview: AMD Radeon RX 480 8GB review door Foritain Hier zijn ze wel, gemeten door Foritain, for the time being.
RX 480;1;0.3114328384399414;Ik zie ook geen grafieken EDIT: nu wel
RX 480;1;0.3721686005592346;Die hadden even een schopje nodig
RX 480;2;0.4038355350494385;Waarom worden bij het benchmarken van 1 losse videokaart ook de SLI/Crossfire opstellingen samen in 1 grafiek gegooid ? Dat de TI en OC versies wellicht meegenomen worden is op zich nog wel te begrijpen aangezien het gaat om 1 videokaart maar de vergelijking met 2 videokaarten geeft een warrig overzicht in de grafieken. (met name ook de kleur blauw/roze) Daarnaast denk ik (aanname) dat de meeste gebruikers een enkele videokaart hebben/draaien tov de SLI/CF opstelling. Of ligt dit aan mij ?
RX 480;3;0.4161337912082672;Omdat het een interessante vergelijking is, vooral bij deze RX 480. Twee RX 480's komen namelijk erg dicht in de buurt bij een GTX 1080 voor een fors lagere prijs.
RX 480;1;0.3854067921638489;Ligt eraan waar je naar kijkt. Qua latency / tijd (van game state update tot beeld op monitor) zijn CF & SLI gewoon een ramp.
RX 480;3;0.352278470993042;En zorgt dit dan voor input lag latency of een andere soort latency ?
RX 480;3;0.4895194470882416;Hogere output latency Afhankelijk van game en hoe je meet kan input latency ook hoger worden.
RX 480;2;0.4220563471317291;"Het is natuurlijk ook gewoon hetzelfde. Waar ""input latency"" om draait is het tijdsverschil tussen het doen van een actie en het zien van het resultaat. Als het resultaat later zichtbaar is (wat jij ""output latency"" noemt), dan heeft dat direct tot gevolg dat de ""input latency"" hoger is."
RX 480;3;0.39915165305137634;Je hebt ook nog de tijd tussen user input en het moment dat de server die input verwerkt in een MP game, volgens mij zit daar niet per se die output latency in, afhankelijk van de game engine.
RX 480;3;0.32223445177078247;Nou moet ik zeggen dat ik de term nooit heb gehoord in die context, maar in dat geval kan dat ook nog ja.
RX 480;4;0.2929629683494568;En ook in een SP game heb je die latency. Genoeg Quake spelers die 200+ fps beter vinden spelen alhoewel monitoren dat nog steeds niet weer kunnen geven.
RX 480;3;0.47514811158180237;Dat heeft meer met floating point afrondingen te maken, die een sweet spot bereiken bij bepaalde framerates waardoor je uiteindelijke snelheid gunstiger uitvalt.
RX 480;2;0.28635796904563904;En een fors hoger energieverbruik.
RX 480;3;0.5614786744117737;Ik vind het wel een reële keuze. De 480 RX is een heel stuk goedkoper dan bijv. de 1070, dus voor ongeveer hetzelfde geld kun je er twee van in het systeem zetten. Het is dan wel fijn om te weten hoe de verhouding is van twee 480 RX'en in crossfire vs 1 GTX 1070.
RX 480;2;0.3976045250892639;Moest CF consistent in elk game werken dan is dat nuttig ja maar dat is helemaal niet het geval.
RX 480;2;0.4422414302825928;Je bedoelt dan dat het niet werkt in DirectX 12. En is dat dan niet juist interessant om te weten, hoe goed hij het doet bij verschillende games? Nu kun je concluderen dat je er in DirectX 12 nog geen fluit aan hebt. Als hij niet was opgenomen in de benchmarks dan kon je die conclusie niet eens trekken en bleef je met vragen zitten.
RX 480;2;0.40034219622612;Ook. Maar het werkt ook niet in alle DX11 games.
RX 480;3;0.40842896699905396;Welke dan niet? Of doel je dan specifiek op Project Cars in 1920x1080? Want op hogere resoluties wordt het verschil weldegelijk relevant. En zie verder mijn aanvulling in mijn vorige reactie.
RX 480;1;0.313449889421463;Is dit een serieuze vraag? Of bedoel je welke werken wel?
RX 480;3;0.2753262519836426;Euh ja Ik weet niet of we nou volledig langs elkaar heen lullen of dat jij gewoon de grafieken niet snapt, maar volgens mij kunnen we concluderen dat het in álle DX 11 games zin heeft om de 480 RX te gebruiken in CrossFire.
RX 480;1;0.5443779826164246;Ik wist niet dat alle DX11 games getest waren? Jouw bewering dat alle DX11 games baat hebben bij een CF setup zal zelfs de AMD CEO niet durven herhalen.
RX 480;2;0.35086771845817566;Oh je hebt het over álle games . Nee inderdaad, je weet van tevoren niet of games het ondersteunen. Maar maakt dat benchmarks onzinnig?
RX 480;3;0.5749735832214355;Eh, de benchmarks zijn natuurlijk niet fout maar als CF/SLI slechts af en toe goed werkt kun je je wel afvragen of ze nuttig zijn ja.
RX 480;3;0.4600648581981659;Maar hier zijn de testen gevoerd in momenteel populaire games die de gpu's het vuur aan de schenen kunnen leggen. Dat er weinig verschil zal zijn met een CF/SLI opstelling in bv Prison Architect, weet het kleinste kind (al is die wel lekker om OC op cpu's te testen). Dus ja een CF test was relevant.
RX 480;3;0.31955161690711975;Ja, hoe doen twee RX480's het eigenlijk met DX12 in Rise of the Tomb Raider, .oisyn?
RX 480;3;0.46085453033447266;Inderdaad, speel je veel games die goed schalen in CF ben je veel goedkoper uit, dat ie wat singleplayer games minder is who cares..
RX 480;1;0.6758460402488708;Je kunt bij Azerty terecht voor 270 dus 540 totaal. Daarmee wordt het gat met de 1080 zo groot dat er niet om heen te kletsen is.
RX 480;2;0.47010520100593567;Toch heb ik het idee dat als het over CF en SLI gaat het vaak bij kletsen blijft. De meesten gebruiken het niet daadwerkelijk.
RX 480;3;0.533702552318573;Ik wel. Bevalt prima.
RX 480;3;0.48239967226982117;Daar ga ik dan ook vanuit. Het is niet bijzonder constructief om de daadwerkelijke prijzen op launch day aan te houden.
RX 480;4;0.39728355407714844;Lukt het nog om een score neer te zetten voor dual rx480’s in de steamvr benchmark? Leuke review en de cf schaling is impressive, ben benieuwd of alle dx12 games zo gaan schalen want dan veranderd de markt zeer snel.
RX 480;5;0.3815237879753113;Ik ga de benches even draaien.
RX 480;3;0.282886266708374;Super!! Edit: en thanks! Schaalt in de steamvr bench dus niet lekker naar 1080 niveau.
RX 480;3;0.3478843569755554;Heb de bench even voor je gedraaid. De RX480 zet een score neer van 6.6, de RX 480 in CrossFire doet 8.6.
RX 480;5;0.5188118815422058;Ben ik ook heel benieuwd naar. Zou top zijn als 480CF overal boven de 90 zit.
RX 480;3;0.5800284147262573;Grafiekjes werken niet? Voorderest valt het een beetje tegen kwa performance t.o.v. 290x/390x alleen verbruik is heel wat beter.
RX 480;2;0.35902705788612366;Eens, had gehoopt dat ingame prestaties on par zouden zijn met 390X en bijna op GTX 980 niveau..
RX 480;2;0.404293417930603;Ja vind het ook wat teleurstellend, hij zit tussen de Geforce GTX 970/980 in, en de Geforce GTX 970 is niet veel duurder, en zelfs net zo duur als de 8GB versie (ja weet de Geforce GTX 970 heeft maar 4GB), en dat de Geforce GTX 9** kaarten al weer +/- 2 jaar oud zijn, en dat Nvidia bijna met de Geforce GTX 1060 uit komt, en die hoogstwaarschijnlijk sneller zou wezen, en daar had de Radeon RX 480 mee moeten concurreren. En BTW waarom is de Energieverbruik systeem - Ingame niet met 2x Radeon RX 480 CrossFire gemeten, zie allen het van een Radeon RX 480????? Verders een mooie review.
RX 480;2;0.2767222821712494;De 980 is een duidelijke high end kaart. De 480 is mid range en is zo door AMD ook gepresenteerd. Als je sec kijkt naar de 980 dan valt de nieuwe AMD 'tegen'. Alleen dat is geen eerlijk vergelijk. Die kaart is 150 Euro duurder. Zelfs nog met de 1080 op de stoep... Nu weet ik ook wel dat (volgens geruchten) AMD claimde 980 prestaties te leveren voor $200. En als je kijkt naar de DX12 benchmark, klopt die claim ook. Kijk je naar de prestaties en de prijs, is de 480 een no brainer voor 1080p Gaming. Vsync aan en gaan. Steady 60 FPS op een dikke setting gaat prima af. En wat nog belangrijker is, Energie verbruik is dik, maar dan ook dik in orde. Over 3 weken ben ik jarig, en ik weet al wat ik mijzelf cadeau ga doen!
RX 480;3;0.42182791233062744;Dat zal best, maar die Geforce GTX 980 is +/- al weer 2 jaar oud, en je hebt een Geforce GTX 980 al voor €370,- Ik weet dat hij €150,- duurder is, maar hij is wel sneller in 99% van de spellen, en ja je kan 2x Radeon RX 480 kopen, maar dan verbruik je weer 135w meer, dus het is maar wat je wil. En een Geforce GTX 1080, is niet veel duurder dan 2x Radeon RX 480 8GB, een Radeon RX 480 8GB kost €269,-, en 2x dat is dan €538,-, en een Geforce GTX 1080 kost €677,50, en dan scheelt het nog maar €139,50, en dat de Geforce GTX 1080 in bijna alle spellen even wat sneller is.
RX 480;5;0.34529605507850647;Nu weer wel
RX 480;2;0.43417197465896606;Kritische noot: De GTX970 is alleen al zeker meer dan 1.5jaar op de markt... Dat AMD nu een passend antwoord heeft is niet meer dan logisch? Ik krijg een beetje het gevoel dat de efficiëntie voornamelijk uit het 14nm proces komt en niet zozeer de beter ontworpen GPU.
RX 480;5;0.4789137840270996;Voorzover ik weet is de tegenganger van de GTX970(en 980) altijd de Fury lijn geweest, beide op 28nm en gericht op highend. De RX 480 is een midrange kaart wat dus evengoed presteert als een GTX 970 uit het highendsegment.
RX 480;2;0.5460106730461121;Als je naar de prijs-/prestatietabel kijkt zie je dat de RX480 en GTX970 akelig dicht bij elkaar liggen. M.a.w. een GTX 970 kaart die als high end 2 jaar geleden verkocht werd is nu vergelijkbaar met een nieuwe mid-range kaart van AMD. Ik zie de GTX970 op dit moment niet als high end meer maar als mid-range met GTX980Ti, GTX1070 en GTX1080 als high end. Juist op het punt van prijs-/prestatieverhouding vind ik AMD teleurstellen.
RX 480;2;0.309152752161026;Die efficiëntie zie je alvast niet aan het stroomverbruik.
RX 480;3;0.4742409884929657;"""kan de RX 480 wel eens heel populair worden bij mensen die niet per se het snelste van het snelste willen"" Mensen die niet per se het snelste van het snelste willen, kiezen wel graag voor een iet of wat stillere kaart. Jammer dat deze nu net één vd luidruchtigste kaarten blijkt te zijn"
RX 480;3;0.31405705213546753;Dit is een referentie model. Aftermarket zal vast een stuk koeler/stillen zijn.
RX 480;5;0.296701043844223;Referentiemodellen zijn altijd luidruchtig.
RX 480;2;0.4406038820743561;"Waar ik wel benieuwd naar ben; Wat zijn de ontwikkelingen op het gebied van CrossFire? Ik heb 2 stuks R9 290's in mijn kastje hangen maar de games die echt goed werken in CF zijn op een hand te tellen. Mijn volgende stap (1x 1080, als die wat in prijs zakken) wordt dan ook weer een single kaart. Overigens heeft AMD me niet weggeblazen met deze nieuwe kaart. Voor onder € 100,- was het een prima midrange kaart geweest maar de prijzen die ze nu hanteren zijn absoluut niet realistisch."
RX 480;1;0.3779700994491577;Tsk. De 480 is net zo snel als jouw 290 en kost de helft bij introductie.
RX 480;3;0.4527716338634491;Het beeld bij CrossFire is hetzelfde als bij voorgaande series: als een game ondersteunt wordt, dan schaalt het prima. Maar die ondersteuning moet er wel zijn. DirectX 12 kan hier potentieel een oplossing bieden omdat het dan aan de gameprogrammeurs is om multi-gpu-opstellingen te ondersteunen en je dus geen driverprofielen meer nodig hebt. Het is alleen nog afwachten in hoeverre dit door gameontwikkelaars opgepakt gaat worden.
RX 480;5;0.45185622572898865;Ik heb meer en meer issues met de Crimson drivers dan ooit tevoren met men 290X kaartjes, gewoon verplicht gedowngrade terug naar de oude catalyst versies... Terry Makadon get your act together....
RX 480;3;0.36040636897087097;"Vergelijking prijs na 2 jaar met gemiddeld idle gebruik. Aanschaf GTX 970 OC - €280 4 uur per dag * 730 dagen = 2920 uur 71.8 * 2920 = 209.656 209.6 * 0.22 = 46.112 Eindprijs: €326 Eindprijs 24/7 On: €556.80 Aanschaf RX 480 - €260 4 uur per dag * 730 dagen = 2920 uur 78.3 * 2920 = 228.636 228.63 * 0.22 = €50.29 Eindprijs: €310.30 Eindprijs 24/7 On: €561.8 Punt wat ik hiermee maak: Beide kaarten zijn qua kosten gewoon gelijk. In vergelijking met de NVIDIA kaart heeft de AMD;Toch op sommige plekken wat minder performanceMeer hitteproductieMeer geluidsproductieMinder goede softwareMinder goede driver supportAl met al komt AMD dus twee jaar later met een minderwaardig product. Begrijp deze reactie niet verkeerd, ik ben geen ""NVIDIA Fanboy"". Als AMD met een betere kaart zou komen voor gunstigere prijzen zou ik direct overstappen."
RX 480;2;0.3932206630706787;De 4GB is voor 220 te koop de 8GB voor 270, voor een goede vergelijking zou je de 4GB moeten nemen die dus 60euro goedkoper is. Maar natuurlijk wil je meer geheugen dus neem je de 8GB. Plus vergeet je dat je met de RX480 meer ondersteuning hebt voor nieuwe technieken zoals hdmi 2.0b, HDR, HEVC en VRS. Maar blijkbaar koop jij liever oude auto's met oude technieken voor hetzelfde bedrag als een nieuwe. tenzij je natuurlijk een voorkeur hebt voor een bepaald merk En hoe kom je erbij dat AMD minder goede software en driver support heeft ?
RX 480;2;0.40372011065483093;Ben het grotendeels met je eens hoor behalve je laatste vraag. AMD HEEFT gewoon uber slechte drivers en dat is een feit (en Nvidia heeft UBER goede drivers). Waarom zeg ik dit? Omdat Nvidia kaarten meestal vanaf het begin TOP presteren terwijl AMD kaarten eerst een lange aanloop nodig hebben voordat ze TOP beginnen te presteren. Voordeel is dat AMD kaarten over langere periodes vaak BETER gaan presteren dan hun groene tegenhanger (bijv. 280x/7970 vs 770 en zelfs 780 en 290x/390 vs 970, 390x vs 980, alles dus behalve de 980ti wat gewoon een zieke kaart is). Wat ik hiermee wil zeggen is dat de 480 lange na niet TOP presteert (relatief gezien: het heeft meer in zijn mars). Kijk ook maar naar de driver release...pas vorige week (!!!) uit gekomen voor de reviewers, in den beginnen hadden de reviewers geeneens drivers voor hun kaart (480 draaide wel maar werd niet gezien als de 480). Een week voor launch pas komt AMD met hun eerst drivers voor de kaart. Dit betekent ook dat de drivers van de 480 'volwassener' gaan worden en dus beter gaan presteren. Kijk niet raar om als over een jaartje de 480 (ref) de 390x evenaart zou ik zeggen. Jammer dus dat dat gewoon niet vanaf het begin zo is alleen (zelfde geldt voor de 290x die de 780Ti nu wel evenaart/verslaat in huidige games). Dat is een grote verschil tussen drivers/ R&D van AMD en Nvidia. Ikzelf draai nog de 7950 (280 in Crimson) en hoewel de 480 een mooie upgrade zal zijn ga ik hem niet halen totdat de aftermarket coolers komen + de drivers sterk volwassener worden. Hell wie weet wacht ik wel tot Zen en kijk ik dan wat voor GPU zal passen bij mijn nieuwe build (Phenom x6 ftw lol).
RX 480;4;0.37243208289146423;Het feit dat AMD driver pas na een tijdje beter worden is omdat er veel Nvidia games worden gemaakt en daar AMD meestal geen optimalisatie voor de tijd kan doen. Dit zie je goed bij de game Hitman waar AMD een flinke voorsprong heeft omdat AMD daar wel zijn optimalisaties van te voren kan doen. Ook hebben veel Nvidia-only elementen veel invloed op de peformance bij AMD kaarten. Al met al zijn de drivers dus niet slecht, en werken gewoon naar behoren. Ik volg de release van de 480 al een tijdje en ik kan je vertellen dat de zogenoemde review drivers al wel langer in de omloop waren dan afgelopen week.
RX 480;3;0.4707038402557373;Je hebt zeker gelijk, hoewel het nog steeds deels waar is wat ik zeg. Kijk maar naar de 770 vs 280x (laatste die nu rond de 780 3gb presteert), waar de kepler kaarten gewoonweg gegimped worden door Nvidia bleef de 280x/7970 reeks groeien (volwassener wordende drivers). Het zal niet anders zijn dan met de 480 die nog steeds een geweldige potentie heeft (zie dit voor de architectuur van de kaart), wat dan 'jammer' is dat er met de ref. modellen gewoon niet meer uit wordt geperst, bij Nivida is dat vaak wel anders. Met mijn verhaal wil ik zeggen dat Nvidia kaarten bij de top begint vaak en AMD kaarten een aanloop nodig hebben en naast ref design heeft het dus te maken met driversupport. En ja, je ziet inderdaad bij AMD favored games (ala echte DX12 games) ze enorm goed presteren. Over de drivers vind ik het zelf nog steeds een raar verhaal. De reviewers hadden al een tijdje de kaarten voordat de juiste drivers gereleased werden. Dat vind ik maf (drivers horen allang al klaar te zijn, tenzij de R&D geen tijd/geld voor hadden).
RX 480;5;0.33901697397232056;Dat wat je aanhaalt voor de drivers dat ze die zo laat krijgen heeft te maken met de NDA afspraken als ik het goed heb. De drivers die ze eerst hadden waren met opzet lagere prestaties zodat er nog geen zinnige benchmarks etc. kunnen worden uitgeleverd.
RX 480;1;0.687711238861084;780Ti staat 7e en 290x staat 17e...gaat hem niet worden ook niet met drivers
RX 480;3;0.5084288716316223;Ik denk dat je even moet wachten op de aftermarket varianten van de RX480. Voor een wat eerlijker beeld.
RX 480;2;0.4875102937221527;"Deels mee eens. Zelfs met aftermarket versies zal de performance ongeveer gelijk staan, met aftermarket OC's zal het energieverbruik (fors) toenemen wat de prijs alleen maar meer maakt. De 970 is inmiddels al een tijd op de markt, gaat de prijs ook alleen nog maar verder zakken.. Ook zal het 2e hands aanbod toenemen en kan je binnen een paar maandjes een (jong)gebruikte 970 wel voor +-200 aanschaffen. Zou de RX480 voor €200 aangeboden worden dan zou ik zeggen 'Kijk, dat is een geweldige kaart voor een instap-gamesysteem"""
RX 480;3;0.46417105197906494;Ik denk dat de kaarten nog wel wat gaan zakken qua prijs. De 8GB versie staat nu voor €269 te koop bij Azerty in plaats van de €319 van Alternate. Het idle stroomverbruik is jammer genoeg wel hoger maar in-game is die weer lager. Uiteraard staat de kaart vaker idle dan onder load. Een gemiste kans van AMD. Dit neemt trouwens niet weg dat de 970 in de grafieken een overclockte aftermarket kaart is en de RX480 stock is. Misschien was het een goed idee om de 970 terug te clocken naar referentie waarden? Ik denk dat de 480 toch wel iets beter gaat presteren dan de 970. En als de 4GB versie rond de €200-225 gaat kosten dat het een erg goede deal zal zijn.
RX 480;2;0.3601576089859009;Dat is natuurlijk de vraag. Ik verwacht dat de aftermarkets van producenten nog wat duurder gaan worden zelfs. Je maakt natuurlijk ook goede punten hoor, begrijp me niet verkeerd Wat ik gehoopt had, was dat AMD nu eens met een echte NVIDIA killer zou komen waar de bang for buck zo goed was dat je eigenlijk niet eens meer zou moeten twijfelen tussen NVIDIA en AMD. Dit is op elk punt van de kaart het gewoon nét niet. Zou de 4G €170 kosten en de 8G €200 dan zou ik dit een geweldige low-budget kaart vinden.. Vooral met de GTX1060 die er binnenkort aan gaat komen en die volgens geruchten ook onder/net boven de €300 gaat zitten. Waarschijnlijk gaat die met performance deze AMD wel inhalen en zal dan direct met bang-for-buck alweer een beter koop zijn dan deze AMD. Maar dat is natuurlijk maar speculatie!
RX 480;2;0.3623201549053192;Tja je realiseert dat een maandje geleden de 970 nog 330+ euro kostte? Vergeleken met dat bedrag is 220 (voor de 4GB, waarom de 8GB vergelijken met de 970?) een stuk aantrekkelijker. Hell het is nog steeds aantrekkelijker dan de 280 euro wat nu gevraagd wordt voor een nieuwe 970. Clock voor clock is de 480 verder wel sneller dan de 970 (minus gameworks fratsen maar dat wordt weer gebalanseerd door async fratsen). De vraag rest nu alleen hoe ver hij over te clocken is met aftermarket designs (lees betere coolers + PCB). Als hij even goed over te clocken is als de 970 dan zou ik niet voor 30 - 40 euro minder een tweedehandse 970 aan raden (of even duur voor een nieuwe) die zijn top al bereikt heeft terwijl het common sense is dat de 480 vanaf nu alleen beter zal presteren (betere drivers, meer DX12 games etc.). Ja als je een 970 hebt is de 1070 (of tweedehandse 980ti) op dit moment je enige upgrades en is een 480 op zijn max een sidegrade. Heb je een oude kaart (270/950 of lager en voor sommige de 280x/380x/960) dan is de 480 vele malen beter dan de 970 zodra de prijzen genormaliseren en is het beste advies gewoon te wachten op de aftermarkets en tot de prijzen normaal zijn (of kijken wat de 1060 in zijn mars heeft). Als je NU een kaart wilt, koop dan gewoon de 480, nu de 970 kopen zal alleen maar tot spijt leiden (zeker wanneer nieuwe games async meer gebruiken, meer VRAM gaan gebruiken en de drivers van 480 volwassener worden). Meeste mensen doen jaren met een GPU, de 480 is dan gewoonweg de enige echte keuze voor nu als je onder de 300 wilt blijven.
RX 480;3;0.5927844643592834;Het is inderdaad een over ge-hypte kaart. Ik had er zelf ook wat meer van verwacht. Iets dichterbij de 980... Wat minder stroomverbruik want 14nm finfet etc etc... Ik denk dat ze een goede kaart hebben gebakken alleen dat de verwachtingen iets te hoog waren. Het is inderdaad geen Nvidia killer helaas. Met de 1060 wordt het pas helemaal interessant. Ik heb nu zelf een 950 maar hield de 480 wel goed in de gaten. Als Nvidia de 1060 ook wat lager positioneert om VR wat toegankelijker te maken kan het nog wel eens spannend worden.
RX 480;1;0.3144170045852661;Je bent gek als je nog een 970 koopt en helemaal een gebruikte ...
RX 480;1;0.4703948497772217;Of juist heel slim om een gebruikte 970 te kopen als je geen free sync hebt en gewoon je 1080p monitor wilt blijven gebruiken....hoe lager de winkel prijs des te lager de tweedehandse prijs. De 970 heeft momenteel één van de beste prijs-/prestatieverhouding, wie is er dan gek?
RX 480;3;0.2797287404537201;Miss moet je wat meer reviews lezen, de 970 wordt aan alle kanten verslagen door de 480...de beste prijs/prestatie verhouding yeah right, wat de gek geloofd zeker.
RX 480;2;0.32001739740371704;Lees de conclusies dan nog eens door van verschillende reviews, ja de 480 is in de meeste gevallen op full HD wat sneller maar een lichte OC is vergelijkbaar met de 480. Freesync, DX12 buiten beschouwing gelaten.
RX 480;3;0.3574540615081787;De meeste conclusies neem ik met een korreltje zout, ze zijn namelijk allemaal afhankelijk van sponsoring en advertisement en zullen daar ook rekening mee houden, vandaar de data vindt ik veel interessanter.
RX 480;1;0.6208682656288147;Real life game tests geven aan dat 480 op niveau zit van 970, dat zijn voor nu de feiten. Kan me voorstellen dat na driver optimalisaties de 480 kan uitlopen maar voor nu kan je niet stellen dat de 970 aan alle kanten verslagen wordt, pure onzin.
RX 480;1;0.4211793839931488;Er zijn spellen waar jou 970 maar 7 fps doet op 1440p, en de 480 dik 40 fps, dus ja pure waarheid.
RX 480;5;0.3016786575317383;En voorbeelden genoeg dat de 970 het stukken beter doet dan de 480, ook pure waarheid. Discussie is zinloos. Heb trouwens geen 970. Heb zelfs nog AMD 6870 in gebruik (ex CF opstelling), GTX 770 in game pc voor de kids en 980M in mijn laptop. Die GTX770 is aan vervanging toe, de 480, 1060 of goedkope tweedehandse is goed genoeg.
RX 480;1;0.5878562927246094;gezien de voorraad kan ik me niet voorstellen dat die eerste 2 winkel prijzen representatief gaan zijn, of lang stand zullen houden. edit: dat was snel, nu (uurtje later) voor 269 te krijgen in Nederland. 9 euro meer als AMD's advies prijs. niks aan de hand dus.
RX 480;2;0.4810170829296112;De prijzen komen op dit moment naar buiten en in Duitsland lijkt de kaart fors goedkoper dan hier. Ik verwacht dan ook niet dat de hoge prijzen in Nederland heel lang stand kunnen houden.
RX 480;5;0.6399286389350891;Heel fijn dit. Stel uitzuigers bij Alternate. Nu de 4GB nog. Zijn er ook unbranded kaarten van AMD zelf te koop? Zoals in jullie review?
RX 480;1;0.42298001050949097;Nee, de referencekaarten worden alleen uitgegeven voor reviews.
RX 480;5;0.37831079959869385;Gewoon een combi met de Tweakersexpress Zaterdag eerst de beurs en daarna de stad in en de kaart kopen
RX 480;2;0.504755973815918;helaas net niet goed genoeg voor mijn 2560x1440 behoefte. 1070 dan maar
RX 480;1;0.4343371093273163;dan maar een GPU die 300 euro meer kost (wat een vergelijking) En de 1070 heeft ook nog eens geen hardware matige ondersteuning voor asynchronous compute.
RX 480;1;0.41325363516807556;300 meer? weet niet waar je dat vandaan tovert MSRP RX480: $220 MSRP GTX1070: $ 375 volgens mij een verschil van $155 maar tel het gerust even na.
RX 480;2;0.4644872844219208;altijd nog steeds een stuk duurder, en dan ook nog zonder hardware matige ondersteuning voor asynchronous compute.. en de 1070 kost op dit moment 448 euro, zoals het er nu na uitziet kost de 480 bijna 200 euro minder.. (dus is dat het waard?)
RX 480;1;0.42231717705726624;oh wow +5%... die async haalt nu toch nog geen hol uit (behalve in AOTS benchmark game). en DX12 drivers staan ook nog steeds in de kinderschoenen. ik maak me niet druk iig. Ik koop een videokaart om NU goed mee te kunnen gamen, niet om over X jaar als async in alle games word toegepast (als dat uberhaupt al gaat gebeuren). Dan koop ik tegen die tijd wel een AMD kaart als dat al nodig is. Wie weet heeft Nvidia tegen die tijd ook een hardwarematige async ondersteuning.
RX 480;2;0.4530963599681854;Nvidia blijft gewoon overpriced, ongeacht hoe je het ook wilt goedpraten...
RX 480;2;0.39978712797164917;Volgens welke definitie van overpriced? Helaas heeft AMD nog steeds geen videokaarten die even goed preseteren als nV's high-end (voor een lagere prijs).
RX 480;5;0.6167376041412354;Een dual RX 480 is sneller en goedkoper als een GTX1080.
RX 480;2;0.47298353910446167;En werkt niet op een flink deel van de games
RX 480;3;0.2666398286819458;Wat maakt dat nu uit als je niet op zoek bent naar een top-end-kaart?
RX 480;2;0.44279566407203674;Voor AMD zou het een stuk gezonder zijn als ze wel lekker konden meespelen op high-end gebied, zowel qua GPUs als CPUs. Voor het kopen van een enkele videokaart maakt het nu niet zoveel uit. Als er geen AMD meer is betaal je daar echter flink meer voor.
RX 480;2;0.4391838312149048;Gezonder voor de reputatie bedoel je? Ik denk dat niet enkel AMD schuldig is aan de kwalijke reputatie die ze hebben bij het plebs. Ik snap trouwens niet wat een concurrent voor de GTX 1080 zou kunnen betekenen in de keuze van een kaart voor ongeveer €200. En in dat budget ligt de grootste markt, toch? Uiteraard. Dus wees slim en weiger de hegemonie van de grote bedrijven te sponsoren door de kleine concurrent te blijven kopen. Afin, voor CPU's kan je beter wachten tot Zen uit is.
RX 480;3;0.3654424846172333;Ik bedoel vooral gezonder voor de financiele situatie van AMD. Meer concurrentie, lagere prijzen, betere performance voor hetzelfde geld. Dus misschien ook een nog betere kaart voor 200 euro.
RX 480;2;0.3038652837276459;Dus indien de GTX 1080 zwaarder werd beconcurreerd zou die een lagere prijs hebben en zou alles daaronder ook naar beneden worden geduwd? Ik denk niet dat het allemaal zo dynamisch is, elk marktsegment wordt afzonderlijk ingedeeld en geprijsd adhv de waardige tegenstander van de concurrentie. Inderdaad, trouwens, dat volkomen concurrentie zonder vormen van concurrentievervalsing zou zorgen voor een speelveld dat gunstiger is voor de consument en brutaler voor de fabrikant, maar om eerlijk te spelen moet je wel met twee zijn.
RX 480;3;0.2501355707645416;Dat heb je als er geen concurrentie is.
RX 480;3;0.4285886883735657;En toch zijn er voorbeelden genoeg die hoog scoren als je kijkt naar prijs-prestatie.
RX 480;3;0.7291942238807678;Dit valt me toch wel een beetje tegen helaas, ik was zo gehyped voor deze kaart. Maar ik verwacht dat toekomstige drivers nog een redelijk verschil gaan maken.
RX 480;1;0.6276848912239075;Precies, overhypete gedoe altijd... Er is dus niks revolutionairs aan deze kaart. Zelfs als de prijzen op de adviesprijzen uitkomen dan krijg je gewoon weer dezelfde performance voor net een paar tientjes minder. Fijn, maar absoluut niet de grote opschudding van de markt die van tevoren verwacht werd. edit: Kijkt iedereen hier trouwens alleen maar plaatjes of zo? Ik heb het artikel aandachtig gelezen en ben 40 minuten verder, en dan zie je vervolgens dat iedereen een paar minuten na het liften van de embargo al zit te reageren.
RX 480;1;0.5126408934593201;Een paar tientjes? De GTX 970 kostte recent nog 320 euro dus dat scheelt bijna 100 euro. Dat de 970 ineens goedkoper wordt heeft een reden..
RX 480;2;0.38472819328308105;Klopt inderdaad, ik had alleen even naar de huidige prijzen gekeken en niet naar het prijsverloop. 100 euro is echter weer de andere kant op rekenen want op dit moment is alleen het 8 GB model beschikbaar die duurder is, en het is afwachten hoe zwaar de performance hit op het 4 GB model gaat zijn.
RX 480;1;0.8274960517883301;idd nog steeds geen vervanger voor mijn 7970 OC Het word of een 1070 dan maar als de prijs wat gezakt is of even wachten op de 490 die hopelijk wel een acceptabele volle 384 of beter nog 512 bit geheugen bus heeft en alle 2800 rekenunits ipv maar 2300. Het is overduidelijk dat deze kaart aan alle kanten geknepen word maar in huidige vorm echt zijn geld niet waard tenzij je nog niets heb of echt een bagger kaartje heb. Moet tegenwoordig alles van amd met een kilotje zout nemen. Deze kart is wat mij betreft een flop voor de serieuze gamer die graphics wil. Ook het hele VR geneuzel trap ik niet in. VR is helemaal niet interessant voor iedereen die een losse videokaart koopt en dus niets dan een wat mij betreft mislukt verkoop babbeltje. De echte nummers laten zien dat deze kaart niet mee kan en er voor bijna niemand reden tot upgraden is. Verslagen door de 290x uit 2013...
RX 480;1;0.671001672744751;"Elke keer dat er een nieuwe Radeonkaart uitkomt, bekijk ik eerst de staatjes met geluidsdruk. En elke keer, inclusief vandaag, denk ik: ""Bah, weer niet."" Teveel lawaai, zonde."
RX 480;1;0.28725865483283997;2 woorden, aftermarket kaarten
RX 480;1;0.9660235047340393;Slecht excuus! DAMn, het is toch belachelijk dat bedrijven het niet voor elkaar krijgen een fatsoenlijke stock koeler te ontwikkelen. Nvidia kan dit toch ook! Er werken toch geen halve zolen! Ondeugdelijk = ondeugdelijk.. Kijk naar de consumenten wet. AMD moet eens kappen met bagger koelers. Wachten op aftermarket koelers vind ik gewoon je reinste onzin. Gewoon boycotten die slechte producties.
RX 480;1;0.6996679306030273;Lees je wel eens wat je zelf schrijft en/of het artikel hier boven en eventueel extra informatie? Slecht excuus? Als je graag een extra stille kaart wilt zal je altijd een aftermarket kaart kopen, dit geld voor AMD maar ook voor Nvidia ( De kaart van AMD is niet eens merkbaar luider dan een reference GTX 1080 met vaper chamber of GTX 1070. Bagger koelers? Dit is natuurlijk meteen weer de reinste onzin, misschien zijn de koelers van AMD iets luider dan die van Nvidia, maar je koopt een AMD reference kaart ook niet op zijn stilte. Je koopt hem om de prijs/performance die hij levert voor de prijs die hij kost. Wat mij betreft kan Nvidia hier nog heel wat van leren voor we beginnen te zeiken over de koelers (let op: AMD heeft 200 euro om een koeler en een kaart te bouwen waar Nvidia in dit geval er 550 euro voor heeft) Als je perse een stille kaart wilt koop je gewoon een aftermarket kaart, en zo niet dan moet je niet zo zeuren hier over.
RX 480;2;0.4146966338157654;Uiteraard. Maar dan nog. Het verschil in geluidssterkte met de referencekaarten van Nvidia is enorm. Ik heb er een hard hoofd in of zelfs Asus & MSI de warmte van de 480 stil weg krijgen. Time will tell.
RX 480;1;0.7684530019760132;Onzin, op auto settings doet de RX 480 39.4 decibel en doet de EVGA GTX OC 1070 37.2 decibel (let op, dit is een reference kaart!) en een reference GTX 1080 met vaper chamber 38.5 decibel. Dit is zo'n klein verschil dat dit niet eens hoorbaar is en deze kaart is 500+ euro goedkoper... Het hele geluidsprobleem is dus sowieso onzin! Source:
RX 480;4;0.2785136103630066;Ik keek naar het staatje hier op Tweakers. Daar zag ik 54,3 decibel. En dat vind ik, voor een videokaart, irritant hard. Vandaar de opmerking. Het geeft geen pas de 480X te vergelijken met de 1070/1080. Die kaarten zijn van een heel andere orde. In hetzelfde prijssegment zijn Nvidia-kaarten veel stiller. Je gaf terecht aan dat het het afwachten waard is wat Asus & MSI hiermee gaan doen, mijn vraagtekens dat aangaande ten spijt. Er is geen enkele reden om je op te winden. Een fijne dag gewenst.
RX 480;2;0.36735644936561584;Ik wind me heus niet op hoor, ik maak me alleen op voor een leuke discussie ^^ Maar die 54,3 decibel zijn alleen onder 100% fan load (5200 RPM) die ze in real life nooit gaan bereiken (hoogste wat ze doen is 2600 RPM (50%)) in real life (gewoon fabrieksinstellingen) wat 39,4 decibel is. Dit is heel acceptabel voor een videokaart van dit kaliber met een reference koeler. Daarbij zijn dit geloof ik geen delta T decibel metingen voor sommige kaart en ofzo? (opnieuw kijk deze video eens, daar leggen ze een stuk beter uit:
RX 480;5;0.2832946479320526;Hey Tweakers, nogmaals mijn absolute complimenten voor het toevoegen van een VR-benchmark, dat zie ik nog niet op veel andere sites. Hebben jullie ergens al uitgelegd hoe jullie die benchmark doen? Ik kan me voorstellen dat hoofdbewegingen veel invloed hebben op de gevraagde rekenkracht en dus prestaties van een GPU en ik kan me dus voorstellen dat het lastige is om resultaten te reproduceren. Daarnaast lijkt een toevoeging van Elite Dangerous in VR mij erg waardevol in deze. Dit is toch een van de weinige full blown AAA games die nagenoeg perfect speelbaar is met een ton aan grafische instellingen waarmee je ook SLI 1080's mischien nog wel op de knieën brengt (voorzover FPS<90 Hz 'op de knieën brengen' is.
RX 480;3;0.561906099319458;Op dit moment testen we nog met een statische hoofdpositie, simpelweg omdat het beter te reproduceren is. Er gebeurt op dit moment echter genoeg in vr-benchmark-land en ik verwacht dat de testmethoden nog flink gaan veranderen de komende tijd
RX 480;3;0.5004778504371643;Aha, interessant. Misschien een keer een stagaire aannemen die een apperaatje bouwt waarmee de headset een aantal bewegingen maakt tijdens het meten. Maar zoals je al zegt, dit zijn veranderingen voor de toekomst, nogmaals kudo's voor het er nu al starten met VR benchmarks. Daaropingaand denk ik trouwens dat gezien het maar net aan is ik toch moet gaan uitwijken naar de veel duurdere 1070. Dat wordt samen met mijn Rift / Vive (ik ben er nog niet helemaal uit, maar ik wacht op de touch controllers) een flinke aanslag op de portemenee.
RX 480;3;0.47801366448402405;Toch niet echt de 980/980TI prestaties waarop iedereen van Team Red had gehoopt. Achja, wat verwacht je ook voor 240 euro.
RX 480;3;0.6369103789329529;Met een wat stevigere overklok verwacht ik nog wel wat verbetering
RX 480;5;0.3981439471244812;Mwah, eerst zien, dan geloven
RX 480;4;0.5376733541488647;Lekker uitgebreide review heren, CF -en- VR, smexy! Er was wat kritiek rond de review van de 1000-serie, maar het mag van mij dan ook gezegd worden wanneer een review snel en zeer uitgebreid is. Meer van dit aub
RX 480;4;0.4490487575531006;Leuk dat jij ook meteen van de partij bent En thx, we luisteren wel degelijk
RX 480;2;0.3835444450378418;Best wel tof toch om als halve zool een beetje mee te mogen zwemmen met grote visjes. Hopelijk hebben al die hongerige gamers er wat aan om wat extra leesvoer te hebben, wat verschillende games en kaarten in vergelijkingen, kunnen we enkel beter van worden als collectief nerd lijkt mij. Mbt het luisteren, dat wisten wij natuurlijk wel he, ook hoe absurd veel tijd er in dingen als dit gaat zitten. Kan mij ook goed voorstellen dat jij een reviewer spontaan een dag kwijt raakt als nieuwe GPU's en VR brillen binnenkomen Minpunt mbt de review: gebrek aan VR selfies. Give.
RX 480;3;0.3037198483943939;Nu snap ik dat het voor de meeste mensen niet belangrijk is, maar ik zou toch graag ook wat benchmarks gezien hebben van hoe het performed in Linux, met gebruik van de nieuwe AMDGPU drivers. Als een Linux gamer zou ik dit heel interessant vinden. Gelukkig heeft Phoronix hele uitgebreide benchmarks gedaan.
RX 480;3;0.4502258002758026;Voor de geïnteresseerden, hier de link naar de Phoronix review: Inderdaad een knappe prestatie en mooi van de open source drivers.
RX 480;3;0.4348450005054474;Ik was niet helemaal zeker of ik mocht linken naar andere nieuwssites
RX 480;2;0.4101566672325134;Reality check: Dus geen GTX980 prestaties aan 200 euro maar stuivertje wisselen met de GTX 970 (2 jaar oude kaart) aan 315euro (alternate.be). Dus zelfde prijs als de 2 jaar oude GTX970 met gelijkaardige prestaties en met een belachelijk klein overclock potentie ? Die 8GB blijkt niet eens een verschil te maken tov 4GB. GTX970 lijkt me dan nog steeds de betere koop , met een beetje overclock haal je bijna GTX980 prestaties. *not impressed*
RX 480;2;0.4203048348426819;tot je dx12/vulkan gebruikt en de 970 nergens meer is. verder verbruik de 970 meer en met 280 euro voor een 970 is hij bijna 30% duurder als het model met 4GB (De 970 heeft immers ook maar 4GB.. of.3.5).
RX 480;1;0.5972548723220825;Dit geloof je toch zelf niet. Not nader order is er nog nergens bewijs.
RX 480;1;0.582146942615509;ik had je reactie verkeerd begrepen, ik dacht dat 315 op de prijs van de 970 sloeg. reactie aangepast. maar toen je poste was de 480 al te krijgen voor 269 euro. dus nauwelijks meer als de advies prijs en 10 euro minder als een 970's die maar 4GB heeft.
RX 480;1;0.41516757011413574;Je kletst, als de 4GB voor 220 komt is ie mooi honderd minder dan die 970 van je. Die dan overigens al van de markt gehaald zal zijn.
RX 480;1;0.49917417764663696;De 970 staat al voor 260 in de PW hoor.
RX 480;3;0.3445771634578705;Nog wel, als NV over week de 1060 aankondigt zal ie heel snel weg zijn.
RX 480;1;0.5523875951766968;Waarom wordt VR getest alleen op een Oculus Rift getest met een Nvidia gameworks title zoals Project cars? Zie bron: En is er gekeken separate OC van elke kaart? Want volgens Linus Tech + andere reviewers op Youtube is de kaart te OCen over de 1300 mhz. En er is zelfs 1 persoon die 1.6+ heeft gehaald met zijn Rx 480. Zie bronnen: Rx 480 1.6 bron: Ik zie ook al prijzen van <280 op Tweakers komen van de kaart en er wordt gesproken over 300+?? Ik krijg lichtelijk het gevoel alsof Tweakers bias is. Kunnen we andere VR tests krijgen op de ViVe en andere VR games??
RX 480;3;0.3664577901363373;Toen Project Cars net uitkwam draaide het inderdaad veel beter op Nvidia- dan AMD-kaarten, maar tegenwoordig is dat niet meer zo. Net als bij veel andere benchmarks zie je bij Project Cars dat de RX 480 nipt sneller is dan de GTX 970. Het is dus een prima game om vr mee te testen, zeker ook omdat er een simulatiemodus aanwezig is waarbij de AI de auto bestuurt. Dat maakt het makkelijk om representatieve, reproduceerbare tests te doen. Overklokken hebben we vziw alleen geprobeerd met het reference design, niet met de kaart van Gigabyte. Die kregen we pas flink later binnen en is daarom eigenlijk continu gebruikt om alle CrossFire-benchmark op tijd af te krijgen. Wat betreft de prijzen. Toen deze review online ging lagen ze boven de €300 en dan hebben we dan ook opgeschreven. In de laatste anderhalf uur zijn de prijzen flink aan het schommelen geweest en ze komen nu gelukkig naar beneden. Dat is overigens ook precies waar we op hoopten zoals je in de conclusie kunt lezen
RX 480;4;0.46517011523246765;Yup Thanks voor de snelle reactie! Fijn om te weten dat jullie de reacties zo meelezen . Stel dat de prijzen stabiliseren rond de 240-290, zouden jullie je eigen conclusie aanpassen? Omdat de prijs/performance heel anders is. En komt er later tests van 3rd party companies? en overclocken etc?
RX 480;5;0.5236294865608215;Gezien het feit dat de kaarten in Duitsland nu al €219/269 kosten, en we in de EU vrij verkeer van goederen hebben, denk ik dat de Nederlandse prijzen ook snel die kan opgaan. En voor dat geld levert de 480 echt een hele leuke prijs-prestatieverhouding.
RX 480;3;0.3644356429576874;de prijzen zijn al wat omlaag gegaan op Azerty. 219 voor de 4 gb versie, 269 voor de 8 gb(sapphire versie)
RX 480;2;0.6162698864936829;valt tegen, hopelijk kunnen betere drivers en aftermarket kaarten met meer stroomkabels nog wat meer uit de chip halen.
RX 480;2;0.40580007433891296;Hoezo valt tegen? No offense, maar als je voor €220 betere prestaties krijt dan een 970 en een 290x die wel ouder zijn, maar ook fors duurder, is hier toch weinig mis mee?
RX 480;3;0.3680187463760376;Precies! Krijg een beetje het idee dat er een hoop gtx 970 gebruikers de prestaties van deze kaart proberen omlaag te praten. Ik snap dat het een tegenvaller is wanneer je net een 970 hebt en deze ineens de helft in waarde is gedaald.
RX 480;1;0.7080180644989014;970's zijn al ruim een maand voor onder de €300,- te koop... en de 8GB rx 480 doet op dit moment €320,-.
RX 480;1;0.5102885365486145;Ja en dan komt over 1 jaar alleen maar dx12 games uit en ga je huilen omdat je met een 970 opgescheept ziet terwijl je voor het zelfde en binnenkort zelfs minder geld 980+ fps had kunnen krijgen. En nvidia is nooit fan geweest om lang optimalisatie voor games toe te passen op hun andere modelen. Dus verwacht niet dat de 970 nog lang optimalisatie krijgt in nieuwere games.
RX 480;1;0.49793609976768494;Wie iets zoekt in dit segment en slim is, wacht gewoon nog een maand. Dan zijn de prijzen wat gedaald, dan zijn er AIB RX480's, en dan heeft ook Nvidia de 1060 uit zodat je generatie tegen generatie kan vergelijken. Wie weet is een AIB RX480 dan nog een zeer goeie deal, of niet. Binnen een maand kom je sowieso niet bedrogen uit. Dan liggen alle kaarten op tafel. Nu blijft het gokken.
RX 480;2;0.43831750750541687;En de 8gb 970? Precies. De prijzen zullen snel gaan dalen en ik kijk persoonlijk niet naar de 8gb maar naar de 4gb.
RX 480;2;0.4462375044822693;Waarom zou ik een 4GB 970 niet kunnen vergelijken met een 8GB RX480, prestaties zijn hetzelfde volgens de benchmarks.En als ik kijk naar de prijsontwikkeling van de 1070 en 1080 dan dalen die prijzen zeker niet snel... Misschien met een wat minder rode en groene bril naar zaken kijken en gewoon de feiten behandelen...
RX 480;3;0.33153489232063293;De GTX 970 lijkt mij de meest logische vergelijking juist. Dat is tenslotte het enige alternatief in die prijsklasse (nu althans wel) waar je een beetje mee kan meten. Azerty heeft overigens al RX 480's gelist voor 269 euro. In dat geval ligt de keuze voor een RX 480 8GB, mede dankzij het hebben van die 8GB, maar ook gezien de directe vergelijking van de prestaties, wel redelijk voor de hand. Het concept 'feit' is wat vloeiend de eerste uren en dagen na een release edit: (link)
RX 480;3;0.4477660655975342;"Wellicht ook belangrijk; deze release is een stukje jonger. Performance van een kaart waar spellen al sinds eind 2014 op geoptimaliseerd worden versus een verse release. Ik denk dat we nog een paar procentjes performance increase kunnen verwachten van de RX480 in de komende maanden. Daarnaast verslaat de kaart de vergelijkbare maar oudere kaarten in de meeste DX12 spellen vanwege de nieuwe architectuur. Ik ben erg benieuwd hoe de benchmarks er over een paar maanden uit zullen zien."
RX 480;3;0.4762446880340576;"De rx 480 zal naar verwachting veel beter leverbaar zijn vandaar dat hij ook sneller zal dalen in prijs. Kwestie van meer aanbod. Ik maak trouwens gebruik van zowel ""groene"" als ""rode"" kaarten op dit moment dus heb 2 brillen op."
RX 480;5;0.3599788248538971;Je hebt een bril op met een rood glas en een groen glas. Daarmee zie je dus 3D.
RX 480;2;0.41803309321403503;Alleen relevant voor mensen die nu meteen een kaart kopen. Al de rest koopt over een maand aan een veel lagere prijs.
RX 480;1;0.6499994993209839;Nee hoor, 270 bij Azerty.
RX 480;2;0.3675813376903534;De eerste dag dat er prijzen bekend zijn, zegt niet zoveel over de uiteindelijke prijs. GTX 970 voorraden moeten op, dus worden deze gedumpt.
RX 480;2;0.4934919476509094;Daar lijkt helaas wel op. En zolang er nog gtx970 voorraad is houdt shops als alternate de prijs nog even kunstmatig hoog. Ikzelf hoopte binnenkort voor een prikkie een 970 op de kop te tikken.
RX 480;2;0.5852359533309937;Hier is de 8 GB versie getest he, zelfs als de adviesprijzen straks overeen komen met de straatprijzen, dan is dat maar een paar tientjes lager dan de 970. Daarmee heb je procentueel bij de hier geteste games net ietsje meer performance, maar wel met meer herrie en idle verbruik. Of de 4 GB versie net zo goed presteert dat moeten we ook maar weer even afwachten, er is vast een reden waarom AMD alleen 8 GB modellen uitgestuurd heeft. Verder valt het vooral tegen omdat er van tevoren zoveel beloofd werd (misschien niet door AMD zelf maar wel door al die fanboys die momenteel zo druk elk kritisch geluid zitten te minnen). Deze kaarten zouden de markt eens flink gaan opschudden en dat gaat zo te zien gewoon echt niet gebeuren. Je kunt misschien een paar tientjes uitsparen bij je aanschaf, nou, joepie de poepie...
RX 480;2;0.4884440302848816;Volgensmij komen de kleinere varianten later (net zoals de andere RX-modellen). Voor lagere modellen worden meestal kaarten gebruikt die de prestaties niet voldoende halen. Zelfde verhaal als met processor modellen, allemaal de zelfde chip, maar sommige cores werken niet, of zijn uitgeschakeld. Je kan ze dan nu wel handmatig uitschakelen, maar dan gooi je geld weg, als je straks een voorraad mindere kaarten hebt.
RX 480;5;0.2713560163974762;"';"
RX 480;1;0.6543718576431274;319 eu kost deze bij alternate wat gewoon veel te duur is voor deze kaart.
RX 480;1;0.3960149884223938;wacht dan gewoon even...
RX 480;1;0.46522149443626404;Ga voor 270 naar Azerty.
RX 480;2;0.562216579914093;de prijs is prima, maar het is niet fantastisch. het is nu een normale vooruitgang in plaats van de enorme stap in prijs/performance die zoveel gehyped werd. ik ben dus teleurgesteld door alle hype eromheen.
RX 480;3;0.295414537191391;Het is vooral interessant als je op 1080 gamed en laag budget zit en nu geen vergelijkbare kaart hebt.
RX 480;3;0.5526706576347351;Het gaat hier wel over een 14 nm kaart. De kaart zou veel beter moeten presteren aangezien twee generaties produktieprocede's worden overgeslagen. Het komt deels doordat de gpu veel lager geklokt is dan de nVidia gpu's.
RX 480;1;0.6367281675338745;€320,- op dit moment en dus duurder dan een 970 die nog maar €280,- kost over het algemeen.
RX 480;1;0.5501243472099304;in frankrijk al 260. In nederland zijn ze duur ja,.(net zoals de gtx-1070). Wacht een maandje(mischien zelfs enkele dagen) en hij is ook gewoon rond de 200 te koop.
RX 480;1;0.4176091253757477;Waar? We gaan net een systeem samenstellen en we wonen in Frankrijk vandaar de vraag...
RX 480;1;0.41509121656417847;Hoe betrouwbaar de webshop is weet ik niet. credits gaan naar Blue_Fear in 'nieuws: AMD-website bevestigt komst Radeon RX 490 voor eind 2016'
RX 480;1;0.6434001922607422;Ja, hoog boven de advies de advies prijs, ze proberen eerst even al je zakken leeg te halen en ze willen eest eerst van de GTX 970 af om dat ze die nog in stok hebben. Maar echt goedkoop zal hij hier in Nederland niet worden,. Nederland is altijd al duur geweest met computer componenten.
RX 480;3;0.5806248188018799;Qua prijs valt het niet tegen, maar hij presteert niet veel beter dan de inmiddels een jaar oude 970 die bijna gelijk geprijsd is. De 1070gtx is misschien 2x duurder maar is bijna net zo snel als een crossfire opstelling ati, verbruikt aanzienlijk minder 280 vs 480watt en is ook aanzienlijk stiller 36 vs 57,3(54,3 enkele +3 voor twee kaarten) db(a). Wil je geen 500€ uitgeven aan de 1070 heb je nog de 970 die voor 1080p voorlopig zeker nog voldoende is (en tweede hands goedkoper is). Nu zal de 490 binnenkort geshowed worden maar die zal geen 300€ kosten en zal misschien in de buurt van de 1070 komen qua prestaties maar die komt pas uit eind van het jaar of begin volgend jaar dan is de prijs van de 1070 al gezakt naar de 450/400€ staat nvidia al bijna klaar op de 1170/1070ti en 1180/1080ti op de markt te brengen.
RX 480;4;0.5732028484344482;Een hele mooie kaart. Wel jammer dat hij toch minder is geoptimaliseerd voor vr dan er eerst werkelijk bekend werd gemaakt. Toch blijft dit een hele mooie kaart, zeker vanwege de prijs, en aangezien je de minder grafisch hoogstaande games gewoon kunt spelen in vr.
RX 480;1;0.5610091090202332;Kan je hem in CF niet voor VR gebruiken? Wordt niks over gezegd helaas..
RX 480;3;0.6904698014259338;Hmmmm vind het een beetje tegenvallen. Ik wacht wel tot de 980 Ti in prijs is gedaald zodra de 1070 en 1080 goed verkrijgbaar zijn.
RX 480;2;0.4119115173816681;dat gebeurt niet, die wordt gediscontinued als de prijs te hard gaat zakken, gebeurt standaard met vorige generatie kaarten om al te grote 'koopjes' te voorkomen.
RX 480;3;0.4396251440048218;Per saldo, het is een gelijkwaardige aan de 290X, zeker als je dus FullHD blijft. Heb je enkel als meerwaarde dus de HDMI2.0 en HDR, dat voor wel een aanzienlijk lagere prijs. (al mis ik in de verdere vergelijkingen verbruik etc van de 290x).
RX 480;1;0.42610862851142883;Zie hier de vergelijkingen van het verbruik van een 390X. De 290X en de 390X verschillen nauwelijks van elkaar, behalve het geheugen.
RX 480;1;0.43798205256462097;Waarom is de 8gb versie 260 euro terwijll hij in amerika 239 dollar is klinkt niet echt logisch
RX 480;1;0.8178097009658813;Importeer het eens dan? ben je zo 300 euro kwijt en je vergeet ook nog eens de BTW die je in bijna elke staat moet betalen.. (prijzen in de VS zijn altijd zonder BTW, dat betekent niet dat je geen BTW hoeft te betalen.)
RX 480;5;0.48258882761001587;Klopt, vergeet iedereen..
RX 480;1;0.5271527171134949;ik vroeg het me gewoon af denk je dat de prijzen zullen droppen?
RX 480;3;0.48198574781417847;Hm, ik had eerlijk gezegd iets betere resultaten verwacht. Hopelijk dalen de prijzen iets, want nu is het niet de instant buy wat ik had verwacht/gehoopt. edit: toch wel voor 269 te koop her en der, dan is het toch wel een goede koop/
RX 480;1;0.3983067572116852;Helemaal als de 4GB voor 220 binnenrolt.
RX 480;1;0.4746240973472595;Zitten op al deze 480x kaarten zo'n lelijke koeler of moeten de custom kaarten nog komen?
RX 480;3;0.3081052601337433;rx480* em dit is alleen het reference model de aftermarket versies moeten nog komen
RX 480;2;0.2834108769893646;Pas aan de plaat? Dacht het niet! Een kaart waarvoor je overduidelijk geen small loan of a million dollars betaald zoals bij het groene kamp aan de andere kant, maar toch je performance van krijgt die je nodig hebt! Veel te vroeg de loftrompet gespeeld? Waar is dit in hemelsnaam op gebaseerd, AMD beloofde een kaart waarmee je VR zou kunnen gamen en die niet meer dan 200 DOLLAR zou kosten. Dit is ze gelukt en ik zie op het moment geen kaart bij het groene kamp waarover we het zelfde kunnen zeggen. Prijs anders? Ze gaven aan 200 DOLLAR en wat is de prijs? 200 Dollar, of met belasting en invoerheffing 220 euro... Dat is accurater dan die 350 euro voor de GTX 1070, als ik me het goed herinner? AMD nogsteeds tweede plaats? Hangt er vanaf hoe je er naar kijkt, qua prijs/performance hebben ze nu de beste kaart en volgens mij is de R9 295x2 in quad crossfire ook nog niet verslagen (vs de max van dual sli van de 1080)? Ik denk dat ik in de achtergrond Nvidia hoor afdruipen Maar voor de Nvidia fans, veel succes met jullie nieuwe kaarten
RX 480;3;0.40723031759262085;Precies, het is gewoon een nette kaart voor een nette prijs, maar de prijs moet teveel oplopen- dan is het voordeel wel weg. Vraag mij wel af, of deze kaart met al dit HDMI en DP geweld, ook futureproof is mbt de 8K-schermen, in de toekomst. Kan deze kaart ook via twee DP-kabels, een 8K-scherm aansturen...?
RX 480;2;0.5551134943962097;Ik zou het niet weten, ik vond het ook erg jammer om te zien dat er geen dvi(-d) aansluiting meer op zit... Mijn beeldscherm heeft namelijk gewoon nog VGA of DVI...
RX 480;2;0.34705135226249695;"DVI en HDMI is qua signaal toch hetzelfde? Enkel de vorm van de aansluiting verschilt maar verder zijn ze identiek als je louter naar het beeld-signaal kijkt. Je hebt idd wel geen audio bij DVI en ik weet ook niet hoe HDCP er op zou reageren. => Gewoon zo een 'omzetstukje' van een paar euro en je kan verder, of een kabel kopen met aan ene kant DVI en aan andere kant HDMI; ik heb er zo ook eentje in gebruik en dat werkt perfect. (PC = HDMI, monitor = DVI)"
RX 480;2;0.40711480379104614;Ja dat is het probleem ook helemaal niet, ik vind het gewoon jammer dat ik nu zo'n omzet stukje moet kopen (zou me alsnog niet tegenhouden om deze kaart te kopen). Maar dvi heeft idd geen geluid er bij, dvi-d wel, dus ik denk dat ze gewoon hdmi naar dvi-d stekkertjes verkopen waar dan ook dvi in past.
RX 480;1;0.4096704423427582;Waarom zou de prijs trouwens oplopen? Ze staan nu al in de pricewatch voor (misschien 9 euro meer) de prijs die hier boven genoemd wordt voor de kaarten... Zolang AMD gewoon zijn voorraden hoog houdt en dus niet Nvidia achter na gaat met hun 1000 series (waar ik alle vertrouwen in heb) zou ik niet zien waar het fout zou gaan met de prijs? Zeker als de aftermarket kaarten er bij komen! ^^
RX 480;2;0.4565809369087219;Hmmm..omdat de meeste aftermarket kaarten al duurder worden, volgens tweakers (>319 euro voor Asus bijv.) en bij Azerty hebben zij nog weleens dat zij bij intro de adviesprijs aanhouden en daarna omhoog schieten, dat zag ik bij een Dell P2415Q monitor ook en een Kyocera kleuren laserprinter (beiden aangeschaft en werden later een stuk duurder!) Ik vind 269 euro eigenlijk net aan netjes, zelf ben ik toch wel benieuwd naar de 1060 van Nvidia, ik denk dat dit door met name Pascal (16nm-archi) een betere, maar vooral zuinigere kaart is. Ik lees in diverse reviews, dat de RX 480 bij een multi-mon setup, idle al 20-40 Watt extra trekt, dan bijv. de GT970 op 28nm. Dus Polaris of niet, ik vind het een mooi kaartje- maar sla even over. Wij staan nu aan het begin van nieuwe standaarden zoals hdmi 2.0 en DP1.4 +HDR etc. er komt nog veel meer aan mbt kaarten die dit supporten dadelijk. Als niet echt gamer wordt het dadelijk mogelijk interessanter om de 470 of zelfs 460 te kopen, die je dan ook makkelijker van een eigen koeling (of water-cooling) kunt voorzien, dan deze referentiekaart. Dus als je deze kaart wilt voor 269 euro, beter bestel je hem dan- ik denk dat de prijs nog wel omhoog gaat, als hij straks echt geleverd wordt door Azerty. Edit: daarnaast schijnt de RX 480 meer te trekken dan 150Watt (zelfs 166Watt max.) las ik bij full load, waarbij er meer dan 75Watt op je PCIe-slot getrokken wordt, wat je mobo niet ten goede komt, ook vwb de levensduur. Ik vind dit vrij slordig en al een goede reden, om deze kaart niet te nemen. Risico lopen, omdat men niet heeft nagedacht over een 8-pin aansluiting ipv 6-pins, mij niet gezien...
RX 480;3;0.26385682821273804;Als dat echt het geval is zal dat wel worden opgelost met de aftermarket kaarten. Maar ik ben het met je eens, ik draai nu een R7 260x en sla ook een ronde over, ik denk dat ze volgende ronde aangezien ze dan beter bekend zijn met hun 16nm process veel meer efficiëntie uit hun kaarten kunnen halen met de nieuwste technologie (ik wacht nogsteeds op HBM 2.0).
RX 480;1;0.5216090679168701;Voor VR is deze kaart dus niet te gebruiken, je zt meteen al aan de onderkant dus als het dadelijk ook nog aantrekt, dan kun je het wel vergeten...
RX 480;1;0.29804468154907227;Ach joh, je kan er vr mee gamen, als je perse de hoogste settings en nieuwste games wil zet je er gewoon nog één bij in crossfire, is alsnog goedkoper dan 1 1070 atm...
RX 480;5;0.6485689878463745;Performance/dollar is deze kaart de nieuwe koning
RX 480;3;0.6121423840522766;Goede kaart voor een goede prijs (over een paar maanden als die goed verkrijgbaar is), maar helaas niet de markverstorende factor waar sommigen, waaronder ikzelf, op hoopten.
RX 480;3;0.7105541825294495;Ok, dit zou voor mij een kaart kunnen zijn maar ik wacht nog wel even tot Nvidia met de 1060 komt om te zien hoe de verhoudingen dan liggen, iig is AMD weer relevant voor mij al hoop ik wel dat er betere koel opties komen van de OEM's.
RX 480;5;0.3158606290817261;Als de verkoopprijzen normaliseren is het gewoon een hele goede kaart voor het geld! Ik wacht even tot de hype wat gedaald is dan bestel ik er wel eentje Was tot vandaag aan het twijfelen om eventueel toch voor een tweedehands 970 te gaan, mits ik deze kon kopen voor max 175,-, maar nu de benchmarks daadwerkelijk bekend zijn hou ik het toch maar bij een nieuwe 480 voor iets meer dan 200 en hopelijk over een tijdje voor 199,-
RX 480;5;0.3437737822532654;Voor mij het moment om mijn hd6870 te gaan vervangen
RX 480;3;0.4853236675262451;ik heb de asus variant van de rx480 maar besteld, hopen op aardig wat leuke game uurtjes
RX 480;2;0.5907796621322632;Wat me eigenlijk het meest opvalt is het matige energie verbruik? (Even rondgeneusd en bij elke andere review is het energie verbruik vergelijkbaar met de 380x/970/1070 ingame. Misschien een foefje van AMD bij de 3D benchmarks?) Efficiëntie van het nieuwe ontwerp/architectuur valt dus best tegen.
RX 480;4;0.42519882321357727;Op Steam VR komt ie binnen op 6.8. Dat is voldoende voor VR. Niet het 390X level (7.8) waar sommigen op hadden gehoopt maar dat is toch boven de GTX 970 (6.5). Maar gezien de prijs/kwaliteit verhouding helemaal niet slecht.
RX 480;3;0.4980241358280182;Toch wel teleurstellend. Nauwelijks verbetering in prijs/performance in vergelijking met de oude gen =/
RX 480;3;0.4503571093082428;Wat ik in deze review mis, maar wat Anandtech wel opmerkt is het verschil in geheugen clock en bandbreedte tussen de 4GB en 8GB versie. De 8GB versie gebruikt sneller geheugen, waardoor er een memoy clock van 8GB/s gehaald kan worden t.o.v. 7GB/s bij de 4GB versie. In de benchmarks valt op te merken dat de 8GB versie hierdoor net even wat sneller is. Link naar de Preview van Anandtech:
RX 480;1;0.4471262991428375;4 stuks op voorraad bij alternate wel € 319,-
RX 480;3;0.48955968022346497;ik miss een beetje informatie over de warmte en informatie over geluid van crossfire 480?
RX 480;2;0.47273293137550354;Wellicht kan Tweakers nog toevoegen welke Nvidia kaart zij bedoelen die als echte concurrent geldt voor de videokaart van AMD. Ik de prijsprestatieindex zie ik alleen maar veel duurdere van Nvidia en bij de vergelijking van benchmarks staan ook alleen duurdere. Op was is de conclusie eigenlijk gebaseerd? Ik vind het maar een kromme conclusie die kant noch wal raakt zonder toevoeging van wat jullie suggereren (bijvoorbeeld de concurrent, als je dit zegt, benoem dan ook specifiek welk model jullie bedoelen als concurrent).
RX 480;2;0.5211001634597778;de prijs wat de winkels nu vragen is gewoon te hoog. 269 is ook al te hoog voor die kaart. voor mij gewoon 19 euros teveel. max 250 voor de 8gb. prestaties hier doen wel beter dan op ons ander nl buren. daar lag de prestaties nog onder de 970. komende maanden kunnen we de 1060 verwachten. en als deze zo presteerd als de 980 voor de prijs van een 970, dan lijkt dat meer bang for buck. we zullen zien wat nvidia gaat doen. voor amd advies prijs is het wel een hele erge concurent voor de 970. maar voor huidige winkel prijzen is het niet.
RX 480;2;0.3983212113380432;Pas op met Overclocken, het schijnt dat de RX 480 tegen de 100 Watt uit je PCI-E trekt. Niet zo gezond voor je Moederbord. Bron: Toms Hardware
RX 480;1;0.3432668149471283;Als zo'n pci slot 75 watt kan leveren, en in hun test gemiddeld 100, met een piek van 200, hoe is dat dan niet gefrituurd?
RX 480;3;0.8172390460968018;Prima kaart voor de prijs. Maar kwa performance vind ik het een beetje tegenvallen. Ze bereiken eigenlijk nu pas het zelfde niveau van efficiëntie wat Nvidia al twee jaar geleden had op een veel grotere die (28nm vs 14nm). De warmteproductie is ook hoog en het overclockpotentieel is niet om over naar huis te schrijven (dat is bij de Maxwell generatie kaarten wel anders). Het is voor mij wel bewijs dat GCN flink achterloopt op Pascal. Amd zal dus vooral moeten concureren op prijsniveau. Wat ze blijkbaar zelf ook doorhebben.
RX 480;3;0.4197053015232086;Prestaties zijn binnen verwachting. Tussen 970 en 980, met drivers en groeiende DX12 support wordt dat nog wel wat beter. Uitstekende prijs/prestatie voor adviesprijs. Jammer dat de Nederlandse winkels weer extreem waardeloos zijn. Lekker vangen met 30%+ boven adviesprijs. Ik mis AotS en dx12 in dit stukje. Bij de 1070/1080 werd dit nog extra aangestipt, waarom hier niet?
RX 480;2;0.40388378500938416;"Ik vind het eigenlijk een ""net niet"" geval, 1080p ultra lukt nipt op 60 fps voor pakweg witcher 3, prima denk je dan en ja op zich is dat ook wel zo maar dat betekent dat je bij de nieuwe grafisch intensieve titels die in 2017 uitkomen allicht onder die 60 fps gaat duiken. Met andere woorden goed voor nu maar futureproof not so much. In dat licht gezien vraag ik me af of je dan ook niet beter gewoon wat langer kan sparen en de 200 euro extra neerleggen voor de GTX 1070. Nu duurder, maar op langere termijn gezien goedkoper dan over 2 jaar nog eens te moeten upgraden."
RX 480;1;0.3868468403816223;Zie ik het nu verkeerd of is de rx 480 nu helemaal niet veel sneller, bij de dx 12 games, dan een gtx 970?
RX 480;2;0.5077466368675232;de 4gb versie presteerd aanzienlijk minder vergeleken met de 8gb versie. neem een kijkje bij anandtech. 4gb versie kan echt niet mee met de 970.
RX 480;5;0.7792120575904846;Wat een prachtkaart! Kun je nagaan hoe hoog de verwachtingen zijn van de RX 490
RX 480;2;0.49753469228744507;Het energieverbruik van deze kaart is niet wat ik ervan gehoopt had. Bij de review op tomshardware waren ze zeker niet positief over het verbruik, dit bij testen van de kaart zelf en niet het gehele systeem. De prijs is nog altijd een stuk beter dan die van de gtx 1070, maar ligt in de lijn van de gtx 970 (een oudere kaart, maar wel een waar ik goede ervaring mee heb). De reviews die ik vandaag dus gelezen heb maken het mij dus niet makkelijk om te kiezen welke kaart ik nu in mijn nieuwe pc wil. Mijn voorkeur gaat uit naar een gtx 1070, maar de prijs houdt me nu nog enigszins tegen. Moest die in de buurt van de introductieprijs van de gtx 970 zijn dan direct, maar 50% duurder dan die prijs is er toch iets over. De RX 480 is een goedkoop alternatief, maar voldoet niet echt aan mijn verwachtingen en nu nog een gtx 970 kopen lijkt me niet echt een goede future proof keuze. Het zal dus nog even wachten worden voor mij tot de prijzen van de nvidia kaarten dalen of op de custom modellen van de RX 480.
RX 480;4;0.6228209733963013;Mooi kaartje, blij dat er weer een concurrentieslag gaande is tussen Nvidia en AMD. Goed toekomstvoorbereid, met deze aansluitingen- nu maar hopen dat wij dit op meer kaarten terug gaan zien. Ben benieuwd of software zoals AutoCad etc. ook nog een beetje profijt heeft van deze Radeon GPU, ook als het geen CAD-kaart qua drivers. Belangrijke zaken zitten erop (HDMI en DP) in de laatste versie's, als het stroomverbruik en geluid ook te verdragen zijn, wordt het moeilijk kiezen tussen Nvidia's GT9xx of deze jongen.
RX 480;4;0.270643949508667;"Hier nog een uitgebreide review/benchmark: Vergeet trouwens niet dat dat we zeker AMD nodig hebben, anders had Nvidia nog niet eens de moeite genomen om concurrerende kaarten uit te brengen. Ik heb 1x een klein tijdperk meegemaakt waar het bijna alleen Nvidia was na de overname van Voodoo techniek (mooie tijden). De prijzen bleven toen zo duur, ook voor 3 jaar oudere kaarten betaalde je nog teveel. Ook was er bijna geen vooruitgang meer in de kaarten. Totdat ATI weer eindelijk met concurreerde kaarten op de markt kwam met hun fantastische ATI Radeon R200, waar ze zich weer konden meten met Nvidia, en later soms zelfs beter waren dan Nvidia. Nvidia werd weer wakker geschut en de prijzen gingen dalen, en er werd weer vooruitgang geboekt. Wij als consument werden er alleen weer beter van. Ook al ben je fan van groen, of rood. maakt niets uit, dat is je eigen keuze, maar willen toch allemaal dat we kwaliteit voor een goede prijs krijgen, en daar zorgt nu net concurrentie voor. Ook al is de ene of de andere net altijd iets beter of sneller, het zorgt er gewoon voor dat ze beide scherp moeten blijven. Voor de AMD RX480 kan ik alleen maar zeggen dat ik (als de ""advies"" prijzen kloppen). het een prima kaart vindt. Niets mis mee. Meeste reviews/benchmarks zijn zeer te spreken over deze RX480."
RX 480;3;0.44203320145606995;Ik had toch wel gehoopt dat de RX480 iets dichter bij de 1070 in de buurt zou komen, op zich, maar al met al is het een nette kaart, zeker voor die prijs. Nu nog maar afwachten wat de RX490 later in het jaar gaat brengen. offtopic: was ik de enige die bij WattMan aan Suske en Wiske moest denken?
RX 480;3;0.30274179577827454;Grappig om te lezen dat een 3D kaart die hetzelfde presteert als de 970 of 290X (beide al ca 2 jaar beschikbaar) hier in NL een slechte markt intro te wachten staat omdat wij er de hoofdprijs voor moeten bestalen t.o.v. andere landen. Dit zou een EUR 200,- (8GB) kaart moeten zijn omdat een CF opstelling (AMD6870 CF tijden herleven) dan voor veel mensen bereikbaar wordt die bij de meeste games zich met de topkaarten kan meten. Leuke kaart. ik wacht de concurrentie en prijsdaling af voordat mijn GTX770 vervangen gaat worden.
RX 480;1;0.4395853579044342;en zo scroll je dus door de comments... nadat je de review hebt doorlopen.. conclussie: 'haalt het net niet' een overklokte 970 kwa performance.. en dat voor een gloednieuwe 14 NM kaart. die gewoon 8 gb ram zou hebben.. eeuwige zonde dat AMD weer de boot mist op deze manier.. en ze wederom weer de 'budget boy' zijn totdat Nvidia met de 1060 komt en hun kaarten op alle fronten verslaat. dit kan liggen aan het feit dat alles voor CUDA en Nvidia related spullen is gemaakt. maar ze komen dus wederom met iets wat 'het net niet haalt' en dat zal met de 490 straks net zo zijn. spijtig. de prijsvechter uithangen kunnen ze als de beste.. maar als het op raw performance per watt aankomt is kamp groen toch nog altijd (zoals de afgelopen 10 jaar ofzo) ruler and king.
RX 480;4;0.4582747220993042;Ik ben benieuwd hoeveel extra performance er extra uit zal worden gehaald wanneer vergeleken wordt met drivers over pakweg 2 jaar. Ik zie deze kaart als een soort Radeon HD 7850 in 2012: destijds ook een uitstekende upper-midrange. De rebrand naar R7 265 (2014) en R7 370 (2015) vond ik een compliment voor hoe goed deze kaart is verjaard. Ik verwacht dat toekomstige games meer en beter gebruik zullen maken van de technieken die deze kaart heeft (maar afwezig zijn door onbreken ondersteuning in benchmarkscores van de populaire games van nu). Ik ben met name benieuwd hoe deze kaart zich dan zal verhouden tot de oudere GCN architectuur van de HD 7000 en R9 (200/300) serie. Die driver optimalisaties kunnen negatief opgevat worden: dat deze bij de introductie 'niet op peil' zou zijn, maar ik ervaar de prestatietoename positief, een extraatje!
RX 480;3;0.4665657579898834;Jullie vergeten wel allemaal dat dit de referentiekaart is he. Gezien de problemen met power draw (zie hier zou het zomaar kunnen dat de AIBs wel beter presteren en ook beter kunnen overklokken. Dus waarschijnlijk is het beter om nog even te wachten daarop voordat er harde conclusies worden getrokken. Dat gezegd hebbende, gezien de enorme hype had ik er wel iets meer van verwacht. Is dus niet worth om van een 390/970 hierop over te stappen, maar is wel interessant voor nieuwe klanten of degenen die een oude(re) GPU hebben.
RX 480;5;0.34217920899391174;Mijn conclusie is dan toch weer dat die AMD 290@290x kaart die ik ruim een jaar geleden heb aangeschaft voor 200 euro echt de beste prijs/prestatie heeft. Ik mag blij zijn met die keuze, de kaart kan nog goed mee en is gelijk aan de RX480 en kost minder. Hij mag dan wel wat energie slurpen, maar daar geef ik helemaal niets om.
RX 480;1;0.6102313995361328;Aargh damned! Heb 2 maand geleden mijn gtx980 verkocht voor €335 Nu zit ik gewoon op intel h530 in afwachting... Maar de 1070 kost nog +/- €500.... En een single Rx480 is niet beter als een Gtx980 Ik wil geen crossfire.. Dus is gtx1070. Enkel prijs die. Nog wat moet dalen. Ondanks verschil in kaarten vind ik het zo stom dat het ene spel dan weer beter is met nvidia en andere radeon... Waarom is dat eigenlijk en kunnen ze dit niet uitbalanceren? Is dit software matig?
RX 480;1;0.34911996126174927;een single RX480 is amper beter dan een 970....
RX 480;2;0.5498536229133606;Ligt aan de feautures die de kaarten hebben en de specificaties. Maar ook drivers, de game zelf. Sommige games zijn bijv. geoptimaliseerd voor nvidia, of amd. Dus dat is idd. software en hardware matig. Uitbalanceren kan in theorie, maar is niet echt praktisch. Ik denk dat voor de latere games die uitkomen amd de winst pakt met dx12-vulkan ter vergelijking met de 970.
RX 480;3;0.27008795738220215;Ligt het aan mij of schaalt CF met de RX480 zeer, zeer goed?
RX 480;1;0.3215733766555786;Klopt. CF is echt geen nadeel dat het prijsverschil met GTX 1080 goedpraat, zoals Nvidia boys doen geloven.
RX 480;5;0.5142457485198975;Bij de 390x gaat het al zeer prima. Haal 90fps piek 150 op 4k ultra settings bf4
RX 480;5;0.4834446609020233;Mijn HD7850s hebben ook bijna geen problemen, behalve bij games die sowieso slecht geport zijn (ac unity, cod advanced warfare)
RX 480;3;0.5286868810653687;Ja, de verwachting is dat dat straks nog veel meer games ondersteunen.
RX 480;3;0.29410725831985474;wanneer zullen ze beschikbaar zijn op webshops als azerty?
RX 480;5;0.6421511769294739;Ze zijn bij verschillende webshops al beschikbaar, en ook direct leverbaar!
RX 480;3;0.2944856584072113;webshops als?
RX 480;3;0.3025054335594177;Bijvoorbeeld Alternate en Afuture op dit moment. De komende uren zal op deze pagina een overzicht van kaarten met prijzen verschijnen.
RX 480;1;0.3477957844734192;ver boven de adviesprijs nog..
RX 480;1;0.46161261200904846;€9 boven de adviesprijs..
RX 480;3;0.3194814920425415;Die had ik nog niet gezien alternate moet er 319 voor hebben.
RX 480;3;0.4571060538291931;Klopt. Hopelijk dalen ze nog iets meer. En de aftermarket versies zullen er vast wel iets meer uitpersen.
RX 480;3;0.30757936835289;Alternate kan de pot op.
RX 480;3;0.2928212285041809;Mee eens sinds het begin kocht ik altijd bij Alternate maar voortaan naar Azerty. Alternate is allang niet meer de goedkoopste.
RX 480;3;0.2963181138038635;Klopt, valt mij ook op.
RX 480;1;0.24392539262771606;is er 1 van.
RX 480;1;0.5895128846168518;Bizar duur! 269.- op dit moment in DE ff wachten dus
RX 480;1;0.6534308791160583;Yep. Maar de hele voorraad in europa in bagger. Ook 269 is nog te duur. newegg heeft hem voor 239 dollar. Wat 15 dollar boven de advies prijs is die AMD hanteert. Wacht enkele weken en ze zijn hier ook 200-240 euro.
RX 480;3;0.35483112931251526;Ben ik de enige die deze kaarten vind tegenvallen? Had toch wel gehoopt dat ze high end zouden worden
RX 480;2;0.4023480713367462;"Het is de vervanger van de 380(x) dat was toch ook geen highend ? Ik zie vaak de reactie ""Het heeft AMD 2 jaar gekost om gelijk met de vorige generatie van Nvidia te komen 480=970"" voorbijkomen Hoewel het waar is dat de 480 ietsjes beter presteert dan de 970 is het toch een vertekend beeld dat de 480 daardoor ""slecht"" zou zijn. de 480 is een veel kleiner chip dan de 970 (232mm vs 398mm) De 970 is eigenlijk een highend part (gecutte 980) dus wat dat betreft is het volslagen logisch en knap dat ze zoveel uit een veel kleinere chip halen."
RX 480;4;0.3836270868778229;Mee eens, de 480 was nooit als high end kaart bedoelt, echter kan ik niet wachten tot ze wel een high end kaart op de markt brengen die de strijd aan kan met de 1080 (met meer performance voor de zelfde prijs of even veel performance voor minder geld)
RX 480;3;0.3604947030544281;Ik lees dat deze kaart vergelijkbaar is met de AMD R9 290X. De AMD R9 290X is toch weer vergelijkbaar met de XFX 7970 kaarten of mis ik nu iets?
RX 480;3;0.3770275115966797;Ik denk dat je in de war bent met de 280X. De 280X is namelijk net iets sneller dan de 7970 maar min of meer de zelfde kaart. De 290X is een stuk sneller en iets langzamer dan de 390X.
RX 480;4;0.38949936628341675;Dank! Ik was inderdaad in de war met de 280x en 290x serie. Dan is deze nieuwe 480 RX weldegelijk een interessante upgrade voor 280x / 7970 eigenaren. Toch (A)?
RX 480;3;0.27185893058776855;Jazeker! Ik zou nog wel even wachten tot de prijs daalt naar de vraagprijs.
RX 480;3;0.5296057462692261;Ja, maar vooral als je er direct twee neemt
RX 480;1;0.2876680791378021;Als mensen geïnteresseerd zijn in de kaart. Kun je deze uit Duitsland halen, hier is die 50 euro goedkoper dan in Nederland. 269,- is geen slechte prijs.
RX 480;1;0.49235832691192627;Hier zijn ze ook €269! Mits je hem bij Azerty koopt!
RX 480;2;0.40799978375434875;Guru3d komt een stuk verder met overklokken, en kan de voltage wel verhogen. Edit: De hele tweakers staff zit te down voten, omdat ze hebben gefaald met de voltage haha
RX 480;1;0.4840826988220215;Tweakers meld een prestatieverbetering van zo'n 10% bij overklokken. Guru3D ook. Dus hoezo komen ze een stuk verder?
RX 480;3;0.5415018796920776;Wat interessant is, is dat de overklok van 54MHz (4%) voor de gpu toch 10% extra prestaties oplevert. Met andere woorden, de overklok van het geheugen is dus grotendeels verantwoordelijk voor de 10% hogere prestaties. Maw, blijkbaar zit de gpu enigzins om bandbreedte verlegen.
RX 480;2;0.4898495078086853;Wel waarschijnlijk meer een ROP probleem. Ook al zijn de ROP's in theorie 40% verbeterd tegen de vorige generatie, AMD heeft beslist om deze mid-end kaart maar de helft mee te geven tegenover een 290-390. Hierdoor heb je inderdaad al snel een bottleneck. Ik vraag me dan ook af waar de hele 8GB hype vandaan komt. OK het geheugen is sneller geklokt maar al bij al zal je die 8GB nooit of te nimmer efficient kunnen gebruiken door een serieuse bottlenecks op je ROP's ..
RX 480;2;0.42869454622268677;De meerwaarde van 8GB is niet die hele 8GB. Juist als je over de 4GB gaat is het opeens extreem belangrijk. Liever te veel dan te weinig. Met dat gezegd te hebben met 6GB was ik ook blij geweest.
RX 480;1;0.4065715968608856;Klopt. Ook hardware.info meldt dit, en zij zien een prestatiewinst van 20% voornamelijk door de geheugen overclock.
RX 480;2;0.3543165326118469;En wat is je punt? Sommige kaarten overklokken nu eenmaal beter dan andere.
RX 480;1;0.41642919182777405;Ze hebben gefaald het voltage te verhogen.........
RX 480;2;0.4643734395503998;Spijtig. Indien de advies prijzen gevolgd worden, dan heb je voor prijs/prestatie een volwaardig alternatief voor de 1070, misschien zelfs een verbetering. Maar dan kom je weer in het oude verhaal dat watt/prestatie AMD dan weer heeeeeeeeeel ver achterop zit.
RX 480;1;0.5834506154060364;319 euro bij Alternate.... wordt even wachten voor mij dus
RX 480;5;0.40174761414527893;Ze zijn weer aan het graaien
RX 480;1;0.6309811472892761;Fuck Alternate, Azerty heeft 'm voor 269
RX 480;3;0.49987897276878357;De GTX 970 lijkt mij duidelijk de betere keuze, en stiekem had ik ook verwacht dat AMD een beest uit zo brengen zodat de prijs van de GTX 1000 serie omlaag zou gaan. Het is nu wachten op pricedrops van de GTX 900 serie. Volgens mij loopt AMD (nu) alweer 2 jaar achter Nvidia aan wat jammer is voor de prijzen voor de echte gamers. Benches gezien op Guru, toch wel aardig!
RX 480;1;0.6842609643936157;idd alternate 310!!!!!! euro voor 4gb versie.......... geen budget kaart meer in nl
RX 480;5;0.6652241349220276;Wow, gaan weer prijzen kelderen!
RX 480;3;0.3680349886417389;Nog even afwachten is, of Apple ook Polaris volledig gaat ondersteunen...bouw je een Intel-pc'tje en wil je voor de hobby eens klooien met Hackintosh, dan is het wel zo prettig als je weet of deze kaart supported is. Ik sta voor een nieuwe build en vind dit (naast de GT960 en 970) een zeer interessante kaart, met name omwille vd 8GB DDR5 op 256bit en de aansluitingen, redelijk future-proof. Ik las ook ergens dat deze kaart van AMD 10bit kleurdiepte biedt en dat je daarvoor bij Nvidia, minimaal een Quadro kaart nodig hebt...(of het waar is, weet ik niet- ben geen graka-expert.) Wellicht dat iemand hier meer over weet?
RX 480;5;0.2639402747154236;Eind van het jaar de RX 490:
RX 480;2;0.4404439926147461;Heet zoiets niet gewoon z-buffering, een techniek zo oud als 3D graphics zelf? De toelichting die in veel bronnen wordt gegeven over de Primitive Discard Accelerator is het volgende: Om eerlijk te zijn schiet ik hiermee niet veel op. Hardwarecanucks zegt iets meer waar de functie op ingrijpt: Dit vertelt me dat die PDA al ingrijpt voor de geometry processors. Een klassieke z buffer grijpt later in (z-buffer wiki): Bij klassieke z-buffering werd pas tijdens het renderen van de pixels bekeken welke pixel achter de voorstelling terechtkwam en gediskwalificeerd kon worden (na onnodig te zijn gerenderd). Door voor het doorrekenen van de geometrie al te kijken wat in de voorstelling voor en achter ligt maakt dat zowel de geometry engine als de graphics core meer effectief kunnen worden gebruikt. Als we bedenken hoeveel geometrie in een voorstelling verscholen blijft dan zou dit veel kunnen uitmaken.
RX 480;5;0.5109797716140747;Al met al helemaal in mijn nopjes met mijn vorige week aangeschafte asus gtx 970 oc kaart tegen een dumpprijs van 200 euro. Game alleen full HD op 1920. Had nog twijfels over de aanschaf door de (opgeklopte) verwachtingen qua energieverbruik en prestaties van deze kaart ...... maar gelukkig geen miskoop .
RX 480;2;0.41934943199157715;En niet even een gewone 390 in de test. Best jammer, aangezien een concurrent. (ja ik weet, die hebben jullie moeten terug geven / geen geld voor)
RX 480;1;0.484903484582901;AMD vega is de videokaart die gaat opnemen tegen 1080,ik weet niet hoeveel mensen gaan 700 of 550 euro gaat neertellen voor 1070 en 1080,op dit moment meerdere review site's geven aan dat iedereen moet wachten met kopen van high end gpu,2017 is het jaar van high end gpu's als nvdia en AMD op komt met snellere hbm2 geheugens,dus op dit moment geld strooien is niet verstandig naja als jij een top verdiener bent is niks aan de hand maar niet iedereen heeft geld over voor 700 euro gpu's...
RX 480;5;0.2620209753513336;Ik heb nu een MSI GTX 760 draaien met een Nvidia Surround met 3 schermen. Mocht ik overstappen naar de RX 480 hoe zit dat met aansluitingen en het spelen op 3 schermen? De 480 heeft 1x HDMI en 3x Displayport.
RX 480;2;0.5694044828414917;Een beetje teleurgesteld in deze kaart moet ik eerlijk zeggen. Verbuikt even veel als een 1070 met 50% performance ? Ze lopen nog steeds hopeloos achter kwa perf/watt bij AMD helaas. En als de 1060 over een maand uitkomt, minder verbruikt, sneller is dan deze kaart en niet veel meer kost .....
RX 480;2;0.3066833019256592;Hoe zit het eigenlijk als je wat oudere monitoren hebt? M'n TV is wel via HDMI aangesloten op het moment met m'n 2 7790's maar m'n monitoren zijn aangesloten via DVI-D en DVI-I, kunnen hoorstens verder nog VGA aan en geen displayport. Krijg je er converters bij?
RX 480;1;0.5385934710502625;Net even 3Dmark Fire strike gedraaid, 6473 punten met 2 HD7790's.. Heaven op 739. De RX 480 lijkt dus zo'n 2 keer sneller te zijn in z'n eentje en 't is duidelijk de hoogste tijd voor een upgrade.
RX 480;2;0.5014154314994812;"AMD RX480 voldoet niet aan de PCI-E specificaties waarin staat dat je max 75W uit de PCI-E mag trekken. De AMD RX480 gaat daar in veel gevallen DIK overheen (90W+ is door diverse reviewers gemeten). Een goed (OC) moederbord kan dit wel aan, maar bij de goedkopere moederborden kan dit wel een issue zijn. Daarnaast is het gewoon slecht dat dit niet compliant met de PCI-E standaard is. Verder prima kaart, maar AMD moet dit serieus oplossen voordat het de nieuwe ""GTX970 3.5GB"" debacle wordt."
RX 480;5;0.757976233959198;Een heel grote verbetering tegenover R9 380 - 50% meer kracht voor hetzelfde geld.
RX 480;2;0.40429428219795227;Erg bevooroordeelde review, oordeel aan prijs hangen die door schaarste kunstmatig hoog is. De reviewer had beter moeten weten, had meer objectiviteit verwacht. Maarja, reclame inkomsten zijn ook nodig. AMD brengt high-end grafics voor een midrange prijs, dat spreekt al in hun voordeel. En daar wordt volledig aan voorbij gegaan. En de rx 480 heeft alle nieuwe features incl. volledige DirectX12 ondersteuning. En hij schaalt in CF ook nog eens erg goed. Gewoon een erg goede kaart en ook al zou de GTX 1060 15% sneller zijn, om hoeveel meer fps gaat het dan om, absoluut? Te klein om het met het oog te zien iig. AMD probeert de markt open te breken en dat moeten wij supporten ! Kijk naar Freesync en Mantle. Wat doet Nvidia G-sync en Gameworks. Gameworks zorgt ervoor dat de 700 kaarten en ouder slechter presteren.
RX 480;5;0.2111794799566269;Waarom vergelijkt iedereen een midrange kaart met een high end kaart. Hij presteerd beter dan een 290x. En aangezien die vergelijkbaar is met een 390. Heeft AMD zich aan hun belofte gehouden. Dat de advies prijzen niet worden gebold door iedereen is amds schuld niet. 220euro voor zon kaart is daarnaast lekker ouderwets goedkoop(4gb)
RX 480;1;0.5876002311706543;"Ik vraag me af of je uberhaupt de review heb gelezen. Deze kaart is: 1. een reference kaart die je vergelijkt met een aftermarket kaart met degelijke drivers 2. DX12 geoptimaliseerd, wat nieuwere games ten goede komt dus meer future proof 3.4/8GB, waarbij de 8GB meer futureproof is. 4. aan de hoge kant qua prijs ivm release dag. De gtx 970 is na 2 jaar nog steeds duurder dan de RX480 over een paar weken zal zijn 5. bedoeld voor de ""algemene gamer"". Dit is 80% van de gehele markt, dus dit is een prima zet van AMD. Plus de GTX 970 is 2 jaar oud en dus minder future proof. o.a. door de genoemde punten hierboven. Ik snap mensen als jou echt niet. Cynisch van nature of gewoon een hater?"
RX 480;2;0.6190046668052673;Zeer matig is wellicht overdreven, maar de kaart kan de hooggespannen verwachtingen ook echt niet waar maken. Er werd een prijs/prestatierevolutie beloofd en die is er gewoon niet, nee ook niet als we van de adviesprijzen uitgaan. Feit is gewoon dat je vergelijkbare performance krijgt als een 970 die net een paar tientjes meer kost (ten opzichte van die adviesprijzen dus). Is hij meer futureproof? Misschien, maar dat heb ik AMD al te vaak zien beloven, dus ik ga uit van werkelijke resultaten. En ja, dan valt hij mij ook tegen.
RX 480;3;0.714131772518158;Oke daar kan ik me wel in vinden. Wat is volgens jou de beste kaart rond de 200-250 euro in jou ogen? Ik sta op het punt om een nieuwe PC aan te schaffen namelijk. Ik zie alsnog de RX480 als beste optie in deze prijscategorie tho.
RX 480;2;0.44864389300346375;is het inderdaad want de 970 is niet echt bepaald futureproof meer 480 veel meer vram en kan je ook wel een eindje mee voorruit
RX 480;1;0.4428621530532837;Oh dat zal waarschijnlijk inderdaad de RX480 zijn ja, mijn punt is alleen dat het 'gewoon' weer een volgende videokaart is die iets beter is op zijn prijspunt dan de vorige kaarten. Dit riedeltje herhaalt zich elk jaar of elke twee jaar, het is gewoon de volgende stap en niet de sprong die ons beloofd is. Daarom zijn velen van ons zo teleurgesteld.
RX 480;4;0.409077912569046;De RX480 is in dat budget zeker de beste prijs/kwaliteit kaart (in elk budget overigens, volgens het Tweakers grafiekje in deze review). Als je nog wat extra geld wilt besparen kan je eventueel kijken naar de 4GB versie, al zou ik eerder de 8GB versie kopen. De GTX970 is ook nog steeds een goede kaart, maar wel wat boven dat budget.
RX 480;1;0.5152578949928284;Vermomde fanboys die hier hele tijd t rode kamp afkraken, wat in niemands voordeel is ..
RX 480;1;0.644298791885376;Je kletst onzin. Wat kostte de 970 bij introductie? Oh ja..
RX 480;3;0.28986817598342896;Wat maakt dat nou uit? Je moet kijken wat de 970 nu kost.
RX 480;1;0.3711838722229004;Ook dan is de 970 bagger want 3.5 GB en geen async / dx12
RX 480;1;0.7810416221618652;Thats right. Nvidia slaat echt helemaal door... op den duur moet je 2 jaar wachten na release voordat je eens wat fatsoenlijks hebt voor 500 euro....
RX 480;1;0.4035494029521942;We weten nou toch wel dat AMD niet aan dat topsegment kan komen, waarschijnlijk ook niet met VEGA (maar moeten we maar even afwachten) en dat is ook helemaal niet belangrijk, gezien in het high-end segment niet zo veel te verdienen valt. Wat we van AMD willen is degelijke concurrentie in het low-mid segment, en dat is met deze videokaart UITSTEKEND gelukt. Dus is er gezonde concurrentie in dat segment, niets te klagen dus. Dat nVidia ondertussen peperdure high-end kaartjes verkoopt is geen enkel probleem, er is maar een kleine groep die zulke kaarten koopt. AMD get your shit together? Zeer ongepast, ze hebben nu mogelijk voor het eerst sinds tijden hun shit together en de juiste strategie aangenomen. Even nadenken kerel
RX 480;2;0.45804691314697266;Toch is het wel een probleem, en ik ben ook bang dat je je een beetje verkijkt op hoeveel mensen voor zulke kaarten gaan. Het probleem ligt hem hier in, Nvidia loopt nu vrij ver voor op AMD, wat als resultaat zal hebben dat veel games van dit en volgend jaar vooral voor Nvidia hardware gemaakt gaan worden, met alle drama van AMD fanboys die daar bij hoort, AMD verliest ondertussen steeds meer markt (mede door de nog steeds opkomende VR hype, waar ze eigenlijk niet goed in mee kunnen) waardoor Nvidia's ego alleen nog maar groter word en die kaarten alleen nog maar duurder zullen worden.
RX 480;2;0.40093350410461426;Alsof nV dat wel doet... 1070 performance voor 250 euro is toch gewoon niet realistisch?
RX 480;1;0.6778656244277954;Niks raars aan, performance gaat altijd elke generatie omhoog. dan hoeft de prijs nog niet omhoog te gaan. 970 was mooi voor 300 euro, maak de 1070 300 euro of 250. Want met dat andere logica in gedachten, dan zou de 970 ook 800 euro moeten zijn omdat de 780ti 700 kosten ervoor. Dat op dezelfde generatie performance kijken is gewoon onzinnig. 1070 is gewoon eeen mid range kaart voor een veelste hoge prijs op het moment. Totaal niet waard.
RX 480;2;0.4556710720062256;1070 is overdreven, ik had eerder iets in de richting van 970 prestaties voor approx 200 euro verwacht, en gehoopt op prestaties tussen de 970 en 1070 in voor 270-300 euro.
RX 480;3;0.3215159475803375;Heb je voor bovenstaande twee beweringen ook bronnen toevallig? Daar heb ik namelijk nog nooit eerder van gehoord. Die zijn er wel: Plays.tv en OBS Studio.
RX 480;1;0.6146667003631592;Ik teste beide hardware oplossingen, ik ben zelf van 290's naar een 970 gestapt voor de rede dat cpu overhead van de kaarten gewoon te hoog waren, de framepasing ook met een enkele kaart is gewoon slechter. Een google result op de nieuwe 480 zal al weergeven hoe enorm framepasing problemen zijn met de kaarten en drivers. Het is dus ook absoluut een slecht idee om een amd kaart te kopen als je een budget cpu heb voor nieuwere spellen, Vooral als je naar misleidende testen kijkt zoals hierbij. Ze gebruiken vrijwel altijd een beest van een cpu, die totaal niet realistisch zijn tegenover wat mensen nu daadwerkelijk hebben. Er was ooit is een test gedaan op een overclock forum, met een 280 op verschillende cpu oplossingen, en dit bleek dus duidelijk ook het geval te zijn bij hun. Dus ik ben niet de enige die dit constateerd. Nu snap ik niet waarom mijn reactie -3 krijgt, aangezien het gewoon gebaseerd op feiten is.
RX 480;3;0.3442801535129547;Welke verwachting heeft AMD volgens jou gewekt ? volgens mij hebben ze precies de performance gehaald die ze beloofd hadden, enige kantpuntje is het verbruik.
RX 480;3;0.3788979649543762;Performance vergelijkbaar met de 980? terwijl het amper de 970 kan evenaren... Naast dat de werkelijke prijs toch ook hoger is dan aangegeven..
RX 480;1;0.2654079794883728;Toon eerst maar eens aan waar AMD dat geclaimd heeft
RX 480;1;0.4467346966266632;Exact ze hebben de 970 aangegeven welke hij overduidelijk voorbij gaat. Mensen hoopte echter dat hij aan de 980 kon toppen, dit heeft AMD echter nooit geclaimd en dus ook niet waar gemaakt. Zie niet wat daar mis mee is?
RX 480;3;0.465900182723999;"Maar hij ""evenaart de GTX 970 ook niet amper"" hij is weldegelijk sneller Questie van interpretatie"
RX 480;2;0.2810012996196747;Volgensmij was er iets met hun review. Volgensmij had hij z'n nda gebroken, of heeft hij niet eens een nda, en dus ook niet de goede drivers. Maar hoe en wat zal je hier even moeten checken, iedergeval flinke grote discussie hierover.
RX 480;1;0.718986988067627;gewonnen waarop ? op consumenten uitmelken ? Zolang er nog geen 1060 is heeft nvidia nog niks gewonnen.
RX 480;2;0.3988769054412842;Waar heb jij het in vredesnaam over? Natuurlijk is de RX480 niet bedoeld om het op te nemen tegen de 1070 of 1080. Dat is een compleet ander prijssegment en bovendien nooit de bedoeling geweest van Polaris. Pas later dit jaar, met Vega, zal AMD high-end kaarten op de markt zetten. Polaris is mid-end. De RX480 presteert overigens ongeveer even goed als een 970 (in sommige games iets slechter, in andere games iets beter). Maar in DX12 verliest de 970 het wel duidelijk van de RX480. En de RX480 is natuurlijk goedkoper dan de 970, vergeet dat ook niet
RX 480;1;0.5474953651428223;"Hoezo moet een 480 tegen een 1070 opboxen ? Bij de vorige generatie hoefde de 380 toch ook niet tegen de 970 of 980 op te boxen Wat een onzin argument Een kaart die 200 euro goedkoper moeten gelijk presteren ?? Doe eens even normaal zeg... De 480 moet het ""eigenlijk"" met de niet bestaand 1060 concurreren. Probeer het nog eens zou ik zeggen maar denk deze keer even wat harder na voordat je wat neer typt."
RX 480;1;0.3946506083011627;Geruchten gaan dat de 1060 op 7 juli uitkomt, dan zullen we zien welk kamp het gaat winnen in de 250 euro pricerange... bron:
RX 480;2;0.2883761525154114;Feit is dat je geen enkele feit geeft in je comment. De 380(X) is bedoeld als tegenhanger voor de 960 en niet 970. Wederom zit je kaarten te vergelijken die in een heel ander segment zitten. De 390 is de concurrent van de 970. En de 300 series zijn allemaal rebrands dus je kan het ook omdraaien en zeggen dat nvidia pas na een jaar (2014) de 200 serie kon verslaan die al in 2013 eerst geïntroduceerd.
RX 470;3;0.4002513885498047;Hier enkele (non ref) reviews: MSI Radeon RX 470 Gaming X 8GB ASUS Radeon RX 470 STRIX OC 4 GB VIDEO REVIEW: Sapphire RX 470 Platinum en de Powercolor review van de buren. De review van tweakers lijkt hierbij een vertekend beeld te geven, gezien deze saphire kaart met de ref koeler gewoonweg minder presteert (zie verschil tussen MSI kaart review en de review van Tweakers). De MSI ziet er voor mij het meest belovend uit. EDIT: zie de OC van de MSI kaart -> de 470 overtreft de 480 dan bij de getoonde benchmarks.
RX 470;5;0.22977964580059052;En deze van Foritain op Tweakers: productreview: AMD Radeon RX 470 review door Foritain
RX 470;3;0.35669344663619995;Grappige is dat er twee dx12 benchmarks zijn waar de 1060 beter doet dan de 480/470. En dat zijn de enige benchmarks welke Tweakers uitvoert.
RX 470;1;0.6063367128372192;Mooi die paper launches. Helaas krijgt AMD werkelijk niks geleverd waardoor de prijzen (de enige reden op papier om een AMD kaart te kopen) op dit moment echt totaal debiel zijn. De 470 (280 euro) van Sapphire is ondertussen (op moment van schrijven) zelfs duurder dan de 480 (255) bij amazon in Duitsland. En dat terwijl de 480 werd gebracht als de $200 GPU. Met de 1060 uit is deze kaart DOA.
RX 470;2;0.5436992049217224;Geen paper launch. Ik heb een 480X hier in m'n systeem zitten met 8GB. De 470x zal op korte termijn ook gewoon beschikbaar zijn. Maar het probleem is is de vraag vs aanbod. Het aanbod is kleiner dan de vraag en daarmee worden de prijzen door leveranciers & webshops ook een beetje verhoogd. MSR ligt gewoon op de prijs alleen je moet de parasietjes ertussen ook even in acht nemen. Over 2 maand is die kaart wel gewoon beschikbaar voor de prijs als de 1060 ook wat 'meer' leverbaar zou zijn.
RX 470;2;0.4695385694503784;"Kleiner, te klein, of geen aanbod is een paper launch. Als je er niet aan kunt komen, bestaat de videokaart alleen op papier. Kijk maar hier hoeveel GTX 1060 videokaarten (die later dan de RX 480 aangekondigd en gelanceerd is) tussen de Sapphire Nitro+ RX 480 4GB en 8GB staan. Ik heb alle reference videokaarten en videokaarten zonder prijs niet opgenomen in de vergelijking."" De vergelijking is gesorteerd op ""prijs oplopend"" op dit moment, dus de volgorde kan bij een prijswijziging veranderen zonder dat dit in de (statische) vergelijking zichtbaar is. Tussen de Sapphire Nitro+ RX 480 4GB en de Sapphire Nitro+ RX 480 8GB zitten op het moment 15 GTX 1060 videokaarten. Die zijn alle 15 dus goedkoper (en sneller en beter overklokbaar) dan de Nitro+ RX 480 4GB en 8GB. Het probleem ligt bij AMD (lage productie), bij de AIB's (geen prio), of bij beide. Eigenlijk ben ik het wachten op de andere RX 480 custom AIB kaarten zat. Ik wacht weer op AMD. Die RX 480 houdt namelijk de bestelling van mijn X99 systeem op. Op AMD Zen wacht ik niet meer. P.S. Ik heb dit als basis voor de vergelijking gebruikt. Gewoon alles aangevinkt wat niet reference is en een prijs heeft van boven naar beneden over alle 3 pagina's."
RX 470;2;0.41802823543548584;Is de 1060 ook geen paper launch dan? Als ik naar je vergelijking kijk zie ik zelden dat ze binnen enkele dagen leverbaar zijn. Bij dat ene waar het wel het geval is, betaal je een premium van 50 euro ook nog eens. Buiten dit, is de 480 een mooie kaart om te minen (jammer genoeg) en hebben miners letterlijk dozijnen 480's binnen gesleept waardoor hij nu minder beschikbaar is. Bij de 290 was ongeveer hetzelfde verhaal en pas na 4 maanden werd hij normaal leverbaar. In mijn ogen is het niet werkelijk een paper launch ook, gezien mensen wel die kaart gewoon hebben.
RX 470;1;0.5378803610801697;Ben het met je eens. Het was geen paper launch, hij was leverbaar. Echter ik moet wel vaststellen dat AMD wel flinke leverproblemen heeft. Zie marktaandeel op userbenchmark.com: RX 480 Mei: 0% Juni 0.8%, Juli 0.8% GTX 1060: Mei: 0% Juni 0.3% Juli 1.7% GTX 1070: Mei: 0.6% Juni 3.5% Juli: 4.4% GTX 1080: Mei 1.3% Juni 2% Juli 2.5% Het marktaandeel is dus niet meer toegenomen (sterker nog, eerder deze week stond het op 0.7), blijkbaar zijn die dingen wereldwijd slecht leverbaar. Opvallend genoeg heeft AMD de marktaandeel race überhaupt voorlopig verloren. De verhouding is nu 0.8% vs 8.6%. Dus voor elke 100 Pascal kaarten werden er 9 van Polaris verkocht. Het doel van Polaris was juist marktaandeel terug winnen. *Disclaimer: het kan zijn dat de high-end kaarten oververtegenwoordig zijn op een benchmark website, de 1060 zal dat wat minder hebben.*
RX 470;1;0.30789560079574585;denk dat mensen die de RX kaarten kopen om ze in een mining rig te stoppen ook niet zullen benchmarken. wat dat betreft is dat ten nadelen van de steamcharts en de benchmark charts. ben benieuwd waar de officiele sale-charts op uit komen.
RX 470;2;0.37368080019950867;Dat kan verklaren waarom het marktaandeel achter blijft tov Nvidia, maar niet waarom de groei er volledig uit is. Sterker nog: vorige week was er zelfs een krimp van 0.1% in het marktaandeel van de RX 480 waar te nemen. Dus de markt groeit momenteel harder dan AMD kan leveren, zelfs met het huidige beperkte marktaandeel.
RX 470;1;0.3520815074443817;Minen ? Dat doe je tegenwoordig toch op een ASIC ?
RX 470;1;0.5538955926895142;Als je het woord paperlaunch letterlijk neemt, was de release van de RX480 geen paperlaunch, want AMD heeft 3 maanden lang alle chips opgespaard en de reference editie bij de release op de markt te kunnen brengen, maar daarna is er nauwelijks meer wat geleverd waardoor de smachtende consument zonder zit. Je kan dus op je strepen staan omdat jij er toevallig eentje hebt, maar in mijn ogen heb jij gewoon geluk gehad. Mensen die graag een 3rd party kaart hebben hebben pech, vooral als ze op de van te voren beloofte prijs wachten. Ondertussen kan NVIDIA iets beter leveren, maar dat houdt ook niet over, met als gevolg dat de kaartenbouwers misbruik maken van de hoge vraag door de kaarten boven adviesprijs aan te bieden. Lose-lose voor de consument van AMD en Nvidia kaarten.
RX 470;1;0.5198912620544434;Kan Amd daar iets aan doen? Ik heb hem in Duitsland de MSI voor 239.99 gezien en de goedkoopste rx480 was 249.99. Dus valt reuze mee. Nvidia deed het met de 1080/1070 slechter.
RX 470;2;0.34508827328681946;Prijs de enige reden om AMD te kopen? Freesync vs. G-Sync? Crossfire vs. SLI? Vulkan performance door Async Compute en DX12? Future proofing. Ethische redenen (geen Gameworks)? Er zijn zat redenen bedenken afgezien van de prijs.
RX 470;3;0.2801850140094757;Hoe kom je daarbij? Vulkan c.q mantle is juist ontwikkeld om de overhead op CPU's te doen verlagen. Zo kan een systeem met een midrange CPU toch 'genieten' van snellere graphics in combinatie met een high-end of mainstream kaart.
RX 470;5;0.3283596634864807;Een Interesant artikels is dit: DX12 heeft een enorm toekomstperspectief omdat het de hardware van een pc abstrageerd zodat game developpers net zoals bij consoles gewoon met een API kunnen communiceren ipv aparte componenten om hun games te optimaliseren. Los van sli/crossfire kan de onboard gpu van uw cpu ook bijspringen, dit is in dx11 ondenkbaar. AMD lijkt hier heer en meester in en het feit dat DX12 specifieke voordelen heeft voor ontwikkelaars mag je er vanuit gaan dat ze allemaal staan te springen om over te schakelen. En de consument kan rekenen op extra frames/second met de zelfde hardware. De rx479 heeft naast de rx480 geen bestaansrecht (wie kijkt er nu nr 20 euro? Koop dan een keer geen pizza.) maar wat je ook koopt, value for money krijg je zeker en vast! Het feit de dx12 ondersteuning bij Nvidia aan de lage kant zorgt ervoor dat de gtx1060 een minder goede koop wordt dan de rx480w. Ik heb veel goede reviews gelezen over MSI, die zou de rx470 fluister stil houden, zelfs onder load. Zolang je geen VR gebruikt of UHD is de rx480 de beste kaar die je op dit moment kan kopen. Als je ooit kracht tekort komt kan je nog altijd 2dehands een crossfire setup maken voor 100euro terwijl je met de GTX1060 geen sli-mogelijkheid hebt en zich een nieuwekaart opdringt.
RX 470;1;0.640679121017456;Waarom toont de benchmark andere resultaten dan? Het is juist de 1060 die nauwelijks prestatie verlies heeft bij low end cpu's bij de Vulkan benchmark. Snap niet dat ik gedownvote word voor pure waarheid. Bekijk de Steam hardware survey en dan zul je zien dat een absolute minderheid een high end cpu heeft.
RX 470;1;0.28178825974464417;met een 1060 die ook langs geen kanten leverbaar is en waarvan de prijs ook even sterk oploopt
RX 470;1;0.5759125351905823;Is geen paperlaunch, de voorraad was relatief de grootste ooit bij launch, en er zijn al gigantisch veel gekocht. Daarnaast is het ook een populaire kaart bij miners, wat ook niet mee helpt.
RX 470;1;0.42724040150642395;Je bedoelde wellicht EOL? End of Life? DOA wordt volgens mij voornamelijk gebruikt in het fenomeen dat nieuwe producten die bij klant aankomen bij testen direct stuk zijn/gaan.
RX 470;3;0.3920009136199951;Death On Arrival is anders dan EOL, want EOL veronderstelt dat er eerst leven is geweest.
RX 470;1;0.4003446102142334;Als je dan toch gaat corrigeren, maak er dan iig Dead on Arrival van.
RX 470;3;0.30571281909942627;Jah, maar death* impliceert ook dat ie eerst leefde </mierenneuk>
RX 470;3;0.5785273313522339;Wel, hun DirectX12 performance is wel een groot voordeel. De 480 springt van 10-15% trager dan de 1060 ind DX11 naar 0-5% sneller in DX12. Je kan er van uitgaan dat heel binnenkort game developers allemaal DX12 zullen gebruiken, gezien de mogelijkheden die het biedt (ook voor Nvidia kaarten is het een mooie boost). Qua prijs/performance zit AMD echt wel goed hoor. Nvidia heeft dan de 'founders edition', voor wie zijn kaarten wilt tentoonstellen, en ze verbruiken minder. Dat verschil in verbruik is eigenlijk zowat verwaarloosbaar tenzij je dag en nacht gamet. En voor koeling/geluidsdruk is Nvidia ook niet echt veel beter, zoals deze review ook toont.
RX 470;3;0.3431670665740967;Mijn punt: de AMD kaarten zijn nu goedkoper, ongeveer evenredig met het prijsverschil tov 1060. Op dit moment dus zeker geen slechte deal, maar dat zal alleen maar beter worden naarmate DX12 algemener gebruikt wordt. AMD kaarten schalen beter met DX12 dan Nvidia kaarten, kijk eens naar de prestatieindex van de 1060 vs de 480. Een beetje hoger postte iemand een review van Guru3d. Daar testen ze Tomb Raider, Hitman en Total War:Warhammer. Allemaal toch wel vrij grote titels, nee? Ook Doom dat gebruikt maakt van Vulcan en daar vergelijkbare prestatiewinsten toont als DX12.
RX 470;1;0.4134942293167114;Natuurlijk ook niet vergeten dat amd deze generatie heeft uitgebracht voor de mensen met een budget zeer acceptabele framerates tegen een acceptabele prijs ?
RX 470;3;0.38344013690948486;ik geef je ook geen ongelijk dat de 1060 gelijk geprijsd is? Maar uit verschillende benchmarks blijkt wel zeker op dx 12 dat amd in sommige gevallen gelijk soms minder maar zeker ook in sommige gevallen beter presteert, natuurlijk zijn deze kaarten van beide teams nog maar net gereleased en zullen we nog even moeten afwachten op andere titels en hun onafhankelijk benchmarks om te zien of beide teams nog gelijk presteren in dezelfde price range.
RX 470;1;0.41811463236808777;De adviesprijs ligt €20 onder die van de RX480? Kan iemand mij uitleggen met welke insteek dit wordt gedaan? Of is dit gewoon een minder goed gelukte RX480 variant?
RX 470;2;0.42057618498802185;Misschien om de 480 te verkopen? Net als bij de mac, klein 1,50, middel 2,00 en groot 2,15. De 460 is echt een niveau (of 2) onder de 470, maar de 470 en 480 zitten erg dicht bij elkaar. En misschien dat men dan toch die 480 overweegt, en dan voor nog een paar tientjes meer heb je een 8 GB.
RX 470;1;0.23129723966121674;Dus je zet een hele productie in gang om de 480 aantrekkelijker te maken en beter te verkopen met een 470 die €20 goedkoper is? Kijk als dit afgekeurde 480's zijn dan kan ik mij er nog iets bij voorstellen.
RX 470;2;0.4045962393283844;Dit zie je vaker bij videokaarten. Omdat de chips zo groot zijn neemt de kans op fouten toe. als een chip niet goed genoeg is voor je top model disable je de slechte delen en gebruik je hem voor een goedkopere kaart. dus inplaats van de slechte chips weg te gooien verdienen ze er nog geld mee. Later als de z.g. yields beter worden en er minder fouten voorkomen, maar de vraag naar de 470 hoog blijft kan het zijn dat je een gezonde 480 chip in je 470 hebt zitten. Je zou dan je 470 kunnen unlocken. Dan heb je betaald voor een 470 maar je hebt een volwaardige 480 in bezit.
RX 470;1;0.609000027179718;uiteraard zijn dit afgekeurde 480 chips, dat is waarschijnlijk ook de enige bestaansreden van deze kaart zelf bij goede yields zijn er altijd chips die net niet zijn, via dergelijke kaarten kunnen ze deze chips toch nog verkopen
RX 470;5;0.5560001134872437;En met geluk unlocken naar een volledige 480x.
RX 470;1;0.4155206084251404;Droom verder, dit kan al jaren niet meer. Of zie je nu ook mensen die een 380 hebben unlocked naar een 380x?
RX 470;5;0.3966605067253113;En de eerste lading(misschien nu nog steeds). Kan je de 4gb ook een 8gb maken. En dat is enkele weken.
RX 470;3;0.294283002614975;Ja, dat kon met een simpele BIOS mod.
RX 470;3;0.26857051253318787;Met bepaalde R9 290 kaarten kun je die unlocken naar een 290X, dat is nog niet heel lang geleden.
RX 470;3;0.43006962537765503;R9 290's gingen anders prima door als 290x.
RX 470;1;0.35098743438720703;Tegenwoordig worden die lanes door een laser gewoon afgesneden toch?
RX 470;2;0.4751201272010803;"Als het ""gepland"" is wel ja. dus langzaam tekorten van de ene kant en overvloed aan de andere ja. Maar als je bij de r9 290 mining rage keek hadden ze er geen tijd voor binnen enkele weken was vrijwel de hele wereld voorraad op. En dan moet je wel. Heb nog een r9 290 factuur waar mijne 1 van de 300 is:P."
RX 470;3;0.4451468884944916;Dit doen bedrijven wel vaker. Zo lanceerde Apple de 5c. Minder toestel maar prijstechnisch net onder de gewone 5.
RX 470;1;0.399009644985199;Zijn ook afgekeurde Polaris 10 gpu's, is in het verleden nog gedaan en dan als je veel geluk had kon je de defecte shaders/CU's toch nog activeren en had je een volledige hogere kaart. * Damic twijfelt om een 470 te bestellen aan 219€
RX 470;1;0.3850506842136383;Omdat zelfs €20 voor sommige mensen een hoop geld is. Het is toch een goedkopere kaart en soms heb je gewoon niet meer nodig. Waarom dan toch een duurdere kaart nemen? Zie t als bij een auto: waarom een 1.8 liter als er ook een 2.0 is? Sommige mensen nemen liever de 2.0 zodat ze sneller weg zijn bij het stoplicht, anderen vinden de 1.8 snel genoeg. Ofwel, kwestie van hoe dik je portemonnaie is dus...
RX 470;3;0.4098190367221832;Ik begrijp heel goed wat jij bedoelt alleen bij een aanschaf van een videokaart rond de €200 kan ik mij niet voorstellen dat €20 een verschil maakt. Ondanks dat het in lijn is met het percentage van het prestatieverschil.
RX 470;2;0.3514588475227356;Maar het telt zo aan. Nét even betere GPU, nét even snellere SSD, nét even... En voor je het weet ben je €200-€300 verder op (wat begon als) een budget van ~€1000ish.
RX 470;2;0.3937596082687378;Met een 470 of 480 kom je niet aanzetten als je budget rond de 1000 ligt, dan ga je voor een 1070. deze kaarten zijn meer bedoelt voor de 700-800 budget range.
RX 470;4;0.33870455622673035;Goed punt, moet zeggen dat ik al even geen PC meer samengesteld heb. €20-€30 wordt overigens alleen maar relevanter op een kleiner budget
RX 470;2;0.40724125504493713;Hangt er maar net vanaf waar je prioriteiten liggen. Als je meer focus legt op video edditing dan op gamen. Zal die 20 euro snel opgaan naar extra werkgeheugen om maar even een voorbeeld te noemen.
RX 470;3;0.31846073269844055;Tenzij je monitor en keyboard etc er ook nog bij moet.
RX 470;2;0.2812647521495819;Klopt helemaal, heb het zelf gehad net met een PC-tje dat ik had gebouwd, dacht om en bij de 500 voor een relatief simpel systeempje kwijt te zijn, ben uiteindelijk ruim boven de 700 beeindigd omdat ik toch iets duurder wilde iedere keer.. Maar nu is het ook meer toekomstproof
RX 470;2;0.35967519879341125;Ben het met je eens, maar kan me voorstellen dat mensen gewoon een budget hebben. Voor die 20,- kunnen ze bijv. ook aan de ssd besteden bij een nieuwe build. Daarnaast denk ik dat de prijs verschillen straks beter worden. Alles gaat nu behoorlijk boven de adviesprijzen veelal.
RX 470;5;0.32154151797294617;Ik denk ook oem's voor hun is elke € veel winst over x aantal computers.
RX 470;2;0.5110381245613098;en anderen vinden dan weer een 2.0 veel te mager
RX 470;2;0.4405992031097412;Maar voor die 1.8 liter worden niet de wegen over een tijdje zwaarder te berijden. En als je een aanhanger wilt gebruiken heb je spijt. Games worden steeds zwaarder en misschien heb je nu nog een kleine monitor die je binnenkort wilt vervangen voor een grotere met een hogere resolutie. Dan zou je wel gek zijn als je voor die € 20 meer niet de RX480 zou kopen.
RX 470;5;0.5308850407600403;De mensen die deze kaart kopen, maken nu een upgrade. Die zijn al lang blij dat ze deze prestaties kunnen kopen. Ze kopen geen kaart die het goed doet in de toekomst, maar voor nu. En als je puur de goedkoopste prijs wilt hebben dan is het is toch 20 euro extra terwijl een 470 al goed genoeg is voor nu. Dus als dit of dat, of misschien wordt het zwaarder geldt dan niet. Puur de beste kwaliteit voor zo weinig mogelijk.
RX 470;1;0.3967006504535675;Als 20 euro een hoop geld is dan is 180 (of ja, 200+ voor nu) en enorm hoop geld Iedereen die nu 180 euro kan ophoesten, kan 20 over een maand (of twee) ook wel 20 euro er boven op hoesten (betaal je 20 euro minder op je zorgverzekering ofzo lol). Anders even lief familieleden aankijken of wat extra werk verrichten. Bij een auto kan het verschil echter wel enkele honderd tot duizenden euro verschil zijn. Is imo geen goede vergelijking. Zelfs vind ik de 470 niet interessant voor 180 euro, tenzij het een non ref model is, die met een goede OC boven de 480 ref komt. Moet ook eerlijk bekennen dat op dit moment je het beste gewoonweg kan wachten en over een maand (of 2) wat spaart en een non ref 4gb 480 haalt of een 1060. Maar okay, dat is mijn mening.
RX 470;4;0.2950506806373596;PCIe
RX 470;1;0.5346755981445312;Is dit hier gewoon een fout die 8gb? Niet de enige RX470 8gb trouwens: In het artikel staat namelijk dat er enkel 4gb varianten zijn (in de conclusie en bij de specificaties)...moet fout zijn, hier werd een 8gb versie reviewed:
RX 470;3;0.34557583928108215;Interessant, ik had van AMD begrepen dat de RX 470 alleen in 4GB zou komen, maar lbijkbaar is er ook een 8GB-versie, ik pas het aan!
RX 470;1;0.5142754912376404;Vergeet het dan ook niet aan te passen in de conclusie
RX 470;1;0.3936280310153961;Oei...is gefixt
RX 470;1;0.6761071085929871;Zeer slimme zet. Ze hebben gewoon de mislukte 480x chips omgedoopt naar de 470x. Ze dachten zeker. Nja we gunnen je de zelfde chip met iets minder performance voor 20 piek minder.
RX 470;3;0.36590898036956787;Deze strategie is toch vrij standaard?
RX 470;5;0.417575865983963;Klopt. En gelijk hebben ze.
RX 470;2;0.40537208318710327;Nee, er komen ook 8GB versies van de Radeon RX 470. Die maken alleen nog minder sense dan die van de RX 480. In tegenstelling tot bij de RX 480 is de 4GB-versie van de RX 470 ook waar de focus op ligt bij AMD en de AIB's.
RX 470;1;0.5591774582862854;Het zou natuurlijk ook een fout bij die webwinkel kunnen zijn, maar wellicht brengt men de 8gb versie hier gewoon niet op de markt?
RX 470;2;0.46321532130241394;Ik hoor hier mensen weinig over, maar deze nieuwe lijn zegt mij dat AMD de race tegen nvidia voor de snelste gpu een beetje heeft opgegeven?? Het is eigenlijk een beetje hetzelfde als Amd en intel toen. Op een gegeven moment kon amd niet meer mee komen en toen zijn ze zich gaan focussen op minder voor weinig, wat hun op de cpu markt niet veel goed heeft gedaan.. Het zou jammer zijn als de enige concurent van nvidia nu een beetje de handdoek in de ring zou gooien. Dan is de noodzaak voor nvidia om nog sneller en beter te maken er ook een beetje uit. Ofterwijl bij nvidia kan de R&D dicht en de winst kan omhoog! Ik ben helemaal niet blij met deze lijn van AMD. Het zou zomaar kunnen zijn dat de gtx 1080 nu het laatste snelheids monster is wat we zullen krijgen. Kijk maar naar intel al iteraties lang zijn het nog maar snelheids winsten van een paar procent sinds amd eigenlijk is opgehouden met de race. In gpu land lagen de prestatie winsten met elke nieuwe chip beduidend een stuk hoger altijd. Een nieuwe concurent zit er ook niet echt in meer. Dit soort gpu's nu vanaf scratch gaan maken en dan nog sneller willen zijn dan nvidia is vrijwel onmogelijk. Dat is zo geadvanceerd proprietary spul waar jaren en jaren aan research omheen zit. Dat ontwerp je niet gewoon even vanaf scratch. Ik vind het zorgelijk, maar hoor echt bijna niemand erover. Het lijkt iedereen maar normaal dat AMD met hun nieuwe lijn gpus uitbrengt die zich amper kunnen meten met een 980. Bizar
RX 470;2;0.3275778293609619;Polaris is gewoon geen grote chip, vega wel en dat wordt gewoon de nieuwe high end en die komt gewoon eind dit jaar of begin 2017. Zo moeilijk is dat toch niet te begrijpen ? Je hoort niemand erover omdat de meeste mensen dit gewoon weten.
RX 470;2;0.5127556324005127;En omdat dit tot in den treuren is herhaald, AMD prestenteert éérst de midrange en dán de High End. Dus de 480 vergelijken met een 1070/1080 gaat dus nergens over. Zeggen dat AMD de snelheids race heeft opgegeven, gaat dus nergens over. OK het is vakantietijd en lang niet iedereen zal in de vakantie het laatste nieuws hebben bijgehouden. Je moet toch wel in een behoorlijke uithoek hebben gezeten, maandenlang, om dit gemist te hebben. AMD heeft een prima kaart gepresenteerd (de 480) voor een nog betere prijs. Voornamelijk gericht op de mainstream gamer die op FullHD gamed. Waar het gros van de gamers toch op zit. Als je ergens niet blij om zou moeten zijn is het feit dat nVidia de prijzen voor hun lijn gigantisch omhoog heeft gegooid. Nu zijn er mensen die klakkeloos die GPUs kopen, want sneller. En dan toch vergeten dat ze een flinke meerprijs neerleggen. ter vergelijking, de 1070 (non ref) is bij introductie $100 duurder geworden dan de 970. De 970 was $330 bij introdutie, En het ref model is $500. Om over de 1080 nog maar te zwijgen! Dat zijn behoorlijke prijzen en tegen performance uitgezet, eigenlijk veel te duur.
RX 470;3;0.4319528341293335;Vega is de nieuwe architectuur van AMD met grote chips (4000/6000 shaders). echter word deze pas Q1 2017 verwacht. als het goed is brengt deze ook meer verandering in architectuur met zich mee dan Polaris.
RX 470;5;0.5668965578079224;Ik ben erg benieuwd naar de crossfire prestatie van deze kaart
RX 470;2;0.21664825081825256;Ik was zelf even geïnteresseerd naar de prestaties van mijn eigen 'oude' R9 290 crossfire setup van anderhalf jaar oud. Nog even op gezocht. Scoort nog 15080 in 3D mark Firestrike. Misschien kun je ook een enkele R9 290 stat vinden bij een oude review. Als ze ongeveer het zelfde schalen kun je een goede schatting doen ... .
RX 470;2;0.4834349751472473;Sinds de release van de kaart zijn de scores flink omhoog gegaan dankzij betere drivers. Oude reviews (dus scores) zijn NIET representatief
RX 470;5;0.5042991042137146;Mooi
RX 470;1;0.5221076011657715;"""De adviesprijs van AMD is 179 dollar, wat uitkomt op 195 euro"" Denk dat dit andersom moet?"
RX 470;1;0.5877978801727295;Nee hoor, 179 usd is ~160eur+ 21 procent btw = ~195 eur
RX 470;1;0.3391420245170593;Kan ik daaruit concluderen dat adviesprijzen altijd zonder BTW zijn?
RX 470;1;0.5836673378944397;Nee, Amerikaanse dollarprijzen zijn zonder btw, omdat iedere staat daar zijn eigen sales tax (btw) heft. Als een kaart 199 dollar kost is dat dus vrijwel nooit de prijs die je betaalt, maar zal er een paar procent bijkomen, afhankelijk van de staat waarin je je aankoop doet. Als er ergens een prijs in euro's staat kun je er vanuit gaan dat het inclusief btw is.
RX 470;4;0.3036319613456726;"Ik kan je reactie moderen om mijn dankt te uiten, dus; Bedankt:)."
RX 470;1;0.473954439163208;Maar 195 euro is geen 20 euro goedkoper dan de rx 480: De goedkoopste rx 480 staat nu op 257 euro voor de 4 gb versie en op ongeveer 280 euro voor de 8gb versie...
RX 470;3;0.36992064118385315;Ze vergelijken denk ik met de adviesprijs van de RX480, hoewel ik niet na heb gerekend of het dan wel klopt
RX 470;1;0.5085100531578064;Vanaf adviesprijs gerekend met 21% BTW klopt het inderdaad. €195 voor 470 om €215 voor de 480 4GB versie. Dat de kaarten uiteindelijk voor een (veel) hoger bedrag in de winkel ligt, is door vraag en aanbod. Er lijkt toch een behoorlijke vraag te zijn naar de 480 als ik in mijn omgeving kijk. Zegt natuurlijk niets.
RX 470;5;0.272014856338501;Ah, on that bike.
RX 470;3;0.4274364113807678;De dollar is inderdaad minder waard dan de euro, maar de Amerikaanse adviesprijzen bevatten nog geen belastingen en dergelijke. Vandaar onder de streep toch een wat hogere prijs vaak.
RX 470;5;0.3443158268928528;Ben ik blij dat ik geen btw betaal
RX 470;1;0.6090841889381409;Tweakers gooit er voor ons Nederlanders alvast het btw erop aangezien wij die moeten betalen. Adviesprijzen in de VS zijn zonder belasting.
RX 470;1;0.5413581132888794;Ik denk dat Tweakers direct van BTW uit gaat. en zal dus 195 euro inc BTW zijn.
RX 470;1;0.4419683516025543;Ik vermoed dat het klopt, want de adviesprijs van AMD is waarschijnlijk exclusief BTW. Als je 179 dollar omrekent naar Euros en er BTW bij optelt kom je wel ongeveer op 195E.
RX 470;1;0.8420401811599731;Sorry hoor, maar 21% BTW is niet de EU, maar Nederland dat hebzuchtig is. Het is een van de hoogste BTW tarieven in Europa. En Amerikaanse staten vragen ook deels vors BTW, dus die zijn dan net zo hebzuchtig als Nederland.
RX 470;1;0.6010875105857849;Sorry hoor maar dat valt wel mee. VK, estland, bulgarije, frankrijk, slowakije, oostenrijk = 20% Nederland, belgie, litouwen, spanje, letland, tsjechie = 21% Slovenië, italie = 22% Ierland, portugal, polen, griekenland = 23% Finland, roemenie = 24% Denemarken, zweden, kroatie = 25% Hongarije = 27% dus een hoop landen die we graag onder Nederland plaatsen op de succesladder betalen hetzelfde of zelfs fors meer!
RX 470;1;0.5679037570953369;En mochten er heren & dames zijn die alle europese tarieven willen weten, plus meer, dan kunnen deze hier terecht Klik En wat ik uit deze lijst haal op de gelinkte pagina hierboven, zit Nederland zo'n beetje op het gemiddelde qua BTW. Dus we hebben het zo slecht nog niet naar mijn idee Al heeft deze reactie niks met de hele review te maken, maar goed, goed bedoeld.
RX 470;1;0.3744824528694153;"Ondertussen in Amerika: ""The five states with the highest average combined state and local sales tax rates are Tennessee (9.46 percent), Arkansas (9.30 percent), Louisiana (9.01 percent), Alabama (8.97 percent), and Washington (8.90 percent). The five states with the lowest average combined rates are Alaska (1.78 percent), Hawaii (4.35 percent), Wisconsin (5.41 percent), Wyoming (5.42 percent), and Maine (5.55 percent)."" Welkom in de EUSSR."
RX 470;1;0.49732568860054016;"Dus de Amerikanen doen het goed zeg je. Terwijl de infrastructuur zwaar verouderd is en op instorten staat, omdat ""ze"" niet genoeg geld hebben voor onderhoud"
RX 470;1;0.5516522526741028;"In de VS heet het sales tax (wat overigens iets anders is dan VAT oftewel BTW) en het hoogste wat je zult zien is zo'n 13,5% in delen van Alabama. Sales tax is op staatsniveau geregeld, waarbij er ook nog lokale belastingen geheven mogen worden, tot een grens. Daarom geldt die 13,5% niet eens in heel Alabama. Gemiddeld zit sales tax rond de 7-8%. Echter zijn internet aanbieders niet verplicht deze belasting te heffen en dat doen de meesten ook niet. Zelfs Newegg heft deze niet, tenzij je in één van vier staten woont (California, Indiana, New Jersey, Tennessee). Voor sommige aankopen moet je ze dan wel zelf aan de IRS opgeven in sommige staten. In veel gevallen betaal je dus echter helemaal geen belasting. En ons BTW tarief valt inderdaad in het midden als je het met de rest van Europa vergelijkt. Laagste is 17%, hoogste is 27%. Zelfs landen als Polen en Slovenië rekenen meer; laat staan Scandinavië."
RX 470;2;0.42036280035972595;Scandinavie heeft vergelijkbare btw tarieven alhoewel dit pas recent is gewijzigd in bijvoorbeeld Noorwegen, wat uiteraard velen een doorn in het oog is. Voordien werd in noorwegen amper belasting geheven op invoer. Ik heb daar vrienden wonen die erg veel uit usa bestellen en daar helemaal niet blij van worden dat Noorwegen deze stap heeft genomen. Ik denk dat uiteindelijk er mogelijk een standaard tarief gaat komen voor heel europa was rond die 20 a 25% gaat liggen. Hierdoor slepen de regeringen toch een mooie smak geld voor hun geld verslindende waanzin projecten binnen.
RX 470;1;0.44175228476524353;man man man. Moet je eens kijken hoe hoge tarieven sommige 'arme' landen rekenen. Neem bijvoorbeeld Griekenland, waar gewoon 24% BTW gerekend wordt. (bron: 21% is best een schappelijk bedrag. Tuurlijk is het veel, dat weet iedereen, maar zo is het nou eenmaal. In ruil daarvoor leven we wel in een van de 'beste' landen in de wereld.
RX 470;1;0.6647341251373291;Griekenland moest van de EU hun btw opvoeren naar 24% anders zou het land geen hulp geld meer krijgen. Inmiddels blijven de miljarden weer vrolijk deze bodemloze put invloeien.
RX 470;1;0.7966286540031433;Waar haal je dat vandaan? Grootste probleem was dat heel veel banken en andere financiële instellingen staatsobligaties tegen woekerrentes hadden gekocht omdat ze er van uit gingen dat die toch niet failliet ging. Ze hebben gegokt waarbij een heel groot deel niet verloren heeft. Een klein deel hebben een deel van de obligaties moeten afschrijven, maar het gros is opgekocht door ECB en andere landen. Sinds er fatsoenlijke rentes worden gehanteerd is de schuld stabiel alleen heeft het land onterecht een ridicule reputatie gekregen waardoor investeerders het mijden als de pest. Ziedaar een prachtig recept om een hele natie naar de klote te helpen waarbij 25% van de bevolking geen werk heeft. Totaal off-topic maar populisten wijzen vaak naar Griekenland want dat is lekker makkelijk, maar de partijen die er van hebben geprofiteerd ontspringen de dans! Ruwe schatting, stijging van 60mld tegen minimaal 10% rente, er is dus minstens 6 mld ergens naar toe gevloeid, heb je daar ooit iemand over geboord? Vergis je niet, iemands put is zeer goed gevuld, alleen vult zowat de hele wereld het gat dat is ontstaan.
RX 470;1;0.5429661273956299;Je moet toch echt maar eens gaan lezen welk pakket men de grieken heeft opgedrongen. Waaronder het privatiseren van heel wat overheids instantie's en de btw verhoging, Verder staan er kortingen op pensioen en andere zaken op de lijst welke veel te lang is om hier ooit te bespreken. Er ontstonden wilde stakingen omdat de btw verhoging zeer negatief zal uitvallen voor het tourisme. Zeker omdat het land heel veel moeite heeft om de concurrentie aan te gaan met het veel goedkopere turkije. Mogelijk kan dit op langere termijn veranderen als mensen gaan snappen dat het land niet echt westers meer is.
RX 470;2;0.45119205117225647;Mijn dagen van AMD fanboïsm zijn enigszins voorbij, ik root altijd voor de underdog De volgende kaart zal wss Nvidia worden. Helemaal sinds de nieuwe gpu's die bizarre performance leveren met amper 150w tdp. AMD levert uiteindelijk technologisch inferieure producten helaas. De performance is haast bijzaak. Maar goed, de r9 390 die ik bezit heeft nog flink wat rek begrijp ik. Kan iemand me uitleggen, waarom AMD het alleen in DX11 zo slecht doet? In het geval van DX12, Vulkan en OpenGL zie je de performance direct gelijk trekken met Nvidia of zelfs her en der een voordeeltje geven aan de AMD kaarten. Nvidia blijft alsnog de betere kaarten leveren, simpelweg omdat ze gelijkwaardige performance leveren tegen een lager verbruik. De overclock capaciteiten zijn ook weer om van te smullen, voor de tweakers
RX 470;4;0.2587006986141205;Dat ligt gewoon aan chip binning, de beste chips gaan de laptops in, de mindere chips krijgen de desktop kaarten in het lagere segment, waar ze overtuigend de beste prijs-prestatie verhouding hebben. De GCN architectuur is beter afgestemd op DX12/Vulkan/OpenGL, de Maxwell/Pascal architectuur is beter afgestemd op DX11. (En Nvidia heeft erg goede DX11 drivers, waar ze rustig de shaders van games verwisselen voor shaders die beter draaien op de Nvidia kaarten, ik meen dat ze dat niet doen met de DX12 drivers)
RX 470;1;0.4822382628917694;Ja dat is dus waarom ik gebruikelijk voor AMD sta te juichen. Nvidia speelt zulke 'vieze' spelletjes. Zo kwam ik er een tijd terug achter, dat tesselation standaard op 64x ingesteld staat. Die naar 16x geforceerd in de drivers, en in sommige games vallen framedrops ineens weg en loopt het als een zonnetje Had te maken met Nvidia die dit support, en AMD niet.
RX 470;2;0.5518937706947327;"De prijs/prestatie verhouding van de AMD kaarten in de nieuwste technology is een heel stuk beter dan die van nVidia. Daarmee zou ik zeggen dat AMD technologisch superieure producten levert. De RX470 heeft een adviesprijs die op zo'n 60% van die van de nVidia 1060 ligt (180 versus 300) terwijl de prestaties in Dx12 maar zo'n 10% lager liggen (.87 versus .8). Dat vindt ik nogal een verschil. De RX480 zit op 66% prijs en heeft betere prestaties..... AMD levert technologisch superieure producten. Helaas lopen ze echter, opnieuw (net als met 64 bit processoren) te ver voor op de markt waardoor je er als gebruiker niet zo heel veel aan hebt. Ikzelf koop eens in de 5-6 jaar een nieuwe grafische kaart. Voor mij is de keuze dan ook eenvoudig; AMD is voor mij de enige juiste keuze."
RX 470;2;0.4752727746963501;Ze leveren niet technologisch superieure producten, de prijs prestatie verhouding ligt zoals gebruikelijk in hun voordeel. Maar is dat nog wel zo? De 1060 bied voor €280 euro betere DX11 performance, gelijke DX12/vulkan performance, met 120w tdp. De rx480 heeft idd in sommige games betere prestaties, en in sommige games de 1060. Ik hou het op gelijk, ook in DX12. Hitman is de enige dx12 titel met een voordeel voor AMD, en ik kan uit ervaring zeggen dat DX12 nog steeds een ramp is in die game. Ik begrijp je redenatie dan niet dat AMD de betere producten maakt? Ze maken goedkopere producten, niet beter. En zoveel goedkoper zijn ze dus niet meer, kijkend naar de 1060. Ik zeg niet dat je verkeerd zit bij AMD, ik heb zelf een r9 390 zoals ik al aangaf. Maar Nvidia maakt logischerwijze de betere kaarten in het hoge segment, ze zijn immers aanzienlijk groter als bedrijf. In het budget segment staan ze op gelijke hoogte nu. Als de Vega kaartjes uitkomen zullen we het zien. Maar na de vorige HBM hype, heb ik het vertrouwen enigszins verloren. Zo geweldig veel sneller was het helemaal niet.
RX 470;3;0.4726758599281311;volgens mij werd de fury x tegengehouden door de 4gb limiet die hbm heeft. Anders hadden ze de 980ti-titan x prestatie's. En ik denk daarom ook dat nvidia eerder hun volta kaarten wil releasen.. Omdat ze bang zijn voor vega met hbm2 die 16 gb kan hebben. En HBM is een technologisch superieure product waar nvidia de vruchten van gaat pakken, wat ik eigenlijk zielig vind voor amd(Die hebben eraan meegeholpen). Maar zo werkt het wel en concurrentie is nodig op de markt. Maar amd krijgt wel voorrang op hbm2. Dus als ze dat genoeg bestellen kunnen ze nvidia aardig dwarsbomen.
RX 470;1;0.5419864654541016;AMD is mede eigenaar van de licenties op het HBM geheugen, maak je over AMD dus geen zorgen. Als nVidia die vruchten haat plukken, dan gaat ze hiervoor ook betalen.
RX 470;4;0.5193358659744263;Nice, dat vind ik leuk om te horen!
RX 470;1;0.32070058584213257;Omdat je het zo leuk vindt heb ik even de moeite genomen om die vorige reactie op te snorren waarvoor ik dat heb uitgezocht: teacup in 'nieuws: Gerucht: Nvidia werkt aan gpu's met HBM2-geheugen' Dat zoeken in 24 pagina's oude reacties is trouwen geen sinecure. Het zoeken op usernaam en trefwoorden heb ik als een mooi feature aangemeld, maar dat terzijde.
RX 470;2;0.42979419231414795;vind het ook raar dat nvidia niet hieraan heeft meegeholpen..
RX 470;3;0.3928413987159729;"Ze hebben de interposer bedacht bedoel je. Dat is dat stukje silicoon op waarop het geheugen rust en communiceren kan met de chip. Maar dat neemt niet weg dat AMD straks op HBM2 ""priority"" heeft t.o.v andere partners zoals Nvidia. HBM2 is niet zo spannend, lagere latencies, minder verbruik en minder ruimte op een PCB nodig maar het gaat om de chip die het werk moet verzetten en het geheugen moet kunnen benutten, das veel interessanter. Dus, alle ogen op Vega straks"
RX 470;5;0.4236689507961273;Zoals ik hierboven aangeef, ivm het peperdure gsync word het toch even kijken wat Vega gaat doen. Ben erg benieuwd dan! Kan me voorstellen dat 4gb een bottleneck is met dergelijke high-end specs.
RX 470;3;0.31168800592422485;Ik ben ook nog steeds besluiteloos. Het enige nadeel voor de 1060 is dus wanneer je tevens van plan bent een nieuwe monitor aan te schaffen. Want ondanks dat de meerprijs hiervan eigenlijk over meerdere GPU's uitgesmeerd zou moeten worden, betekent het wel dat je aan GSync modellen vast zit, en hier dus prijstechnisch significant in het nadeel bent. Dat is voor mij persoonlijk de reden om te wachten tot de prijzen van AMD naar beneden gaan, of eventueel Vega uit komt, en niet nu snel naar een 1060 over te stappen.
RX 470;1;0.6081421971321106;Daar noem je idd wel een punt, die ik even over het hoofd zag. Man wat zijn die gsync monitors belachelijk duur. Voor een 1080p model met 144hz, betaal je min €400. Voor een zelfde Freesync monitor €280. Voor die €400 heb je al bijna een freesync monitor met 1440p 144hz. En mijn huidige monitor is ook een freesync model. Toch maar wachten op vega dus
RX 470;1;0.3677899241447449;"Quote hardware.info: ""We kunnen dus wel constateren dat AMD (RX470) een GTX 960-killer op de markt heeft gebracht - anderhalf jaar na dato, maar soit.!"
RX 470;5;0.4090249538421631;DX12 en openGL zijn uniformer. nVidia zit daar nog niet in met API's voor ontwikelaars die op veel moderne games gebruikt worden. En vooral goed draaien op nVidia.
RX 470;1;0.3582521975040436;Tevens @Toettoetdaan Dat is dus waarom ik een AMD 'cheerleader' ben Nvidia heeft nogal de neiging er 'vieze spelletjes' op na te houden, wss om een monopolie positie na te streven. Zo had ik een tijd terug ontdekt, dat tessellation naar 16x (of 8x) forceren in de AMD drivers, de performance in een aantal games sterk verbeterden. Geen framedrops en stutters meer en dergelijke. Grafisch was er geen enkel verschil waar te nemen. Iets met nvidia en hun gameworks mentaliteit
RX 470;3;0.29351118206977844;gameworks is er vooral om AMD te dwarsbomen kijk maar naar tomb raider (2015).
RX 470;2;0.4003352224826813;Voor mij een meer relevante observatie is dat AMD het in DX12 goed lijkt te doen. Kijkende naar de game benchmarks tenminste. Als ik zie wat de GTX970 kaart er van bakt in de Total War Warhammer game dan is dat bedroevend. Als Total War fan lijk ik dus precies de verkeerde kaart te hebben gekozen . Voor een volgende kaart zou ik mijzelf meer afvragen wat er goed draait in DX12.
RX 470;3;0.3883945941925049;Zou mij niet verbazen wanneer AMD de 470 als decoy gebruikt. Die aanname is makkelijk te maken, gezien het minieme prijsverschil tussen de 470 en 480.
RX 470;2;0.4436478912830353;Het is aannemelijker dat AMD de kaart rond kostprijs aan het leveren is (aan de partners). Dat past namelijk goed bij de beoogde strategie om met een voordelig geprijsde kaart marktaandeel terug te veroveren. AMD wil nu graag omarmd worden door de community zodat het voor devs loont om (weer) te gaan optimaliseren van AMD. Het potentiele voordeel van de console deal richting de pc graphics markt heeft tot heden namelijk helemaal niet plaats gevonden, aldus de topman van AMD Radeon Technology Group in het genoemde interview. De variabele kosten een de kant van AMD zijn niet zo groot. Daarmee bedoel ik dat al zou je de chip halveren dit niet betekent dat de kaart twee keer zo goedkoop wordt om te maken. Er zit nogal wat omheen en je hebt te maken met logistieke kosten, sales kosten, R&D kosten, voorraad kosten, enz. Dus ook al zou een half zo grote chip daadwerkelijk 2x goedkoper zijn dan nog wordt een kaart van 200 dollar daarmee geen kaart van 100 dollar. Eerder 190 dollar. Dat je toch grote verschillen ziet in de prijzen van videokaarten is te verklaren door twee factoren -Prijzen zijn vaak gebaseerd op wat iemand bereid is te betalen -Lagere volumes (gevolg van duurdere prijs) verkopen is relatief duurder
RX 470;3;0.3349437713623047;Nu je het zegt... de RX 470 is geprijsd navenant zijn procentueel prestatieverschil met de RX 480, ongeveer m.a.w. een kleine step up, dus waarom niet voor de 'grotere popcorn' videokaart gaan, de RX 480. Lijkt misschien wat vergezocht en speculatief maar dat wil dan wel zeggen dat yields van volledige Polaris 10 chips er goed zitten!
RX 470;2;0.4874837100505829;"Ik snap het idee en de insteek van je vergelijk, maar dan dient er echter wel nog een beperkende factor aan de GPU kant bij te komen: Hoeveel kan en wil je consumeren. Ik zou persoonlijk namelijk niet zoveel popcorn kunnen of willen consumeren, terwijl ik qua FPS onverzadigbaar ben. De 470 zou in dat geval slechts een decoy zijn indien ik een CPU bottleneck heb en de daadwerkelijke meerwaarde van de 480 dus niet tot z'n recht komt, en de 20€ dus zonde zijn geweest. Daarnaast zou de timing natuurlijk ook erg ongunstig zijn, omdat de 480 niet of nauwelijks beschikbaar is. Deze heeft dus geen decoy nodig om als zoete broodjes over de toonbank te gaan, omdat hij in de perceptie niet als ""te duur"" wordt ingeschat, wat bij het popcorn wel het geval was. Ik denk eerder dat het dus in de andere richting te zoeken is: AMD heeft momentum en een mooie generatie kaarten op de markt gezet, maar kan er van de 480 te weinig leveren. Ook de 480 zal voor vele 1080p gamers DE kaart zijn die ze willen, terwijl wellicht een 470 in hun situatie ook voldoet. Nu in die vraag wordt voorzien is de kans groot dat degenen die zich bij het AMD kamp willen scharen en nu snel een GPU willen een 470 nemen, met iets minder performance, maar dan wel met even extra 20 Euro op zak. In plaats van dat ze de meerprijs voor een 1060 ophoesten."
RX 470;1;0.30359944701194763;Prijsvraag 20euro goedkoper dan 480? vertel me dan maar eens waar ik de RX 480 voor 220euro kan kopen?
RX 470;1;0.6262153387069702;Op dezelfde plek als de RX470 voor 200 euro te krijgen is natuurlijk Er staat in het artikel dat het om de adviesprijs gaat afgezet tegen de adviesprijs van de RX480...
RX 470;4;0.5963042378425598;Interessant kaartje. Ik zie deze nog wel terugkomen in de pre-built systemen van Dell/HP/etc. Voor full-hd gamen doet hij namelijk prima z'n werk op goede kwaliteit. En dat voor een lage prijs.
RX 470;2;0.3123408555984497;Ik verwacht dat die bedrijven eerst nog hun oude kaarten uit het magazijn in die pre-built systemen gaan drukken voordat ze massaal deze 4xx generatie gaan aanbieden.
RX 470;3;0.36006712913513184;Eigenlijk hetzelfde verhaal als tussen de 290/390 en 290X/390X. Die kaarten hebben namelijk ook precies een verschil van 4 compute units... Het grootste verschil in prestatie komt dan ook door het verschil in kloksnelheid, wat ook het geval was tussen de 290 en 290X. Als je bijvoorbeeld kijkt bij de buren dan zie je dat bij een hogere clock de verschillen ineens een stuk kleiner worden...
RX 470;3;0.36556118726730347;Voor zo'n minimaal prijsverschil zou ik toch de 480 nemen. Maar goed, ik heb sinds 2 weken een 1060.
RX 470;3;0.3650021255016327;Men kan toch nogal overdrijven. 'De RX480 is bedoeld voor gamers op een 2560x1440 resolutie'. Ik speel Crysis 3 met een GTX660 op 3440x1440 en natuurlijk staat niet alles op maximum, maar die beelden bewegen zo snel dat ik het verschil tussen medium en ultra toch niet zie. En uiteraard draait dat niet op +60fps maar rond de 40fps en dat werkt ook, stotteren doet het helemaal niet. Heb laatst ook Doom eens getest en dat werkt ook bijzonder vlot met mijn opstelling. Ik vraag mij af wie dat verschil opmerkt bij snelle bewegingen tussen medium en ultra, je bent tenslotte niet naar de pixels aan het kijken, maar naar de actie die er in het spel omgaat.
RX 470;2;0.4272777736186981;Wat doen de consoles dan om 1080p op max 30fps, zonder stotteren te tonen? Voorbeeld : Ik wilde eerst een videokaart voor 1440p@60fps (minimaal 60fps) op max/ultra. Voor gamen op een Dell UP2516D 1440p monitor. Foritain heeft mij duidelijk gemaakt dat ik dan meer moest uitgeven dan voor een RX 480 of GTX 1060, of niet alles op max of ultra moest zetten. (in de reacties bij zijn review). Goed dacht ik, dan wordt het 1440p met lagere settings. Maar toen kwam ik die YT video tegen en geloofde niet dat onsoles de games maar op maximaal 30fps spelen zonder stotteren. Er moet iets aan de hand zijn waar ik geen zicht op heb.
RX 470;5;0.35625261068344116;De 480 is juist een 1080p kaart..... Far Cry Primal draait net lekker dan op gemiddeld 60 fps als je drukke vecht scenes mee berekend een 2K kaart is meer zoiets als en 980ti/1070 of eeN FuryX
RX 470;3;0.33951783180236816;Kortom, als je deze kaart nu gebruikt met de huidige games, dan ga je al regelmatig onder de 60 fps zitten op 1080p? Dan lijkt het me met het oog op de nabije toekomst al verstandig om een net iets krachtigere kaart te halen die op een resolutieslag hoger ook nog goed presteert... je wil niet ieder jaar opnieuw tegen een grens aan komen te hikken, ben het er dus ook mee eens dat je beter die twee tientjes extra kan neerleggen, bedenk ook hoeveel tijd je daarmee extra met de kaart kan doen.
RX 470;4;0.48078280687332153;deze kaart doet het toch heel goed in full hd,hopelijk komt de prijs onder de 200 euro,dan is het een goede koopje.
RX 470;1;0.4521092176437378;Offtopic: Wat is dat voor een ziek geprijsde CPU uit 2013 die jullie gebruiken. Is er ook een hedendaagse variant op de Intel Core i7 4960X Boxed?
RX 470;5;0.32316842675209045;i7 6950x
RX 470;5;0.6205578446388245;Dank!
RX 470;2;0.4940614402294159;De 4Gb versies van de 470 en 480 zouden Nvidia pijn kunnen doen, maar helaas nog steeds problemen met beschikbaarheid van de kleinere geheugenchips. Als ik de reference 8Gb 480 tegen over de goedkopere versies van de gtx1060 zet (In Duitsland schelen die erg weinig qua prijs... zo rond de 270 euro) is de keuze heel wat minder vanzelfsprekend. Aan wie ligt dat tekort ? Samsung die er te weinig heeft gemaakt of toch AMD die te optimistisch waren over het zakken van prijzen van geheugen ? En lijkt me dat er nu snel een gtx1050 komt of dat de gtx970 in prijs zakt ? Edit : gek genoeg is de 4gb rx470 wel beschikbaar en de 4gb rx480 niet in Duitsland. (Wel voor een prijs die voor de 4gb rx480 'beloofd' was) Zou dat dan toch een andere oorzaak hebben? Te weinig marge en wordt de 4gb rx480 een zeldzame kaart?
RX 470;4;0.565009355545044;Leuk kaartje, wordt nog interessanter als de prijzen op of onder de adviesprijs duiken en beschikbaarheid 'normaal' is. Hopelijk gaat AMD in game laptops weer mee doen en krijgt Nvidia concurrentie er bij zodat wij wat te kiezen hebben.
RX 470;2;0.39153045415878296;ik ben nog niet klaar met het artikel, maar hier al: Beetje BS dit. Ze gebruiken gewoon async-compute die daadwerkelijk werkt en wint bij AMD, en ze hebben ook al uitleg gegeven waar in duidelijk werd dat elke 'vendor' (AMD/Nvidia/Intel dus) is gevraagd mee te kijken en te helpen, en dat ze dingen konden optimizen waar ze wouden.. _mits_ die wijzigingen geen negatieve gevolgen hadden voor de andere. Dat is toch juist OK? Dan heb je toch inderdaad maar 1 code-path, waar elke vendor aan heeft mogen geven wat ze liever anders zouden willen zien... dat is toch hoe spelletjes werken? Ik zie games ook geen complete ander code-path hebben voor nvidia en AMD hoor. Beetje krom dit van de reviewer to be honest. Ik had nog ergens anders weken terug een grote reactie post om uit te leggen wat async-compute nou precies is, zou een goede read zijn in dit geval . Ik dacht juist dat de controverse was dat Nvidia niet zo veel won als AMD waardoor hij _juist_ meer geoptmized leek voor AMD? Wat dus niet klopt, bij AMD valt er gewoon meer te winnen in async-compute omdat ze zonder async-compute te veel laten liggen.
RX 470;2;0.3780733346939087;RX 480 voor de 1440p markt? Misschien op 30 FPS ja, en anders heel veel toeters en bellen uit.
RX 470;2;0.4249995946884155;Hmm helaas een beetje een misser vanuit AMD als je het mij vraagt.. Ik had gehoopt dat deze kaart voor max. €180,- op de Nederlandse markt zou komen Tijd om verder te kijken naar iets wat wel binnen het budget zou kunnen passen.
RX 470;2;0.4299094080924988;Ik denk als de voorraad straks hoger is dan de vraag of gelijk aan, dat de prijzen dan beter worden.
RX 470;3;0.2581879198551178;Waarom en voor wie is deze kaart bedoelt?
RX 470;3;0.5092796087265015;Voor het mijnen kan die nog wel interessant zijn wil je niet teveel geld uitgeven. Om te mijnen sneller dan de 1060 en toch goedkoper (kijkend naar Ethereum, dus deze Algo neem ik even als voorbeeld, daar waar nvidia nog enigzins kans maakt). Mining snelheid voor Ethereum zit de RX 470 rond de 24-26Mh/s RX 4xx 8GB rond de 28-31Mh/s De 1060 zit rond de 20Mh/s (edit: 18 max 20Mh/s, om het toch nog leuk te maken, met max overclock 22Mh/s) De 1070 zit qua snelheid rond de RX 480 8GB, de 1070 is maar iets sneller, kijkend naar de aanschaf prijs niet eens echt een optie. Dusja op dat gebied zou je dus dom zijn om een 1060 te kopen, de 1070 zal ook geen slimme zet zijn (performance vs aanschafprijs) Op dit moment zijn oa rigs met de 290 en 390 zeer populair, qua snelheid zitten ze gelijk met de 8GB versies, maar wat is het stroom verschil tussen de 480 en de 290 en 390? Jawel op dit gebied is het wel een leuke kaart om te hebben. Geen perfectie, maar wel een verbetering op wat er is, en het laat nvidia wel deels achter zich. De 1080 is toch wel 1 van de weinige kaarten die er tegen kan opboksen, maar de aanschaf prijs rechtvaardigt het niet. Waarom word ik op offtopic gezet? ik heb het vrijwel alleen maar over de kaart en ik reageer op iemand die niet snapt waarom je deze kaart zou kopen.. Geef ik uitleg waarom er genoeg mensen zullen zijn die hem wel willen kopen, want het is een leuke kaart om te mijnen, en dan ben ik offtopic? Dit terwijl de meeste mensen het over de BTW hebben of over Griekenland en die zijn wel ontopic? nja vaag hoor.. Of is dit omdat ik met voorbeelden kom waarbij deze veel nvidia kaarten achter zich laat? Het is in ieder geval niet mijn bedoeling om nvidia aan te vallen, mijn excuses als het niet gewenst is voorbeelden te geven waaruit blijkt dat deze wél beter is dan veel nvidia kaarten.
RX 470;2;0.4709222614765167;Bitcoin minen is allang niet meer rendabel.. zeker niet met een rx470.
RX 470;1;0.382138729095459;Het gaat dan ook niet over Bitcoin maar Ethereum.
RX 470;1;0.6637612581253052;Hmkay, daar ga ik eens naar googelen. Zegt me niets.
RX 470;1;0.5994644165039062;Er zijn nog zo'n 736 coins buiten Bitcoin (althans, die wat op een markt zitten en op coinmarketcap.com staan). Ethereum hoef je niet echt google voor te gebruiken, komt hier regelmatig op de frontpage van tweakers.net Nog heel veel voor je te lezen schijnbaar. Het klopt wat je zegt dat Bitcoins minen met een GPU niet rendabel is, dat is ook de reden waarom wij dat niet doen, maar wel coins die je alleen met je GPU / CPU kan mijnen.
RX 470;2;0.43201085925102234;Ja er valt nog veel voor met lezen idd. Had je niet een linkje kunnen posten die me op weg hielp Veel sites die een rommelig verhaaltje afgeven helaas
RX 470;3;0.38570117950439453;Nou de beste bron (althans, waar het begint) is denk ik: www.coinmarketcap.com Daar staan alle coins die aanwezig zijn op de exchanges (verhandel sites). Door erop te klikken kan je naar hun website gaan. www.bitcointalk.org (forum met vrijwel alle coins / teams aanwezig) kan je veel informatie op doen en de ''trends'' volgen. Zo heb je nog wel een paar forums. Ook daar wordt met redelijk veel lof over de rx 470 en de 480 gesproken, redelijk veel snelheid vs stroom verbruik vs aanschafprijs. Maar nu ga ik tever offtopic Op google vind je vrijwel alles.
RX 470;3;0.526067852973938;Haha word wel een beetje offtopic idd. Maar erg bedankt, voor de rest kom ik er wel uit idd.
RX 470;3;0.2671302556991577;Voor minen kan je beter een specifieke asic aanschaffen stuk sneller voor je geld dan een gpu!
RX 470;2;0.4297023117542267;Niet waar.. lees mijn reacties hier boven maar aub.
RX 470;3;0.44445258378982544;Ik kan alleen uit opmaken dat de amd gpu wel redelijk is. Niet dat een dedicated asic niet veel beter zou zijn??
RX 470;3;0.33178114891052246;Een ASIC kan rendabel zijn bij bitcoin mining. Voor GPU mining niet. Deze GPU kan dus gebruikt worden voor ETH coin waar er nog geen ASIC voor is. Ook is een gpu in vele gevallen goedkoper in aanschaf dan een ASIC.
RX 470;1;0.5223627686500549;Voor de meeste (en rendabele) coins bestaan geen asics, hoe wil je daar dan een asic voor kopen als ze niet bestaan? Het is dan ook jammer om elke keer die reacties te krijgen aangezien men kennelijk niet weet dat er nog zo'n 500-800 andere coins zijn buiten Bitcoin. Grotendeels bestaan geen asics voor, waaronder in mijn voorbeelden: Ethereum / Ethereum Classic. 1 van de meest rendabele coins om te mijnen, en daar bestaan géén asics voor. Dit heb ik onder dit artikel volgens mij al zo'n 5x verteld, nu zo'n 6x haha Hier een pagina van een paar coins (738 coins): In de top 10 die je ziet, zijn er 3 waar miners (asic/scrypt) voor bestaan( wel bekende bitcoin sha256 - asic miner, en de litecoin - scrypt miner), rest is alles met GPU en/of CPU. Sommige coins die net komen, met een nieuw/ander algo zijn soms alleen maar met de CPU te mijnen, en later met GPU. @darksnake88 In geen enkel geval is een GPU goedkoper dan een ASIC, althans er vanuit gaan dat je de ASIC ook daadwerkelijk wilt gebruiken uiteraard. Bijv bij Bitcoin mining is een GPU gewoon zinloos duur, een beginners asic zit al op 10 euro en die gaat sneller dan een dikke GPU (bewijzen van). Neem je Ethereum om te mijnen, ja dan kan je wel een asic miner van 500.000 euro kopen, je kan er alleen niks nada noppes mee, dus dan is een GPU wel goedkoper ja
RX 470;3;0.3385915756225586;Voor mensen die een beperkt budget hebben. Al is 20 euro verschil met een rx 480 wel erg klein om de rx 480 niet te nemen met 20 euro verschil
RX 470;2;0.2674185633659363;Niet iedereen wil veel geld uitgeven voor een game pc en voor mensen (zoals ik en al mijn vrienden/kennissen) is het FHD scherm wat jaren geleden gekocht is, nog altijd prima en biedt een QHD/4K scherm niet voldoende meerwaarde voor de investering. +/- E200,- is voor ieder budget te betalen en dan kun je toch mooi alle courante AAA-titels spelen, zonder dat je je kinderen hoeft te verkopen voor een high end kaart en nieuw scherm.
RX 470;4;0.34065794944763184;Voor wie was de GTX970, de meest succesvolle kaart ooit van de afgelopen jaren volgens Steam, eigenlijk bedoeld? Dit is een prima kaart voor iedereen die op FHD gamed.
RX 470;3;0.45336201786994934;Voor mensen die willen gamen. Niet zo raar toch? Het is waarschijnlijk ook wel een goed kaartje voor bitcoin mining, waar de kloksnelheid waarschijnlijk iets wordt opgeschroefd voor bijna RX480 prestaties.
RX 470;5;0.3738703429698944;Echt he en dan krijg je een - hahaha wat een volk.
RX 470;3;0.5972650051116943;Toch een decent kaartje!
RX 470;5;0.8228678107261658;Super prestaties, AMD heeft DX12 nodig om een 2 jaar oude GTX970 bij te benen.
RX 470;3;0.3963887691497803;Euhm, ieder weldenkend mens? In nieuwe games (DX12/Vulkan) is deze kaart fors sneller dan de 1060 terwijl hij ongeveer 30% goedkoper is. In oudere games is de 1060 inderdaad sneller, maar zeker niet 30% sneller.
RX 470;1;0.6738250851631165;"Waar haal je die informatie vandaan? In deze review is de 470 in geen van de DX12 of Vulkan benchmarks sneller dan de 1060, ""fors sneller dan de 1060"" slaat dus helemaal nergens op."
RX 470;2;0.45113426446914673;"Ik denk dat ie de 470 door de war haalt met de 480, en ""fors sneller"" is erg overdreven idd."
RX 470;4;0.39010101556777954;Dat is toch een heel ander prijsniveau...en dit is een prima kaart voor FullHD gaming.
RX 470;5;0.27194052934646606;Mijnen, sneller dan de 1060, goedkoper in aanschaf win-win. Is maar een voorbeeldje hoor, zullen vast nog wel andere toepassingen zijn, niet alles draait om mensen die alleen maar spelletjes willen spelen gelukkig
RX 470;1;0.7780590057373047;Iemand die wat geld wilt overhouden. 'Wie gaat er nou in godsnaam een GTX1060 kopen in plaats van een Nvidia Titan X?' Snap je nu wat een domme vraag dat is?
Radeon VII;2;0.4637797176837921;In typische AMD stijl, staat de voltage weer veel te hoog ingesteld. In deze test van ComputerBase, zakt het verbruik van 277W naar 207W door de voltage 0,1V te verlagen.
Radeon VII;1;0.5643734931945801;Kan niet anders concluderen dat deze kaart gewoon het geld niet waard is. De prestaties zijn all over the place van in een enkele situatie sneller als een 2080 tot zelfs het niveau van een 2060 met een gemiddelde van trager als een 2080 en dichterbij een 2070. Zelfs het stroomverbruik is ondanks de '7'nm gewoon teleurstellend te noemen. Deze kaart zou het prima doen met een 450 euro price tag maar 700+ hell no voor minder heb je al een 2080 die stabiel presteert met betere gemiddelden. Laten we hopen dat de volgende hype NAVI wel waar voor zijn geld is want dit is gewoon een domper die zelfs groter is als dat ik verwacht had door zijn zwaar wisselvallige prestaties.
Radeon VII;2;0.5052808523178101;Geld niet waard zou ik niet zeggen, maar de doelgroep is wel erg beperkt. Voor Linux gebruikers is AMD geweldig, ja ze lopen iets achter op gaming gebied dan NVidia maar de driver buiten gaming om is echt dag en nacht. Ik heb zo ongelovelijk lang problemen gehad met de NVidia driver op mijn laptop die animaties verkeerd af speelt en de text scaling verprutst waardoor alles wazig wordt in KDE, wayland werdt tijden lang niet lekker ondersteund en om maar een beetje gebruik te maken van de kaart moet je binaries installeren waardoor je weer beperkt bent met je kernels. Sinds ik een AMD Vega 64 in mijn gaming PC heb zitten werkt Linux daar top, de driver wordt op de Vega kaarten out of the box ondersteund al moet je wel je Kernel en Mesa updaten om stabiele ondersteuning te hebben zeker op de LTS releases. Doe je dat heb je een super stabiele kaart die alles doet zoals het hoort en je dus niet voor rare verassingen komt te staan (Buiten een paar recente features die nog ontbreken in Mesa). Als ik voor dit budget zou kunnen kiezen tussen een 2080 of een Radeon VII dan is voor mij de betere rendering performance en fenomenale linux ondersteuning belangrijker dan Raytracing en alle andere toeters en bellen. Dus ja ik ben het eens, de doelgroep is enorm klein gezien NVidia voor gamers meer bang voor de buck biedt. Maar dat de kaart geen geld waard zou zijn ben ik het niet mee eens.
Radeon VII;2;0.45540913939476013;tja gamen en linus is dan ook weer niet echt een standaard iets. je moet gewoon bijna altijd een vm draaien met windows om dan weer te kunnen gamen. want de paar games die wel op linux draaien draaien ongeacht de videokaart al voor geen meter. Dus voor gamen ben je standaard al beter af met een dual boot voor game support. Ik ben voornamelijk een gamer met wat foto en video editing en gamen is hoe deze kaart in de markt gezet word. Echter laten prestaties zien dat dit een prima zij het zeer onzuinige kaart is als je er maar niet mee gamed want daar is hij gewoon minder geschikt voor voor het geld dat je ervoor neer telt. Ik zal dan ook verschillende reviewers gaan vragen hoe dit zich vergelijkt met een quadro kaart want dat lijkt in dit geval toch een betere vergelijking dan game fps.
Radeon VII;4;0.29476216435432434;Dat je per se Windows in een VM moet draaien om op Linux te kunnen gamen is een beeld dat al enkele jaren achterhaald is. Bekende recente titels zoals beide Total War: Warhammer games, Divinity: Original Sin, X-COM 2 en Stellaris draaien allemaal native (en goed!) op Linux. Met een blik op Steam blijkt dat dit lang niet de enige games zijn. Daarnaast zijn er enkele uitgebreide initiatieven om op een toegankelijke manier Windows games direct, zonder VM onder Linux te draaien. Zo werkt The Witcher 3 fantastisch onder Lutris en heeft Valve met Proton, een evolutie van Wine, een functie in Steam ingebouwd die voor het spelen van veel Windows games weinig meer vereist dan het eenmalig aanzetten van een vinkje. De nieuwste DOOM draait daar moeiteloos onder. Toegegeven, Proton is nog in beta, maar het is nog niet eens een jaar vrijgegeven en heeft toch al mooie resultaten geboekt.
Radeon VII;5;0.6356731057167053;Ik game zelf (ook) op Linux en er zijn genoeg native ports en dan kan je ook nog wine/dxvk/proton gebruiken. Gebruik een rx 580 en heb weinig problemen.
Radeon VII;2;0.5178796052932739;Het is niet zozeer een zeer onzuinige kaart. De stock voltages zijn vaak erg bagger van AMD. Met de 56/64 was dit ook het geval. Hij scored qua perf per watt beter dan een 2080. Een stock 2080FE. Deze kan je uiteraard ook undervolten. Ik ben wel benieuwd of het exemplaar van Tweakers ook zo goed te undervolten valt.
Radeon VII;5;0.2483004480600357;Met zo'n opmerking laat je merken geen verstand te hebben van Linux en zomaar een ballontje uit de lucht getrokken te hebben. Ruim twintig jaar geleden draaiden we onder andere Half-life en Counter-strike op Linux met een een ATi en/of Nvidia kaart. Soms met betere performance dan op Windows (95/98). Vele games draaiden destijds al op Linux en onder andere met behulp van Steam is dit alleen maar beter geworden. Nu worden zelfs grotere en bekendere games native voor Linux ontwikkeld. De performance is bij Windows versus Linux verschillend, per kaart, per merk. Soms onder Windows beter, soms beter onder Linux. Dit geldt tevens voor de verschillende ATi/AMD versus Nvidia kaarten.
Radeon VII;3;0.3193231523036957;Onder Linux kan tegenwoordig alles wat onder Windows kan. Een enkel speciaal programma misschien uitgezonderd, maar dat ligt meestal an de fabrikant. Linux heeft altijd het nadeel gehad van achterlopende en/of niet geheel doorontwikkelde drivers. Desondanks draaiden en draaien nu nog steeds bepaalde games beter dan onder Windows. Daar zijn inmiddels al vele benchmarks van gemaakt (phoronix). Inmiddels is onder Linux een grote inhaalslag gemaakt en helaas soms lopen de drivers iets achter. Maar zeker gezien de ontwikkelingen bij AMD met de Mesa driver, en de voortstuwing van de opensource community, doet deze onder Linux nauwelijks onder voor de Windows driver. Voor games onder Linux geldt dit ook. Steeds meer Linux games worden ontwikkeld, zelfs een aantal grote namen bieden een native of port van hun bekende game. Maar helaas nog niet alle en dat zal nog wel even zo blijven. Steam en GoG zijn mooie voorbeelden van het aanbod van games onder Linux.
Radeon VII;4;0.5245092511177063;Eigenlijk draaien er verrassend veel spellen gewoon prima op Linux tegenwoordig via Steam. Wel ff de beta optie aanzetten om de eigen Wine versie van Steam te enablen voor alle games ipv alleen de officieel geteste games. Zijn een paar games die het niet doen of glitchy zijn, maar merendeel werkt perfect. Heb wel voornamelijk wat oudere games, en dat zal wel wat helpen natuurlijk.
Radeon VII;5;0.419636994600296;Typisch, want ik speel al jaren al mijn games op Linux. No Problemo
Radeon VII;2;0.48320120573043823;Ik zou zeggen wel heel erg beperkt, hoe groot zal de Linux doelgroep zijn?, iig ruim minder dan 5% denk ik zo, de kaart is ongeveer €150~200 te duur voor gamers, want een RTX 2070 is te koop voor net even €500. En hoewel voor een flink deel niet geheel terecht, heeft nVidia de betere naam voor gamers als eerste keuze voor een nieuwe kaart, en de meeste kopers kijken niet eens (echt) naar AMD, terwijl ze meestal best een goed prijs/fps verhouding hebben. Hoop dat Navi snel is en snel uit komt, want een goede concurrent zou nVidia weer eerlijk kunnen maken, net als wat Ryzen heeft gedaan met Intel. En daarnaast persoonlijk hoop ik dat reviewers een stoppen met het gebruik van non argumenten als: Ik zeg niet dat RayTracing niet een dingetje gaat worden in de toekomst, maar in welke wereld heeft de huidige implementatie van RT nut voor gamers, de huidige cheats die game developers gebruiken om RT te emuleren doen nauwelijks onder aan echte RT, maar met een veel minder zware fps impact.
Radeon VII;2;0.4418993592262268;Daar voor zou je ray tracing toch zelf moeten ervaren, het verschil is namelijk wel goed te zien en emulaties laten helemaal niet het zelfde zien, zo ja in welke game want dan wil ik dat eens vergelijken.
Radeon VII;2;0.46822282671928406;Een vriend van me heeft een RTX 2080Ti, en ik heb zijn kaart geleend voor een week dat hij vakantie was, en ik heb RT tijdens gamen RT gewoon uitgezet, de verschillen waar veel te klein voor de prestatie impact die het kost. Begrijp me niet verkeert, heb niks tegen RT op zich, vind het zelfs een aanvulling, maar het moet niet ten koste gaan van een derde van mijn fps.
Radeon VII;3;0.4437310993671417;Klopt hij zit dicht bij de 2070 aan en erg duur voor zijn klasse.Maar wel weer sneller als de vega varianten.Ik zou deze kaart wel eens willen zien met gddr5 (x) of gddr6 geheugen aan boord. Misschien is de snelheid dan verwaarloosbaar of hij is een stuk sneller. Die Nvidia 2080 is gemiddelt genomen toch nog zo een 25% sneller en pakt toch nog weer het bovenbouw en de top van de markt.
Radeon VII;1;0.521108090877533;Kaart werkt niet op GDDR6, weet niet waarom precies maar buildzoid had het erover op YouTube, zonder gigantische bandbreedte worden deze kaarten al vrij snel erg traag.
Radeon VII;2;0.4920269548892975;Overigens is AMD erg lang rampzalig geweest onder Linux, vooral om games. Het is misschien sinds 1 of misschien 2 jaar op een niveau dat AMD ook voor niet-puristen een optie is. Tot dan waren de meeste kopers in te delen in 2 groepen: gedupeerden die niet wisten dat AMD zoveel issues had, en mensen die uit ethisch oogpunt Nvidia vermijden, bijvoorbeeld omdat Nvidia best wel belabberde business practices heeft, of omdat ze hun software zo open source mogelijk willen. Op zich zijn het geen slechte redenen. Nvidia is soms erg naar met diverse gebruikersrestricties en valse antocompetitieve acties. En open source drivers hebben ook wel praktische voordelen. Maar tot voor kort moesten veel mensen continue wisselen tussen open source en closed source AMD drivers, omdat sommige spellen maar op 1 van de 2 fatsoenlijk draaiden. En sommige spellen werkten helemaal voor geen meter. Maar gelukkig is die tijd nu wel een beetje over.
Radeon VII;3;0.361572265625;Dat klopt Amanoo, jaren geleden was ik juist overgestapt naar Nvidia omdat die propriatary driver zo ongelovelijk instabiel en slecht was en ik met NVidia minder problemen zou ervaren. Ik vond het dan ook jammer dan het bij NVidia niet veel beter ging maar was blij dat op z'n minst spellen goed functioneerde. De afgelopen 3 jaar toen AMD besloot in te zetten op de opensource community en gezamenlijk een nieuwe driver te ontwikkelen is alles gekeerd. De Radeon driver heeft nog nooit uitgeblonken maar de AMDGPU driver is erg goed en heel stabiel. Ik heb ze vorig jaar in mesa zien gaan van een OpenGL 3.0 ondersteuning naar volwaardige OpenGL 4.5 ondersteuning (Iets waarvoor je bij Ubuntu LTS overigens wel een PPA moet installeren om hier van te profiteren) en OpenGL 4.6 komt waarschijnlijk dit jaar. Enkel Vulkan vind ik ze momenteel tegenvallen omdat ze achterlopen met de implementatie van een aantal belangrijke features voor DXVK. Je ziet dus dat sommige spellen prima werken op NVidia maar bij AMD door niemand kunnen worden gespeeld en ik hoop dat het dit jaar lukt om dat bij te trekken. Al met al ben ik dik tevreden met de Vega 64 en heb ik geen twijfel dat mensen die meer zoeken dan een gamekaart blij gaan worden met deze nieuwe kaart. Maar het zou wel leuk zijn als ze qua performance een sprong weten te maken zodat het performance vs features wordt in plaats van een race om NVidia bij te blijven met hun featureset.
Radeon VII;3;0.3999650180339813;AMD is wisselend stabiel (geweest) zowel onder Windows als onder Linux. Onder Linux gebeurde het wel vaker. Het probleem met AMD was altijd de driver, waarbij het kon gebeuren dat de ene driver versie geweldig performde, en de opvolgende driver versie dramatisch kon zijn. In het latere Radeon kaart tijdperk van de afgelopen jaren is de Mesa duidelijk beter geworden. Ook de vele Radeon kaarten waren heel goed (soms beter) ten opzichte van Nvidia kaarten. Ik heb vele Radeon kaart versies gebruikt en het performde zeer goed, uitgezonderd de dramatische R9 Fury kaart. Daar werkten vele games onder Linux niet of nauwelijks, en onder Windows performde het ook niet altijd lekker.
Radeon VII;4;0.5436820387840271;Ik denk wel dat er een plek is voor deze kaart, ik werk veel met 3d simulatie, en een grote framebuffer en 1/4e 64 bit compute is erg goed te noemen voor die prijs. Nou snap ik dat dat een niche is, maar voor mij is deze kaart toch wel ideaal. Kan er prima mee gamen, en goed mee werken, zonder extreem hoge aanschaf.
Radeon VII;1;0.4722052812576294;dit vergelijkt zeer goed met Quadro, en is dus veel goedkoper. en zo'n slechte gamekaart is het helemaal niet. hij zit tussen de 2070 en 2080 in en boven de 1080ti. dus traag allerminst, ja verbruikt wel meer stroom, zeker, maar absoluut geen trage kaart, en de prijs ligt ook gewoon in lijn. dus een slechte gamekaart vind ik wat gechargeerd. ik zou zeggen, wil je gamen, en heb je ook compute werk te doen, dan is dit de perfecte kaart. en met 16gb rambuffer ben je redelijk future proof voor zwaardere workloads in de toekomst. (al zullen er volgend jaar ook snellere zijn met 16gb frame buffers natuurlijk) speel je alleen games, dan is de 2060 wss de betere koop, die geeft je 85% van de 2070 en 75% van deze kaart voor de helft van het geld. wil je de hoogste performance dan is de 2080ti de optie, Maarja mag je wel voor dokken. maar mocht je twijfelen tussen een 2070-2080 of een Vega 7 dan is die laatste helemaal nog zo'n gekke optie niet hoor. zeker als je bijv streamt, of media werkt doet en veel bezig bent met encoding of andere compute.
Radeon VII;2;0.5747010111808777;Het grootste probleem dat AMD heeft, is dat de hardware wel uitstekend is en veel performance te bieden heeft, maar dat de drivers en het design van de hardware simpelweg minder goed uit de verf komt in spelletjes. De ruwe performance van een Vega 20 of 64 is een tikkie meer dan een GTX2080, maar omdat de ruwe performance lastiger te vertalen is naar je dagelijkse spelletjes, zijn de benchmarks er niet naar. Het Nvidia aanbod is qua architectuur veel beter gemaakt voor de specifieke load die je spelletjes vragen en dat merk je. Daar staat tegenover dat de meest succesvolle mining rigs gevuld waren met AMD kaarten, zeker in het begin. Die waren heer en meester en Nvidia kaarten die 3x beter presteerde in spellen, presteerde 3x minder in minen. Je merkt ook dat relatief gezien de xbox one en ps4 betere graphics neerzetten dan je zou verwachten gezien de specs. Ook dat komt deels omdat er daar directer voor de hardware een spelletje gemaakt kan worden. Dat is al heel lang zo en gek genoeg doet AMD er vrij weinig aan. Het geeft ze wel de ruimte om hardware veel langer te gebruiken met kleine upgrades. De Tahiti serie GPU's heeft meer dan 5 jaar dienst gedaan en kan zelfs nu nog meekomen in de nieuwere spellen. Dat zit 'm vooral in driver updates en kleine verbeteringen.
Radeon VII;4;0.26191312074661255;De oude Fermi GPU's van Nvidia kunnen ook nog wel mee komen in de nieuwe spellerjes. Het verschil in prestaties met vergelijkbare AMD GPU's is vrijwel het zelfde gebleven. Ik heb hier nog een GTX 580 en een ATI 5870 werkende voor nood gevallen.
Radeon VII;2;0.4549984037876129;Nou is het wel zo dat veel compute taken vooral bij content creation weer CUDA nodig hebben, dat blijft een issue bij AMD. Anders had je met dat geheugen een hele mooie gpu voor Octane, Redshift etc maar die werken nu niet..
Radeon VII;2;0.40306556224823;CUDA werkt niet op AMD. Bij AMD kun je kiezen uit Stream (AMD versie van CUDA) of je kunt OpenCL gebruiken. OpenCL is door de abstractie laag marginaal langzamer dan CUDA/Stream direct aanspreken, maar heeft als voordeel dat je alleen OpenCL hoeft te gebruiken.. Adobe, AutoDesk en andere ontwikkelaars hebben eigenlijk in de regel altijd wel OpenCL ondersteuning. Echter vanwege het enorme marktaandeel van NVidia in zowel de gaming als desktop wereld (hoewel hier minder extreem dan bij gaming), zijn er wel een aantal ontwikkelaars welke extra optimaliseren voor NVidia..
Radeon VII;2;0.4387841522693634;"""CUDA werk niet op AMD"" ; dat was de strekking van mijn post Adobe gebruikt idd meer opengl en opencl versnelling.. Bij autodesk gaat het meer om vieuwports, compute versnelling is vaak weer bijv een renderprogramma, wat weer vaak CUDA is. Al heeft Autodesk bijv wel mental ray / iray (van nvidia) uit 3dsmax gepleurd, bijv."
Radeon VII;1;0.6793618202209473;Helemaal mee eens. De prijs is gewoon... Bizar. Ik begrijp dat fabrikanten het grote geld geroken hebben na de eerste helft van 2018, maar dit kan gewoon niet doorgaan, er is toch geen markt voor zo'n prijzen?
Radeon VII;2;0.4994249939918518;Het probleem zit hem in de 16gb HBM: deze is gewoon zo duur dat ze de kaart wel zo hoog moeten prijzen als ze er niet massaal verlies aan willen leiden. Eigenlijk hadden ze beter een 8gb versie gemaakt, die ze dan ook een stuk lager konden prijzen.
Radeon VII;2;0.44769173860549927;de kosten voor die redesign van 16gb naar 8gb is het voor AMD niet waard, ze verliezen met de huidige prijs waarschijnlijk al geld voor elke verkochte kaart.
Radeon VII;5;0.47348281741142273;wie weet komt die nog eerst de grootste/sterkste
Radeon VII;2;0.36840108036994934;Volgens mij verlies je met HBM dan ook gelijk de helft van je bus.
Radeon VII;2;0.4147987961769104;Als je de helft van de modules gebruikt wel, als je net zo veel chips gebruikt die halve capaciteit hebben wss weer niet.
Radeon VII;1;0.4682314395904541;Jawel hoor. Alleen niet hier.
Radeon VII;2;0.36108511686325073;Moet helaas hetzelfde zeggen, Deze is alleen interessant als deze fors goedkoper is dan de 2080, of goedkoper dan de Instinct MI50 dan kan je hem voor dergelijke doeleinden inzetten (Niche markt?) Of je moet echt AMD fan zijn dan ja. Als je nog niet over moet wachten tot navi anders is Nvidia nu nog steeds de betere keuze aan de top.
Radeon VII;2;0.457364022731781;"AMD wou gewoon iets om mentaal in the picture van consumenten te blijven. Meer dan enkel het ""lagere""' segment. Deze kaart is een filler. Navi is nog niet klaar of wordt nog duchtig getweaked. En zelfs Navi zal niet het mirakel zijn. Eerder een waardige opvolger voor de R480/580/590. (Jaja, met een stuk hogere performance, maar vergeet niet dat we dan ook weer even verder zijn en de maatstaven mettertijd verschuiven.) Het lijkt me eerder wachten op de generatie daarna. Daar zal AMD wel met vereende krachten aan aan het werken zijn. Alleen duurt ontwikkeling een paar jaar tijd. (Tenzij je grove fouten in een eerder design kan rechtzetten en zo enorme quick wins kan behalen. Dat lijkt me hier niet het geval.) TOEVOEGING:Nieuwe drivers gaan de VII ook geen enorme boost geven, denk ik. De architectuur gaat al even mee, en zelfs de Mi50 is al even uit.Neem de uitspraak van AMD dat ""board makers are free to create their own custom designs"" ook even met een korrel zout. Waarom zouden de board makers dat doen als er maar weinig chips beschikbaar zijn? En als de winstmarge klein of onbestaande is. Bovendien dan nog voor een kaart die geen enorme aantrekkingskracht heeft."
Radeon VII;2;0.3238494098186493;Als je ermee gaat minen is ie juist redelijk goedkoop. 90 mh/s
Radeon VII;3;0.3500578999519348;Ik hype niet ,het is wat is . En gezien AMD GPU tak op laag pitje draait. is dit wat ze nog konden doen. Daarbij verwacht ik dat Navi een nextgen GCN midrange is. Ik vergelijk de Vega VII met mijn Vega 56 en de drang om te upgraden is er niet dus wacht wel op Navi en wat next gen brengt voor DX12 SDK . RTX2060 hooguit om DX12 SDK te testen
Radeon VII;3;0.33511078357696533;Hoe reëel is het dat er nog serieuze driver optimalisaties plaats moeten vinden? Vraag me af of het niet wat voorbarig is om nu al harde conclusies te trekken?
Radeon VII;2;0.3748367130756378;Aangezien het feitelijk een die-shrink is van de huidige Vega kun je geen grote prestatie-winst meer verwachten van drivers, tenzij AMD het afgelopen jaar optimalisaties aan de drivers heeft laten liggen.
Radeon VII;2;0.39935043454170227;Der8auer heeft al een video geplaatst betreffende overclocking en daar lijkt de kaart, door de driver, heel raar te reageren. Normaal overclocken geeft hele vreemde resultaten, zelfs met droogijs werkt het niet. Echter, laat je via de software de kaart zichzelf overclocken (op droogijs), gaat ie ineens wel een stuk sneller. Oftewel: ja, de drivers hebben zeker nog wat werk nodig, maar of dit alles gaat oplossen, ik denk het niet. Bij deze nog de video:
Radeon VII;2;0.3678579330444336;Ik had verwacht dat die 7nm kaart efficienter ging zijn qua stroomverbruik ... foute redenering?
Radeon VII;4;0.4484891891479492;Hij is natuurlijk efficiënter. 7% zuiniger en 20+% sneller dan de voorganger, dat is een redelijke verbetering van de performance/watt. Ik denk dat als je kijkt naar de voordelen van 7nm die AMD aangaf: Of 50% zuiniger bij dezelfde performance of 25% sneller bij gelijk verbruik (of iets daartussenin) dat 7% zuiniger en 20% sneller aardig in de buurt komt van de efficiëntie stap die 7nm zou moeten betekenen.
Radeon VII;2;0.4010193347930908;Jammer dat de resultaten van de GTX 1080TI niet zijn meegenomen in de grafiekjes.
Radeon VII;4;0.2764430344104767;ik ga hem er ff bijzetten, moment...
Radeon VII;3;0.2716045677661896;zou je ook een GTX1070 willen toevoegen?
Radeon VII;5;0.2722916901111603;De 1070 is nog niet opnieuw getest op het nieuwe platform: we hebben gezien de tijd eerst de echte concurrenten getest... De 1070 gaan we uiteraard aan de database toevoegen
Radeon VII;2;0.4060205817222595;En hebben we nu eindelijk wel betrouwbare metingen? En niet weer met de standaard fouten die we elke keer weer opnieuw krijgen? Zoals videokaarten koud testen waardoor de prestaties met dynamische klokfrequenties, zoals tegenwoordig elke videokaart, veel beter presteren dan de praktijk. Computerbase heeft hier bijvoorbeeld een mooie test die het verschil in koud en warm laat zien. Bij een RTX 2070 Max-Q gemiddeld 10% verschil. Dat is dan wel een laptop videokaart, maar bij desktop videokaarten heb je dit net zo goed. Zelfde geld voor andere tests zijn die nu wel betrouwbaar of moeten we die nog steeds negeren? Hoe wordt het verbruik bijvoorbeeld precies gemeten? Zijn die tests wel betrouwbaar of ook niet? We zien alleen maar wat cijfertjes zonder enige uitleg. En hoe zit het met het geluid? Is dit nu wel goed? Dus niet meer met de meter veel te dicht bij de videokaart wat er voor zorgt dat de metingen niet betrouwbaar zijn.
Radeon VII;2;0.4342978894710541;Ik kan je hier zeggen dat we nooit kaarten zullen opwarmen alvorens benchmarks te draaien. Niet omdat we dat niet willen, maar omdat de tijd die het dan kost om alles te benchen astronomisch toeneemt. Dan zouden we nooit meer een deadline halen. En dat voor een mogelijk verschil dat niets zal wijzigen aan het algehele beeld en onze conclusie. Ik ben het niet met je eens dat de cijfers daarom compleet onbetrouwbaar zijn. Omdat custom koelers van fabrikanten en hun bios-instellingen invloed kunnen hebben op langetermijnprestaties/throttling, draaien we ook een duurtest. Zo brengen we in kaart in hoever de prestaties per kaart na 30 minuten teruglopen. Die hadden we ook graag voor de Radeon VII gedraaid, maar de software die we gebruiken kon helaas nog niet met deze kaart overweg. Daarbij wordt dit pas echt interessant als je kaarten van aib's gaan vergelijken, aangezien je dan met verschillende koelers te maken hebt. Het argument 'dit is een probleem is bij laptops, dus dat zal bij desktops ook zo zijn' is niet meer dan een aanname en gaat geheel voorbij aan het enorme verschil in koelvermogen. Met diezelfde denkwijze zouden desktop-cpu's ook enorm moeten throttlen en we weten allemaal dat dat niet zo is. De vergelijking tussen laptops en desktops gaat mank.
Radeon VII;1;0.5073952674865723;Nee die vergelijking gaat niet mank. Jullie eigen duurtest heeft het vaak genoeg laten zien dat de klokfrequentie bij veel kaarten de eerste minuten drastisch inzakt. Als je dan testen uitvoert die soms maar iets van een halve minuut duren (zoals aangegeven door iemand van HWI een tijd terug) dan krijg je dus slechte metingen in de standaard tests. De duurtest die jullie uitvoeren bevestigd juist mijn punt. Ook kost dit helemaal niet meer tijd. Jullie doen nu iets wat HWI altijd al doet. Kwantiteit verkiezen boven kwaliteit. Maar het is veel interessanter om minder tests te doen en die goed uit te voeren dan veel tests en dan die slecht uitvoeren. Ook hier weer waarom zou je een kaart als deze gaan testen op medium instellingen op full HD. Dat slaat gewoon nergens op. Sowieso waarom zou je überhaupt een kaart als deze testen op Full HD? Als je al die nutteloze tests eruit haalt en de tests gebruikt voor goede metingen dan geeft dat een veel beter beeld. De geluidsmetingen en verbruiksmetingen zijn dus ook nog steeds onbetrouwbaar? Aangezien je dat gewoon negeert.
Radeon VII;5;0.3991420567035675;Dank je Mask, iemand moest het zeggen.
Radeon VII;1;0.392017126083374;Het argument 'duurt anders te lang' is voor een échte techsite (je weet wel, die technologie op de proef stelt) geen reden om een dertien-in-dozijn review te publiceren.
Radeon VII;3;0.3873097896575928;Misschien een leuk idee om de rauwe data van de verschillende runs met de verschillende kaarten ook beschikbaar te maken? (Of anders wat meer info over aantallen samples, gemiddelden, afwijkingen, statistische significantie van de verschillen tussen de kaarten enzo)
Radeon VII;2;0.5361652374267578;Volgens mij verwar jij het begrip betrouwbaar met vergelijkbaar. Koud testen lijkt me prima, als je dat maar weet, dat maakt de waarde niet onbetrouwbaar, hooguit onvergelijkbaar met die van een ander test protocol. Ik vind het daarnaast nogal (wereld)vreemd om van een reviewer te verwachten dat hij de kaart eerst opwarmt, dat kost nog meer tijd. Ik vind het ook hard dat je dat een fout noemt. Het zijn gewoon keuzes. Zelfde geldt voor die geluidsmetingen, waar je die microfoon neerzet is gewoon een keuze, als je het maar elke keer op dezelfde plek doet. Dat heeft niets met betrouwbaarheid te maken, maar maakt het wel lastig te vergelijken met testen waarbij een microfoon verderweg gepositioneerd stond.
Radeon VII;3;0.4177514612674713;Wat heb je aan niet realistische resultaten? Een gemiddeld potje gamen duurt een stuk langer dan een benchmarkje draaien. dus zal de kaart een stuk warmer worden/blijven.
Radeon VII;1;0.6111370325088501;Nee ik verwar dat niet. Tweakers weet trouwens ook al heel lang van de problemen, maar ze doen er gewoon niks aan. Zelf uitleg geven doen ze niet. Het is onbetrouwbaar omdat elke kaart anders reageert. De ene heeft een klein verschil in prestaties tussen koud en warm de andere heel groot. Daarom moet je een test minimaal 5 minuten uitvoeren. Terwijl sommige tests nu soms minder als een minuut duren is mij verteld door HWI. (HWI en Tweakers delen dezelfde gegevens) Dit zorgt dus voor grove meetfouten zoals te zien is in de test van Compiterbase. Voor geluid is het ook onbetrouwbaar daarom is er ook een hele standaard voor het meten van geluid. Nu hoeft men zich hier niet perfect aan te houden, maar de huidige methode werkt simpelweg niet goed.
Radeon VII;2;0.39309975504875183;Bij de desktopkaarten net zo goed? Dat lees ik nergens. In het artikel dat je aanhaalt worden de mobiele Max-Q versies juist vergeleken met de desktopkaarten omdat die consistent presteren.
Radeon VII;3;0.4671013653278351;Het is bij desktop kaarten alleen minder groot. Maar wordt ook daar steeds groter door steeds dynamisch wordende klokfrequenties. Aantal jaar terug waren de klokfrequenties gewoon vast wat betreft 3D en was dit geen probleem.
Radeon VII;1;0.4121268093585968;Heb je misschien links naar benchmarks hiervan? Ik denk dat je dit alleen heel misschien op ref kaarten gaat zien. En dat na eerst een uur een stresstest runnen ofzo.
Radeon VII;3;0.6221016049385071;Ja je kunt het bijvoorbeeld zien in de duurtests van HWI/Tweakers. Daar zie je de klokfrequentie in de eerste maximaal 2 minuten vaak flink inzakken. Is inderdaad vooral iets wat je meer ziet bij reference videokaarten, maar die worden hier ook getest, dus het is hier ook van toepassing. Bij non-reference versies met goede koeler en die niet zeer beperkt wordt qua verbruik zie je het veel minder. Omdat dit echter per videokaart type anders is en de tests van HWI/Tweakers naar eigen zeggen zeer kort worden uitgevoerd is dit nogal een probleem hier.
Radeon VII;1;0.6667402982711792;Outrage en dan nog steeds geen links is vervelend.
Radeon VII;3;0.42347848415374756;Ik ga daarom liever voor Custom kaarten met top koeling maar OC niet
Radeon VII;3;0.24009007215499878;Geluid is gecombineerd met een duurtest vooraf, dus wat het geluid is na 30 minuten gamen. Dit wordt wel nog altijd gedaan op 10cm afstand, maar dat is puur om de getallen uit te vergroten zodat de verschillen duidelijk zijn. Het stroomverbruik wordt dmv. een PCIe riser gemeten die een log genereert met het verbruik voor PCIe, PEG1 en PEG2 indien die is aangesloten. Dit allemaal met een gemiddelde van een run 4K Ultra in Rise of the Tombraider.
Radeon VII;3;0.41057440638542175;Voor een groter verschil moet je de geluidsdrempel omlaag brengen. De microfoon dichter bij plaatsen zal alleen maar zorgen voor een grotere meetfout. Het is niet voor niks dat de standaarden voor geluidsmeting altijd op een veel grotere afstand wordt gedaan. Maar hoe precies? Hieraan is totaal niet te zien hoe er gemeten is en of het wel betrouwbaar is uitgevoerd. En zoals in mijn andere berichten is te lezen is het nogal lastig om maar gewoon aan te nemen dat het wel goed zal zijn. Want de andere tests laten duidelijk zien dat jullie flink wat steken laten vallen.
Radeon VII;3;0.3497005105018616;Hoe precies wat? Wat wil je precies weten over de stroommeting anders dan dat het een riser is die stroom meet op alle punten die stroom leveren aan een videokaart? Ik snap een paar van je andere punten wel mbt de warmte en terug klokken van de kaarten, maar probeer jij dat maar eens binnen een bepaalde tijdslimiet te doen en dat het ook nog betrouwbaar is. Maar je kan natuurlijk ook als een 'echte tweaker' waterkoeling op je videokaart nemen en nooit last hebben van kloks die omlaag gaan door warmte
Radeon VII;1;0.5219137072563171;Wat voor elektronica is er gebruikt voor de metingen? Waarom is dit gebruikt? Hoe is dat allemaal aangesloten? Hoe is het geijkt? Waarom zijn die games gebruikt? En zo zijn er vast nog wel een paar dingen te bedenken. Al zou vaak aangegeven, minder nutteloze tests. Men moet af van de HWI filosofie van kwantiteit over kwaliteit. Alle tijd die bespaart wordt door de nutteloze tests weg te laten kan dan worden besteed aan goede tests. Want zeg nu zelf. Liever één goede test dan 10 slechte.
Radeon VII;1;0.5420242547988892;Ik denk dat 99% van de gebruikers liever 10 verschillende games heeft die getest zijn op een 'koude' kaart, dan 1 game heel grondig.
Radeon VII;1;0.5209264159202576;99% van de gebruikers weet ook niet wat de problemen zijn van alle reviews. Als je hun uitlegt wat de problemen zijn dan kijken ze daar toch echt heel anders tegen aan. Neemt niet weg natuurlijk dat het niet 10 maal zoveel tijd kost om de review goed te doen. Dus de keuze is niet 10 of 1 game.
Radeon VII;3;0.4210372865200043;Als je de kaart rond de 10 minuten op moet warmen voordat je een test doet, denk ik dat het wel degelijk 5-10x zo lang gaat duren. Daarnaast ken ik letterlijk geen enkele website die het op deze manier doet. Die vergelijking met laptops gaat zoals al eerder aangehaald door anderen, behoorlijk mank.
Radeon VII;2;0.4586923122406006;Ik zei niet dat je hem voor elke test minimaal 10 minuten moet opwarmen. Er zijn meer dan genoeg manieren te bedenken om er voor te zorgen dat het helemaal niet zoveel meer tijd hoeft te kosten. De tijd die je bespaard door nutteloze tests weg te halen kun je prima gebruiken om de huidige tests wel goed uit te voeren. Dan kun je evenveel tijd kwijt zijn aan de tests, maar voer je ze dan wel goed uit. Ben je alleen maar een paar nutteloze tests kwijt en uiteindelijk even lang bezig. Iedereen blij, lezer blij doordat die betere test kan lezen, de persgroep omdat ze geen extra tijd uurloon hoeven te betalen en de schrijver omdat die dan ook voldoening kan halen. Ik kan mij namelijk zo voorstellen dat je weinig voldoening overhoudt aan een slecht stuk. Wat betreft dat het een manke vergelijking is. Net zei je nog dat je mijn uitleg snapte. Daarnaast laat jullie eigen duurtest zien dat het probleem wel degelijk ook aanwezig is bij desktop videokaarten. Het is dan ook geen manke vergelijking, bij laptops zie je het alleen nog beter. Maar ook bij desktop wordt het elke generatie net wat meer. Er is uiteindelijk maar één probleem dat overwinnen moet worden en dat is de wil om het goed te doen, die lijkt al jaren te ontbreken.
Radeon VII;3;0.4823576807975769;De wil is er wel, maar het moet ook haalbaar zijn. En zelf kom je ook nooit met fatsoenlijke ideeën die niet ongelooflijk veel tijd kosten. Het is vaak alleen maar alles afknallen omdat jij denkt dat het niet goed is, maar er missen altijd concrete voorbeelden van hoe het wel kan en als gebruikers zoals hierboven om linkjes vragen, krijgen ze die ook niet. Niet alles is met een zwart/wit plaatje op te lossen. Zeker wanneer het gaat om boostkloks icm warmte en vervolgens ook performancedegradatie, zeker op desktop, is het niet allemaal even simpel. Een laptop blijft meestal wel even warm, dus is een koud/warme laptop relatief makkelijk te vergelijken. Een beetje desktop kaart is tegen de tijd dat je bepaalde levels van een game hebt geladen alweer koud genoeg dat ie opnieuw warm gemaakt moet worden. Als je zoiets goed wilt doen, zou je met bepaalde intervallen performance moeten meten over een bepaalde tijd, maar daarbij komt dan dat zoiets altijd in een bepaalde loop gebeurd moet zijn. Je kunt ook niet elke keer een level opnieuw laden en dus moet je iets hebben dat continue draait en betrouwbaar performance kan loggen met een gemiddelde fps, anders krijg je simpelweg onze duurtest grafieken, waarvan je overigens ook prima zelf een inschatting kan maken wat voor invloed het heeft.
Radeon VII;1;0.8890184164047241;Dit is gewoon weer slap gelul omdat de wil er niet is. Ik heb meermaals manieren gedeeld en nee het hoeft zoals meermaals aangegeven niet extra tijd. Dat je iets niet ziet betekend niet dat iets niet bestaat... Het is echt puur willen. Want dit is nu de zoveelste nieuwe testmethode. Maar elke keer worden de grote problemen niet opgelost. Ik heb alles al geprobeerd om het beter te krijgen, pagina's vol geschreven tot zelfs langskomen maar niks werkt, simpelweg omdat de wil er niet is. Zolang dat niet veranderd zal er ook geen goede test komen.
Radeon VII;1;0.4243619441986084;Klinkt meer als een slap excuus om zelf geen linkjes aan te hoeven dragen of methodes waarmee het kan wat JIJ wilt. Wij denken regelmatig na om dingen te verbeteren, maar key hierin is altijd haalbaarheid, herhaalbaarheid en vergelijkbaar. Daarnaast moet er natuurlijk ook een bepaalde tijdslimiet zijn en kunnen we niet zomaar alles doen.
Radeon VII;1;0.7266945242881775;Volgens mij wil iedereen betere reviews, de enige die het niet lijken te willen zijn de mensen die het moeten maken. En zolang dat zo is heeft het voor mij helemaal geen enkele zin om voor de zoveelste keer alles weer vanaf het begin aan alles uit te leggen, aangezien er toch niks mee gedaan wordt. Misschien moet je dus je collega's eerst maar overtuigen en veel weten ongetwijfeld ook wel de hele opstellen die ik al geschreven heb.
Radeon VII;2;0.49945318698883057;Je hoeft niet meteen zo op de man te spelen hoor. Maar even een paar dingen. Je beticht ons, of een collega ervan bewust dingen te negeren in de bovengestelde vragen, maar als ik vragen aan jou stel over linkjes (andere gebruikers ook) en voorbeelden vraag van sites die doen wat jij vraagt, negeer je dat net zo graag en krijg je dingen terug zoals bovenstaande reactie. Ik sta altijd open voor suggesties en verbeteringen, maar als ik geen suggesties krijg aangeboden en alleen maar te horen krijg 'dat heb ik al zovaak gedaan', krijg ik niet de indruk dat je graag wilt dat het verandert en klaag je gewoon graag. Voor zover ik weet heb je meer met collega's van me gesproken en hier en daar zal er vast een topic zijn over e.a., maar daar weet ik verder vrij weinig van, dus jouw bovenstaande reactie kan ik dan ook weinig mee. Je geschiedenis met zowel HWI als Tweakers is langer dan dat ik er werk, dus kun je me het moeilijk kwalijk nemen dat ik niet alles heb meegekregen. Wat mij betreft is dit nu af en stel ik voor dat je of een topic maakt, of pm's gaat sturen, want dit gaat niet meer over de Radeon VII.
Radeon VII;2;0.3192767798900604;Op veel punten heeft -The_Mask- gelijk, alleen zie ik op dit gebied het steeds commercieler worden van Tweakers.net van de laatste jaren als boosdoener hierin. Ondanks dat mijn account het niet reflecteert, ben ik al sinds eind 1999 een trouwe bezoeker van Tweakers.net en heb gezien dat zeker de laatste Jaren de technische diepgaande inhoud steeds minder is geworden. Het zou mooi zijn als Tweakers.net zich weer zou afscheiden van de andere zogenaamde techsites door weer de diepte in te gaan. Zodat er weer vraag is naar engelse vertalingen van een review, zoals in het verleden wel eens het geval is geweest.
Radeon VII;1;0.7118782997131348;Ik denk dat je het beter bij de techniek kunt houden, want met dit soort bijdragen bewijs je jezelf geen dienst. Hardware Info is gestopt met de Engelse publicatie om meerdere redenen, die gewoon zijn toegelicht in een bericht voorafgaand aan het offline gaan. Geen daarvan heeft iets te maken met wat jij zegt. Sterker, bepaalde artikelen, met name grote vergelijkende tests, werden op de Engelse site vele malen beter gelezen dan op de Nederlandse. De achtergrond van deze beslissing is mysterieus noch geheim. De Engelse site kreeg al te lang te weinig aandacht, door tijd- en geldgebrek (Nederlandse [takken van] bedrijven adverteren niet op Engelstalige media, Engelse adverteerders moet je werven, daarvoor heb je iemand in de UK/US nodig]. Vanwege een technische afweging werd het eind vorig jaar noodzakelijk de knoop door te hakken, want code updaten kost geld. De keuze was investeren en ontwikkelen, óf vooralsnog de stekker eruit. Investeren in een niet-Nederlandstalige titel strookte niet met het Persgroep beleid, waarmee de beslissing feitelijk was gemaakt: een puur bedrijfsmatige beslissing. Je mening over het nut van Hardware Info publicaties is de jouwe. Onze bezoekcijfers geven aan dat niet iedereen die deelt. We blijven uiteraard waar mogelijk verbeteren, in samenspraak met bezoekers die open staan voor een discussie met meer dan alleen het eigen perspectief.
Radeon VII;1;0.45253148674964905;Dat is een hele grote, en ook verkeerde aanname over de reden dat we gestopt zijn met HWI Engels. Maar ik denk dat iedereen er baat bij heeft dat jullie weer fijn over gaan op het onderwerp van deze review.
Radeon VII;3;0.49225395917892456;Mijn GTX 1070 houdt eerlijk gezegd altijd zijn maximale boost clock van 1980 Mhz vast. Bij de GTX 660 Ti PE was dat niet anders (had MSI een mod voor). Het hangt sterk van je airflow af. Dus dat wordt voor een reviewer lastig om eerlijk mee te nemen. Immers ik heb een grote Dark base 700 met prima airflow, maar iemand met een ITX kast zal dat anders ervaren.
Radeon VII;5;0.32463639974594116;Zou je ook een nvidia 8800GTX willen toevoegen? Gewoon voor de fun
Radeon VII;3;0.2933792173862457;nee, we testen eerst nog een 3dfx voodoo 2 op het platform, die heeft voorrang
Radeon VII;4;0.448586106300354;Prima! De 8800GTX start heel misschien nog een enkele game uit de lijst
Radeon VII;3;0.46590617299079895;En de niet-TI 1080? Gezien de AMD kaarten nog vrij rap in prijs dalen, is het handig om andere items er ook nog bij te zetten, gezien ie over een half jaar misschien in de buurt komt qua prijs en je dus wel wilt weten hoe het zit. Ook mis ik eigenlijk de VR benchmarks. Hoe doet ie het op redelijk hoge resolutie met de focus op hoge FPS? Wat moet je dan wel of niet laten zitten?
Radeon VII;1;0.43966376781463623;We hebben die nog niet getest op het nieuwe testplatform, want net als bij de 1070 is tijd een factor: het kost een paar dagen om een kaart te testen. We hebben voor deze launch voornamelijk op concurrenten geselecteerd die nog te koop zijn: een 1080 kan ik gek genoeg nergens meer kopen. (da's een discussie op zich: hoe heeft Nvidia al die Pascal-kaarten, waarvan er zoveel over zouden zijn gebleven, allemaal uit de verkoop gekregen? D'r is nauwelijks nog iets van 10xx te koop...)
Radeon VII;2;0.43063032627105713;Ik heb nog een 1080 voor je liggen, ruilen voor een RTX2080? Ik zou er nog een Tweakers mok bij kunnen doen, maar ja, dan is het geen eerlijke deal meer natuurlijk.
Radeon VII;1;0.3594014048576355;We hebben de 1080 nog wel liggen (maar toch bedankt voor het gulle aanbod ), maar met prioritisering van welke kaarten getest worden hebben we voor verkrijgbare kaarten gekozen, dat bedoelde ik... De 1080 gaan we nog toevoegen no worries
Radeon VII;2;0.40937167406082153;Nouja goed, ik weet niet wat er allemaal nog te koop is en ik denk dat het ook niet nodig is om elke test met elke kaart te doen om een idee te krijgen. Het gaat hier voornamelijk om de directe concurrenten, waarbij de iets verdere concurrenten er alleen voor beeldvorming zijn. Als je een Mercedes alleen met BMW's en Mercedessen gaat vergelijken, dan mist er ook nog wel wat imo.
Radeon VII;2;0.48257049918174744;Dan heb je toch niet helemaal goed opgelet. Nvidia had al eerder aangegeven dat de hogere kaarten al vrij snel op waren maar dat vooral de x6 kaarten het probleem zijn. Daarover heeft de CEO verklaart dat dit komt door een onverwachte grote hoeveelheid RX 580 kaarten die onverwachts het retail kanaal in zijn gepushed. Blijkbaar leverde AMD deze kaarten eerst aan miners en gaan ze nu naar de retail. Waar het eerst iets van 20 Nvidia op 1 AMD was het ineens iets van 50/50 aldus de CEO. Nvidia had dat niet verwacht. Hence honderden miljoenen aan overschotten. Los daarvan lijkt het mij voor GTX 1080 bezitters in de basis goed om te weten of deze Radeon VII een interessant alternatief is. (Niet dat de kans groot is dat ze er één kunnen bemachtigen, maar dat is een andere discussie).
Radeon VII;5;0.8413233757019043;Top! Zo te zien kan de 1080TI nog steeds heel goed mee met de nieuwe generatie videokaarten op de 2080TI na.
Radeon VII;2;0.3359081745147705;Nou als je zo kijkt lijkt de 7 gelijk aan de 1080 ti. Bij hwi staat die er wel bij.
Radeon VII;3;0.5651617646217346;Wel jammer, maar je kan het zelf ook ongeveer vergelijken. Een 1080Ti zit in veel gevallen namelijk tussen de 2080Ti en 2080 in. Dus als je bij de test het gemiddelde van allebei pakt heb je ruw weg de performance van de 1080Ti. Edit: Nu dat hij geupdate is zie ik dat ik toch niet helemaal gelijk had. Oops..
Radeon VII;3;0.41863951086997986;Dus amd loopt nog steeds achter de feiten aan, Beetje jammer weer. Ik had het al verwacht dat het een dud zou worden en mijn vermoedens zijn dus waar gebleken. And so Nvidia stays king of the hill, and so the prices go up again... yeey.
Radeon VII;2;0.45650529861450195;Volgens mij mis jij wat in de verhaal lijn. Dit was geen game kaart en is er allen als een gat opvulling waar amd laat zien dat ze met zelfde generatie zonder rtx toch mee kunnen komen in high end met een kaart die daar niet voor ontworpen is en toch even een rek gewft voor de veryraagde navi
Radeon VII;2;0.5972891449928284;"Tussen een 2070 en 2080 zitten is nou niet echt ""meekomen"". Met productie is het heen en weer, dus ook geen winnaar daar, en de prijs en oplage (5000 wereld wijd) is ook niet competitive genoeg om het een viable product te noemen als de concurentie betere offers heeft. Dit is een poging om snel wat centjes in de zakken te krijgen. Navi komt pas ergens 2020 zo niet 2021, dan heeft Nvidia weer een opvolger van de 2080 en lopen ze weer achter. If anything, is het voor de AMD fanboy die wat anders wil, maar als je al een vega 64 hebt dan is deze kaart overbodig gezien de performance difference en de prijs. Ik zou het het niet waard vinden iig om te upgraden van een vega 64 naar deze kaart."
Radeon VII;1;0.7581048011779785;je moet echt even verder lezen, deze kaart kost ze geld en is puur voor marketing het is niet bedoelt als viable marketing. deze had nooit als release plaats moeten vinden.
Radeon VII;2;0.4157305359840393;Er is niks verder te lezen, wat de achtergrond van de kaart is doet niet echt ter zake, AMD heeft hem op de markt gebracht, en zet hem als game-kaart neer. Daar wordt hij op beoordeeld.
Radeon VII;3;0.4136274456977844;Het is geen winnaar idd, maar ze kunnen prima meekomen. Het is maar een klein deel van de GPU gebruikers die deze kaart kan betalen. En de 2080ti is voor de meesten al helemaal buiten bereik. Met een midrange kaart en een goede prijs bereik je meer mensen.
Radeon VII;2;0.4366658627986908;Nou nee ze komen uit zeer zwarte tijd waar ze met Zen en nu eens wel goede foundry support tegen iNtel kunnen concurreren. Maar ze zijn wel zeer beperkt zie ook chiplet stratigie voor CPU dat is meer voor productie efficentie of Monolitische chips zit er kwa R&D Bin en Yield rendement gewoon niet in. Daarnaast dat GPU tak even opzij moet voor de CPU tak. Het is goed dat iNtel nV zwaar geld vraagt voor hun producten gezien gamers Retail markt volk ook AMD bij minste verschil al lieten vallen als baksteen.
Radeon VII;3;0.6428964138031006;Niet overtuigend in alle games, maar toch verslaat hij een GTX2080 in Battlefield V en Far Cry 5 bijvoorbeeld. Behoorlijke power hoor en AMD kan weer meevechten aan de top. En als je de compute functies goed kan gebruiken dan lijkt het zelfs een erg goede koop. Voor gaming maakt de overdaad aan hbm geheugen hem misschien te duur. Had wel meer verwacht van het energieverbruik op 7nm, daar is de achterstand op Nvidia toch erg groot helaas.
Radeon VII;5;0.323007196187973;en nvidia moet nog een stap naar 7nm maken.
Radeon VII;3;0.3907175064086914;AMD moet bigger high-end chip maken voor 7nm concurentie. nvidea zou 1,5 meer transistor kunnen besteden voor 7nm RTX3080 AMD zou 2x meer transistor kunne besteden voor 7nm high-end op lage klok verbruik handicap is kleinere chip op hogere klok is niet goed voor TDP
Radeon VII;3;0.4531121253967285;Toch raar dat het verschil van Battlefield m.b.t. andere games zovele malen groter is. Komt dat dan de andere games zoveel minder zijn geoptimaliseerd voor AMD GPU's? Je zou zeggen dat AMD een voordeel heeft omdat devs weten hoe ze ondertussen moeten omgaan met de GPU die ook in de Playstation en Xbox zit. Anderzijds Battlefield is altijd wel een game die niet gericht is pracht en praal, maar op zoveel mogelijk mensen in een zo groot mogelijke map. Ofwel veel textures en daar is de VII erg snel in dankzij het HBM2 geheugen. Iets wat je ook zag bij de Fury reeks.
Radeon VII;4;0.3635919988155365;DX11 scoort AMD altijd wat minder in dan wanneer in DX12. Maar BF laat juist de power van AMD kaarten een stuk beter zien dan vele andere games.
Radeon VII;1;0.331851065158844;Ik denk niet dat ik de enige ben die bij VII eerder dacht aan het getal 7 dan aan Vega 2...
Radeon VII;5;0.44728460907936096;Over de naam, ik zag in een video met iemand die AMD representeerde dat de naam voor zowel Vega 2 als 7 staat gezien de chip op 7nm is. Zeer leuke speling op de naam!
Radeon VII;3;0.32301226258277893;Zie VII als Vega-2
Radeon VII;3;0.4858640432357788;Prestaties vallen me niet tegen. Een goeie concurrent voor de RTX2080. Wat me erg opvalt is dat de kaart erg consistent is wat betreft frames (Zie Wolfenstein II grafieken). Waar de RTX2080 en zelfs de RTX2080TI een gat heeft tussen de laagste en de hoogste van ruim 60fps zit, is bij de VII erg klein van ongeveer 20fps verschil. Ook doet de VII het erg goed op hogere resoluties, mede door de hoeveelheid HBM, 16GB is niet niks.
Radeon VII;2;0.4754750728607178;FPS in dit geval is geen goed vergelijking. Percentueel komt het op de zelfde neer bij beide kaarten.
Radeon VII;4;0.4212437570095062;Mogelijk komt dat door de HBM2 aanzienlijke bandbreedte voor 4K
Radeon VII;2;0.4496929943561554;"Het blijft me verbazen dat iedereen nog steeds zo negatief reageert op deze kaart. Alles is toch redelijk naar verwaching? Vantevoren was al duidelijk dat het niveau van een 2080TI niet gehaald zou worden. Gekeken naar de stroomproductie de laatste jaren van AMD is het ook naar verwachting dat het stroomverbruik van deze R7 nog steeds beduidend meer zou zijn dan die van Nvidia. Wat de meeste mensen daarentegen vergeten is dat AMD (1160 mil; 2017) een behoorlijk ander R&D budget heeft dan Nvidia (1797 mil, 2017). Wat we dan nog vergeten is dat AMD dit gehele budget gebruikt voor zijn CPU en GPU lijn, en Nvidia dit gehele bedrag alleen maar gebruikt voor het ontwerpen van GPU's (software en dergelijke niet meegerekend). Eigenlijk is dit nu vrijwel hetzelfde als de CPU lijn. Nog steeds niet zo goed als de grote concurrent, qua pure kracht (IPC) en stroomverbruik maar wel een prima product. En toch reageert iedereen bij Ryzen direct verbouwereerd en onder de indruk, en als het om GPU's gaat is het direct bagger en lijkt het nergens op. Is het in beide situaties niet vrijwel hetzelfde? Namelijk indrukwekkend dat AMD nog met de grote jongens mee kan komen, ondanks hun budget/marktpercentage? Terug naar het product: als je op dit moment zo'n 700 euro wilt betalen voor een GPU en je wilt de beste prestaties dan neem je een 2080. Wil je graag betere ondersteuning voor Linux of gewoon graag wat anders dan Nivida. Dan heeft AMD toch weer een prima product."
Radeon VII;5;0.4074990451335907;"Yep. Het is tegenwoordig k*t of het is: ""WoW! Epic! Super! Vet, man!"". Het is zwart of wit, grijs bestaat niet meer. Deze Radeon VII is gewoon een degelijke kaart, niet meer en niet minder en zeker wanneer je aan content creation doet en/of Linux draait. Wanneer je op 4k gaat gamen, dan zijn de prestaties van het niveau 2080 RTX. Dus ik ben het met je eens. De soep wordt niet zo heet gegeten; de negatieve reacties zijn overtrokken. De kaart presteert gewoon goed en naar verwachting. Ja, ik had ook graag gehad dat AMD nVIdia van de wereld had geblazen met een videokaart van maar €650.-. ...Maar laten we wel enigszins realistisch blijven met zijn allen."
Radeon VII;2;0.5026223063468933;Op CPU gebied lopen ze dichter bij de concurrentie en is cijfer neuken. Op GPU gebied doet AMD het stuk rustiger aan als zijnde stuk minder in de pijplijn. sinds paar jaar ook geen full nextgen productlijnen uitrol. Hoop rebranding. AMD Comeback is zwaar vanuit diepe dal waar de GPU tak even de CPU tak moet laten voor gaan. Ik denk eerder het is zwemmen of verzuipen was en uit diepe afgrond omhoog kruipen. Dan kan je niet wild doen om zwaar uit te pakken zonder cash om TMSC GF productie pipeline te vullen Ook R&D zal afgelopen jaren iig GPU tak stuk minder hebben dat minder teams parallel bezig zijn aan meerdere chips
Radeon VII;4;0.488711953163147;Helemaal mee eens. Je zou ook verwachten dat wie niet competitief gamed voornamelijk in 4k speelt (zo duur is een 4k monitor / tv niet meer), waarmee deze kaart snel veel interessanter wordt, zeker met een iets lager voltage: met 0,1V minder verslaat hij de 2080 in stroomgebruik, zie Granted, dat je zelf het stroomgebruik moet tweaken is wel minder.
Radeon VII;5;0.24789837002754211;"Bedankt voor de ""Davinci Resolve"" test. Het zal dus een AMD worden"
Radeon VII;2;0.4530380666255951;Als je hem kan krijgen, volgens de geruchten gaat deze kaart zeer moeilijk leverbaar worden.
Radeon VII;1;0.5549529194831848;Hier te koop voor 769 euro.
Radeon VII;1;0.29518383741378784;Ondertussen staat ie op levertijd: minimaal 10 dagen.
Radeon VII;3;0.3219178318977356;Ja, na iets van 4 uur was die uitverkocht. Nog wel bij Caseking die ook naar NL verzend voor iets meer.
Radeon VII;3;0.36461347341537476;Dus in het kort, hadden ze vaak een kaart die net niet mee kon komen, of aanzienlijk goedkoper was dat het het overwegen waard is, hebben ze nu een kaart die best okish is, maar waarschijnlijk niet leverbaar zal zijn. Dezelfde troef die dus ook het probleem geeft, net als de 5xx series die best interessant voor gamers waren, maar interessanter voor coin-miners, resultaat dat die dingen bijna nergens te koop waren. Maar waar is de 1080/1080ti in de test? Natuurlijk is de 2xxx al de opvolger, maar de 1080 is daarmee nog steeds een sterke concurrent van deze kaart én beschikbaar, zeker tweedehands nog aardig aanwezig.
Radeon VII;2;0.4428069293498993;Ik denk niet dat hier een groot probleem in gaat zitten. Voor minen was pure performance belangrijk, in andere GPGPU toepassingen worden vrijwel nooit AMD kaarten gebruikt. Een AMD kaart moet significant goedkoper zijn dan een vergelijkbare Nvidia kaart om echt veel afzet te halen voor GPGPU.
Radeon VII;3;0.3987487256526947;Het is jammer dat we de verkoopcijfers niet hebben, maar kijkend naar de synthetische benchmarks is AMD momenteel daar beter in. Kijk je puur naar workstations dan heb je mogelijk gelijk. Al heeft dat natuurlijk wel tijd nodig. Ivm ondersteuning en gewenning. Bij Big Data en modelberekeningen kijkt men toch echt naar de totale prijs. Dus ook naar hoeveel stroom kost het per uur en hoeveel berekeningen kunnen er per uur uitgevoerd worden.
Radeon VII;2;0.5662274956703186;Jammer, voor de (super) high end nog steeds geen AMD kaart.. Ik had gehoopt op meer maar misschien is dat voor een lange tijd valse hoop.
Radeon VII;3;0.43844231963157654;Voor mijn gevoel is AMD de Dark ages nog niet te boven, het gaat beter met Zen en jaren sucses moeten ook nog eens jaren verlies compenseren . De R&D voor de GPU tak kruip wat omhoog maar zal verre zijn van wat ATI voor AMD overname had. Ik verwacht iig de komende 2 jaar nog geen Bigchip High-end dat ze gooi naar de kroon willen.Tenzij ze sowieso niet voor onrendabel top high-end gaan want dat zal je teneerste heel goed bij kas moeten zitten om onrendable hoge prestige en imago producten op te leveren.
Radeon VII;2;0.4368201196193695;Ik snap dat het grote geld in de mid range kaarten zit maar de marge op de high end is groter. Bovendien doet het veel met imago en ook met enige vorm van competitie. Nvidia heeft nu een monopoly in het high end segment. Als ze deze kaart met 8 GB HBM2 hadden gemaakt en voor 500 euro hadden verkocht had Nvidia al klamme handen gekregen. Ik hou erg van AMD, vooral de vrijheid die ze geven maar ik ben nu geforceerd Nvidia te gebruiken.
Radeon VII;2;0.4761575162410736;Ik vind het een beetje gek dat ze voor 16GB zijn gegaan. Op Anandtech staat dat ze van 2 naar 4 stacks van HBM geheugen zijn gegaan en de verdere extra ruimte door naar 7mm te gaan niet gebruikt hebben omdat de chip anders te groot wordt en de yields dan niet goed genoeg zijn. Er zijn zelfs maar 60 units t.o.v. de 64 units in de Vega 64. Ik vraag me af of AMD ook een variant had kunnen maken met 12GB geheugen en 64 units (of misschien zelfs wat meer dan 64 units). Dan zou de kaart een stuk goedkoper zijn geweest en met die 4 extra units zou het gat met de RTX 2080 misschien net gedicht zijn. Het blijft voor mij wel vaag dat AMD met 7nm t.o.v. nVidia's 14nm, en met minder features, daar door minder transistoren, en met HBM2 geheugen dat volgens mij zuiniger is dan GDDR5/6 toch zoveel meer stroom verbruikt (30%+). Ze draaien beide op ongeveer het zelfde aantal mhz.. Kan iemand dit verklaren?
Radeon VII;3;0.5438216924667358;"Als AMD volume gaat leveren een redelijke kaart voor degene die een kaart uit het rode kamp wil hebben en toch redelijke high end performance, ik ben er echter bang voor dat de geruchten waar zijn dat Radeon VII een ""stopgap"" gaat zijn tot Navi en we dus maar een beperkt aanbod gaan zien (geruchten hadden het over 5000 kaarten bij launch, wereldwijd). Jammer voor ons gamers, maar voor AMD mogelijk goed, des te minder chips ze gebruiken voor Radeon VII, des te groter de kans is dat zoveel mogelijk chips als MI50 verkocht worden tegen voor AMD veel gunstiger marges. Zo heb ik bijvoorbeeld wat geruchten gelezen dat de Radeon VII verlies verkocht wordt. Het verbruik valt me wel iets tegen, daar had ik wat meer van verwacht maar AMD lijkt ervoor gekozen te hebben het beschikbare ""budget"" in prestaties te steken en minder in het verlagen van het verbruik."
Radeon VII;3;0.5356230139732361;verbruik kan je vergeten met vergelijkbare CU zelfs iets minder en ietwat hogere klok. Klok vermogen relatie is niet lineair. Dus zuinig is er sweet spot waarbij je grotere chip ontwikkeld die moet presteren bij efficiëntere klok. probleem is dat AMD niet veel verschillende chips in ontwikkeling heeft en dus geen directe tegenhanger kan leveren. Ik wacht wel op Navi zal meer in mijn prijsklasse vallen als mid tot sub high-end
Radeon VII;2;0.4854443669319153;Ik mis het geluidsniveau van de kaart in deze review. Ik zie op andere sites dat het vergelijkbaar is met een Vega 64 op luchtkoeling. Tijdens belasting is die ongeveer 10 dB luider dan een FE 2080. In mijn ogen is dat verschil absurd! Beetje spijtig dat dit niet is meegenomen in deze review...
Radeon VII;4;0.2846148908138275;
Radeon VII;1;0.527560830116272;Het is een triestige constatering dat AMD na 2 jaar wachten eindelijk met een GTX1080Ti performance kaart komt en dat deze op een kleiner procedé alsnog meer verbruikt… Nu volgt er een heel verhaal van getriggerde AMD fans die gaan zeggen dat deze kaart voor de helft niet gebruikt wordt en dat AMD zich niet op de high end markt heeft gefocust, maar dat interesseert me allemaal als neutrale partij geen zier: ik vind het triestige constatering. NVIDIA (en de enthusiast gamer markt) heeft veel meer concurrentie nodig.
Radeon VII;2;0.5123214721679688;Dit is normaal. Gebrek aan R&D Gebrek aan refreshes gebrek aan full chip line zoals grote bigchip voor top-end met lage klok maar massive parrallel performance. En regelmatig refreshes zodat chips ook periodiek verbeter worden. Die luxe heeft AMD GPU tak al lang niet meer Nvidia wel. Omdat AMD CPU langdurig donkere tijden heeft gehad en GPU even op de handrem moet. Als je AMD wat eerder met concurrentie komt moet er ten eerste capitaal binnen komen dan kan de R&D voor de GPU tak omhoog. Zodat je na GPU product cyclus AMD meer had kunnen doen en dus ook beter met Nvidia kan meekomen zo ook high-end chips. Dan moet men massaal vanaf release tijd van de Ryzen AMD CPU kopen en iNtel links laten liggen . Maar men blijft braaf intel kopen
Radeon VII;3;0.6061514616012573;Prestaties zijn zoals verwacht jammer genoeg zat er niet meer in maar was te verwachten. Verbruik is lager dan dat van de Vega 64 dus dat is wel mooi. Ondanks dat de kaart 4 CU's minder heeft is hij clock voor clock wel duidelijk sneller dan de Vega 64: Het lijk er op dat ze wel wat zaken hebben kunnen verbeteren/gefixed hebben. Bij Vega leek die Rasterizer niet echt veel te doen. Ik vraag mij af of die nu beter werkt. (Je zag tussen Fiji en Vega relatief weinig verschil op gelijke clocks. Soms zelfs achteruitgang en met een goed werkende rasterizer verwacht je dat niet) Hopelijk kunnen ze bij Navi een serieuze stap maken op dat gebied.
Radeon VII;4;0.5802234411239624;Mooie kaart als je perse een AMD kaart wilt of aan bepaalde content creation doet. Overigens een erg mooi alluminium reference design.
Radeon VII;3;0.5584637522697449;Zoals verwacht, maar +10% in vergelijking met een goeie LC of aftermarket Vega 64 (behalve in BF5 om een of andere reden). Ik wacht op big Navi of wat daarna komt..
Radeon VII;2;0.5641244053840637;Jammer... Ik had op meer gehoopt.
Radeon VII;2;0.4275228977203369;Hele domme vraag maar ligt het aan mij of is deze kaart redelijk nutteloos? prestaties onder de 1080ti, veel later uitgekomen, moeilijk verkrijgbaar? Ik snap niet zo goed in welk segment deze kaart moet passen.. Alle highend games hebben al 1080ti.s of 2080ti's gekocht.. Niet bashen gewoon echt nieuwsgierig
Radeon VII;1;0.38036903738975525;Ik weet niet hoor, maar op nvidia's eigen site is de 2080 799, geen 849
Radeon VII;3;0.6605969071388245;MacOS heeft betere ondersteuning voor de AMD kaarten. Ik heb op dit moment een VEGA 64 als EGPU aan mijn Macbook hangen. Prima prestaties. Niet geweldig, maar ik zou een upgrade naar een VII zeker overwegen, alleen niet voor elke prijs.
Radeon VII;2;0.30283012986183167;"Zonder RTX is het gewoon de 3de snelste kaart van het moment. En voor de hogere prijzen kan je Nvidia bedanken. Toen ik mijn Vega 64 kocht voor 450€ ( via een Tweaker ), was het een no brainer. Ik wou ook een nieuw Ultra Widescreen. Die combo met aan 34"" Freesync van LG , 144Hz was veel goedkoper dan een GTX 1080 van Nvidia met een gelijkwaardig G-sync scherm. Na 2 jaar, blijkt zoals altijd bij AMD, met wat extra performance uit hun drivers, dat mijn Vega 64 even snel is als een 1080. Hij verbruikt dan wel weer, maar eer je dat verschil in aankoop ooit terug zou verdienen met je stroomrekening, dan moet je toch werkloos zijn en niet meer slapen... Ik koos destijds voor een 2K scherm ( 2560x1080 ultra wide ), omdat een 1440p scherm met hogere resolutie mijn in Quake Champions geen continue 120 fps zou geven. Nu heeft AMD dus een kaart waar een 144Hz scherm op 1440p of 4K waarschijnlijk goed uit de verf komt... Mooi toch ?"
Radeon VII;3;0.29457804560661316;"@willemdemoor die ""onderliggende verhoudingen""-tabel is een behoorlijk raadsel om te ontrafelen, was dit de manier om de data zo te representeren ?"
Radeon VII;2;0.416524738073349;Nou het is nu 100% zeker dat ik geen Vega VII ga aanschaffen. De prestaties staan bij mij niet op de eerste plek, maar dan moet je wel leuk kunnen overclocken en goeie koeling hebben. De stock koeler kan de kaart niet voldoende koelen zonder enorm lawaai. Jammer. We wachten wel weer op Navi.
Radeon VII;5;0.48626962304115295;Luid, drivers matig, toch besteld! Beste AMD kaart tot nu toe en ik weiger langer te wachten!
Radeon VII;5;0.6068850755691528;veel plezier ermee!
Radeon VII;2;0.350769579410553;"Same here, was aan het wachten op de VII voor mijn volgende build. Ergens een beetje irrationeel om deze te kopen, zeker gezien de ""launch day premium"" die je betaalt doordat de kaart eerder 800 dan 700 euro kostte, maarja... niet alles in het leven moet rationeel zijn, dacht ik dan"
Radeon VII;1;0.5588361620903015;Precies om 15:00. Dus @Dennism je hebt gelijk. Volgens onderstaande link kost alleen al het geheugen 320 USD Dus ze gaan met deze kaart ook niet op prijs concurreren. Jammer.
Radeon VII;1;0.8356045484542847;Ik begrijp niet wat AMD bezielt om voor HBM2 te kiezen terwijl het reteduur is. Die fout hebben ze gemaakt met Fury, Vega 10 en nu weer met Vega 20. Zo nemen ze het enige vlak waarop ze met Nvidia kunnen concurreren weg(de prijs). De kaart presteert iets minder dan de 2080 met een hoger verbruik. Als je die dan voor een lagere prijs kan verkopen zou het misschien nog iets kunnen worden maar dat kunnen ze niet. Telkens opnieuw maakt AMD dezelfde fout, dan moeten ze het zelf maar weten. Wat een falend beleid.
Radeon VII;2;0.46094176173210144;Simpel, Vega heeft het nodig anders kunnen ze niet op performance concurreren... En dit ding is niet meer dan een symbool om te kunnen zeggen dat ze toch iets hebben op highend vlak. Het wachten is op Navi, maar die is helaas iets vertraagd. Geef het nog een paar maanden en hopelijk kan AMD dan eindelijk echt weer wat tegenover Nvidia zetten (hoger dan de low tot midrange).
Radeon VII;2;0.2844999432563782;Tja, dat heb ik eerder gehoord, zoals bij de introductie van Polaris.. toen was het wachten op Vega, want dat was het high-end antwoord op Nvidia...
Radeon VII;3;0.41536012291908264;Ja ik wil het ook eerst zien voor ik het geloof hoor (en zelfs dan... ik heb 'helaas' een gsync monitor)... Maar AMD heeft wel één voordeel, en dat is dat Nvidia zo nodig met RTX moest komen. Dat kost Nvidia een heleboel ruimte op hun chips, waardoor hun chips relatief duur zullen blijven voor de performance die ze leveren als je RTX niet gebruikt. Daardoor is de kans wel wat groter dat AMD een betere deal kan bieden aan mensen die geen RTX willen.
Radeon VII;2;0.6190707683563232;Erg jammer dat de Radeon VII gemiddeld net zo snel is als de Geforce GTX 1080 Ti FE, die al weer 23 maanden bijna 2 jaar terug uit kwam, en nog is 50w meer gebruikt dan de Geforce RTX 2080 en net zo duur of duurder gaat kosten als hij uit komt. In DX12 en 2560x1440 is hij aardig snel, en sneller dan de Geforce RTX 2080, met Battlefield V en Deus Ex Mankind Divided en Strange Brigade, behalve in Forza Horizon 4 en Shadow Of The Tomb Raider en Star Wars Battlefront II. Voor mij geen waardige upgrade, omdat hij niet veel sneller is dan de Geforce GTX 1080 op DX11, waar ik nog steeds de meeste spellen van speel.
Radeon VII;1;0.6632750034332275;Je snapt het niet. Ze moesten voor hun high-end gpu nooit voor hbm gekozen hebben. Dit gaat terug tot Fury, daar zijn de problemen begonnen voor AMD. Hoe komt dat denk je? Door het veel te dure hbm-geheugen. Kan goed zijn dat Vega hbm nodig heeft om beter te presteren dan met gddr omdat hij ervoor ontworpen is. Maar dat is het punt niet. Als je al een achterstand hebt op Nvidia kies je voor zekerheid en ga je niet beginnen experimenteren met hbm-geheugen. Hadden ze toen(met Fury) voor gddr gekozen was er nu niets aan de hand geweest en kon AMD rustig op prijs concurreren met Nvidia. Maar nu zitten ze zwaar in het problemen. Daarom zeg ik dat het slecht beleid is. Het HBM2-geheugen alleen van de Radeon VII zou $320 kosten, dat is bijna de helft vd prijs vd kaart. Dat zegt toch genoeg?
Radeon VII;2;0.4664457440376282;Ja ik snap het wel... Mijn punt is dat ze toen misschien de fout maakten door op hbm te gokken, maar zeggen dat ze nu met VII weer de fout maken dan snap je de positie van AMD op dit moment niet. Ze weten echt wel dat dit pad niet de juiste is (voor games), daarom zijn ze druk bezig met Navi. Ze hadden nu Navi willen presenteren maar helaas is daar iets bij mis gegaan waardoor deze een paar maanden vertraagd is. Ze hadden dus de keuze om helemaal niks te presenteren... Of deze kaart die eigenlijk helemaal niet meer bedoeld was voor de game markt toch beperkt uit te brengen zodat ze in ieder geval toch iets hadden en niet helemaal afwezig zouden zijn op de highend markt (ook al is het vooral de schijn ophouden). Het is dus niet zo dat ze het nog steeds niet snappen, dit is gewoon een zoethoudertje wat eigenlijk helemaal niet nieuw is (en zelfs nauwelijks verkrijgbaar zal zijn). Of AMD het licht gezien heeft en nu wel zal kunnen concurreren dat moeten we helaas nog even afwachten als ze uiteindelijk wel met Navi kunnen komen.
Radeon VII;1;0.668124258518219;Vega was ook al geen game gpu maar meer voor de professionele markt. En Navi zou voor de midrange zijn en niet high-end. Het zou wel eens nog heel lang kunnen duren voordat AMD weer met een high-end gpu komt die gericht is op gamen. AMD heeft ATI helemaal kapotgemaakt. Ze zouden het beter verkopen aan een bedrijf dat wel financieel gezond is. Zoals de zaken nu gaan is dit voor de consument helemaal niet goed. Doordat ze eerst met hun cpu's in de problemen gekomen zijn(omdat ze op 28 nm bleven hangen) hadden ze geen geld meer om een deftige gpu te ontwikkelen. Aangezien een gpu voor de professionele markt meer geld in het laatje brengt hebben ze zo'n gpu ontwikkeld en geen game-gpu. Maar voor de consument is dit natuurlijk heel slecht. AMD zou zich veel beter richten op alleen nog cpu's ontwikkelen. Een gpu-divisie erbij kunnen ze eigenlijk financieel niet aan. Hun beste tijden hebben ze gehad met de Duron/Athlon en dan hadden ze ATI nog niet opgekocht.
Radeon VII;3;0.3763565421104431;Ze hebben op moment niet veel opties. Dus iets pro wat ze al hadden als high-end gamer kaart op de markt zetten. Ze moet nu roeien met riemen die ze hebben op GPU gebied. R&D was laag is meer geworden maar de vruchten daarvan komen uiteraard later. Daarnaast is 1 op 1 dieshrink ook save . minder risico dan met een nextgen architectuur op dieshrink.
Radeon VII;2;0.4106256067752838;Schijnbaar kon het bedrijfstechnisch niet echt anders als ik youtuber AdoredTV moet geloven. Vanaf Vega zijn de kaarten ook niet echt primair voor gamen bedoeld. Al zeggen ze dat misschien wel en je kan er ook prima mee gamen, al is het t.o.v. de concurrentie geen sterke zet zo is (achteraf) gebleken. We weten dat de marketing afdeling niet werelds beste is ^_^ . Het was de bedoeling om nu Navi uit te brengen en dat lag schijnbaar ook lange tijd op schema. Echter is er waarschijnlijk toch ergens een fout in geslopen en hebben ze daar 2-3 maanden vertraging in opgelopen. Nu hebben zet een beetje uit noodzaak en om het gat voorlopig te dichten, maar deze Radeon VII uitgebracht. Zo snel even de theorie van AdoredTV en wat ik er, in een notendop, nog van herinner.Wil je het fijne er van weten...
Radeon VII;2;0.4071187973022461;Kan ik inkomen. Maar het is al vanaf Fury dat het misloopt in de high-end voor AMD. Vanaf die gpu heeft AMD voor hbm gekozen dat veel te duur is. En die kaart had er ook maar 4 GB van wat ook al een nadeel was want Nvidia had er 8. Fury kostte €700 als ik het mij goed herinner wat toen meer was dan Nvidia's kaarten.
Radeon VII;1;0.7356972694396973;Wat ook nooit de bedoeling is. Dit is geen gamer card wat AMD al tig keer heeft gezegd.
Radeon VII;1;0.3964686095714569;Dat is natuurlijk niet direct waar, de MI50 die dezelfde GPU gebruikt als deze kaart (al is deze kaart een cutdown naar 60 CU's uit mijn hoofd) is inderdaad geen gamekaart. Deze kaart, de Radeon VII zoals hier besproken, is echter op CES volmondig als gamekaart gepresenteerd door AMD.
Radeon VII;5;0.6041712164878845;THE WORLD’S FIRST 7nm GAMING GPU Amd schreeuwt het op hun eigen website:
Radeon VII;2;0.35388171672821045;"Los van alles is het iig heel fijn dat er ""concurrentie"" gekomen is voor NVidia kaarten! Dat is alleen maar gunstig voor de prijzen. Maar toch vind ik het ook mooi voor AMD dat deze kaart toch zo goed presteert! Dit zag ik niet aankomen .. zelfs de 2080 blijft soms achter. En dat voor een 7nm refresh Nu nog een échte nieuwe architectuur lanceren en Nvidia klop geven .. disclaimer: niet dat ik anti-RTX ben of anti-Nvidia, maar die prijzen slaan nergens op! Ik heb altijd ATI kaarten gehad, en na mijn R9 270 nu voor het eerst een Nvidia RTX 2070 als primaire gaming kaart. Zodra de nieuwe AMD ook voor € 525 voorbij komt, zal het bij mij wel even gaan knagen .. hoewel de 2070 met RTX toch nog een streepje voor heeft is het nog koffiedik kijken hoeveel dit geïmplementeerd wordt .. maar qua performance is AMD in de win!"
Radeon VII;2;0.25995713472366333;Mee eens. Concurrentie op high end is hard nodig, en dit is concurrentie. Ze hoeven niet gelijk met alles bovenaan te staan, maar ze staan er nu wel. Daarnaast kon deze kaart over 2 jaar nog best up-to-date zijn voor 4K gaming. Dat HBM geheugen biedt bizar veel bandbreedte, en AMD kaarten blijven over het algemeen vrij goed in prestaties vooruit gaan met nieuwe driverupdates en nieuwe games. Verder is dit beter dan niks doen. We hebben bij Intel gezien wat geen concurrentie jarenlang betekent heeft, dat is stilstand...dat zijn 4 cores...dat is 3.6ghz. Dit is AMD die, ondanks een tegenslag, toch concurrentie weet te bieden. Terwijl ze er geen winst op maken. Ik hoop dat de gamers deze kaart zullen kopen, en wacht af waar AMD later dit jaar mee komen gaat.
Radeon VII;3;0.5062177777290344;En natuurlijk zullen we verbeteringen zien in wel meer games wanneer de driver updates (en/of game updates) voor deze kaart uitkomen. Als er nog meer games zo'n efficiëntie teweeg kunnen brengen zoals bij BFV dan zal het een knappe prestatie zijn. Maar gezien de leverbaarheid lijkt me dit een korte tussenstop tot de Navi generaties.
Radeon VII;3;0.47782233357429504;nvidia loopt voor omdat hun wel de capitaal hebben om biggest chips te ontwikkelen en ook refreshes om architecturen en efficiëntie en bottleneck te verbeteren. Bij AMD ligt het nogal stil en zijn er in heden meer rebrands die tegen refreshes moeten uitzingen. En bigchip high-end laten ze overwegend varen. Nu met enige 7nm kunnen ze iets wat mee komen op huidige high-end. Voor toekomstige 7nm begrippen is dit niet high-end kwa chip diespace size.
Radeon VII;5;0.5638102293014526;Ben altijd wel benieuwd wat AMD doet. laatste AMD kaart die ik heb gehad is alweer een tijdje terug. Ben in de vorige generatie uiteindelijk toch voor de groene broeders gegaan. Dit omdat de prijs van AMD vertaalt in Euro's totaal niet overeen kwam met datgene wat werd gecommuniceerd. Benieuwd wat deze kaarten gaan doen. AMD heeft nog steeds leuke features waaronder meerdere schermen zonder dat alle schermen van exact hetzelfde type moeten zijn.
Radeon VII;3;0.3432512879371643;Hardware prijzen komen eigenlijk altijd wel overeen met de dollar prijzen. Men maakt alleen bijna altijd de fout door prijzen zonder VAT met prijzen te vergelijken die inclusief BTW zijn.
Radeon VII;1;0.3617386817932129;Klopt daar kan ik in mee. Waarschijnlijk destijds net de pech dat de mining hype begon en dat mn 1070 op launch vrijwel direct beschikbaar was. Maar dat staat verder los van dit nieuws
Radeon VII;3;0.3357643485069275;"Maar ondertussen zijn er ook geruchten dat er maar 5000 Radeon VII kaarten zouden zijn. En dan is 't ook nog eens een omgebouwde Radeon Instinct MI50, je zou dan eerder denken dat ;t met die productie wel goed zit maar dat valt dus tegen. En het lijkt er ook op dat deze kaart een noodgreep van AMD is omdat ze anders niet tegen de 2080 op kunnen boxen. Afgelopen jaren is 't toch vaak zo geweest dat de topkaart van AMD maar marginaal sneller was dan de budgetkaart van Nvidia. En dat als je maar weinig budget hebt dat je ook beter voor AMD kan kiezen omdat je toch en topkaart hebt voor je geld ipv een entry level kaart zoals bij Nvidia. En ik denk dat een hoop consumenten voor die prijs toch liever voor de 2080 gaan, die is stiller, gebruikt minder stroom, is ongeveer net zo snel maar heeft wel raytracing als voordeel. Voor de content creators is de Radeon VII wel weer erg aantrekkelijk, je hebt voor een relatief goedkoop bedrag een kaart die veel tijd (en dus geld) kan besparen. Ben zelf (vooral door buudetbeperking) jarenlang een AMD fanboy geweest maar als ik een groter budget zou hebben dan zou ik toch liever voor Intel en Nvidia gaan nu. AMD redt 't steeds nét niet, soms winnen ze even wat met wat noodgrepen maar worden daarna alweer snel ingehaald. Al wordt AMD ook wel weer een steeds sterkere concurrent en in elk geval een prima producent voor degenen met minder budget."
Radeon VII;5;0.7258821129798889;AMD gebruikt de top producten voor de pro markt en de low bin voor de gamer. Chiplet beleid werkt ook zo en daar mee zijn AMD instapper Ryzens de mindere chips en de Epyc hebben de beste. intel gaat nog voor monolitische chips dus binnen product type heb je do low en high yield exemplaren. Nvidia heeft meerdere chips en heeft dus ook dekking voor product lijn.
Radeon VII;3;0.3315299153327942;Onder kopje 3, test methodes: > Assassin's Creed: Odyssey - Oktober 2017 Dat is 2018 . AC Origins is van 2017
Radeon VII;5;0.3135242462158203;2017 2018
Radeon VII;2;0.3026750087738037;Voeg je ook nog even de geluidsmetingen toe?
Radeon VII;3;0.5390881896018982;Vooruit Maar bedenk dat het allemaal referencemodellen zijn, wat Asus, Gb of MSI met hun koelers gaan doen wordt hopelijk wat stiller (ik vind persoonlijk dit design wel erg mooi, maar dat terzijde)
Radeon VII;3;0.4285324811935425;Verschijningsdatum van de games klopt niet helemaal. BF:V en AC.
Radeon VII;5;0.30737701058387756;Filmpje van Linus over deze GPU:
Radeon VII;2;0.4352201521396637;Zodra DLSS beschikbaar komt is het game over helaas.
Radeon VII;2;0.3655886650085449;heb nu 1440p en 4K scherm en AA is niet zo nodig voor mij
Radeon VII;2;0.27600908279418945;Uit verschillende reviews blijkt dat hij een bak met herrie maakt tijdens het gamen. Kijken wat aftermarket koelers doen.
Radeon VII;2;0.47749075293540955;Teleurstellend. Ik was deze aan het overwegen maar het gaat nu toch de 2080 rtx worden!
Radeon VII;2;0.48714807629585266;Jammer dat deze kaart, ondanks dat ie het goed doet in deze content creation testen, niet gebruikt kan worden om te renderen met populaire 3D engines en applicaties. Octane, Vray en Redshift verkiezen toch echt CUDA cores of werken niet zonder. Anders was het best het overwegen waard geweest, aangezien 16GB snelle VRAM zeer welke is bij dat soort taken.
Radeon VII;3;0.4312005043029785;Ik heb niet de hele review gelezen, maar zou er veel verschil in de prestatie van de videokaart zijn wanneer de cpu word vervangen door het high model van AMD ?
Radeon VII;2;0.4385737478733063;De conclusie van de Nvidia topman dat deze kaart niet 'Overwhelming' is iig gerechtvaardigd. Jammer want dat ze de prijzen van de RTX niet onder druk.
Radeon VII;3;0.4410732090473175;Ik doe het voorlopig wel met een RX 590 of Vega 56, zijn ook prima kaarten. Nvidia komt er niet in.
Radeon VII;1;0.49926626682281494;hopelijk drukt dit de kost vd 2080
Radeon VII;1;0.3550216257572174;Precies de reden waarom ik voor de RTX 2080 Ti ben gegaan, wat een beest zeg.
Radeon VII;2;0.5487176775932312;Probleem bij AMD is dat de CPU tak lang op 32nm voor high-end jaren moest concurreren tegen 13 / 14 nm rivaal. Dan kom met TDP al niet mee op laptop en server gebied. En GPU afdelng diende meer als liveguardboei die geheel liet drijven. Maar ja die boei is op en moet dus ook van die zware last bloeden voor de CPU tak te boven komen.
Radeon VII;3;0.26183438301086426;Kaart is was leverbaar bij Megekko, zie post de Mask.
Radeon VII;1;0.5535138249397278;Nee hoor, of ze zijn al uitverkocht, maar minimaal 10 dagen levertijd.
Radeon VII;3;0.2669304609298706;nu wel idd , ik ken de inkoper goed , ze hadden er 10 , binnen een uur op.
RX Vega Liquid;3;0.4661395847797394;ik vind het jammer dat er niet naar undervolten gekeken is bij de 56 en 64 air reviews. met deze kaart snap ik het omdat hij van een tweaker is. maar je kan extreem goede resultaten halen door de vega kaarten iets te undervolten. de performance per watt stijft gigantisch en de grote nadelen : hoog stroom verbruik en geluid productie pak je flink aan. als je extreem bezig gaat kan je de max fan speed van 2400 naar 2000 verlagen waar na je hem helemaal niet meer zo duidelijk hoort. het verbruik daalt al snel 80watt terwijl de prestaties in verhouding niet ver omlaag gaan. het echte overclocken van vega kaarten is totaal niet interessant sterker nog de lucht gekoelde versies verliezen prestaties bij gelijke fan speed en de stock voltages laat staan hogere voltages. als je de voltages verlaagt zie je de prestaties al omhoog gaan. ook zonder overclocks maar gewoon standaard instellingen gaan de prestaties omhoog omdat de kaart minder throtteled. ik heb de stock scores even niet bij de hand deze kon ik nog wel terug vinden ik zal deze later ook toe voegen. ondanks dat de clocks wat lager zijn valt het verschil in prestaties reuze mee. het verbruik is wel aanzienlijk lager namelijk 90watt performance per watt is ruim 40 % hoger met de extra undervolt en de hoge vega score is al een stuk beter qua performance per watt dan de standaard kaart! ( de 56c modes zijn de geoptimaliseerde standen een voor performance en een voor performance per watt. ) voor mensen die het interessant vinden wordt er veel getest en besproken in het : amd rx vega evaringen topic edit : extra stock vega 56 scores toegevoegd + een vega 56 met 64 bios ik zal later echte 64 cores toevoegen. maar die zitten er heel dicht op. voor meer resultaten kan je hier terecht : rx vega uv / oc results - unigine superposition mocht je een vega kaart hebben vul gerust ook je eigen scores in
RX Vega Liquid;1;0.38032037019729614;Heel simpel, undervolten is een aanpassing aan de videokaart die niet standaard is, en producten wil je reviewen zoals ze uit de doos komen. Dan weet je als koper tenminste wat je krijgt zonder dat je er eerst allerlei dingen aan moet veranderen. Dat undervolten is leuk voor een tweaker, maar de gewone gamer/consument gaat daar natuurlijk niet aan beginnen. Die wil gewoon een kaart uit de schappen halen (of een kant en klare PC met AMD Vega 64) en er direct de beste prestaties uithalen, zonder eerst te moeten undervolten. En trouwens, als alle kaarten zo makkelijk te undervolten zijn en 40% beter performance per watt kunnen halen, waarom heeft AMD dat dan zelf niet gedaan?
RX Vega Liquid;2;0.36278975009918213;Dat is met overclocken ook en dat wordt ook getest. In dit geval niet want het is geen review sample maar een kaart van een gebruiker maar in het geval van Vega is Undervolten interessanter dan Overclocken. Bijna alle grote review sites testen OC sommige hebben ook UV getest. En we zitten op tweakers.net dus mensen hier hebben daar wel interesse in. Ja waarom AMD het niet gedaan heeft is een hele goede vraag. Zodra ik alle gegevens heb ga ik kijken of ik met AMD in contact kan komen al vrees ik dat dat moeilijk gaat worden.
RX Vega Liquid;2;0.4590429663658142;Het kan natuurlijk zijn dat ze geen stabiele undervolt met significante resultaten hebben kunnen bereiken. Ik heb destijds namelijk ook veel gelezen over het undervolten van een Fury (non-X) welke ik in plaats van een 480 heb gekocht. Ondanks vele pogingen is het me nog altijd niet gelukt om een stabiele undervolt te krijgen terwijl er online velen zijn met resultaten van -70mV bij stock speeds. Daarnaast heeft AMD wellicht gekozen om de doelstelling zuinig te zijn volledig te laten varen, omdat ze toch niet bij de 1070/1080 in de buurt kunnen komen, en vervolgens dan tenminste wél de gewenste prestatie te behalen. Anders Vega over de gehele linie als een teleurstelling worden gezien. ps. Bedankt voor het uitvoerlijke testen van undervolts, ik ga me er toch nog maar weer eens in verdiepen om te kijken of ik jouw ervaring kan gebruiken om toch nog tot goede resultaten te komen.
RX Vega Liquid;2;0.4757838547229767;-70 is even wat anders dan -300 wat ik hier aan het doen ben. Bij Fury kaarten was dat ook lastiger als je de clocks niet wilde verlagen. Dan was er maar weinig mogelijk. Met het verlagen van de clocks was er meer winst te halen. Bij Vega is dat anders. Ik heb 4 Fury kaarten getest ook met UV maar daar moest ik al snel clocks verlagen. Bij Vega niet en na 7 kaarten die het redelijk vergelijkbaar doen kan ik toch wel zeggen dat -100mv en -100-150MHz een betere balans zou zijn. Ja heel jammer want Vega heeft dus wel een goede performance per watt mits je dus geen hoge voltages gebruikt 1.2v is gewoon erg veel. nVidia gebruikt ook lagere voltages. Ik moet nog meer tests doen maar ik denk dat de performance per watt bij de sweet spots beter is dan Polaris en ook beter dan Fiji maar dat ga ik nog testen. Heb nog te weinig tijd gehad om alles te doen. Zitten nu al heel wat uren in ben al een week of 2 bezig. Dus er nog niet aan toe gekomen om oude kaarten weer opnieuw te testen. Staat wel op de planning.
RX Vega Liquid;2;0.38776639103889465;idd spijtig dat ze default 1.2V aanhouden. Met de oudere HD 7xxx reeks kon je ook al wat bereiken met UnderVolt. Wat zou de reden zijn? gemak bij de productie? moeilijk te testen? Zouden ze bij productietesten een waarde kunnen bepalen, en ergens neerschrijven? Dan zou je betere/slechtere kaarten hebben...
RX Vega Liquid;3;0.39050978422164917;Klopt ik heb met de 7970 ook wel leuke resultaten gehaald. Ik zou denken Yields zodat meer chips in aanmerking komen maar tot nu toe kunnen alle chips wel aardig undervolten. Ik blijf op zoek naar heb nog geen slechte gevonden welke die voltages echt nodig zou hebben. Ook niet bij andere op het internet. Wellicht bij bepaalde compute load dat het nodig is? dat ze hem getest hebben met 100% chip load datacenter werk? maar dat is speculatie en niet iets wat ik zelf kan testen. Lijkt mij wel wordt ook wel gedaan om bv cutdown versies te maken zoals de Vega 56 en bij nVidia de 1070. Als een deel defect is of ze de clock targets niet kunnen halen gaan die chips naar een lager model. Al lijkt het er op dat ze weinig moeite hebben gedaan om deze chips echt goed te binnen. Met geheugen modulen gebeurd dit heel goed. Vooral bij die hele snelle OC repen. Die hebben dezelfde memory chips alleen zijn deze speciaal uitgezocht en samen gebracht zodat de reep die 4000MHz kan halen. Als een chip het niet aan kan heb je problemen. Chips die dat niet halen zie je dan terug op een 3600 model. Is natuurlijk wel kostbaar werk. Die snelste modules zijn mede daardoor ook aanzienlijk duurder. Maar AMD laat met Ryzen zien dat ze het wel kunnen. De turbo speeds daar zijn hoger dan de all core OC's die mensen zelf halen. En je ziet ook dat de 1800X gemiddeld wel 100MHz hoger clockt dan de 1700X en die weer 100 hoger dan de 1700. (weet zo even niet meer waar maar er is ergens een lijst met resultaten en de analyse er van)
RX Vega Liquid;1;0.537879228591919;"ondertussen, door die 'slechtere' binning van Fury/Vega heb je wel een heethoofd, en iedere website zegt gewoon ""Vega verbruikt heel veel"". Performance is ook gewoon slechter door die hogere voltage (thermisch beperkt -> downclocking / niet aanhouden van maximale frequentie). Ik weet wel dat ""tester-tijd"" een duur ding is... maar zouden ze in hun chips een goeie self-test kunnen inbouwen, dan kunnen ze gemakkelijk de laagste bruikbare voltage vinden, wat een win-win situatie is. Hoe moeilijk kan het zijn, om wat je nu doet (zelf undervolt bepalen) te automatiseren en in het productieprocess te verwerken?"
RX Vega Liquid;2;0.350799560546875;Een kaart (net zoals een auto) moet theoretisch onder alle omstandigheden kunnen werken. Dus zo ook de GPU en HBM2 chips die op de interposer met elkaar verbonden zitten. De trade-off is als je een te laag voltage hebt is dat er de kans bestaat dat er rekenfouten optreden. Weet je hoeveel RMA zaken dat wereldwijd op gaat leveren? Ze spelen gewoon liever op safe heb ik het idee. Ja je kunt wat winnen, net zoals de 9570 AM3+ CPU van AMD welk met 60W aan het stopcontact 'bespaard' kon worden door de chip te undervolten. Denk gewoon niet dat ze alle tijd van de wereld hebben gehad de kaart op een goede manier te kunnen testen. Iedere chip presteert net even iets anders weer en vaak heeft dat een one-size-fits-all approach nodig.
RX Vega Liquid;1;0.5856615900993347;ja uiteraard. En idd, werkt hier in huidige omstandigheid... maar de kaarten gaan ook naar warme en koude landen. Goeie en slechte voedingen. En wat werkt onder bepaalde games/benchmarks is misschien niet de worst-worst-case.
RX Vega Liquid;3;0.3494988977909088;Uit de doos is natuurlijk mooi, maar ik ben het er zeker mee eens dat dit voor de gemiddelde Tweaker geen probleem moet zijn. Ik prefereer AMD altijd boven Intel/NVIDIA, mits er iets van te maken is. Nou, dat kun je met die kaart dus al doen. Als je de doelgroep voor deze kaarten apart neemt, dan heb je een groep mensen die wel meer kunnen dan een schroefje in en uit draaien. En probeer zeker contact op te nemen, want wie niet schiet raakt sowieso niet he
RX Vega Liquid;1;0.515613317489624;dat vind ik een slecht argument, meltedforest. dat zou een argument kunnen zijn om ook de out of the box performance te testen, het is geen valide argument om niet daarnaast de undervolted performance te testen. je hoeft niet wekenlang of zelfs urenlang erop te studeren om te weten hoe je moet ondervolten. als reviewers hun werk doen dan weten de gebruikers dat je gemiddeld bijvoorbeeld rond de 0, 1 v uitkomt ( dat lijkt gebruikelijk te zijn ), je opent amd settings / wattman, je stelt de spanning 0, 1 v lager in, je draait firestrike, je speelt een spelletje en als het stabiel is dan verlaag je de spanning met stappen van 0, 005 v verder totdat het crasht. als het niet stabiel is dan verhoog je de spanning met stappen van 0, 005 0, 01 v totdat het wel stabiel is en vanaf daar kan je weer beetje bij beetje omlaag. er zijn een aantal mogelijke oorzaken : - een zeer klein deel van de kaarten ( 1 %, 0, 1 %, 0, 001 %...? ) is niet stabiel bij de lagere spanning - een zeer klein deel van de applicaties ( die bijna niemand gebruikt ) is niet stabiel bij de lagere spanning dit hoeft nog niet eens met de gpu zelf te maken te hebben, het kan door een combinatie van de gpu en het powerregelmechanisme ( hardware + software in het geval van amd ) komen. dat is echter allemaal niet zo belangrijk voor de doorsnee klant, de doorsnee klant moet weten dat hij een zeer grote kans heeft dat hij de kaart met ongeveer 0, 1v kan onvervolten. merk op dat dit niets nieuws is, dit was voor polaris en fury ook al bekend bij het grote publiek, alleen besteedden de meeste reviewers er geen aandacht aan ( tomshardware is een positieve uitzondering ). @ astennu succes met het contact leggen met amd. advies : houd het beknopt en meld zoveel mogelijk harde feiten : aantal kaarten + modellen + spanningen + klokfrequenties... zodat ze in 1 oogopslag zien dat jij niet een gewone gebruiker bent maar iemand die flink wat ervaring heeft met de hardware en flink wat data heeft verzameld.
RX Vega Liquid;3;0.39217761158943176;Bedankt voor de tip. Ben dat ook van plan maar heb eerst nog heel wat uren test werk voor de boeg om alles compleet te maken. En dan even kijken hoe ik het slim kan verpakken dat het inderdaad beknopt is.
RX Vega Liquid;3;0.4109768569469452;Ik wil best helpen met het opstellen van de tekst en het goed verpakken. Ik zou er in ieder geval grafieken en staafdiagrammen voor gebruiken. Grote stukken tekst lezen ze minder snel maar zo'n grafiek/staafdiagram trekt direct de aandacht en laat zien dat je niet over 1 nacht ijs gaat .Het geven van referenties (Tomshardware, GamersNexus, AdoredTV, mogelijk mensen op Reddit of zo die hier veel data over hebben gedeeld) waarbij succesvol werd ondervolt helpt wellicht ook.
RX Vega Liquid;3;0.4352664351463318;Goed idee. Ik zal tzt de draft wel doorsturen.
RX Vega Liquid;2;0.3054940104484558;Iets alleen standaard testen, en dan jezelf tweakers.net noemen. En gebruikers die het normaal vinden dat kaarten niet getweakt worden. Tjonge, wat is de site en haar doelgroep toch veranderd door de jaren heen.
RX Vega Liquid;1;0.409796804189682;Dit is toch Tweakers.net? Zijn we die roots inmiddels al vergeten?
RX Vega Liquid;2;0.3882080614566803;Dan hadden ze ook niet moeten overklokken. Dat zorgt zelfs nog voor meer warmte en powerdraw. Undervolten juist minder....
RX Vega Liquid;3;0.664734423160553;"Oké allemaal goed en wel, maar als je het verbruik van een vega64 met 90 watt kunt verminderen verbruikt ie nog steeds veel meer dan een 1080. Plus dat de prestaties dan nog slechter worden t.o.v. de 1080. Bovendien wil ik gewoon een kaart kopen, die in mijn pc prikken en dan gamen. Niet eerst allemaal moeten duiken in de voltages en dagen testen naar de optimale prestaties. We kunnen allemaal wel eromheen draaien maar ik denk dat we het wel met elkaar eens zijn dat de algemene tendens op tweakers is: Intel/Nvidia zijn de evil multinationals die ons opzadelen met 0 innovatie en AMD is de redder in nood, de prins op het witte paard die de draak Intel/Nvidia een lesje gaat leren. Same als met Microsoft/Linux. Dat we op Tweakers zo van AMD houden is omdat zij in het verleden kaarten en procs met een betere prijs/prestatieverhouding maakten. Wij hoopten allemaal dat deze vervlogen tijden weer terug kwamen. Nu AMD een mietje van een ridder heeft afgeleverd waar de prinses niets aan heeft, en Nvidia wel een ferme ridder in huis heeft, proberen sommige Tweakers toch de AMD ridder te verdedigen, met ""ja maar hij zal goed voor je zorgen en Nvidia ridder is een bad boy, o.d."" Anyway, dit stoot heel veel Tweakers tegen hun tenen, maar don 't shoot the messenger. Zolang de Vega kaarten niet een stuk goedkoper zijn dan hun Nvidia tegenhangers is er geen reden om als gamer voor AMD te gaan, zeker niet met dat belachelijke verbruik."
RX Vega Liquid;2;0.41736340522766113;Zo denk ik er dus niet over. Ja ik ben het niet eens met de manier van bedrijfsvoering van Intel en nVidia maar dat is een andere discussie. Er zijn ook genoeg mensen die zonder goede rede ook in de tijden dat AMD de betere keuze was toch blind voor nVidia en Intel gaan voor die mensen is dat merk heilig en ook zonder meer het beste en is AMD altijd troep zonder dat ze dan goed kunnen onderbouwen waarom enige argument is dan: dat is gewoon zo punt. Dus merk trouwe fans heb je altijd zul je ook altijd houden. Qua verbruik kan vega denk ik gelijk zijn aan de 1080 maar dan wel wat lagere prestaties leveren. Was in mijn ogen een betere keuzen geweest dat is mijn persoonlijke voorkeur. Ik heb dat nu ook makkelijk voor elkaar gekregen. Ik snap dat de meeste mensen gewoon de kaart kopen er in prikken en dan gaan gamen. Vega is in dat opzich met de huidige prijzen gewoon totaal niet interessant dus daar ben ik het helemaal mee eens. Het was meer om aan te geven dat AMD in mijn ogen gewoon een steekje heeft laten vallen en het verbruik niet zo hoog had hoeven zijn. Dat had lager gekund zonder te veel op prestaties in te leveren. In mijn ogen dus een gemiste kans. Deze ronde heeft AMD het gewoon niet goed gedaan. Mag hopen dat ze dit met Navi toch echt anders gaan doen anders doen ze voorlopig even niet meer meer in het High end CPU segment. Wel jammer want de CPU afdeling doet het nu net goed gaat het weer niet goed met de GPU afdeling.
RX Vega Liquid;1;0.4739428162574768;Oh ik hoop ook dat ze de volgende ronde weer competitief zijn. Even de mining gekte buiten beschouwing gelaten zal concurrentie er alleen maar voor zorgen dat wij onze spulletjes goedkoper kunnen krijgen, of dat dit nu Nvidia of AMD is. Niet alleen in de high end maar ook in de lagere klassen heeft Nvidia niet echt concurrentie helaas. Ben het ook met je eens dat het verbruik bizar is zeker als jij met wat tweaken en finetunen 90 watt vanaf krijgt. 90 watt, dat is echt bizar veel! Wat bezielde AMD eigenlijk? Wilden ze koste wat het koste een paar procent prestatiewinst toen ze zagen dat hun processor niet de ridder werd die ze dachten dat ze maar alle power erin pompten die ze erin konden pompen? Of dachten ze dat het toch niks uitmaakt of je gamebak 250 of 350 watt verbruikt? Deze ronde kan AMD praten als brugman maar ze hebben het gewoon verloren en ze gaan het ook met optimalisaties niet eens gelijk spelen. Het spijt mij voor alle AMD fans maar het is zoals het is.
RX Vega Liquid;3;0.41897648572921753;Misschien is de mining gekte wel de reden waarom ze hogere voltage draaien. Dan is het voor de miners niet interresant, en dat zou de beschikbaarheid evt ten geode moeten komen.
RX Vega Liquid;1;0.45308324694633484;Lijkt me sterk, verkopen is verkopen. of nou miners je kaarten kopen of gamers, voor AMD is elk geld wat ze binnen kunnen krijgen zonder lange termijn grote schade op te lopen nogal belangrjik.
RX Vega Liquid;2;0.4344160258769989;"Miners hebben al uitgevonden dat: met undervolten en andere optimalisaties de VEGA beter presteert dan verwacht. Dus ook daar is het ""KOPEN KOPEN KOPEN! EN IN DE FARM DRUKKEN RAAAAAAH!"" geworden. Moraal van het verhaal: Consoles zijn ineens weer aantrekkelijk geworden . Sigh, je zou maar een PC only gamer zijn (zelf PC en Nintendo consoles)."
RX Vega Liquid;2;0.4680650234222412;"Enigszins snap ik je redenatie wel hoor, AMD had met een lager voltage wellicht mindere prestaties gehad, maar wel een beter gebalanceerd product. Echter, jij kijkt naar verbruik en warmte e.d. De gemiddelde consument, en dus niet alle tweakers die hier reageren, kijken gewoon naar welke kaart het snelst is, en dus is de keuze van AMD om boven de 1070 en 1080 proberen uit te komen begrijpelijk. Sneller dan 1070 of sneller dan 1080 doet marketingtechnisch nou eenmaal meer dan ""lager verbruik"". Kaarten moet je gewoon vergelijken zoals ze uit de doos komen, met dezelfde reden als hiervoor. 90% van de gebruikers gaat die kaarten niet undervolten, overclocken, etc. Tevens de grootste reden voor veel van de discussies hier, bedrijven baseren hun beslissingen op de grootste gemene deler. En veel tweakers hebben nogal eens het idee dat zij tot die groep horen...terwijl dat echt niet zo is. Daarnaast kan ik mijn 1080 ook undervolten..."
RX Vega Liquid;1;0.6714409589767456;Ik snap eerlijk gezegd ook niet waarom iedereen zo valt over verbruik bij een high end graphics kaart. Electriciteit is helemaal niet duur en een 2x zo hoog verbruik betekent niet dat je honderden euro's per jaar meer electriciteit verbruikt. Bovendien kan je hoog verbruik prima koelen met een goede case. En in gameland geilt iedereen op rgb fans en extravagante koelingsoplossingen waar dat makkelijk mee opgevangen kan worden
RX Vega Liquid;2;0.5059512853622437;Het gaat mij persoonlijk niet om de stroomrekening (die extra kosten kunnen met gemak gedragen worden), maar om het grotere plaatje: ik ben van mening dat je vandaag de dag simpelweg niet meer kan aan komen zetten met een alternatief product dat 50-100% meer energie verbruik bij - op zijn best - gelijkwaardige prestaties. Die mening staat helemaal los van merk: nu is het AMD, maar was nVidia de energie-slurper geweest, dan had zij in mijn verdomhoekje gezeten. Ik gun AMD het beste (want de consumentenmarkt heeft het hard nodig), maar dan zullen ze toch met iets beters op de markt moeten komen: niet vanwege de prestaties, maar vanwege het energie-verbruik, welke dermate veel hoger is dan de concurrentie dat het (imho.) bijna gênant is om daar anno 2017 nog mee op de proppen te komen. En daarvoor ga ik mijn principes niet opzij zetten, hoe spijtig dat ook is voor AMD.
RX Vega Liquid;2;0.36900848150253296;Reken het eerst nou even na. Het prijsverschil tussen een high end amd en Nvidia kaart zal in de eerste 5 jaar niet in het voordeel van Nvidia wanneer je dat ding heel het jaar aanlaat. Dan is het toch te gek voor woorden dat je voor Nvidia kiest? Want je bent in principe een dief van je portomonee. Mijn oordeel hierin is neutraal want een game pc heb ik al 10 jaar niet meer en mijn laptops gebruiken alle Intel graphics, omdat ik er toch niet op game Energie verbruik kan enkel maar de doorslaggevende factor zijn als de prijzen dicht bij elkaar liggen (wat absoluut niet zo is!)
RX Vega Liquid;2;0.4388831853866577;Je moet wel goed lezen, beste mikesmit, want dan had je gezien dat het gaat mij niet om de centen gaat, zowel qua aankoopprijs als de energiekosten. Het gaat mij om het energie-verbruik. En zolang AMD in de hoek van de onnodig zware energie-verbruikers zit, komt het er bij mij niet in. Zodra ze nVidia qua zuinigheid en performance/watt voorbij weten te streven, dan staat AMD weer bovenaan mijn lijstje, maar eerlijk gezegd zie ik dat - helaas - niet zo snel gebeuren (ik zie AMD liever nVidia liever beconcurreren op efficiency in plaats van ruwe kracht). Erg cru gesteld is een Vega kaart kopen is vergelijkbaar met een energie-label F huishoudelijk apparaat kopen: nauwelijks te rechtvaardigen als duurzaamheid je een beetje aan het hart gaat. En er is geen enkele mate van merkenliefde dat dat in mijn optiek kan recht-praten.
RX Vega Liquid;1;0.5270935297012329;Zo had ik het niet bekeken. Ik dacht dat wij Nederlanders beroemd waren om onze kosten/baten mentaliteit. Je behoord tot de zeldzame groep die meer uitgeeft enkel voor de harten van Greenpeace en soortgelijke. Ik wens je dan veel succes in je missie!
RX Vega Liquid;2;0.4924110472202301;Vergeet niet dat een hoger energieverbruik ook meer warmte betekent. Veel mensen hebben een game PC op hun kleine (studenten/of studie, dan wel zolder) kamer, als die warmte in je kamer blijft hangen dan is dat niet prettig. Voor mij was dit een rede om over te stappen naar een NVIDIA1080, terwijl ik eigenlijk een hekel heb aan de bedrijfsvoering van NVIDIA en voorheen altijd AMD GPU's kocht.
RX Vega Liquid;1;0.23845452070236206;Aanbevolen door linustechtips: Edit: kijk de video op zijn kanaal. Hij heeft er een gamepc build in gemaakt
RX Vega Liquid;1;0.3885129690170288;Een fanloze kast? Ik volg even niet hoe dit relevant is aangezien de hardware nog steeds warmte genereert en dus in de kamer terecht komt.
RX Vega Liquid;1;0.3259023427963257;Heb je een kamer zonder raam dan? Warmte betekent herrie van de fans, die herrie kan ontnomen worden van je.
RX Vega Liquid;4;0.2846148908138275;
RX Vega Liquid;2;0.3726576864719391;Laten we het hier maar bij houden dan... Ps: jouw punt vliegt inderdaad zo langs me omdat ik gewoon geen logica zie in jouw gedachtegang.
RX Vega Liquid;1;0.4908475875854492;"Ehm, dat gaat hem ook niet worden. Alleen de kast weegt al dik 28 kilo en zal al snel 600 tot 1000,- euro kosten. Dan ben je nog beter af met een VEGA, de extra kosten van de stroom wegen totaal niet op tegen zo'n lompe, dure kast. Of, zoals Gijs al zegt; koop gewoon een Nvidia 1080. Wel, Mike. De logica van jouw gedachte kan ik idd wél volgen maar dat is helaas een niet zo'n bijster slimme."
RX Vega Liquid;1;0.5131353139877319;Het komt erop neer dat Nvidia totaal geen last heeft van AMD en AMD echt ver en ver achter op ligt Stel je eens voor dat Nvidia ook een kaar uitbracht met hetzelfde verbruik als de AMD kaarten. Die met de huidige Nvidia techniek dan echt NOG een keer zo snel is als de kaarten die Nvidia nu al heeft.
RX Vega Liquid;4;0.348765105009079;Het is ongetwijfeld voor sommigen geen issue. Maar persoonlijk geil ik niet op die extravagante koeloplossingen en wil ik een zo compact en stil mogelijke game-pc. Voor mij is die 100W wel belangrijk omdat dit 100W meer warmte betekend maar ook dat de voeding harder moet werken waardoor die ook weer harder moet gaan blazen. Laat ik het zo zeggen, als het niet hoeft (zie Nvidia) waarom wel dan :-)
RX Vega Liquid;2;0.489292174577713;Het is ongetwijfeld geen gemakzucht van AMD, dat kost ze te veel. Wellicht hebben ze geen tijd en resources gehad om dit fatsoenlijk te finetunen, of wellicht verschillen de samples individueel te veel om standaard voor die 1150mV te kiezen. Dat je er 7 getest hebt wil niet zeggen dat 1 op de 100 het gewoon niet zal halen en artefacten toont of misschien zelfs wel crashes. Hoe dan ook, bij nVidia kun je natuurlijk ook per sample gaan undervolten en de grenzen opzoeken. Wellicht boek je hier bij AMD dan wat meer winst bij, en als je uitgaat van jouw 80W voordeel, en er bij nVidia maar 30W op vooruit gaat door te undervolten, zit je nog steeds op een verschil in verbruik van 100W tussen een GTX1080 en een Vega 64, terwijl de eerste sneller is. Het is absoluut interessant om eens met beide kaarten te gaan tweaken als een vervolgartikel, maar we dan wel een kaart van beide kampen, op dit moment is het koffiedik kijken.
RX Vega Liquid;2;0.326855331659317;Als ik dat in een paar dagen voor elkaar krijg kan AMD het ook. Ik zou het graag als vrijwilligers werk voor ze willen doen want dit is gewoon zonde. AMD heeft allemaal binning methodes om chips te sorteren en controleren of ze goed genoeg zijn voor de 56 of d 64. Dus lijkt mij dat dit geen enkel probleem is. Voor Ryzen CPU's hebben ze dit heel goed onder controle. De Max turbo speeds van die CPU's zijn hoger dan de max all core snelheden die je kan halen. Dus dat geeft wel aan dat AMD's cpu afdeling in ieder geval weet hoe je moet binnen. Die 50mv moet echt geen issue zijn. nVidia draait ook lagere voltages zal denk ik een goede rede voor zijn. En wat ik al zij beter 50mv er af en dan de clocks ook 50-100MHz omlaag is nog steeds beter. En alle chips die ik getest hebben kunnen ook werken op 1300MHz of hoger op slechts 900mv meeste zitten dan tegen de 1400MHz aan. 900 of 1200 is een gigantisch verschil. Qua max clocks niet want max halen ze op 1200mv maar iets van 1650-1750 even uit het hoofd allemaal want ik heb mijn gegevens niet bij de hand. Ik zou ook heel graag nVidia kaarten willen testen maar heb er nog geen voor een goede 2e hands prijs kunnen krijgen. Ik wil er niet verlies op maken aangezien ik toch weet dat ik ze niet ga houden. Maar wellicht komen die resultaten ook nog wel. Wat je ook zegt elke chip haalt winst bij UV de een gewoon meer dan de ander en bij Vega scheelt het meer dan bij Polaris.
RX Vega Liquid;5;0.270938515663147;A Lurker Dit zouden revierws die de naam waardig zijn nu moeten uitzoeken!
RX Vega Liquid;1;0.5783918499946594;het is prima uit te leggen dat amd niet naar de undervolt optie gekeken heeft : al maanden was het duidelijk dat vega een mega - probleem had als het ging om de harde cijfers / prestaties. ik bedoel kijk naar de review, hoe slecht ( en laat! ) die ook is, en wat vega nu doet ten opzichte van de fury x. er is gewoon bijna niets veranderd. precies zoals voorspeld, ondanks alle triomfantelijke powerpoint slides van amd over de radicaal vernieuwde gcn in vega, is dit gewoon fury x v2, met een verhoogd power target om het gebrek aan verbetering te maskeren. amd kon niet anders, want fury x zomaar even rebranden was ook niet uit te leggen. dat er een low - power variant van gemaakt kan worden is natuurlijk leuk voor het minen, maar perf / dollar gaat dan wel zo hard onderuit voor gaming dat het de moeite niet waard is. als je dan ook nog ziet dat hbm vrijwel niets doet voor het energieverbruik ( sterker : het is een grote verbruiker en een van de punten waarop veel winst te boeken valt met tweaks ) dan vraag je je echt af wat ze daar bij rtg aan het roken zijn. tot slot, mbt de review : waarom worden games die niet dx12 - native zijn, wel in dx12 getest? zoals tw : warhammer. doe het dan allebei, want zo heb je er niets aan, die game is in dx12 totaal niet stabiel te noemen en zegt dus ook niets over werkelijke prestatie verhoudingen tussen de geteste kaarten. verder : geleend kaartje, drie weken te laat, oude drivers, en we testen medium?! en ultra?! maar niet de high setting dat vaak het beste van twee werelden is? geen temperaturen? geen oc? geen verkenning van de architectuur en geen inzichten over de effecten van over - en undervolten? waren jullie dronken op de redactie? of gewoon volledig clueless... hou het in het vervolg anders maar gewoon bij mobieltjes en televisies, dit is zinloos.
RX Vega Liquid;2;0.4144323170185089;Ik ga er een andere keer beter op in maar Vega =! Fiji. Ja het is nog steeds GCN met toevoegingen. Er zijn meer aanpassingen dan in Polaris. Dus om te zeggen dat Vega gewoon Fiji is is niet waar. Vega is een Compute architectuur gericht op datacenters (Instinct MI25). Schaalt inderdaad niet lekker met gaming en kan dus qua IPC op gelijke clocks tegenvallen tov Fiji. AMD heeft wel transistors besteed zodat Vega hoger kan clocken en dat zie je ook. Polaris doet met moeite 1400. Vega doet dat met lage voltages prima. AMD heeft niet de resources om zoals nVidia een Datacenter chip + gaming chip te maken. Dus dat Vega het dan aflegt tegen een gaming Chip (GP104 en GP102) is imo niet raar. (even heel kort door de bocht) Ik ben het er wel mee eens dat het er op lijkt (ik heb het bewijs nog niet) dat een aantal nadelen die Fiji in z'n frondend had zitten nog steeds aanwezig zijn en de troughput daar niet goed is.
RX Vega Liquid;1;0.31149759888648987;Klopt hoor, ik snap dat gaming absoluut niet de prioriteit heeft bij deze GCN iteratie, maar we gaan het nu in zowel prestatie, warmte, verbruik én prijs terugzien. Dat trucje kun je niet blijven herhalen en eigenlijk was het met de Fury X en Hawaii al gedaan. Het lijkt erop, vwb die throughput, dat het bij de ROPs zit, een vermoeden dat er ook al was bij de Fury.
RX Vega Liquid;2;0.5212671160697937;Dat vermoeden heb ik ook. Fiji schaalde al niet echt lekker qua shader aantallen Vega heeft er nog even wat meer last van. Fiji was het maximale wat mogelijk was met de GCN. Die limieten lijken niet opgeheven. Nu hopen dat AMD de boel met Navi wel flink op de schop gooit.
RX Vega Liquid;3;0.3094549775123596;"Hoezo ""ik snap het omdat de kaart van een tweaker is""? Undervolten kan geen schade aanrichten. En blijkbaar zijn er wel kaarten die niet stabiel zijn op 1150mv, anders hadden ze wel 1150mz gezet."
RX Vega Liquid;3;0.40803059935569763;Ik snap dat je er dan gewoon niet te veel mee wilt klooien. Het kan niet veel kwaad nee maar goed het is netter om de kaart dan gewoon stock out of the box te testen. Dat dacht ik ook dus daarom heb ik inmiddels 7 kaarten getest maar ze halen allemaal top prestaties. 7 is natuurlijk niet genoeg maar als ik kijk naar de max clocks op verschillende voltages zijn de verschillen heel klein. Ook dat ga ik nog in mijn rapport op nemen maar tot nu toe zijn de verschillen tussen de slechtste en de beste maar iets van 60MHz. Dat is heel erg weinig. Ik ben ook informatie aan het verzamelen van andere tweakers en dat in een sheet aan het samen voegen om een beter beeld te krijgen van wat de chips kunnen. Tot nu toe heb ik nog geen kaarten gezien die die lagere voltages niet aan kunnen bij behoud van stock clocks en allemaal hebben ze dan nog wel ruimte over. Maar dan nog steeds had AMD in mijn ogen beter voor lagere voltages + lagere clocks kunnen gaan. Aangezien de kaarten toch vaak throttelen had je amper prestatie verlies gehad wel een veel lager verbruik en minder herrie. Het was toch al stuivertje wisselen met de nVidia kaarten. Iets langzamer maar qua performance per watt wel in de buurt zitten en qua geluid productie ook was in mijn ogen een betere keuze geweest.
RX Vega Liquid;2;0.4449777901172638;"Maar wel overklokken maar niet undervolten? dan is overklokken nog ""gevaarlijker""... Ze hebben het gewoon niet gedaan omdat ze geen tijd hadden of omdat Tweakers al jaren niet echt ""Tweakers"" meer is. Wat AMD had moeten bouwen is een systeem die de kaart undervolt totdat hij niet meer stabiel wordt."
RX Vega Liquid;3;0.3334493935108185;Het is natuurlijk ook van toepassing dat de GTX1080 te undervolten is, resultaat is nog steeds dat het een zuinigere, stillere, goedkopere kaart is vergeleken met de Vega. Dit doet niets af van wat er stock in de schappen aanwezig is.
RX Vega Liquid;2;0.41227054595947266;De ene chip heeft et meer baat bij dan de ander. Alle chips zijn te undervolten maar Vega laat hier meer winst zien dan de 290x, Fury, 980 en 980 Ti de 1080 en 1080ti heb ik zelf nog niet kunnen testen.
RX Vega Liquid;1;0.2309732884168625;Kortom: 1080 en 1080ti heb je geen enkel bewijs voor. Waardoor de enige constante en zekere factor is: 1080 en 1080ti zijn beter op stock. Hou eens op met die what if's en volg de data: Nvidia wins
RX Vega Liquid;1;0.4979505240917206;Ik zou zeggen kom maar met bewijs dat de 1080 zo geweldig te undervolten is zonder in te leveren op prestaties. Ik zie je alleen maar aannames doen zonder met cijfers te komen. Ik heb hier een 980, 980 Ti , Fury Nano, 290X, 295X2, 7970, 3x Vega 56 getest en 4x Vega 64. Kan je het mij kwalijk nemen dat ik het geld nog niet extra heb liggen om ook even de 1080 en 1080 Ti te testen? As we speak ben ik bezig met de aankoop van een 1080 Ti dus die informatie gaat er zeker komen maar het heeft gewoon tijd nodig als consument.
RX Vega Liquid;2;0.4661291837692261;"Hardwareunboxed heeft toevallig vandaag een video gereleased waar hij vega 56 undervolt/oc'd. Ja de performance stijgt (+-15%), maar het verbruik stijgt ook met zo'n 33%. ""Hoog stroom verbruik en geluid productie pak je flink aan"" Lijkt dus niet op te gaan op basis van zijn resultaten. Daarnaast vind ik het een beetje een raar argument dat Tweakers de kaarten zou moeten undervolten. Het resultaat dat je met undervolten/overclocken kan behalen varieert per chip. Het is leuk voor de enthusiast maar het geeft geen enkele garantie. Veel logischer is om het product gewoon te testen zoals het wordt aangeleverd. Dat is performance die gegarandeerd is out of the box."
RX Vega Liquid;2;0.4417615830898285;Ik zal er vanavond even goed naar kijken heb daar nu geen tijd voor. Settings zijn nogal van belang. In het Ervaringen Topic worden genoeg resultaten gedeeld waar je ziet dat het verbruik zeker omlaag kan zonder prestatie verlies. Met dat argument wat jij nu geeft over UV zou je dus ook geen OC tests meer hoeven te doen. Sterker nog dan kun je vandaag de dag ook geen stock kaarten meer testen. Door dynamic clockspeeds is geen kaart hetzelfde. De een 1080 is ook een stuk sneller dan de ander om die rede Bij de 9xx serie het zelfde idem voor de 29x, Fury en Vega kaarten. Out of the box performance is niet gegarandeerd meer want al die clocks zijn tegenwoordig dynamic vroeger in de tijd van de Geforce/Radeon 1 en de HD58xx en de GTX480 was dat nog zo want die clockspeeds stonden vast. Met de komst van turbo verschilt elke kaart weer afhankelijk van lekstroom en hoe goed hij gelukt is.
RX Vega Liquid;2;0.3320101797580719;"""Out of the box performance is niet gegarandeerd meer want al die clocks zijn tegenwoordig dynamic"" Onzin, de clocksnelheden die op de box worden aangegeven zijn gewoon gegarandeerd. Alles wat daarbovenop komt door dynamic boost is een leuke extra. Het verbaast mij eerlijk gezegd in hoeveel bochten sommige mensen zich wringen om AMD producten maar zo goed mogelijk uit de bus te laten komen. Laten we heel eerlijk zijn, AMD heeft dit product op deze manier in de schappen gelegd. Is het dan aan de reviewers om het product maar zoveel mogelijk te Tweaken totdat er een enigszins respectabel resultaat uitkomt? Dat had AMD zelf moeten doen. Het is een leuke suggestie om undervolting als extra op te nemen, naast OC resultaten. Maar de conclusie en de focus van de review hoort gewoon gebaseerd te zijn op out of the box gegarandeerde resultaten, en dat heeft Tweakers hier gewoon goed gedaan."
RX Vega Liquid;2;0.43800875544548035;"Het is niet erg, maar dit soort dingen dragen - elke release opnieuw, of het nu een GPU of CPU is! - absoluut niet bij aan een goed imago voor AMD. Tel daarbij de rare communicatie, de op zijn zachtst gezegd aparte press releases en bijeenkomsten (die er vaak uitzien alsof iemand een zaaltje heeft ge-ghetto-mod en er wat zelfbouw PC's in heeft gezet) en de vele launch-problemen op, en het algehele beeld van een AMD release is gewoon dat het elke keer knudde geregeld is. Kijk nou naar hoe Vega bij reviewers aankomt: je krijgt een hele rits aan BIOS'en en niet eentje is écht goed afgesteld. Het is net alsof AMD na al die tijd zegt 'doe hier eens wat leuks mee, want wij weten het ook niet meer'... Ryzen: zelfde laken een pak; game prestaties gaan van sub-top tot totale bagger FPS, er is geen pijl op te trekken, overklokken is vrijwel non-existent én je moet wel heel secuur kijken of je de goeie RAM latjes pakt. Dat gebrek aan consistentie is gewoon killing voor het vertrouwen in AMD. En dat is jammer, zeker wanneer het eigenlijk prima producten zijn (Ryzen, niet Vega, Vega is gewoon een 'dud' als het om mainstream gebruikers gaat)."
RX Vega Liquid;3;0.4227961003780365;"Best opvallend dat er zo een uitgebreid en mooi artikel geschreven is, maar dat de Vega kaarten getest worden met driver 17.7.1. Dit terwijl vanaf versie 17.8.1 pas expliciet support voor de Vega serie aanwezig is. Niet dat ik niet begrijp waarom hoor; een artikel schrijven kost natuurlijk enorm veel tijd, vooral met zulke aantallen benchmarks. Echter: zou een update in de nabije toekomst, met bijgewerkt drivers, niet aan de orde zijn?"
RX Vega Liquid;1;0.7439979910850525;Verkeerde driver in copy/paste genoemd, de 17.8.1 drivers zijn gebruikt
RX Vega Liquid;1;0.387928307056427;Foutje moet kunnen, baas!
RX Vega Liquid;2;0.3757629096508026;Het schijven hiervan zal niet meer tijd gekost hebben dan de tijd dat de kaarten nu uit zijn. dus een actuele ( lees: gesupporte ) driver had best gemogen.
RX Vega Liquid;1;0.4937776029109955;De volledige specs staan ook op HWinfo. daar zag ik wel ergens staan dat de driver geupdate was naar Crimson Relive 17.8.2 Alle waardes zijn exact gelijk aan de test resultaten hier (heb de FPS resultaten steekproefsgewijs vergeleken... identiek). zit alleen geen verhaal omheen, zijn kale testresultaten.
RX Vega Liquid;1;0.4491211771965027;Ik ben bij pagina 2 opgehouden met het lezen van de review. Wanneer voor de AMD kaarten driver versie 17.7.1 gebruikt word, dan word er een versie gebruikt welke niet eens ondersteuning heeft voor de VEGA chips. Kom op tweakers, versie 17.8.2 is al drie weken uit, daarvoor was er nog 17.8.1 en alle twee hebben in iedergeval support voor VEGA. EDIT: spuit elf.... CykoByte was me voor.
RX Vega Liquid;1;0.863030195236206;Dit begrijp ik dus ook echt niet, dat wil dus zeggen dat deze review al zeker 5 weken geleden geschreven is maar nu pas gepubliceerd wordt????? Ik vind het een slechte review @willemdemoor als je bizar oude drivers gebruikt zonder ondersteuning voor VEGA.. Echt een lachertje.
RX Vega Liquid;3;0.29649221897125244;De juiste drivers zijn gebruikt: 17.8.1 om de resultaten vergelijkbaar te houden met de vorige vega-kaarten. 187.8.2 hebben we ook gedraaid met liquid en air-cooled, zat steekproefsgewijs geen performancewinst in.
RX Vega Liquid;5;0.378993958234787;"Typfoutje dus Net zoals '18.8.2';)"
RX Vega Liquid;3;0.2558930814266205;idd
RX Vega Liquid;3;0.3941361904144287;Ai dit is wel ernstig. Als je dit al doet wil je minimaal voor een kaart alle tests doen met de 17.8.2 driver om te kijken wat het verschil is. (en benoem dit dan ook in het artikel als dat gedaan is anders krijg je de opmerking alsnog). Als er dan een merkbaar verschil is moet je het wel opnieuw testen met een nieuwe architectuur is er altijd meer ruimte voor prestatie verbeteringen via de drivers. Dan zijn up to date drivers wel erg belangrijk.
RX Vega Liquid;1;0.5953096151351929;Zou ook wel 'ns willen weten/lezen van wie ze het testplatform (toevallig intel/windows/samsung) hebben (gekregen). En van wie ze deze specifieke games moeten testen waarvan geweten wordt dat ze niet respresentatief (meer) zijn in vele gevallen tenzij nvidia voorkeur. En ook waarom bij Intel er heen en weer wordt gebeld als er 'n issue is en AMD niet (ja 'n week na 'n artikel). Just curieus. En wat er met HSA is gebeurd... en impact vega op zen apu's als het zo'n shit is.
RX Vega Liquid;2;0.48047009110450745;De conclusie stukje vind ik persoonlijk een beetje teveel Nvidia pro minded. Als de Vega echt zo slechte performance/watt heeft, waarom kopen miners die dingen dan?
RX Vega Liquid;2;0.4593592882156372;Stock perf/watt is inderdaad niet goed. Maar alle miners undervolten de kaarten en daardoor krijg je een veel betere perf/watt. Hetzelfde geldt overigens voor gaming. Als je kijkt in het Vega ervaringen topic dan zie je dat iedereen met gemak de kaart kan ondervolten van 1200mV naar rond de 1000mV. Kloks moet je dan ook wat bijschaven, maar over het algemeen gaat de performance van de kaarten daardoor niet eens zo gek veel naar beneden, maar het verbruik daalt zo enorm. Daarbij wordt er ook in deze review niet stil gestaan bij Radeon Chill. Als je een Radeon Chill game hebt dan wordt de framerate op rustige momenten rond de 40 FPS gehouden. En tot mijn verbazing voor de rest op 60 FPS (ik heb een 60hz scherm), en hierbij loopt de kaart niet sneller dan nodig. Met als gevolg een nog lager stroomverbruik. In Prey of SE4 bijvoorbeeld verbruikt mijn totale systeem niet meer dan 320 watt. En ja, undervolten moet inderdaad niet perse behandeld worden in een review. Een kaart zou op stock goede performance moeten hebben, Maar zoiets als Radeon Chill mis ik dan wel, en dat is een optie die iederen zou moeten kunnen gebruiken.
RX Vega Liquid;3;0.5665087103843689;Radeon Chill is leuk, maar is beperkt tot een klein aantal games (19 op dit moment), dus je hebt er alleen maar profijt van als je voornamelijk die games speelt. Kijk je naar het ander kamp, de 1080, dan heb je een lager verbruik in alle games.
RX Vega Liquid;1;0.5994565486907959;miners kopen eerder een rx 480 dan dit, is gewoon te duur mbt stroomkosten
RX Vega Liquid;2;0.5102020502090454;Probleem is dat een hoop kaarten inmiddels en aanhoudend vele weken levertijd kennen. Dan wijkt men al snel uit naar andere modellen. Deze week nog minen gaat nog tegen de huidige difficulty tenslotte. Inmiddels is het verworden tot graaien en pakken wat je maar even krijgen kan. Dit is de crypto bubbel, als die over is heb je opeens vega's voor het oprapen 2e hands.
RX Vega Liquid;1;0.5097066760063171;Precies, en niemand koopt die tweedehandse kaarten want de levensduur is drastisch gereduceerd vanwege mining. 1.5 Maand 24/7 minen is gelijk aan ongeveer 1 jaar normaal gebruik voor gaming met een gemiddelde van 3 uur per dag. Ik heb persoonlijk liever dan een nieuwe kaart.
RX Vega Liquid;5;0.4300142228603363;Ik zou hier graag een bron van zien. Het constant op warmte houden van zo een kaart is namelijk veel beter dan elke dag in en uit schakelen. Is eigenlijk best prima voor die dingen :-).
RX Vega Liquid;2;0.5186910033226013;En daar zou ik dan graag weer een bron van zien. Ik weet vanuit mijn werk dat chips simpelweg verouderen, en dat gaat sneller bij hogere warmte (en spanningen). Aan/uit zetten maakt daar niet bij uit (behalve als bij het aan zetten er tijdelijk een hoge spanning ergens op komt, waardoor dat sneller zou kunnen verouderen). Voor al die opmerkingen dat minen beter is voor een kaart omdat hij nooit aan/uit wordt gezet heb ik nog nooit een bron gezien. Ik zou me voor kunnen stellen dat door mechanische effecten van het opwarmen en afkoelen verbindingen op de printplaat die al twijfelachtig zijn, echt los kunnen raken. Dus misschien is dat effect er wel hoor. Maar ik weet ook als feit dat elektronica simpelweg slechter wordt als het lange tijd bij hoge temperaturen moet functioneren. (Oftewel, ja een miner die undervolt zorgt voor betere levensduur. Maar dat is nog altijd slechter dan een gamer waar hij voornamelijk uit staat, of in idle 35 graden is).
RX Vega Liquid;2;0.4039887487888336;Een uitspraak doen over wat erger is: constant hoge temperatuur of meer temperatuur cycles, is lastig te maken. Je hebt te maken met verschillende failure modes dus je kan het eigenlijk niet vergelijken zonder zeer uitgebreide tests te doen. Hou er rekening mee dat de kaart is gedesigned op enkele uren gebruik per dag, en hier is dus ook uitgebreid op getest. Ze zijn niet gemaakt om 24/7 aan te staan, dus er zijn bepaalde productie en design trade offs gemaakt die zeker beter zijn voor regulier gebruik.
RX Vega Liquid;2;0.4745507538318634;Ik heb geen bron, maar een condensator houdt over het algemeen niet van warmte, daarvan daalt de levensduur. Dus ik kan me goed voorstellen dat die condensator's dan al behoorlijk gesleten hebben.
RX Vega Liquid;2;0.42331162095069885;Precies zo denk ik er ook over. Er zijn trouwens sowieso teveel mensen die denken een videokaart 2e hands aan te bieden voor bijna dezelfde nieuwprijs.
RX Vega Liquid;2;0.4587569236755371;Dat hangt helemaal af hoe de kaarten behandeld zijn. Zoals Gopher al aangeeft is het beter voor een VGA kaart als deze op een constant temperatuur blijft, dan sterk in temperatuur wisselt wat bij standard gebruik het geval is. Zelf heb ik twee R9 290 kaarten, nu zeker twee jaar, in mijn Pc zitten welke gebruikt zijn voor mining in een datacenter. Hierbij werd de temperatuur op 26 graden gehouden, deze kaarten hebben daar bijzonder weinig van te lijden gehad.
RX Vega Liquid;3;0.41082993149757385;Het hangt zeker af van hoe de kaarten behandeld zijn, maar dat kan ik als koper vaak niet uit opmaken. 26 Graden is inderdaad erg koel, en bevorderd de levensduur. Ik heb geen harde cijfers, waar ik zeker benieuwd naar ben, maar mijn ervaring met koeling op versterkers zegt dat koelpasta bij langdurig gebruik uitdroogt. Natuurlijk kan je als koper zelf nieuwe koelpasta aanbrengen, maar dan moeten ze deze dingen niet gaan verkopen voor bijna de nieuwprijs.
RX Vega Liquid;3;0.3420391380786896;"Voor de FANS geld dat misschien, maar voor de electronica voor zover ik begreep is het juist beter constant ""Warm"" te zijn ipv bij gaming warm/koud/100%/40% etc"
RX Vega Liquid;2;0.4845847487449646;AMD heeft Vega TE ver gepushed. Ze hadden in mijn ogen voor 1050-1100Mv moeten gaan ipv 1150-1200 nu. Ze zitten ver voorbij de sweetspot in OC range qua voltages. De winst door deze stap is 0 en zelfs verlies in prestaties door throttelen (gpu max temp en TDP limits worden te snel gehaald) De performance per watt is een stuk beter dan die van Hawaii (290x) en ik denk ook Fiji (Fury X). Ik ga van de week met mijn Fury Nano wat performance per watt vergelijkingen doen. Vega heeft veel meer winst door undervolten dan andere GPU. Zowel kwa prestaties al hele grote reducties in het verbruik en dus de herrie. De 290X had hier nog meer baat bij omdat die cooler minder goed was dan die van Vega maar ook Vega heeft baat bij een veel lager verbruik. Dan kun je hem best stil maken. Ik zal ook eens proberen een sound level test te doen met mijn telefoon om het verschil duidelijk te maken. Maar lang verhaal kort. AMD heeft Vega in mijn ogen een beetje verknalt en slechter voor de dag laten komen dan de chip echt is. Want je kunt hele goede performance per watt halen.
RX Vega Liquid;1;0.4599916338920593;Bekend probleem van AMD, op het moment dat ze een kaart uitbrengen hebben ze een prima kaart in handen die meestal ook erg efficient is, echter eisen de consumenten (en wellicht aandeel houders) altijd dat de nieuwe kaart een concurrende kaart moet verslaan zo moest de RX 480 de 970 verslaan de Fury moest de 980 verslaan en Vega moest de 1070 en 1080 verslaan Omdat voor elkaar te krijgen lijkt het erop dat AMD hun kaart te ver opvoeren waardoor hun ideale sweet spot lang niet meer bereikt word Het zal waarschijnlijk een afweging zijn, van of gewoon een zuinige kaart, of in ieder geval dicht bij de concurrentie komen. Het laatste wordt blijkbaar altijd gekozen
RX Vega Liquid;2;0.39212360978126526;Daar lijkt het inderdaad op. Vraag me dus af wie dat zo graag wilt. Want het gaat tenkoste van het algehele product. Vega op zich is niet slecht. Vega 56 en 64 in hun huidige vorm tegen deze prijzen wel. Hopelijk zien ze dat snel een keer in en gaan ze het anders doen.
RX Vega Liquid;2;0.4666653275489807;Dat moest omdat AMD met RX 580 al een tegenhanger van de GTX 1060 had. Vega uitbrengen kon dan ook niet anders als tegenhanger van de 1070 en 1080. Concurreren met het eigen Polaris had geen nut, Vega is veel complexer en dus veel duurder om te maken. Vega helemaal niet uitbrengen is ook geen zinnige optie. Er bleef dus weinig anders op dan Vega opvoeren tot het gewenste niveau was bereikt.
RX Vega Liquid;1;0.37594616413116455;Wat is er mis met de huidige prijs? Lijkt me dat de prijs compleet gerechtvaardigd is door de hoge vraag. If anything is 'ie te goedkoop.De enige vraag is, waarom verkopen ze de Vega's niet allemaal als mining kaarten?
RX Vega Liquid;3;0.46276894211769104;Voor msrp en dus +-535 voor een 64 en 430 voor een 56 is er weinig mis met de kaarten. Als je 500-550 voor een 56 en 650 voor een 64 moet betalen kunnen ze gewoon niet concureren met een 1080 & 1080 Ti en zijn die prijs prestatie en performance per watt gewijs een betere keuze. Voor mining is het wellicht wel interessant maar als game kaart dan niet tegen over de concurrentie. De 1070 heeft ook al last van opgeblazen prijzen door de mining gekte dus op dit moment is de 1080 relatief gezien interessanter als je die meerprijs kan betalen.
RX Vega Liquid;2;0.45105937123298645;De Fury X stond tegenover de 980ti, zelfs. Alleen op 1080p was die 980ti echt veel sneller en aangezien het gros van de gamers daar toch nog op zit, én Fury X hetzelfde probleem had als Vega nu (hoger verbruik/AIO geluidsproductie/hitte) moesten ze toch iets gaan produceren dat echt sneller was. Als je hier goed over nadenkt is de enige conclusie gewoon dat er vanaf Fury X weinig tot geen echte vooruitgang in de architectuur zit als het om gaming gaat. Dit lijkt verdacht veel op de clock bump van Skylake > Kaby Lake bij Intel. Het heeft een ander naam, maar onder de motorkap is het meer van hetzelfde, je levert perf/watt in voor marginaal betere prestaties. Dat kunnen we zelf ook wel.
RX Vega Liquid;2;0.5113572478294373;De vergelijking die je maakt klopt totaal niet. Ik snap wel waar je heen wilt maat het is niet waar wat je zecht. Bij Skylake > kabylake is er zeer weinig veranderd onder de motorkap clockspeed bump is ook klein. Als je kijkt naar Fiji vs Vega 10 dan is de clockspeed bump 50-65%! Dat is niet marginaal. Het verbruik is daarbij vergelijkbaar met Fury X dus performance per watt is dan een heel stuk beter. Ik ga deze week testen vs een Fury Nano en dan kijken hoe de performance per watt is tov Vega. Dan de architectuur: -Rapid pack math: FP 8 support op. 8x de FP32 snelheid, FP16 op Dubbele snelheid -primitive shader (faster triangle culling) -high bandwidth cache controller waardoor je meer dan 1tb aan memory kan aanspreken buiten de kaart om. -Improved Geometry engine voor betere tesselation performance -support voor alle. Dx12 feature level -betere display controllers dus nu wel HDMI2.0 -Tile based rasterizer -betere power Management bij lage gpu loads en idle load. Betere clock gating en low frequency Hbm mogelijk als de load lager is. Allemaal zaken die Fiji niet heeft. Ja het is nog steeds GCN maar met veel nieuwe dingen. Veel van die zaken zijn echter wel op het datacenter gericht (Instinct MI-25 kaarten). Een aantal zaken kunnen wel voor winst in games zorgen maar dan moeten game devs die gaan benutten. Zo zijn sommige devs bezig met rapid path math support voor hun game Engines.
RX Vega Liquid;3;0.44779518246650696;De performance in games is anders dan de performance in mining. Dus de performance/watt ook.
RX Vega Liquid;3;0.3288401663303375;Omdat de performance/watt veel beter is met crypto berekeningen dan met DirectX games.
RX Vega Liquid;1;0.4303281903266907;En wanneer je dan beseft dat voor de VEGA ook nog unsupported drivers zijn gebruikt, geeft dat helemaal te denken. OF er moet op pagina 2 een typ fout zitten mbt de gebruikte driver versie.
RX Vega Liquid;3;0.4041111469268799;De grootste energievreter is de GPU. Die is voor mining niet belangrijk en kan je dus gigantisch underclocken. Het geheugen is wel van belang, en juist de reden wat Vega interessant maakt.
RX Vega Liquid;3;0.41163304448127747;"""performance"" is niet per definitie gelijk in verhouding voor games en minen."
RX Vega Liquid;2;0.5032284259796143;Volgens mij wordt hier de kaart gerecenceerd op gaming performance en niet op mining performance. Daarnaast wordt de GPU anders benaderd door mining software. Het is overigens maar de vraag of de schaarse wel lauter door miners wordt veroorzaakt, vooralsnog weet AMD amper te leveren en laten de 3rd party kaarten ook nog wel even op zich wachten. Het lijkt er vooral op dat AMD persé Vega op de markt wil hebben om enigzins in het zog van de concurrentie te blijven. Het resultaat is een halfbakken launch met een enorme schaarse met hoge prijzen, een achterstand op driver gebied en geen geöptimaliseerde TDP. Het is dat Freesync monitoren een flinke klap goedkoper zijn tegenover G-sync, want buiten Freesync zie ik vooralsnog geen reden om voor Vega 64 te kiezen. De GTX 1080 presteert beter en is goedkoper. Ik ben overigens benieuwd of AMD niet alsnog met GDDR5(x) Vega kaarten gaat komen om de prijs wat te kunnen drukken, ik heb niet het idee dat HBM2 enige impact heeft op de prestaties.
RX Vega Liquid;1;0.48977673053741455;Ik wou deze eerst aanschaffen maar was en is nog steeds niet leverbaar. Helaas de GTX 1070 moeten kopen.
RX Vega Liquid;4;0.5148658752441406;Die kaarten zijn perfect leverbaar bij Alternate Duitsland, heb de mijne daar net na de release gekocht en 2 dagen later al aangekregen. Heb er zelfs al een EKWB waterblok op gezet. Is een zalige kaart met een FreeSync monitor. Alleen is de prijs inderdaad vrij hoog. Maar i.v.m. een 1080 en IPS G-Sync monitor kom je toch goedkoper uit met een 64 en FreeSync monitor.
RX Vega Liquid;2;0.4716885983943939;Maar freesync is toch ook niet zalig makend? Ik heb zelf freesync en ik wacht al erg lang op vega. Alleen hitte, stroom en verkrijgbaarheid is voor mij een teleurstelling. Stel ik koop een 1080 TI icm 1440p 144hz. Dat zou toch ruimschoots moeten compenseren voor het niet gebruiken van freesync? Ook de 1080 Ti is in Duitsland wat goedkoper. Ik heb altijd al een amd kaart gehad. Ik gebruik nu een R9 290 en deze wordt soms wel 94 graden zonder oc., silicon lottery laten we maar zeggen. Ik ben eigenlijk ook wel een beetje klaar met die enorme hitte's van AMD en hoger stroomverbruik.Toch nog even kijken wat customs Vega gaan doen:)
RX Vega Liquid;1;0.45308059453964233;Ik zit met precies hetzelfde dilemma, altijd AMD gehad. Eerst een 290x en toen een fury x en daarom ook een freesync 144hz paneel gekocht. Ik ga binnenkort ook een 1080ti halen (ben klaar met het falen van AMD) en dan draai ik hem op me freesync monitor. Dit is uiteraard geen probleem alleen kan je de freesync functie niet meer gebruiken. Ik heb het van de week eens getest of het echt nog nodig is op zulke framerates door de freesync uit te schakelen. Ik merkte er zeer weinig van moet ik zeggen. Maar een ieder zijn waarneming is anders. Ik zou dus zeggen draai een game op 144 fps zonder freesync en test het voor jezelf om erachter te komen
RX Vega Liquid;3;0.3340331017971039;De vraag blijft bij mij dan nog Freesync + vega 64 1440p 144hz vs 1440p 144hz met 1080 Ti. Ondanks het ontbreken van freesync of gsync zou optie twee toch veel beter moeten performen?
RX Vega Liquid;3;0.5969538688659668;Ja dat zeker. Qua FPS zit je veel hoger. De freesync is een extra functie op je scherm die vooral de dips onder de 60 fps opvangt en je beeld smooth houdt. Maar met een 1080ti kom je daar niet snel onder ook niet met 1440p!! Kijk maar naar de verschillende benchmarks op youtube...
RX Vega Liquid;1;0.5153622627258301;Als je een game als Ghost Recon Wildlands pakt en dan op ultra onder de 60 fps dipt mis je het dan ook niet? (Serieuze vraag, zit zelf te twijfelen over G-sync monitor vs 200 euro goedkopere freesync).
RX Vega Liquid;3;0.3741963505744934;Heb met mijn vorige R9 390 ook de FreeSync optie op mijn monitor gebruikt en dat was zo smooth zonder screen tearing. Heb zelf maar een 60Hz FreeSync monitor die OC'ed naar 75Hz als de FreeSync optie geactiveerd is. Vind toch dat dat veel aangenamer speelt. Heb voor men Vega een 1070 gehad en dat speelde ook zeker deftig op dezelfde monitor, maar zonder die FreeSync viel het wel op dat het niet zo smooth speelde. Als je een FreeSync met een 1080Ti koopt dan kan je alleszins niets met de FreeSync aanvangen, het zal sowieso wel smoother spelen, maar verder geen idee van. Heb nooit hoger dan 75Hz gehad. Die Vega stock koelers zijn echt brol, max temps waren 85°C zonder enige moeite en fan in overdrive op 3000rpm, heb enkel een ref. kaart gekocht omdat ik toch al een custom loop had. Nu is de max temp maar 40°C onder 100% load na uren aan één stuk te gamen.
RX Vega Liquid;1;0.5107260346412659;Volgens mij versturen ze nu alleen nog maar naar adressen in Duitsland (was in het begin niet). Hetzelfde geld voor Mindfactory en volgens mij ook Caseking. In de UK kan je wel bestellen en het naar NL laten komen.
RX Vega Liquid;5;0.36737915873527527;Heb de mijne de 16de augustus besteld bij Alternate Duitsland en ze hebben zonder problemen deze aan mijn uitgeleverd. Wat ze in het verleden niet deden bij Alternate. De bestellingen die binnenkwamen van de Duitse site werden gewoon doorgestuurd naar de desbetreffende shop waar de koper woont.
RX Vega Liquid;1;0.36890020966529846;Met de RX480 kon het wel. Maar laatst ik na de launch een kaart had besteld dus de 17-18e werd hij doorgezet naar alternate.nl en die zijden dat ze hem niet mochten leveren. Heb nog aangegeven dat het eerst wel kon ze zijden toen nu mag het niet. Dus geen idee of het nu wel kan. Wellicht mag het alleen bij die goedkope Vega kaarten niet. In engeland mag je van de veel duurdere versie (die de MSRP niet haalt) wel meer dan 1 bestellen. Van de MSRP versie maar max 1. Wellicht is het hier ook zo iets?
RX Vega Liquid;1;0.5315244793891907;Had ook eerst gedacht dat men bestelling niet doorging, maar bleek uiteindelijk zonder problemen te gaan. Misschien inderdaad zoals je zegt dat het enkel met de Black versies kon ofzo. Geen idee. Heb wel redelijk wat betaald voor men 64, dus misschien daarom en dat ze niet verkrijgbaar zijn in België en Nederland ofzo.
RX Vega Liquid;3;0.2278830111026764;Het is al even geleden dat ik nog eens iets bij caseking gekocht heb, maar op hun website kan je nog altijd een adres in België ingeven bij je winkelmandje. Misschien zijn ze hun website vergeten updaten, maar voorlopig ga ik er toch van uit dat ze nog naar het buitenland leveren.
RX Vega Liquid;1;0.3079483211040497;Bij mindfactory is dat ook het geval, je kan perfect alles invullen totdat je bij afrekenen komt, dan ben je verplicht om een geldig BTW nummer in te geven, wat ik dus niet had. Bij Caseking is het al even geleden dat ik daar nog besteld heb, laatste was mijn gaming chair begin dit jaar.
RX Vega Liquid;1;0.4665548801422119;Mwoa, zoveel scheelt het niet meer met sommige monitoren....
RX Vega Liquid;1;0.39976823329925537;Kan nog goedkoper, hoor. Als je een FreeSync kiest. Dus een 1080 plus G-Sync monitor komt nog steeds duurder uit dan een Vega 64 plus FreeSync monitor.
RX Vega Liquid;1;0.33218422532081604;Nee, dat scherm heeft 60hz en 75hz met freesync. De AOC gsync heeft 100hz. vergelijkbare freesync monitoren zijn 800+ euro
RX Vega Liquid;2;0.45137035846710205;"Die AOC heeft ook geen echt IPS paneel, maar een MVA, wat een aantal nadelen heeft, dus effectieve 34"" 1440p IPS G-Sync monitoren zijn nog een pak duurder. Plus 75Hz t.o.v. 100Hz is maar 25Hz verschil, amper merkbaar, imho."
RX Vega Liquid;2;0.3355543911457062;"De monitoren met freesync van 800 euro hebben ook VA. Die zijn dus direct vergelijkbaar (3440x1440, 100hz, VA). De AOC heeft zelfs 35"" ipv 34"". De freesync variant met die paneel is nog niet uit: pricewatch: BenQ EX3501R HDR Curved Aluminium, Zilver, Zwart En 75>100 is een gigatisch verschil. Ik ben overigens al tijden opzoek naar een freesync monitor, en had er misschien al een gekocht als Vega voor een normale prijs beschikbaar was. Maar de verschillen worden steeds kleiner."
RX Vega Liquid;2;0.4078664481639862;Ook de reden waarom ik imho heb gepost. In mijn mening is het verschil minimaal. Vega zit op dit moment ook in vroege Alpha fase, dus prestaties zullen normaal wel nog moeten stijgen. Er moeten nog veel bugs uitgewerkt worden. Net hetzelfde als met de RX400 reeks toen die uitkwamen, toen konden ze een R9 390 nog niet verslaan, nu kunnen ze een GTX1060 al gemakkelijk aan.
RX Vega Liquid;3;0.3335539996623993;Vega kickstarter edition. Alpha version... en toch de hoofdprijs betalen. Kan niet vind ik. Ik wacht inderdaad nog wel even. Misschien rond december een Vega 56 met betere koeler en die nieuwe BenQ...Moet de prijs wel ver onder de 1070/1080, anders ga ik naar nvidia.
RX Vega Liquid;2;0.4689920246601105;Inderdaad, had niet gedacht dat de drivers niet zo schitterend zijn. Zelfs Crossfire is nog niet ondersteund, terwijl dat bij de Frontier Editions wel het geval is. Ze hebben inderdaad een aantal steken laten vallen en misschien het een beetje overhyped. Wilde zelf eigenlijk ook een 56 maar kon niet wachten tot de 28ste, zat tijdelijk met een R5 270 omdat X99 geen onboard heeft, dus ja. Plus als ze zo snel uitverkocht waren bij meerder winkels, zoals de 64 op dat moment, wilde ik het niet riskeren.
RX Vega Liquid;2;0.358242392539978;Dat klopt daar is die wel leverbaar ind. Maar ik persoonlijk heb nog nooit in het buitenland besteld. Hoogstwaarschijnlijk komt de Vega 64 alsnog in me systeem straks Wil hem wel graag hebben.
RX Vega Liquid;1;0.6218347549438477;Is helemaal niet moeilijk. Je plaatst de bestelling via de website en je krijgt nadien een offerte dat je moet bevestigen met welke betalingsoptie je wil betalen, heb de mijne met Paypal betaald, en dan sturen ze je een linkje om de betaling inorde te brengen. Eens dat inorde is krijg je een mail met de tracking en na 2 dagen is het geleverd met UPS. Verzendkosten naar België was €10. Dus valt al bij al goed mee eigenlijk. Wilde die kaart eigenlijk ook lokaal kopen maar hier zijn ze echt te duur, als ze leverbaar zijn.
RX Vega Liquid;1;0.5503587126731873;Wat ben jij een nvidia fanboy, zeg. AMD is helemaal niet slecht. Nvidia zou ook gemakkelijk gebruik kunnen maken van FreeSync moesten ze de drivers van hun kaarten aanpassen. Maar guess what? Dat doen ze niet, ze hebben helemaal geen interesse in de gebruikers, enkel maar om zoveel mogelijk geld te verdienen aan hun G-Sync modules en zo snel mogelijk achter elkaar nieuwe kaarten uitbrengen. Proficiat dat je bij Pascal blijft, ben er zelf van afgestapt, had een 1070 hiervoor, drivers waren zo slecht dat het niet meer leuk was. G-Sync en FreeSync zijn exact dezelfde technologieën, dus geen goede of slechte technologie, het enigste verschil is dat FreeSync Open-Source is en G-Sync een dure module is. Maar dat wist je waarschijnlijk niet, blijf jij maar door je groene bril zien.
RX Vega Liquid;4;0.38651975989341736;grappig, gevalletje your mileage may vary, zo ben ik enkele maanden terug ( mei ) overgegaan naar een 1080ti en ondervind geen enkele problemen met inmiddels al 2 + maanden oude drivers. moet ik wel zeggen dat de pascal architectuur al langer uit is als deze kaart, maar het draait gewoon vlotjes vanaf de release. twee vrienden hebben al langere tijd een 1080 en 1070 en daar ook geen enkel probleem mee, zelfde geldt voor een vriend die rond dezelfde tijd als ik voor 1080ti sli is gegaan. zelfs een vriend met sli 780 is recentelijk over gegaan naar een 1080ti, wilde naar eigen zeggen van nvidia af vanwege hun ' praktijken ' en de vele bugs die hij ervaart met zijn sli opstelling, en is dus bijna tot het einde blijven wachten tot er meer informatie kwam over vega, heb hem echter nog niet horen klagen. gebruik nvidia al zo ' n 15 jaar en eigenlijk weinig last gehad van echte driver problemen, performance degradatie door drivers is in ieder geval lulkoek. overigens tja, dat gedoe omtrent g - sync en freesync, nvidia wilt een ervaring garanderen d. een tweaker is wel slimmer maar de algemene casual gebruiker zal het niet begrijpen, echter vind ik wel dat het configuratiescherm een melding moet tonen dat het pci - e 4x slot niet ondersteund wordt, dan gewoon de gehele activatie optie weg te laten. natuurlijk willen ze op deze manier ook wel r & d kosten terug verdienen, het stelt hun immers wel instaat producten aan de man te brengen die tegenwoordig moeilijk te verslaan zijn. opensource is een leuk idee, maar je krijgt niet altijd hetzelfde niveau van kwaliteit, maar sowieso wel meer vrijheid. maar ach iedereen moet gewoon de producten kopen waarbij ze zich thuis voelen, we kunnen elkaar wel blijven aanvallen om persoonlijke voorkeuren maar dat haalt eigenlijk maar weinig uit.
RX Vega Liquid;2;0.5195916891098022;Heb een aantal maanden achter elkaar steeds de laatste drivers geïnstalleerd, helaas inclusief de bugs en problemen. Nadien gewoon oudere drivers gebruikt zonder problemen. Was zeker tevreden van mijn 1070, maar vind het niet kunnen dat de drivers zoveel problemen hebben/hadden. Op dat vlak zijn AMD's drivers vele beter sinds de RX400 reeks. Met FreeSync heb je een gelijke ervaring enkel zonder die Premium prijs te betalen omdat het Open-Source is. Vind, persoonlijk ook dat CFX toch wat beter functioneert dan SLI. Heb een tijdje geleden met mijn HD5970's in CFX nooit dergelijke problemen gehad, liep heel vlot. De enigste reden dat ik ben afgestapt van nvidia was doordat ik een FreeSync monitor heb en het het niet waard vond om een duurdere G-Sync monitor aan te kopen. Nu met die Vega 64 te hebben gekocht hoop ik toch stiekem op betere prestaties, wat er zo te zien wel aankomt, de laatste nieuwe die vandaag zijn uitgekomen lossen veel problemen op. Dus inderdaad zoals je zegt, to each their own. Vond gewoon die belachelijke comment er een beetje over. Alsof AMD zo slecht is. Heb jaren AMD GPU's gehad en stuk voor stuk nooit problemen gehad. Ben er zeker tevreden van.
RX Vega Liquid;2;0.38493961095809937;Helaas een beter product gekocht? Mis ik iets?
RX Vega Liquid;2;0.5080533027648926;Beetje zuur voor Blaat. Lever je je nieuwe aankoop in om te laten testen, is de conclusie dat het eigenlijk een miskoop is en dat je voor een Nvidea had moeten gaan.
RX Vega Liquid;2;0.46291384100914;Op zich was dat geen verrassing hoor. Gelukkig heb ik het voor elkaar gekregen de kaart 10% sneller te laten lopen tegen ~170w minder verbruik. Icm een freesync scherm maakt dat de boel een stuk behapbaarder.
RX Vega Liquid;3;0.4972706735134125;Dus zonder Crypto miners lagen prijzen lager en zou dat de conclusie beïnvloeden. Leuk...
RX Vega Liquid;1;0.6316960453987122;Tja dat is nou eenmaal de realiteit. Zonder Chineze goedkope arbeid lagen de prijzen waarschijnlijk ook anders... Er zijn zo veel factoren die invloed hebben op verkrijgbaarheid en prijs. Daarnaast is de kaart zonder cryptominer ook wel duurder hoor, en dus nog steeds in vrijwel alle opzichten de grote verliezer
RX Vega Liquid;1;0.7426053285598755;"Dat is inderdaad een punt wat mij ook opviel, het volgende stukje is volstekte onzin: Nu AMD een week of drie de tijd heeft gehad sinds de introductie van de RX Vega-kaarten, is het beeld dat zich langzaam aftekent voor het rode kamp niet best. De kaarten zijn nog altijd slecht verkrijgbaar en zijn veel te duur door die slechte verkrijgbaarheid en dankzij crypto-miners. AMD verkoopt dus gewoon alles wat ze kunnen produceren en ze kunnen daarbij de marges opschroeven. Klinkt alsof ze het fantastisch doen. Het is natuurlijk jammer voor de gamende consument, maar dat mag de pret voor AMD vast niet drukken. Dit is net zo'n klacht als ""Het gaat slecht met BMW want ik kan hun snelste modellen niet betalen"". Well, boohoo.:P"
RX Vega Liquid;1;0.6223748326301575;Het slechte voor BMW als al hun modellen gekocht worden door professionele racers, of verzamelaars die ze in een garage neerzetten, is dat als er jarenlang geen BMW's op de weg te zien zijn, de merknaam BMW vergeten wordt onder de normale consumenten. En je wilt niet alleen afhankelijk zijn van 1 specifieke groep, die er elk moment mee kunnen ophouden.
RX Vega Liquid;3;0.47379282116889954;Er zijn genoeg exotische automerken die zich daarmee prima kunnen redden. Ik ben wel met je eens dat er een zeker risico inzit, maar vooralsnog kan AMD daar weinig aan doen behalve dan bakken met geld verdienen om meer productiecapaciteit te kunnen kopen. Dat eerste stuk gaat ze nu waarschijnlijk wel redelijk af.
RX Vega Liquid;2;0.4077402353286743;Mis ik iets of gaat er iets niet helemaal goed met de weergave benchmarks?
RX Vega Liquid;2;0.4968361258506775;Je bent niet de enige, bij mij is alles leeg.
RX Vega Liquid;1;0.35259807109832764;Inderdaad. Zag alleen maar tekst. Maarja dacht eerst dat het er aan lag dat het nog vroeg in de ochtend is. Hahah
RX Vega Liquid;5;0.2541778087615967;Idem hier , een review zonder grafiekjes
RX Vega Liquid;3;0.309870183467865;Daar ging inderdaad iets mis, als het goed is is het nu voor iedereen zichtbaar
RX Vega Liquid;5;0.5347422361373901;Hulde!
RX Vega Liquid;2;0.4635300040245056;Met andere woorden, je kan het beste de GeForce GTX 1080 kopen, de Vega 64 Liquid is ongeveer net zo snel als de GeForce GTX 1080, maar de Vega 64 Liquid gebruikt 2x zo veel stroom dan een Geforce GTX 1080 in spellen, wat extreem veel meer stroom verbruikt, erg jammer had liever een AMD grafische kaart gezien die net zo snel was en net zo veel stroom gebuikt, en de GeForce GTX 1080 is een stuk goedkoper.
RX Vega Liquid;1;0.5721407532691956;Klopt, de waarden van de RX 580 kloppen voor geen meter bijvoorbeeld, consistent 15-20% slechter dan mijn eigen RX 580 bijvoorbeeld, dus in hoe verre kloppen de overige waarden.
RX Vega Liquid;3;0.6129994988441467;Dus het wordt ook een stuk warmer. Geen probleem voor towerkasten met een stevige voeding en voldoende ventilatie, maar wel een probleem bij compacte builds. De GTX 1080 kan bijvoorbeeld in een kleine behuizing gepropt worden met net voldoende ventilatie. Dat gaat met een Vega niet gebeuren, tenzij je een minioven wilt hebben.
RX Vega Liquid;5;0.52675461769104;Een minioven die tegelijkertijd bitcoins voor je minet en een pizza klaarmaakt
RX Vega Liquid;1;0.5911526083946228;Slow cooking, luchtgedroogde ham Hoef je niet eens de keuken in na een gamingsessie, kast open en bord erbij.
RX Vega Liquid;2;0.40514639019966125;Denk niet dat een watergekoelde kaart zoveel warmer gaat worden als een luchtgekoelde kaart, ook al verbruikt hij het dubbele
RX Vega Liquid;3;0.5285575985908508;Natuurlijk wel, de warmte moet ergens weg, 300w is 300w, hoe je het ook koelt.
RX Vega Liquid;1;0.730536699295044;"""Nu AMD een week of drie de tijd heeft gehad sinds de introductie van de RX Vega-kaarten, is het beeld dat zich langzaam aftekent voor het rode kamp niet best. De kaarten zijn nog altijd slecht verkrijgbaar en zijn veel te duur door die slechte verkrijgbaarheid en dankzij crypto-miners."" Ik zie niet in hoe de populariteit bij crypto-miners niet best is voor AMD. Als er zoveel vraag is naar het product dat alle stock meteen verkocht wordt, zal het hen worst wezen dat het niet gekocht wordt door de initiële doelgroep."
RX Vega Liquid;1;0.5253075361251831;Omdat grote serverfarms worden opgezet met deze kaarten terwijl de consument er niet van kan genieten. Btw, ik ga denk ik een vega halen als ze droppen in price... die 10-20 fps wat nvidia meer heeft kan me weinig schelen >_<
RX Vega Liquid;2;0.4276920557022095;Kwestie van de productiecapaciteit op te schroeven om aan de vraag te kunnen voldoen. Again, een verkochte kaart is een verkochte kaart, wie ze koopt is van minder belang.
RX Vega Liquid;2;0.3983619809150696;dat is helaas geen simpele kwestie
RX Vega Liquid;2;0.45361629128456116;Ik wacht op de volgende generatie Nvidia kaarten.. Op dit moment voldoet mijn (40% over stock overklokte) MSI GTX980Ti nog prima i.c.m. de Acer XB270HU.. P.s. BF1 benchmarks in DX12 steeds, waarom?!? Het heeft geen toegevoegde waarde over DX11 in die game en draait slechter, zeker op Nvidia kaarten..
RX Vega Liquid;3;0.5736676454544067;Zal er wel mee te maken hebben dat er weinig DX12 titels aanwezig zijn die op een goede engine gebouwd zijn en geoptimaliseerd zijn, moeten toch iets hebben om te vergelijken. Maar het zou uiteraard wel fijn zijn de directe vergelijkingen met DX11 te zien. DirectX11: DirectX12: Misschien ook interessant:
RX Vega Liquid;4;0.4657178223133087;Zeer mooie review, nu moeten de kaarten nog beschikbaar worden. Alleen de benchmark velden zijn leeg.?
RX Vega Liquid;2;0.48082947731018066;Ik zit met mijn gsync monitor voorlopig stevig locked in op het groene kamp, maar het is jammer dat het AMD voorlopig nog niet lijkt te lukken om de gpu markt op te schudden zoals ze dat bij cpu's doen dit jaar. Het is fijn voor AMD fans dat zij nou ook een high end optie hebben, maar voor Nvidia verandert er eigenlijk niks en is er weinig wat hun zal bewegen om hun prijzen aan te passen of vlotter met snellere kaarten uit te komen.
RX Vega Liquid;1;0.49297472834587097;Klopt, ik zou ook zo voor een 1080 gaan, maar als je ook wilt upgraden naar een 144Hz 21/9 scherm, dan is het het totaalplaatje wat mij toch voor een Vega+Freesync monitor heeft doen kiezen..
RX Vega Liquid;2;0.44083237648010254;Ja, ik kon toevallig precies op het juiste moment een Dell S2417DG kopen, een QHD 165Hz Gsync panel die me toen 430 euro kostte waarmee hij nauwelijks duurder was dan de Freesync alternatieven. Nu is dat scherm overal weer 500 euro. Op grotere formaten is het prijsverschil vaak nog een stuk groter en dan wordt AMD met Freesync inderdaad steeds interessanter. Edit: En wat bij mij meespeelde was dat ik mijn GTX 770 dan nog wat langer kon gebruiken. Was toen ook naar Freesync en een RX480/580 aan het kijken maar die waren (en ik geloof nog steeds) nauwelijks te krijgen, laat staan tegen redelijke prijzen.
RX Vega Liquid;2;0.31789204478263855;Hier ook helaas lege grafiekjes. Edit: het is gefixed! Mooie review, wijst wel weer uit dat het helaas nogsteeds niet met nvidia meekan. De schaarsheid is punt 1, maar daarnaast is de performance van amd's topmodel net zo goed als een 1080, verbruikt (veel) meer stroom en dan hebben we het ook nog eens over een liquid versie. De 1080 zou ook nog flink sneller kunnen lopen als er een liquid versie van gemaakt word.
RX Vega Liquid;5;0.45103374123573303;Waarom wordt er geen moeite gedaan om iets over het product zelf uit te leggen? Over wat voor fan/radiator hebben we het bijvoorbeeld? Ik ben heel benieuwd naar de temperaturen van dit ding als er ~350 watt op een rad van 140mm wordt gekoeld. Ook een vergelijkingstabel en wat detail foto's zouden niet misstaan.
RX Vega Liquid;1;0.47412413358688354;er wordt niet eens de moetie gedaan om een driver te gebruiken die ondersteuning heeft voor Vega... Ondertussen is 17.8.2 allang uit
RX Vega Liquid;3;0.513450026512146;Het algemene beeld zal er niet echt door veranderen.
RX Vega Liquid;1;0.35046857595443726;Zou het idee zijn om 2 van de games te vervangen door non-gaming tests? Blender en crypto mining bijvoorbeeld
RX Vega Liquid;1;0.33038488030433655;ik had best hoge verwachtingen voor vega, heb er 4 maanden op gewacht, maar nu toch maar een 1080ti besteld
RX Vega Liquid;2;0.4987351894378662;Ik snap niet hoe mensen deze kaarten nog kunnen verdedigen. Iedereen had hier hoge verwachtingen van, eindelijk concurrentie voor Nvidia in het hoogste segment. Helaas blijkt nu dat deze kaart verre van een concurrent voor de GTX 1080 en al helemaal niet van de GTX 1080Ti is. Daarnaast zijn de kaarten niet voldoende leverbaar en gewoonweg te duur voor wat je er voor krijgt. Ik vind de conclusie van deze review helemaal niet pro Nvidia maar gewoon realistisch. De bubbel is gebarsten voor AMD helaas. Gelukkig zijn de CPU's die zij leveren wel een waardige concurrent voor Intel en is het AMD daar wel gelukt een vuist te maken.
RX Vega Liquid;2;0.5553007125854492;Dat is ook niet helemaal waar. Ik was zeer enthousiast toen de threadripper werd aangekondigd. Nu blijkt dat voor mijn toepassing de latency te hoog en inconsistent is, erger nog dan bij Intel's X99, wat qua specs ook veel beloofde, maar waar binnen mijn toepassing nogal een bottleneck scheen te zijn. Het enige dat ik hierdoor hoop is dat Intel gekieteld is weer eens te innoveren in plaats van als king of the hill het lekker rustig aan te doen.
RX Vega Liquid;3;0.5369902849197388;Goed om te zien dat AMD weer mee kan draaien in performance. Alleen wel zuur dat de prijs én de TDP zo hoog liggen. Waar het eigenlijk op neer komt is dat de Vega64 alleen interessant wordt als de prijs (ruim) onder de €500 komt te liggen. Dat zie ik niet gebeuren... jammer!
RX Vega Liquid;3;0.4333135485649109;leuk al die benchmarks...maar waarom wordt er bijna alleen vergeleken met kaarten van de laatste generatie? doe de kaarten van de vorige er ook bij! dan kunnen mensen tenminste zien hoeveel ze er op vooruit kunnen gaan!
RX Vega Liquid;2;0.3824315369129181;Ik hoop van harte dat jullie deze kaarten willen testen met toepassingen waar deze mee in het achterhoofd voor ontworpen is. Compute, compute en compute. Dat je er ook op kan gamen is echt prettig meegenomen maar de echte kracht zit 'm in de (semi)professionele toepassingen. Bijna iedereen blijft zich maar blindstaren op gamebenchmarks (met de verkeerde driver nb.) terwijl daar de echte teraflops er niet echt uit komen. En bijna alle games in deze review zijn niet geoptimaliseerd voor deze architectuur. Doe eens wat DaVinci Resolve, Lightroom, Blender en meer van dat toepassingen benchmarken. De resultaten daarvan zijn zoveel interessanter voor de Vega kaarten. Hoe verhoud deze kaart zich dan met de oudere generaties en de concurrentie? Daar zou ik graag de cijfers van willen zien.
RX Vega Liquid;3;0.33279937505722046;Voor mijn gevoel de review in een notendop: In games die slecht/of voor nvidia geoptimaliseerd zijn en waarbij amd nokit sneller is geweest, loopt vega nog steeds achter. In dx12 games is vega een toppertje. Ps ook jammer dat de pijzen aangehaald worden omdat ze zo hoog zijn, maar dat jullie er niet bij zetten dat er ook diverse interessante kortingen en games bij gegeven worden. (dit werd overgens wel door jullie gezegt bij de aankondiging van vega: nieuws: AMD Vega RX-videokaarten vanaf 14 augustus verkrijgbaar vanaf 399 dollar )
RX Vega Liquid;1;0.7153708338737488;Nu AMD een week of drie de tijd heeft gehad sinds de introductie van de RX Vega-kaarten, is het beeld dat zich langzaam aftekent voor het rode kamp niet best. De kaarten zijn nog altijd slecht verkrijgbaar en zijn veel te duur door die slechte verkrijgbaarheid en dankzij crypto-miners. Ik denk dat AMD zich opperbest voelt wanneer hun producten doorlopend uitverkocht zijn.
RX Vega Liquid;1;0.8897170424461365;Ik denk dat we duidelijk kunnen concluderen dat Vega in z'n geheel, zeker voor de gamer, 1 groot fiasco is geworden. Dit wordt nog eens versterkt door de absurde prijzen en zeer slechte verkrijgbaarheid.
RX Vega Liquid;3;0.40644189715385437;Bedoel je met verkrijgbaarheid dat ie nog niet officieel verkrijgbaar is in Nederland? Want in Engeland en Duitsland vind ik het allemaal wel meevallen met de verkrijgbaarheid. Zowel op Scan als op Mindfactory is er voldoende stock van in ieder geval Vega 64, en af en toe ook 56. 56 is natuurlijk een stuk populairder vanwege de lagere prijs.
RX Vega Liquid;2;0.5430594682693481;De verkrijgbaarheid in Nederland is inderdaad zeer triest. Ik heb het echter meer over de combinatie van verkrijgbaarheid en de prijs. Ook in het buitenland is de kaart gewoon veel en veel te duur voor hetgeen je krijgt. De liquid edition maakt het helemaal bont door een hogere prijs te hebben dan de volledig superieure 1080ti. Je moet wel heel naïef of groot AMD fan zijn om een Vega kaart (ongeacht welke) te kopen. Op bijna elk gebied (prijs, prestatie en vermogen) is de 1070, 1080, 1080ti beter! Hoe AMD omging met de aankondiging van Vega een dik jaar geleden en vervolgens de teasers van de teasers van de aankondigingen had eigenlijk al voldoende moeten zeggen...
RX Vega Liquid;3;0.4315716624259949;Vega is alsnog gewoon een prima product. Als je kijkt naar de performance in professionele toepassingen, dan is de prijs/prestatie onovertroffen.
RX Vega Liquid;3;0.3937622904777527;Ziet er goed uit, maar waarom is PUBG geen Benchmark? Het spel is behoorlijk zwaar en populair
RX Vega Liquid;1;0.6993960738182068;Omdat het een zeer slecht geoptimaliseerde titel is. De enige reden waarom het spel zwaar is is omdat het nog in beta fase zit, niet omdat het zo gruwelijk mooi is om te zien. De 17.8.2 update van AMD gaf ook bijna 20% aan performance erbij in PUBG. Dat feit alleen al zegt al voldoende over hoe weinig het spelen van die game zegt over je hardware.
RX Vega Liquid;2;0.5610290169715881;Mij valt vooral op hoe hoog het idle geluidsniveau wel niet is van een watergekoelde kaart. Mijn doel voor m'n huidige PC was om een zo stil mogelijk systeem te hebben, zowel idle alsook ingame. Als je kast dan op je bureau staat dan lijkt mij dat een dergelijke kaart direct niet meer in aanmerking komt. Wat dat betreft kan je beter een kaart hebben met een overgedimensioneerd koelblok wat er voor zorgt dat bij minimale belasting de koelers simpelweg uit blijven.
RX Vega Liquid;4;0.2994888722896576;Wil je stil zijn bouw dan een custom WC set, gebruik dynamische fan en pomp snelheden (PWM gestuurd) en let op met de componenten. Warmte die je niet in het systeem stopt, hoef je ook niet af te voeren.
RX Vega Liquid;3;0.3840446472167969;Ik ben zelf de gelukkige bezitter van een Strix 1080Ti. Draai nu op een 6600k. Ik ben jaren terug groot fan geweest van AMD(Ati). Maar bij de eerste Intel Dual Core was het al duidelijk. Na de Athlon XP heeft AMD er geen echte concurrentie aan de dag gelegd. Of te warm, of teveel power drain. Dan kun je een fervente AMD fanboy zijn(en daar heb ik vrede mee hoor), maar AMD is natuurlijk ook gewoon een commercieel bedrijf. Ik krijg de indruk dat AMD deze Vega serie niet wil verkopen als een game kaart, maar meer als een allround oplossing. Gezien hij on par is met een Quadro P5000, lijkt het een kaart bij uitstek voor 3D artiesten. En wordt het een geduchte concurrent voor de Quadro, aangezien die bijna twee keer zo duur is. Qua gaming kaart zie ik het een beetje als de rotatiemotor. Hij is verkrijgbaar, sterk op een aantal punten i.v.m. de conventionele systemen, maar er kleven net iets teveel nadelen aan.
RX Vega Liquid;1;0.49257832765579224;Amd heeft het verprutst konden net zo goed wachten tot ze wel iets hadden om beter te concurreren Waarom gaat iemand deze Vega aanschaffen ipv een 1080ti voor mindere prijs? Een nobrainer
RX Vega Liquid;1;0.8186545968055725;Had al ergens gelezen dat die waterkoeling waardeloos is en slecht warmte utistoot. Typisch weer AMD -__- Daarnaast de mensen die hadden opgelet in pricewatch .. de founders 1080 ging een tijdje voor 470 euro weg! Dus iedereen die op VEGA zat te wachten mag zichzelf voor zijn kop slaan, echt next level dombo.
RX Vega Liquid;1;0.321895033121109;ja want wachten op een nieuwe product waarvan de prestaties onbekend zijn is dom ? alleen een dombo zou achteraf zon conclusie kunnen trekken
RX Vega Liquid;5;0.5408948063850403;ja, een heel jaar wachten wat de VEGA fanboys deden sinds 2016..
RX Vega Liquid;2;0.6104114651679993;De conclusie is erg narrow minded geschreven mijn inziens Dat je misschien beter voor hetzelfde geld een 1080TI kunt kopen kan best zo zijn. En ongelijk kan ik de reviewer ook niet geven op dat vlak als je naar de VEGA 64 LC kijkt. Echter de manier van schrijven van de conclusie laat weinig ruimte voor nuance in mijn ogen en daarmee krijgt zelfs een product wat zeker geen topper is prijstechnisch in verhouding tot de 1080/1080TI nooit meer de waardering die het ergens wel zou moeten krijgen op technologisch gebied
RX Vega Liquid;1;0.8606370687484741;deze kaart is waardeloos presteert in maar in een deel van de games beter dan een GTX 1080 en voor hetzelfde geld kun je een gtx 1080 TI kopen bovendien is dit niks meer dan een R390X met moderne waterkoeling en een andere naam. het zal wel duidelijk geweest zijn dat het niet makkelijk is om een R390Xv2 te verkopen.intel/amd hebben 0 innovatie
RX Vega Liquid;3;0.38823994994163513;Ik ben dan aan de andere kant ook wel benieuwd hoe deze (en de andere RX Vega kaarten) presteren tegenover een overclockte GTX1080, want heb niet het idee dat Tweakers een dergelijke vergelijking heeft gemaakt, maar wel de RX Vega kaarten heeft overclocked om vervolgens te meten hoe dan het potentieel is tov de GTX1080. Eigenlijk geeft dat dus (performancewise sowieso) een vertekend beeld.
RX Vega Liquid;3;0.4677245020866394;Dat de GTX1080ti niet in alle vergelijkingen zit, vind ik dan niet zo heel raar. De GTX1080 is (zover ik uit deze review opmaak) in 75% van de gevallen sneller. De GTX1080Ti is op zijn beurt weer 20% sneller dan de GTX1080.
RX Vega Liquid;2;0.4378710389137268;Klopt, alhoewel vroeger de top-end kaarten van AMD tegenover de top-end van Nvidia getest werden, nu is het een trede lager. De R9 290X kan hier en daar tegenwoordig de 780Ti verslaan d.m.v driver updates, maar sindsdien lijkt het niet meer te lukken. Dus een slechter verbruik en sterk wisselende prestaties t.a.v een GTX1080 ruim een jaar later, laat staan beschikbaarheid en prijzen. Eigenlijk had iedereen de 1080Ti killer verwacht een half jaar terug, want zoals we gewend waren van AMD presteerden de top videokaarten toch wel in de top-range samen met de concurrentie. Daarom kan ik de reactie ook wel deels begrijpen waarom dat de 1080Ti niet getoond wordt, maar dat ding lijkt momenteel echt in een 'league of his own' te opereren. Maar 20-35%~ (wisselend per game, misschien soms minder of meer) prestaties bovenop een 1080 tellen voor een 1080Ti is zo moeilijk ook weer niet, dus ach, maar het was mooier voor een completer overzicht.
RX Vega 64;2;0.349263072013855;Bij andere reviews (Rx570 of Rx580) lees ik veel reacties waarin wordt aangegeven dat de review 'oneerlijk' is omdat Intel+Nvidia wordt vergeleken met Intel+AMD. Wat als deze tests worden uitgevoerd met een Ryzen CPU (1800X bijvoorbeeld), zouden de Vega's dan beter uit de test komen?
RX Vega 64;3;0.5728334784507751;Ben bang dat dat nogal tegenvalt:
RX Vega 64;4;0.46647879481315613;Interessant om te lezen! Bedankt voor de tip!
RX Vega 64;5;0.6314813494682312;Wat een prachtige review Tweakers, mooi uitgebreid met erg veel testjes. Dat is de kwaliteit die we graag zien! Ook erg handig vind ik de geluidsproductie testen. Immers kan je daardoor nu al zeggen dat, tenzij je een stofzuiger naast je voor lief neemt, je beter kunt wachten op aftermarket coolers van MSI, Asus, Sapphire, et cetera. Daarbij gebruikt een vega 64 zo'n 120W meer onder load dan een vergelijkbaar presterende 1080. Als je ervan uitgaat dat een computer 3 uur per dag onder load staat dan kost je dat per jaar 29 euro aan elektriciteit meer. Als je dus deze kaart zou willen vergelijken met de 1080 dan zou ik 60 euro bij de prijs van de vega 64 optellen. In het geval dat je de vega 56 wilt vergelijken met de 1070 moet je 36 euro bij de prijs van de vega 56 optellen. Omdat Tweakers niet alleen bestaat uit de reactie, maar ook uit de community, is hier nog wat leesvoer vanuit onze geweldige community. Ik hoop dat er veel reviews van deze kaartenserie (en dan vooral die met aftermarketcoolers) mogen volgen. Eerlijk is eerlijk, iedereen is gebaat bij concurrentie tussen het groene en het rode team. We zien hoe erg veel prestatie we er bij Intel hebben bijgekregen de afgelopen jaren door gebrek aan concurrentie. Okee, we zijn bij de desktop-CPU's er wat Watt ( ) aan efficiëntie op vooruit gegaan, maar in wezen kan een 5 jaar oude overgelockte i5 2600k nog goed meekomen. productreview: AMD Radeon RX Vega 64 8GB review door Foritain Fortain schrijft in zijn conclusie:
RX Vega 64;1;0.5272456407546997;de vega 64 liquid (met een meer dan 15% hogere clock) wordt niet gereviewed, wat dé tegenstander is van de 1080ti, de specificaties zijn fout vermeld (zie de rekenkracht), vreemd op volgorde gezet en een prijs/prestaties vergelijking ontbreekt (ja zelfs al is het maar een paper-launch met richtprijzen), je kan dit dan ook moeilijk een uitgebreide review noemen.
RX Vega 64;1;0.3612971901893616;We hebben de RX Vega 64 op woensdag gekregen, de RX Vega 56 op vrijdag en de Liquid nooit. Dan wordt het lastig reviewen
RX Vega 64;2;0.48584800958633423;Tel 15% op bij de prestaties en een prijs die hoger ligt dan een 1080ti (die aanzienlijk sneller blijft) heb je alleen nog maar een grotere teleurstelling, de AIO versie is een 345watt monster die de vega niet veel interessanter maakt. Vergelijk je een aio 1080ti met een aio vega64 is het huilen met de pet op. Voor de liefhebbers, in NL is hij niet te koop, in DE hebben ze hem wel. Mindfactory hebben ze hem in het systeem staan voor 509€ (vega 64) 609€ (vega64 LI) en 715€ de AIO. Ter referentie: je hebt daar voor 509€ een 1080 van zotac met custom cooler. (in het systeem staan en bestelbaar) EDIT: ik denk dat ze van elke kaart maar 1 hadden wat ze zijn op er blijven nu twee vega64 staan beiden voor 649, aanzienlijk duurder dan de AMD kaart was.
RX Vega 64;2;0.5087007284164429;Als je ziet dat er met de overclock van deze vega 64 meer dan 15 % wordt overgeclocked en de prestaties met maar 9% toeneemt dan raakt je eerste zin kant nog wal. Echter als de prijs hoger ligt dan de 1080ti heb je juist veel meer gelijk...
RX Vega 64;4;0.2645828127861023;De gpu throttled, kan niet anders.
RX Vega 64;2;0.5300534963607788;Ik heb gekeken naar wat de 56 deed(clock) is 200mhz langzamer, de vega 64 deed ~15% beter, de 64 AIO is 200mhz sneller dan de stock 64 dus deed ik daarboven 15% gem. Het zal altijd lager zijn aangezien de gelijk gebleven streamproc. De prijs in DE, 715€, dat is vergelijkbaar met de 1080ti die net even eronder zit en toch echt nu beter is, misschien dat de vega in de toekomst beter doet maar, maar er is natuurlijk geen garantie. Zeker niet als vega een slechte zaak blijkt voor AMD en dat ze het roer compleet omgooien. Dan zal de driver ontwikkeling ook stoppen voor vega, want dat de drivers in het verleden beter werden kwam simpelweg omdat ze GCN gaande weg beter begrepen en optimaliseerde.
RX Vega 64;2;0.45238491892814636;Ik heb geen idee waar je het over hebt? 'het zijn kaarten wat maximaal 200eurie kosten', wat kost 200€ welke kaarten. Ik heb het alleen over de vega64 aio kaart, die kost 700+ euro vergelijkbaar met de 1080ti die gewoon sneller is en bijna 100watt minder verbruikt. De vega 64 kost op zijn minst 500€ als je hem voor die prijs kan vinden, de 1080 paar tientjes meer, heb je een betere koeler dan de standaard (reference) heatsink & fan van AMD en standaar minder vermogen verbruikt en dus stiller is en makkelijker overclockt (nvidia 180watt vs AMD 295 watt), na verloop van tijd wordt de vega misschien sneller maar dat is nooit extreem en onzeker.
RX Vega 64;2;0.41248902678489685;Moet ik nog duidelijker aangeven wat we testen? Er staat bij de specs welke kaarten we getest hebben. We wisten tot vrijdag niet eens dat we de Vega 56 zouden testen. Ik zie de meerwaarde van ons beklagen dat we de Liquid niet getest hebben, terwijl dat evident is, niet. Bovendien is de Liquid niet eens los te koop, alleen als onderdeel van een pack. Lastig ook om daar een prijs/prestatie aan te hangen...
RX Vega 64;3;0.31314244866371155;Hoe werkt het stroomverbruik meten precies? Ik lees dat jullie een nieuwe manier van meten hebben d.m.v. een pci-e riser. Zit er ook nog meetapparatuur tussen de psu 2x8pin aansluitingen? Het lijkt me sterk dat je het verbruik kan meten met alleen de pcie port.
RX Vega 64;5;0.587992250919342;We meten zowel stroom en spanning aan het pci-e-slot als van de peg-connectors, alle stroom die naar de kaart gaat dus. Die riser zit er niet alleen om de slot-power te meten, maar de twee peg-connectors lopen ook via dat pcb om in 1 keer pci-e-slotpower, peg1 (6/8pin) en peg2(6/8pin) te meten en via usb uit te lezen. We kunnen dan netjes slotpower en per peg-connector de stroom/spanning/vermogen zien en uiteraard combined.
RX Vega 64;1;0.4630998969078064;Resultaten 1 op 1 overgenomen van HWI. Die van AMD TR ook al, dus de vraag is wie doet de review jullie of HWI?
RX Vega 64;3;0.46155643463134766;De reviews doen we los van elkaar Ze zijn ook door verschillende auteurs geschreven en daarom komen er verschillende conclusies uit. Wel werken we sinds we onder een dak zitten samen op het gebied van tests, waarbij data gedeeld wordt. Zeker bij producten als deze, waar samples enorm kort voor de deadline aangeleverd worden, kunnen we daardoor meer en beter testen.
RX Vega 64;2;0.4757937788963318;Uitgelekte benchmarks toonden gisteren aan dat de liquid versie niet veel beter presteert dan de lucht gekoelde versie. Zeer zeker geen concurrentie voor de 1080ti iig. Mogelijk als de drivers in orde zijn dat er weer een procent of 10 gepakt kan worden. De liquid versie is ongetwijfeld een stuk koeler, en overclocked nog een stukje verder. Maar dat zie je dan ook wel terug in het verbruikte vermogen. En de positieve overclock resultaten zorgen volgens deze review maar voor een relatief kleine winst begrijp ik.
RX Vega 64;5;0.30760493874549866;Ik kan niet wachten op de vega 64 liquid aftermarket kaarten. Ik ben heel trots op AMD ze hebben laten zien dat met wat ze hebben en kunnen toch wel competitief genoeg is om interessant te zijn om voor AMD ipv NVidia te kiezen en als we de aftermarket kaarten hebben hoop ik dat we gezamenlijk nog trotser kunnen zijn
RX Vega 64;3;0.4578961431980133;"Die voorbij 1080p wens is een beetje dubbel. Ik gebruik een steeds luidere GTX780Ti GHz edition en behoudens de games die struikelen over 3GB geheugen kan ik met redelijke kwaliteitsinstellingen de meeste games op 1440p goed spelen. (En die geheugen problemen kan je vaak ook voorkomen door de textures wat verder terug te draaien. Kijk maar naar de test resultaten: op ultra zit een (naar ik aanneem) stock 780Ti overal al boven de 30FPS. Een RX580 of een 390X presteren al beter dan de 780Ti en hebben geen geheugen tekort ,deze zijn voor 1440p voor de meeste gamers dan ook voldoende. Met een 1070 is het gros van de spelers op 1440p ruim voorzien van FPS. Dan blijft over de groep top gamers en 4k gamers, die eerste wil 144Hz monitoren aansturen en die laatste wil op zijn minst fatsoenlijke FPS halen. De 1080 en daarmee dus ook Vega64 vallen dan een beetje buiten de boot. Te snel voor de 1440p crowd, te langzaam voor de top spelers/4k crowd; die grijpen naar de GTX1080Ti of een Titan (als ze van hun geld af willen). Ik verwacht dan ook dat AMD het van Vega56 zal moeten hebben, die pakt net een mooie sweetspot, maar dan moet de prijs wel kloppen."
RX Vega 64;2;0.49763888120651245;4K met een 1080 is prima te doen hoor. Je moet alleen geen ultra en AA verwachten. Dit test tweakers wel, maar er zijn genoeg instellingen die je niet direct nodig hebt om je game er goed uit te laten zien. En eenmaal gewend aan 4K wil je niet echt meer terug naar 1080p of 1440p. Maybe ultrawide maar dat heeft weer haar eigen problemen. Gewoon wat spelen met de instellingen en je zit zo boven de 60fps. Behalve de titels die sowieso al voor geen meter lopen. Het jammere van deze kaart is dat gamers die op dit prestatie punt (vega64) willen instappen zijn of al een jaar op de 1080 (zoals ik) of verstokte AMD liefhebbers.
RX Vega 64;3;0.7146250605583191;Ik vind dat anti-aliasing met een wat lagere resolutie er soms beter uit kan zien dan een wat hogere zonder. Maargoed, ik game niet zoveel en mijn games zijn al wat ouder dus dat is misschien ook niet meer te vergelijken.
RX Vega 64;3;0.6675093770027161;Tja, ik houd het vooralsnog even op 1440p, visual studio werkt lekkerder op 2x 1440 dan op 1x 4k. Voor gaming ben ik echter een beetje teleurgesteld in Vega, ik denk dat ik de Freesync maar ongebruikt laat en even door spaar voor een 1080Ti.
RX Vega 64;1;0.3034309148788452;"Is AA nog echt nodig als je op een sub 32"" scherm 4k draait?"
RX Vega 64;5;0.6077457666397095;"Ik zit met 28"" en zet het altijd gelijk uit als ik meer fps nodig heb. Je kan het zien, zeker als je gaat zoeken in de kleine dingen en op de plekken waar je standaard die trappetjes kan verwachten. Je moet echt naar 8 of 16K schermen voordat we daar vanaf zijn denk ik. Maar het is echt meer dan acceptabel om zonder AA te draaien"
RX Vega 64;1;0.7107934355735779;Vergeet de VR-gamers niet. Dan is het met een 1080 al schipperen met instellingen
RX Vega 64;5;0.46506601572036743;Dank je (Vanuit het hiernamaals)
RX Vega 64;3;0.435801237821579;Echter, als je in de markt bent voor een nieuwe monitor, dan ligt het erg voor de hand dat er een beeldscherm komt met Adaptive vSync. In dat geval heeft AMD weer een duidelijk prijsvoordeel omdat Gsync schermen een stuk duurder zijn dan vergelijkbare FreeSync schermen. In dat geval kan je dus weer makkelijk 100 Euro aftrekken van de Vega videokaarten en brengt het het nodige weer in balans. Zeker voor iemand die minder dan 3 uur per dag gemiddeld bezig is met gaming. Voor de volledigheid zou dit eigenlijk in jouw post ook vermeld mogen worden
RX Vega 64;3;0.4736166000366211;"Dit is wel een goed punt.. ik heb 2 weken terug een 24"" IIyama scherm gekocht voor 183 euro (nieuw met 1 dode pixel bij Coolblue, normaal 260 euro). Heeft 1080p, 144hz en Freesync. Voor het Gsync alternatief zit je al snel op 350 euro voor het instapmodel. Als je in de markt bent voor een GPU en een monitor dan heeft AMD vrij snel de betereprijs/kwaliteit verhouding"
RX Vega 64;2;0.33951544761657715;het geluid van de koeler zal meer decibels geven dan een strix gtx1080 zoals ik die heb. mijn kaart gebruikt 180 watt op load en de vega 64 gaat daar met 120 watt erbij ruim over heen ( 180 + 120 = 300 watt!! ). dat betekend dat die ene koeler echt heel hard moet werken. die warmte zal toch afgevoerd moeten worden en ik betwijfel het gameplezier als je systeem een stofzuiger is qua geluid. mijn kaart laat al licht van zich horen als ik hem langdurig op 80 % + load zet. dan zitten er op mijn gtx1080 nog 3 grote koelers. de kosten van het extra energieverbruik liggen hoger dan die 120 watt extra alleen. je moet ook de efficientie van de voeding hierin meenemen. het kan waarschijnlijk zijn dat je een andere voeding nodig gaat hebben. het gaat er mij niet om om amd in een negatief daglicht te zetten, maar de wens om de 1080 familie van nvidia voorbij te gaan kost zoveel extra vermogen dat er echt een betere koeling nodig is. die zullen een stuk stiller zijn, maar je hebt dan een behoorlijk raster nodig met minimaal 3 fans om die warmte je kast uit te krijgen. en dan het laatste. je zegt zoveel extra stroom te moeten betalen door een heel jaar heen.... deel je uitkomst door de helft. in de winter is de extra afgegeven warmte van je kast nuttig omdat je cv dan minder hard aan het werk moet. alleen in de zomer op je zolderkamer kan het een probleem worden. hoe gaat de vega zich gedragen bij een omgevingstemperatuur van 35 graden op je zolderkamer?? die mooie vega kaarten lijden heel veel door het enorme vermogen wat door die print heen moet en de daarbij vrijkomende hitte. iemand een youtube filmpje gezien van een vega 64 op 100 % load? maar we mogen niet vergeten dat zonder de innovatie van amd de prijzen van nvidia kaarten omhoog schieten. een monopolist kan vragen wat ie wil. amd blijft een prima competitie in de midrange - markt. ik vind de vormgeving van de vega ronduit prachtig te noemen. laten we wel wezen.
RX Vega 64;1;0.594199538230896;"Vers van de pers: Prijs AMD Vega 64 wordt mogelijk 100 euro duurder. Prijs at launch = ""introductie prijs"". En dan hebben we het over de stand alone black editie zonder extra's. Sources: Als dit waar is dan ben ik persoonlijk erg teleurgesteld, vind het ook erg misleidend om kaarten te laten reviewen en dan een maand later de prijs omhoog te gooien als alle reviews al gemaakt zijn."
RX Vega 64;1;0.35413527488708496;Ik las dus ergens dat het niet eens een maand was maar zelfs alleen het release weekend
RX Vega 64;3;0.39682066440582275;Ik weet niet of die kortings actie die AMD bij de aankondiging noemde in NL ook werkt, maar dat zou het enige interessante scenario zijn, als je een Ryzen systeem wilde bouwen en toevallig een Ultrawide monitor ging kopen. Dan is het een mooie aanbieding, anders niet IMO.
RX Vega 64;2;0.5759281516075134;"1. Niet geldig in Europa: 2. En zo mooi is de actie niet. Je moet meer betalen voor wat korting op producten die ze kwijt moeten. Ze bieden een 1800X in de bundels en dat is juist de minst hardlopende Ryzen 7. En het scherm is nu niet echt een geweldig Freesync scherm. Er zijn nogal wat issues mee; o.a. flikkeren van het scherm als je Freesync gebruikt. Het is overigens een Freesync 1 scherm en niet een of ander nieuw Freesync 2 scherm."
RX Vega 64;3;0.42659828066825867;Dan is het wachten op de volgende generatie van Nvidia. AMD zal wel wat meer budget voor R&D hebben voor de opvolger. ASSP is goed en alles wordt verkocht voor het GPU segment en CPU's verkopen ze ook weer sinds kort...
RX Vega 64;2;0.45270049571990967;Niet als ik al iets vergelijkbaar heb.
RX Vega 64;2;0.3903331756591797;^ Goed punt. Mogelijk cruciaal punt zelfs. Maar de informatie eromheen is erg onduidelijk, het lijkt er zelfs op dat AMD gewoon maling heeft aan Nederland. Hopelijk de komende dagen wat duidelijkheid erover, want gratis games en dikke kortingen op schermen zijn toch sterke argumenten.
RX Vega 64;3;0.44023409485816956;Ondanks dat ik waarschijnlijk het antwoord al wel ongeveer weet ben ik wel benieuwd hoe deze kaarten presteren op het gebied van OpenCL rendering met bijv. Blender.
RX Vega 64;5;0.5475815534591675;Best goed
RX Vega 64;3;0.6375735998153687;Dat is inderdaad niet slecht. Zo te zien valt hier wel een mooie renderbak van te maken zolang je er maar een flinke PSU in zet.
RX Vega 64;2;0.3995489776134491;En dat is exact het probleem met de arch van AMD. GCN is gemaakt voor parallellisme wat je dus nu in die grafiek uitstekend goed kunt zien. Als het komt op games is het een compleet ander plaatje. Zelfs de Fury X is sneller dan de 1080Ti terwijl die kaart momenteel weer op games vlak goed scoord. Ik heb je foto even gejat ter verduidelijking van mijn uitleg op Als ik de review zo lees is het geen slechte kaart. Het presteert, weliswaar tegen een ietwat hogere wattage, maar niet iedereen MOET 144FPS hebben of MOET de laagste latency hebben. De games die ik speel (sporadisch) heb ik helemaal geen fikse eisen verder voor. Ik vindt het wel passen in mijn straks te bouwen HEDT systeem omdat ik vaak met video's ook werk. De Vega maakt gehakt van Nvidia in dat opzicht. De bruteforce power die die kaarten brengen is gewoon ongelofelijk hoog. Check de mining scores maar eens.
RX Vega 64;2;0.38948532938957214;De enige goede keuze is eigenlijk de RX vega nano want de rest ga je nooit terugverdienen. vega 64 liquid heeft verbruik van 350 watt maar de 1080 TI verbruikt 50 watt minder en is krachtiger.
RX Vega 64;2;0.4648289382457733;Maak daar maar 100 watt minder van. En ik lees hier in posts dat de liquid versie zelfs ruim 400 watt trok wanneer overclocked. En dan sporadisch in de buurt komen van de 1080ti. Er komen kleine versie's van deze kaarten iig ja, ik zag dat de PCB van de vega 56 maar de helft van de kaart bezet. De rest is voor het koelblok! Wat ook weer een hoop zegt natuurlijk. De nano versie zal dus behoorlijk lager geklokt zijn ook nog eens. En dan is een 1070 mini toch een stuk aantrekkelijker.
RX Vega 64;3;0.32878953218460083;Nou nee als TDP geen probleem vind dan klok je over. Ik draai liever op stock. Hoge TDP kan ool vetekenen dat fabrikant klok te hoog gekozen heeft. Dan klok je wat onder beetje Vcore minder en je draaid het als een nano.
RX Vega 64;3;0.5962677001953125;De linux prestaties zijn volgens phoronix wel goed. Met als interrant weetje dat de open-source opengl driver sneller is dan de closed-source. Daarbij zouden nog optimalisaties komen die het nog beter maakt.
RX Vega 64;2;0.4491075575351715;"Je moet wat duidelijker zijn want beide Mesa en AMDGPU (van AMD) zijn open-source. Er is geen ""de open-source driver"" aangezien AMD drivers nu ook in de kernel zitten. AMDGPU-PRO drivers zijn wel closed source maar zelfs AMD raadt het niet aan om die te gebruiken als je er geen specifieke nut voor hebt."
RX Vega 64;3;0.44540154933929443;Jammer dat het toch niet de kaart is geworden zoals ik had verwacht, de grootste teleurstelling zit hem toch in het stroom verbruik, De AMD Radeon RX Vega 64 8GB gebruikt bijna 50 Watt meer dat GTX1080 TI. Prestaties zijn opzich wel op orde, vind ik zelf dan. ik ben erg benieuwd hoe de prestatie per watt gaat zijn voor de refresh van deze architectuur. Zijn ze net terug op CPU gebied, lijken ze het een beetje te laten vallen op GPU gebied
RX Vega 64;2;0.39242202043533325;is 50W nu echt zo doorslag gevend? Na 20 uur kost het je 0.20 cent meer. Ik vind dat nog wel meevallen. AMD is de laatsten jaren op GPU gebied minder zuinig geweest. Voornamelijk in het High segment heeft AMD gewoon niet de middelen om zuinig chips te ontwikkelen en bakken. Maar deze zou wel erg interessant zijn voor miners. En dus met een beetje geluk komt de rx 570/rx580 weer een beetje op prijs. Wat wel weer goed is voor de consument.
RX Vega 64;3;0.3833271265029907;Wel als je je realiseert dat het appels met peren vergelijken is als je 'em vergelijkt met een GTX1080 Ti. Het daadwerkelijke verhaal is dat hij 120 watt meer stroom trekt dan een GTX1080 en niet beter presteert. Het gaat niet alleen om de kilowatturen, maar bij zulke verschillen komen ook een aantal andere zaken de hoek om kom kijken: de warmte die afgevoerd moet worden, de herrie tijdens gamen (60 dB is aardig wat), en de voeding die weer een stuk sterker moet zijn. Bovendien is nVidia's refresh ook niet heel ver weg meer, en daar mag je van verwachten dat de performance per watt weer gaat verbeteren. Onder de streep geef ik dan toch wederom liever mijn geld uit aan kamp groen.
RX Vega 64;2;0.4080345332622528;"De warmte en herrie is ook weer afhankelijk van de gebruikte koeler. AMD heeft niet voor niks een watergekoelde Vega 64 beschikbaar, die je qua adviesprijs voor hetzelfde geld haalt als een luchtgekoelde GTX 1080. Een budget-versie van een GTX 1080 met blower-koeler (laat staan de 1080Ti, die bestaan ook...) tikt net zo goed ongezonde geluidswaarden aan. Daarbij verwacht ik dat er heel veel gamers nog een flinke voeding hebben overgehouden aan de GTX400-serie of de R9 200-serie die ze ook nog hebben liggen. Terwijl CPU's alleen maar zuiniger worden. Een Vega 64 + een i7 op Z270 of een Ryzen 5/7 CPU moet prima lukken op een 500w tot 600w voeding van een fatsoenlijk merk, en dat is echt niks bijzonders. Waarop baseer je dat Nvidia's refresh dichtbij is? Er zijn geruchten geweest over een Pascal refresh, maar daar hoor je helemaal niks meer over (terwijl Nvidia normaliter wel wat lekt tijdens releases van de concurrent). En waarom zou Nvidia ook? De marges op Pascal zijn gigantisch; ze gaan denk ik liever met de prijs dalen dan een nieuw product ontwikkelen en lanceren. Sowieso, wat zou een refresh voor de consument opleveren? En Volta gaat echt niet eerder komen dan 2018, daar hoeven we ook niet bang voor te zijn."
RX Vega 64;1;0.3667930066585541;"Op de doos van de Vega 64 LC staat ""Minimum 1000W PSU"". Deze heeft ook 2x 8-pin power connector. 0:22"
RX Vega 64;1;0.40280935168266296;Op de doos van een GTX1070 staat ook dat je een 600w voeding nodig hebt terwijl het vaak wel lukt met een 350watter. Die getallen zijn enorm overdreven voor het geval iemand m aan een Trust voeding hangt.
RX Vega 64;2;0.34234195947647095;Een kwaliteitsvoeding zou er totaal geen problemen mee mogen hebben op 80% van het opgegeven vermogen te draaien: Het verschil in efficiëntie tussen 50% en 100% load is volgens de certificering slechts 3%. Op 600W zou dat dus een rendementsverlies van 18W betekenen. In jouw geval lijkt me dan dat onder normale omstandigheden (lees: alles behalve Furmark) een goede 500W voldoende zou moeten zijn, en 600W al een veilige marge heeft.
RX Vega 64;1;0.5435614585876465;Het gros van de geklommen marge's ivm mining-trend gaat naar de retailer, niet Nvidia zelf.
RX Vega 64;1;0.7252950072288513;Dat klopt. En dan nog weet Nvidia met bizar dikke marges weg te komen. Reken zelf maar eens met de kwartaalcijfers van Nvidia.
RX Vega 64;2;0.26766616106033325;Dat komt niet alleen door het succes van hun GPU's. *edit, hoewel dit natuurlijk hun brood blijft. Zet de koers maar eens op 5 jaar of max, bijna 8 keer in waarden gestegen de afgelopen 2 jaar. Heeft te maken met het uitbreiden van hun bedrijfsportfolio naar *mobiele chips, deep learning, chips voor auto's/autonoom rijden ect. Die zijn voorlopig nog niet klaar met groeien. AMD had zich op CPU's moeten richten, daar al in energie in stoppen. Laat de gpu's lekker gaan zou ik zeggen. Waar Intel schijnbaar concurrentie nodig heeft omdat ze anders lui worden, lijkt Nvidia vrolijk door te ontwikkelen. De gtx1000 serie is een gigantische vooruitgang ten opzichte van de vorige generatie, terwijl er amper concurrentie was. Nvidia heeft letterlijk volledig 'schijt' aan AMD. De verhoogde prijzen van de gtx1000 serie destijds, blijken nu kijkend naar vega, gerechtvaardigd te zijn. Want AMD heeft ongetwijfeld deze kaarten voor de laagste prijs mogelijk in de markt gezet. Ik werk uiteraard niet bij Nvidia, maar ik vermoed dat ze gekozen hebben voor juist een redelijk prijs en een véél groter marktaandeel (en dus meer verkochte kaarten tegen wat kleinere marge's). Tevens kijkend naar de 'performance' toename ten opzichte van vorige generatie. Aangezien een gtx1070 de 980ti met gemak verslaat met een veel lager verbruik.
RX Vega 64;1;0.4622740149497986;Het gros komt echt wel van de GPU's. Er zijn gedetailleerde cijfers te vinden daarover. Zoals je zelf al zegt: het brood komt echt van de GPU's af. Juist door die enorme winstmarges waar Nvidia mee werkt als praktisch monopolist hebben ze ook de ruimte om enorm te investeren in die groeimarkten. Dat maakt Nvidia een ontzettend welvarend bedrijf. AMD zou RTG inderdaad goed kunnen laten vallen. Door gebrek aan investeringsruimte is het zo goed als onmogelijk om Nvidia bij te benen, zeker als je gokt op het verkeerde HBM2-paard. Al moet je er als consument toch niet aan denken dat AMD dat zou doen... Dat zou echt een regelrechte ramp zijn voor videokaart prijzen. Edit: wij verschillen totaal van mening als we het hebben over hoe 'gigantisch innovatief' Pascal was (namelijk, Maxwell op 14nm, that's it) en hoe gerechtvaardigd de bijbehorende prijsstijging was (de consument betaalt gewoon net zoveel voor dezelfde prestaties. Enkel de Ti- en Titan kaarten kan je nu qua specs makkelijk inhalen voor minder geld. Het gros van de consument koopt in €200-€300 segment en die krijgen nu alleen een lager energieverbruik). 14nm zorgt voor een dermate kleinere oppervlakte van de GPU dat de kostprijs voor Nvidia fors lager ligt. Zie kwartaalcijfers. In plaats daarvan heeft Nvidia de prijzen verhoogd. De marges zijn niet verkleind maar fors verhoogd. En tuurlijk is dat super logisch vanuit Nvidia gezien, dat kan niemand ze kwalijk nemen. Maar als je stelt dat Nvidia geen concurrentie nodig heeft omdat ze toch wel 'doorontwikkelen' (Pascal refresh says hi), dan sta je heel ver van de industrie af en heb je geen idee waar we naar toe gaan.
RX Vega 64;3;0.3060751259326935;Ik vind eerlijk gezegd de stap van volwaardige desktop-GPU's naar laptops, met behoud van prestaties (toegegeven, enigszins afhankelijk van de implementatie), een van de grootste ontwikkelingen (lichtelijk overdreven zelfs revolutie) op de GPU-markt van de laatste jaren: voor het eerst is 1080p met alle toeters en bellen geen enkel obstakel meer voor een laptop (vanaf GTX1060), en zelfs 4K gaming is met terugdraaien van enkele instellingen prima mogelijk, ook voor actuele titels (vanaf GTX1070). OP desktop-gebied hoeft nVidia niet per se, aangezien de concurrentie momenteel ver achterloopt. Het feit dat ze zich op de mobiele markt hebben georienteerd geeft in mijn optiek toch zeker wel aan de nVidia een bedrijf is dat volop doorontwikkeld - de focus ligt alleen wat anders.
RX Vega 64;2;0.2316981852054596;Dat is volledig te danken aan de transitie naar 14nm, gecombineerd met hoe zuinig Maxwell standaard al is. Polaris zou ook prima in laptops kunnen eigelijk. Tuurlijk innoveert Nvidia wel. Het punt is dat AMD dat niet kan door gebrek aan geld. Het is een neerwaartse spiraal voor AMD als je Nvidia in die monopoliepositie houdt.
RX Vega 64;2;0.527820348739624;Amd heeft anders die stap niet laten zien naar 14nm. Om het alleen daar op te gooien is naief. En polaris vreet te veel voor wat het biedt in een laptop.
RX Vega 64;3;0.3127771317958832;Het probleem met amd hun ontwerp en videokaarten is dat Nvidia 2 snelwegen gebruikt met ieder 300kmph, en amd 4 snelwegen met ieder een maximum van 75kmph. Nvidia is hierdoor in de regel sneller en gebruikt veel minder brandstof. Echter op flinke workloads zoals bijv te zien is met video encoderen en alles wat baat heeft bij de gcn architectuur is amd weer heer en meester. Opzich geen verkeerde kaart, de prijs lijkt ook goed. Drivers zullen uitblijken of er nog winst te behalen is of niet.
RX Vega 64;2;0.46305373311042786;"Dit slaat nergens op Op welke snelwegen doel je? Het is namelijk overduidelijk dat AMD meer bandbreedte biedt over een bredere geheugenbus, mocht je daarop doelen... AMD biedt al jaren meer theoretische performance dan Nvidia op hetzelfde prijsniveau. Dat zie je in statische benchmarks en coin mining. AMD kaarten hebben al jaren meer tflop's door het gebruik van een grotere chip met meer shaders, waardoor AMD kaarten ook duurder zijn om te maken (Nvidia draait gigantische marges, bij Vega vraagt men zich af of AMD überhaupt verdient aan de kaart) en natuurlijk meer energieverbruik. Waar het verschil vandaan komt is eerder de softwarekant; Nvidia's architectuur wordt beter gebruikt dan die van AMD zo lijkt het. Wat natuurlijk een vicieuze cirkel is: meer Nvidia kaarten in de wereld = betere optimalisatie voor Nvidia = meer Nvidia kaarten in de wereld."
RX Vega 64;2;0.5218390822410583;Ja dat schrijf ik ook. De arch van AMD maakt het harstikke leuk in intensieve berekeningen zoals mining, video encoderen en noem maar op, maar het legt het af tegen het ontwerp van Nvidia. Nvidia daarintegen scoort weer slechter in juist intensieve berekeningen. Zie m'n eerdere bericht. AMD is goed in het bieden van een 4 way snelweg met daarop iedere baan 75kmph. Die van Nvdidia biedt echter maar 2 way met een maximum van 150kmph. Hierdoor is Nvidia met kleinere chunks van data sneller dan die van AMD. Echter veranderd dat plaatje zodra er grote datasets worden verwerkt.
RX Vega 64;3;0.5051873922348022;Maar wat als de AMD dadelijk 450 euro kost? Met 40 uur in de week kom je (met een kWh-prijs van 30 ct) uit op 12,48 euro per jaar meer. Ik denk dat je de aanschafprijs dus zeker in ogenschouw moet nemen. Je hebt dan wat mindere prestaties, en een hoger stroomverbruik dan een 1080, maar ook een lagere aanschafprijs, dat kan een drempel zijn.
RX Vega 64;1;0.7002155184745789;120 watt, oftewel 0,12 kilowatt, a 40 uur per week, 52 weken, tegen 30 cent per KWh... Dat is dus 0,12 * 40 * 52 * 0,3 is zo'n 75 euro. Per jaar... Of verreken ik me hier nou ergens gigantisch?
RX Vega 64;1;0.38307175040245056;30 cent is dan wel een dure leverancier betaal hier bij eneco al een stuk minder 17cent per Kwh
RX Vega 64;2;0.5017858147621155;Dat idee had ik ook al, maar heb de energieprijzen niet echt meer gevolgd dus gewoon gebruikt wat hierboven gezegd werd. Dan is het echter nog steeds een significant bedrag als je redelijk wat gamet dat je eigenlijk wel mee moet nemen als je de prijzen van de kaarten gaat vergelijken. En dat dan los van de hogere lawaaiproductie, of straks bij custom coolers (die niet de warmte naar buiten blazen) dat die warmte ook allemaal je kast in gedumpt wordt waardoor je potentieel op andere vlakken ook moet investeren in betere koeling.
RX Vega 64;2;0.35147857666015625;Dat verbruik -kan- de doorslag geven. Even bij @A Lurker aansluiten dat je appels en peren aan het vergelijken bent, maar we doen even een vlot rekensommetje op basis van de ~100W die ik tussen de Vega en de GTX 1080 zie: Vuistregelje, 100W bij 24/7 is ~200 euro, uitgaande dat je gewoon een normaal particulier tarief betaald voor je stroom (en we gaan even niet mekkeren over een paar procent, we rekenen makkelijk). Speel je gemiddeld elke dag een uur games, dan kost het je grofweg een tientje meer per jaar. In dat geval zijn de pro-Vega argumenten, eigenlijk vooral Freesync, meer dan genoeg. Game je elke dag tig uur.. tja. Het is niet gezond, maar er zijn genoeg gamers die dagelijks 8 uur aan het gamen zijn. Of je GPU dan full load draait terzijde, en AMD kan met hun Radeon Chill oplossing om de FPS te beperken wmb ophoepelen als argument, maar het is niet ondenkbaar dat er gamers zijn die dan letterlijk richting de 100 eur per jaar extra aftikken op hun stroomrekening. Nogmaals: gezond nee, maar het gebeurd wel. Ik heb ook een Freesync scherm, en met een paar uurtjes in de week (als het mee zit), is het verbruik voor mij geen zwaar wegend argument. Maar dat neemt niet weg dat het wel -het- blok aan het been van Vega is.
RX Vega 64;1;0.6218954920768738;"Ik ging even snel van die 50w die vertelt werd... dat was stom... 100 veranderd de boel al. En doet de ""gratis"" freesync teniet helaas."
RX Vega 64;3;0.291875422000885;Nou ja, dat hangt dus af van hoe veel games je speelt. Als je echt uren maakt, jazeker. Ik ben de laatste 3 jaar blij als ik 5 uur in de week haal
RX Vega 64;2;0.37244680523872375;ja 5 uur per week is niet veel.
RX Vega 64;3;0.562999963760376;Wel interessant onderzoek: hoeveel uren gamed de gemiddelde Tweaker nu echt? Ik denk dat ik geen uitzondering ben. Toegegeven: In mn school- en studietijd was het aantal uurtjes ieeeeetsjes anders.
RX Vega 64;1;0.4109719693660736;laten we maar niet doen, ben bang dat het heel confronterend word
RX Vega 64;3;0.3457232117652893;Ik schrok wel eens in games waarin je /played kon doen
RX Vega 64;1;0.6569101214408875;ik schrok pas toen ik op steam keek hoeveel speeltijd ik had in rift. dat is belachelijk. en ik speel het al heel tijdje niet meer via steam zelfs wist ook niet dat het zo hard kon oplopen qua speeltijd
RX Vega 64;5;0.4294995069503784;Ik speel online op de PS4. PC exclusives en singleplayer op PC. Online games nemen de meeste uren. Voor mij is Vega 56 niet een probleem.
RX Vega 64;2;0.38214147090911865;Dat zal van de leeftijd afhangen, toen ik nog een schoolkiddo was haalde ik uren per dag... Nu ben ik inderdaad ook blij als ik een avond even een spel kan opstarten. Echter is mijn beschikbare budget ook veranderd, waar ik destijds blij mocht zijn als ik na veel sparen een X700 Pro kon kopen (voor heul veul geld, en het ding was toen al wat ouder) kan ik nu als ik dat wil morgen de Vega betalen.
RX Vega 64;1;0.5105271339416504;Precies dat, plus de vraag of je er toen minder plezier van had dat je niet het duurste had maar af en toe genoegen moest nemen met wat minder wellicht. Enorm luxeprobleem nu met hardware, maar de tijd... PUBG gekregen van de dame een week of 2 terug, welgeteld 2 potjes kunnen spelen :-/ Komt AMD weer met Vega kaartje aanzetten enzo he, heel vervelend.
RX Vega 64;1;0.2859456539154053;Toen ik nog schoolkiddo was hadden we Pong...
RX Vega 64;5;0.2522166669368744;Ik zat met mijn famicom
RX Vega 64;3;0.41604408621788025;Volgens mij kan je een grafiek maken met de hoeveelheid funds tot je beschikking, en de hoeveelheid tijd die je hebt.... Hoe meer tijd, hoe minder funds. Hoe meer funds, des te minder tijd. Ik denk dat de Tweakers met een dikke 1080ti en een 16-core CPU juist degene zijn die het niet aan tijd hebben om 8 uur per dag te gamen. En ik denk dat de Tweakers die 8 uur per dag gamen, het toch moeten doen met een i5 + GTX 1060/1070.... En als je dan kijkt, dan maakt het stroomverbruik een stuk minder uit... Misschien zit ik er naast. Maar als ik naar mezelf kijk... Mijn PC is krachtiger dan ooit (en duurdere, meer high-end componenten) maar ik heb ook minder tijd dan ooit. 5 uur per week is hier ook een luxe
RX Vega 64;2;0.5314415097236633;Je vergeet de bespaarde kosten van de verwarming eraf te trekken, als je daadwerkelijk meerdere uren per dag speelt kan de verwarming sowieso een graden lager. Scheelt ook in de kosten Voor de casual gamer die een uurtje per dag speelt niet interessant maar de die hard 5+ uur per dag gamer kan het een verschil maken.
RX Vega 64;3;0.41730985045433044;AMD zou ook hier eens serieus op moeten focussen met hun nieuwe architectuur naar mijn mening. Het verschil wordt toch wel pijnlijk als je kijkt naar de huidige generatie GPU's van het groene kamp. *edit* De vergelijking is performance per Watt.
RX Vega 64;5;0.28707942366600037;"Dit maal ook letterlijk het ""groene"" kamp"
RX Vega 64;2;0.42164987325668335;Je bedoelt de NCU architectuur die ze nu gebruiken? Die is nieuw en support meer DX12 features dan Nvidia's Pascal. Het verschil zit 'm meer in softwareoptimalisatie dan in dat de architectuur van AMD slechter zou zijn, ofzoiets...
RX Vega 64;3;0.4166332185268402;Ik bedoel dat AMD rekening had moeten houden met stroomconsumptie met de Vega architectuur. Hopelijk doen ze dit wel met de volgende architectuur. Ik doel op performance per Watt, als je puur en alleen naar performance kijkt dan zijn het gewoon goede kaarten.
RX Vega 64;2;0.3577190637588501;Rekening houden? Je doet nu net alsof het een soort 'keuze' is Je moet bedenken dat door de huidige marktverdeling AMD amper geld heeft voor R&D. Nvidia investeert veel meer en maakt daardoor grotere stappen. AMD heeft simpelweg niet het budget om én zuiniger te zijn én sneller. En als je dan als AMD zijnde dan toch een keuze zou moeten maken tussen specs en performance per watt (waar de gemiddelde gamer die 10 uur per week speelt in de praktijk niks van merkt) dan snap ik heel goed dat ze voor specs kiezen.
RX Vega 64;4;0.4302313029766083;Of ze moeten Net als Ryzen even terug naar de tekentafel. En eens goed kijken hoe ze de High end GPU markt kunnen veroveren. Met CPU is het ook gelukt. Al heeft het ze wel 5 jaar gekost. Maar uiteindelijk is het ze gelukt.
RX Vega 64;2;0.3887574374675751;Met cpu is het AMD gelukt om de markt te veroveren...? Ryzen is nét uit en de enige reden dat ze zoveel hype hebben gecreëerd is omdat Intel sinds Sandy Bridge (2011!!) geen concurrentie van AMD heeft gehad en dus op deze manier 6 jaar lang heeft kunnen melken. Kijk maar eens naar de single core performance tussen een gelijkgeklokte Sandy Bridge (2011) en een Kaby Lake (2017). Het enige dat Intel hoeft te doen is elke serie iets zuiniger maken en iets hoger te klokken. AMD heeft in 6 jaar geen fatsoenlijk antwoord gehad en heeft het op single core performance nog steeds niet, dus gooien ze het over de multi core boeg. Helaas voor AMD heeft Intel daar een eenvoudig antwoord op: Coffee Lake i5 en i7 zijn hexacores en de i3 worden quadcores. Begrijp me niet verkeerd, ik wil niets liever dan AMD weer terug in de strijd bij cpu en gpu, maar zoals AMD de afgelopen 5 jaar is bezig geweest duurt het nog minstens 5 jaar voordat ze weer gezond mee kunnen strijden, tenzij ze natuurlijk in die 5 jaar weer stomme fouten gaan maken.
RX Vega 64;2;0.4895821809768677;Ik doelde meer dat Ryzen een goede concurrent is voor Kaby lake. En uiteraard is 5 jaar zonder zitten geen pretje. Maar door gaan met zulke power hunger GPU's is ook geen oplossing.
RX Vega 64;2;0.41039803624153137;Het probleem is geluid ... Als iemand dat een AMD 290X had en licht gedowngrade heeft naar een GTX 970, puur en gewoon wegens het geluid. Je hoorde de 290X veel meer omspinnen en op hogere toeren draaien, tot vervelend toe. Dat was ook mijn vrees doen de eerste Vega FF resultaten bekend werden op gebied van stroom verbruik. Ja, men zal er een aftermarkt cooler kunnen op smijten dat stiller is maar als we dezelfde cooler op bijvoorbeeld: 56 vs 1070. Welke zal meer lawaai maken? Die dat het meeste verbruikt. Je gebruik 50W als voorbeeld en ja, dat klinkt niet zoveel. Maar je vergelijk de TI en niet de 1080, waar de 64 tegen concurreert. Maar als je het in percentages uitdrukt: 1070 vs 56: 51% meer stroomverbruik bij AMD ... 1080 vs 64: 66% meer stroomverbruik bij AMD ... Dat betekend dat je GPU cooler, 51 tot 66% meer warmte moet verplaatsen. Aangezien beide dezelfde GPU coolers zullen gebruiken ( er is geen magische technologie dat AMD of Nvidia krijgt ), betekend het dat de coolers op de AMD kaarten meer wind ( = geluid ) moeten generen voor te koelen. Sommige mensen hebben geen last van fans met kop telefoons op maar niet alles dat je doet is gaming, wat je GPU kan doen opwarmen En dan nog het feit dat deze kaarten met Nvidia hun 1 jaar oude oplossing concurreren. Ik zie gerust een 1070Ti op 1 van deze dagen
RX Vega 64;3;0.26983121037483215;Als je kijkt wat sommige koelers met een Ti doen hoeft dat zeker geen issue te zijn. Neemt natuurlijk niet weg dat het niet ideaal is, en eerst zien, dan geloven
RX Vega 64;3;0.4573417007923126;Ik kwam er ook achter dat het niet 50w was maar 120w. Lang leven lui zijn en uitgaan van andere... en ja geluid productie kan een issue zijn. Maar voor de meeste after marker cooler zal het geen probleem zijn. Mijn r9 290 doet kwa geluid productie niet echt onder van de MSI 970 die mijn vrouw heeft. Ja in de Zomer is de extra wat niet fijn nee. In de winkel scheelt het wel in stook kosten😀. Nu maar hopen dat ze met drivers iets extra uit krijgen. Of dat miners er zo blij mee zijn dat ze de 570/580/1070/1080 links laten liggen. En er voor ieder wat te koop is.
RX Vega 64;4;0.4400290250778198;Ook geen probleem voor mij al ben ik wel benieuwd. PS4 en PC in de woonkamer dus er zit vaak iemand TV te kijken. PC Geluid staat voor gamen zacht.
RX Vega 64;3;0.4576805531978607;Voor miners niet echt gezien de niet overweldigende hashrates en het hoge wattage wat gezien de Nederlandse prijzen een aardige impact heeft op je winstgevendheid. Al zullen er genoeg miners zijn die wel een aantal van deze kaarten laten draaien doordat de GTX1070 amper te verkrijgen is. Hiernaast zijn er genoeg landen waar stroom amper iets kost en zullen de kaarten hoe dan ook winstgevend zijn. zie de review bij de buren:
RX Vega 64;2;0.5624951124191284;Dat is te simpel gesteld. Vega is zo'n stroomvreter omdat de GPU op hoge clocks word gezet zodat de 1070/1080 bij te benen is. Maar als je gaat minen zal je die kaart flink undervolten en de GPU enorm downclocken. Praktijktests hebben we nog niet gezien, maar in theorie is HBM2 (door veel minder, kleinere chips met minder afstand tot de GPU) zuiniger dan GDDR5. Tot slot lijkt de hashrate van Ethereum juist flink hoog te liggen - dit is vergelijkbaar met twee GTX 1070's voor Vega 64. Mocht je Vega 64 voor de adviesprijs kunnen krijgen, dan is dat een prima deal... HWI kijkt nu enkel naar Nicehash en concludeert niet voor niks dat hier softwarematig nog winsten te halen zouden zijn.
RX Vega 64;1;0.49779197573661804;De nederlandse prijzen van stroom hebben weinig invloed want die kaarten worden al verkocht voordat ze uberhaupt Europa binnen komen.
RX Vega 64;2;0.5510191917419434;"Ik denk dat je naar de totale package moet kijken, 50W in een vacuum is natuurlijk niet doorslaggevend. Maar 50W meer verbruik en ook nog +- 30% mindere prestaties is gewoon niet goed, een GTX 1080 Ti tegenhanger zal het iig niet worden (maar werd eigenlijk gezien de geruchten ook niet meer verwacht). Net als 120W meer verbruik en +- gelijke prestaties en nog eens een jaar ""te laat"" op het feestje komen is ook gewoon niet goed t.o.v. de GTX1080. Als je een jaar te laat bent moet je in ieder geval zorgen dat je een flinke klap sneller bent, dan zal er namelijk door de meesten ook niet naar stroomverbruik gekeken worden. Prijs en FreeSync v.s. G-Sync kan hier echter nog wel wat doen. Ik heb nu een 10tal reviews doorgenomen sinds 15:00 en de Vega RX64 lijkt gewoon een hard sell te worden, tenzij toekomstige verbeteringen de performance nog flink weten op te krikken. De Vega RX56 lijkt echter een stuk beter voor de dag te komen en mits verkrijgbaar (dit schijnt mogelijk nogal een probleem te gaan worden) een aardige kaart te gaan worden in zijn prijssegment t.o.v. de GTX1070"
RX Vega 64;1;0.457448273897171;"Sorry, maar AMD had een tijdje terug met de ""Cyprus?"" volgens mij een zuiniger ontwerp dan Nvidia, met de GTX280 ofzo..."
RX Vega 64;3;0.47223857045173645;Mogelijk is die tijd wel ja. Bedoelde meer afgelopen jaren. Zal het even aanpassen.
RX Vega 64;1;0.7908822298049927;Ik vind het meer uit principe een zware teleurstelling. Eén jaar en drie maanden na de 1080 komt AMD met een kaart die de 1080 nauwelijks kan bijbenen en ook nog eens meer verbruikt om dat magere resultaat te halen. Zoveel hype, zo'n enorme teleurstelling.
RX Vega 64;3;0.43615347146987915;Er komen nog driver improvements aan dus ik denk dat vega zeker nog wat beter gaat worden.
RX Vega 64;3;0.6623430848121643;De performance is in lijn van verwachting en optimalizaties zullen zeker nog positief effect hebben echter verwacht ik niet dat ze de 1080 TI van de troon kunnen stoten. Het zijn ook gewoon goede kaarten echter is het stroomverbruik gewoon hoog.
RX Vega 64;3;0.5290748476982117;Al is dat wel mogelijkheid. Drivers hebben nogal tijd nodig dus 3 a 7 drivers verder. NV haald uiteraad nu tegen maximale eruit om de release benches van Vega onderuit te halen. Voordeel van extra refresh die al langere tijd uit is. Deze vers architectuur anders dus kan het ook wat extremer zijn.
RX Vega 64;4;0.5221655368804932;Precies. Een 290x doet het hedendaags nog erg goed. Dat is bij de tegenhanger van nvidia van die tijd wel anders.
RX Vega 64;5;0.5943765640258789;50 watt meer dan een 1080Ti die ~30% sneller is. 120 watt meer dan een GTX1080 welke grofweg gelijk presteert. Je kunt 2 GTX1070's in SLI laten draaien op de powerconsumption van een RX64.
RX Vega 64;1;0.49660101532936096;Wilde net de andere der bij zetten maar je was me voor. Het is inderdaad een bittere pil die stroomconsumptie
RX Vega 64;2;0.46977153420448303;Mwah. Ik zie iets andere waarden: 1080 premium lucht: 188W gemiddeld, 235W top. (2X8p) 1080 premium water: 216W voor 2089mhz. (*Dit is sneller dan de Vega 64 op 1645mhz.) 1080 Ti middeklasser lucht: 246w gemiddeld (lastig om em echt >98% te krijgen) 298w top. 1080 Ti water: 267W gemiddeld (2030mhz) 312W top. (enkel in Synth) - hoewel na veel optimalisatie. Vega64 stock, aftermarket kaart: 288W peak 295W Vega64 OC: 312W, peak 345W (1645mhz) Hij gaat dus 100W over het verbruik van de 1080 (2e versie) héén, voor 5-8% tragere results buiten een aantal titels om. Hij gaat dus nog over een niet-geoptimaliseerde 1080Ti OC heen, voor 30% minder performance. Maar veel 1080Ti's hebben wel ongeveer dezeflde peak power use als een stock Vega64. Maar daar concurreert deze kaart helemaal niet mee. In een systeem met forse air cooler, scheelt een Vega64 vs 1080 (1e versie) al snel 5 graden op de CPU in idle na 30 min runtime, bij matige acceleratie. In games is het ook een goede 3-10 graden warmer. Hitman 2016 spelen op Vega loopt het hele systeem echt een heel stuk heter...
RX Vega 64;2;0.36189454793930054;"Het valt me op dat AMD deze kaarten heel stilletjes gelanceerd hebben invergelijking met de rx470/80. Ze wisten goed genoeg dat deze kaart geen potten ging breken. Amd is niet volledig kansloos. Apple maakt trouwens gretig gebruik van midrange kaarten in hun Macbook pro 15"" en Imac's en Mac pro's. Dat zijn geen game stations maar ze sturen wel als enige 5K schermen aan, zijn stil en zuinig. Heel wat anders dan deze Vega. Nvidia verdient goed zijn boterham met de gtx 1060/70/80 mede dankzij de mining hype. Zolang dag doorgaat is het niet ondenkbaar dat ze lang gaan teren op deze architectuur. Ze hebben helemaal geen haast, zeker nu AMD niet meteen met een ijzersterk alternatief op de proppen komt. In CPU-land liggen de kaarten anders. Intel moet een stevig tandje bijsteken door extra cores toe te voegen aan de volgende 'generatie' i5 en i7's. Maar ook hier scheert AMD spijtig genoeg geen hoge toppen in de gamingmarkt omdat de meeste games geoptimaliseerd zijn voor intel al is het verschil daar wel veel kleiner en soms is AMD zelf zuiniger en ook goedkoper. Maar canon lake ligt op de loer waardoor AMD het weer en zwaar weer kan komen als ze niet in de intel-pace kunnen lopen. Besef dat intel de ontwikkelingskost al lang heeft kunnen spreiden. Al bij al de de Vega 56 geen slechte kaart. Ze verbruikt maar 40 a 70 watt meer dan de gtx1070 in game en de kans is groot dat ze de gtx 1070 met gemak kloppen wat prestatie/prijs betreft. Mede omdat Nvidia gebukt gaat onder de miners Hype. Dat Amd iets minder zuinig is kan voor de consument nog wel eens positief uitdraaien in de huidige marktevolutie."
RX Vega 64;1;0.6565204858779907;Helaas is de Vega een flop gebleken, kaart kwam een jaar te laat op de markt en dat stroomverbruik kan hedendaags gewoon echt niet meer. Met overklokken kom je tegen de 400W aan, bij de LC FE Vega bleek het zelfs 440W te zijn. Dit is eigenlijk een kaart die het op had moeten nemen tegen de grote Maxwells, nu blijkt dus de, meer dan een jaar, oude Pascal buiten bereik terwijl Volta al om de hoek staat.
RX Vega 64;2;0.44819778203964233;Vega is geen flop, maar de prijs wel, knap wat AMD gedaan heeft met zo weinig uitgaven aan r&d, maar het is niet voldoende gebleken. Was de prijs laag genoeg geweest was het een mooie kaart en meer keuze voor budget en midrange. Maar nu blijft nvidia dominant en de prijzen zullen niet snel veranderen en heeft nvidia weinig haast met volta (tel daarbij op de mining hype= inflatie prijs en late verkoop van vega in b.v. NL). Alle hoop is nu voor AMD fanatici op navi gericht, maar 7nm... intel heeft een budget waar amd van kan dromen(r&d budget intel is 8 miljard meer p/j dan AMD aan inkomsten genereert..) Ik zie voorlopig intel nog niet komen met 7nm(3 tot 4 jaar nog) en AMD denkt in 2018 navi op de markt te hebben met 7nm(global foundries dan)? Ik gok wederom op uitstel. Helaas maar het ziet er nu uit dat AMD uit de race is bij gpu's, gelukkig hebben we ryzen/threadripper en is die markt wel weer open, maar gpu markt blijft zo stilstaan. no bueno voor consument.
RX Vega 64;2;0.4086465537548065;Vega is wel een flop, als je de prestaties afmeet tegenover het stroomverbruik. De prijs daarbij kan alleen nog maar meer pijn doen.
RX Vega 64;2;0.46671217679977417;De prijs is zeer pijnlijk en dit is nog maar reference, boardpartner prijs schijnt nog hoger te liggen. Nog pijnlijker is de hashrates, deze zijn niet zoals de gefantaseerde 70/100MH/s maar gewoon wat te verwachten was op basis van GCN met hogere klock. Mining hype zal ook wel mee vallen.
RX Vega 64;3;0.5764896869659424;Doet wel pijn btw. Heb altijd Radeons gehad maar als ze op deze lijn blijven zitten, ga ik serieus nadenken om over te stappen. De RX470 doet het voorlopig nog goed hier, dus er is geen haast.
RX Vega 64;1;0.5869601368904114;Waarom kijk je altijd dan naar het groene kamp, het maakt mij niks uit, gewoon welke het beste is?
RX Vega 64;4;0.4198865294456482;Natuurlijk, altijd de beste ongeacht de kleur. Voor mij niet perse de snelste, maar die kaart met de betere prijs/prestatie verhouding. Echter ben ik nogal gewend geraakt aan AMD's Control Center en alleen daarom al valt de keus toch telkens op AMD. Als ze nu qua stroomverbruik ver achterblijven heb ik echter een heel goede reden erbij om wél over te stappen.
RX Vega 64;1;0.2821822762489319;Jij en velen met jouw, maar de schuld ligt bij AMD en hun marketing.
RX Vega 64;1;0.4416520297527313;Neen, je vergeet een heel belangrijk ding - STROOMVERBRUIK. Ook al zou de Vega exact op het niveau van een 1080Ti presteren en net zo veel kosten, dan was deze alsnog niet interessant geweest, behalve voor de echte AMD fanboys. Ik zie al professionele reviews voorbij komen waarbij de non-FE Vega LC tegen de 500W kruipt na overklokken. Dat is 2x een GTX1080 terwijl de kaart in de meeste gevallen langzamer is dan een enkele 1080 (en dan ook nog eens de 1080FE, wie koopt die nog? Een grof meerderheid van de 1080's zijn allemaal custom PCB's die stock tegen de 1950-2000MHz klokken).
RX Vega 64;2;0.32877078652381897;Nee dat vergeet ik niet, AMD kan het gewoon niet winnen van nvidia met efficiency, het verbruik ligt hoger, de kaart was geen flop geweest als de kaart deze prestaties had en verbruik, maar simpelweg goedkoper was. Jij had het over of de kaart een flop was, dat is hij inderdaad met deze prijs, niet of hij goed overclocked, als je graag een spelletje speelt en goede gpu wil is het vermogen niet zo ernstig. Mensen kijken als eerste naar de prijs, als prestaties gelijk zijn en een aantal ook naar vermogen. Ik kijk er ook naar en ik weet wat het verbruik is, niet alleen is de kaart aan de prijs, met zo'n vermogen mag je ook meer uitgeven aan een aanzienlijk sterkere voeding, wat de prijs nog meer omhoog gooit (en daarnaast betaal je ook meer aan energie kosten per jaar, wat je tco ook omhoog gooit). Als je flink wat achter je pc zit en gamed kan dat flink oplopen. (gem 4 uur per dag tav 1080, p/j 35€ meer, of als je seti@home/boinc gebruikt 24/7 aan is het 210€ meer per jaar, dat is na 3 jaar al meer dan de kaart heeft gekost. Met mijn verbruik kom ik per jaar op ~100€ meer per jaar, meestal doe ik 3 a 4 jaar met een kaart, is toch een aardige meerprijs dan)
RX Vega 64;1;0.7838942408561707;Waarom krijgt deze post een -1? Het gaat niet offtopic toch, ik probeer het weer naar 0 te krijgen iig.
RX Vega 64;1;0.4888496994972229;Ik zou het geen flop noemen naar idd een jaar te laat op de markt. Het is eigenlijk niets dat de markt pushed. De Ryzen deed iets wat vele niet mogelijk acht. Echte 8/16 CPUs voor een schappelijk prijs. Ja, AMD kon niet winnen op Single thread maar ze gaven wel iets interessant terug op een ander gebied. Dat is wat er ontbreekt bij de Vega ... Eindelijk terug concurrentie JA! Maar tegen een kost. Die verbruik cijfers zijn pijnlijk. En het geeft ook aan dat AMD in de toekomst nog altijd problemen gaat hebben met verbruik/hitte op de hogere markt segment. Je zal maar weinig mensen een Vega in een SSF zien steken ( of AMD moet die grondig downclocken ).
RX Vega 64;2;0.4653911888599396;Nou, als je na een hoop bombarie ruim een jaar na de release van de 1080 van Nvidia zelfs die kaart niet weet te verslaan met je topproduct en ook nog eens behoorlijk meer verstookt, mag je toch wel van een flop spreken lijkt me. Daar weegt zelfs een aantrekkelijke prijs niet tegenop, en afgaande op de schattingen gaat die prijs ook niet echt aantrekkelijk worden.
RX Vega 64;2;0.4709688425064087;Nou vriendelijk prijsje zou fijn zijn maar ja HBM2 en mining gooit wat roet in. Denk dat volgend jaar de driver toppie zullen zijn en performance sowieso bump krijgt.
RX Vega 64;3;0.47639012336730957;Mooie kaartjes. Alleen dat stroomverbruik, dat blijft toch wel een dingetje hoor. Nog steeds te twijfelen tussen 1070, vega 56 of zelfs wachten tot 20/11 refresh van Nvidia.
RX Vega 64;2;0.4875134527683258;Zou voor de AMD RX Vega 56 gaan als je alleen kijkt naar de snelheid, de AMD RX Vega 56 is sneller in meeste spellen, vooral in DX12 spellen, maar hij gebruikt wel 75w meer dan de Geforce GTX 1070. Wel een grote teleur stelling de AMD RX Vega 56 en vooral de RX Vega 64, na 20 maanden na dat de Geforce GTX 1080 uit kwam hij nog steeds iets langzamer in de meeste spellen, ik had dus gelijk dat hij bijna gelijk zou zijn qua snelheid als de Radeon Vega Frontier Edition, klopt ook wel ze hebben de zelfde specs, zelfde GPU en geheugen, erg jammer voor AMD, en erg triest dat de RX Vega 64 wel tot 130w meer gebruikt dan de Geforce GTX 1080. Maar de drivers zijn wel beter geoptimaliseerd nu met de AMD RX Vega 64, en het scheeld maar een paar fps in de meeste spellen, en en ook anders om is de AMD RX Vega 64 een paar fps sneller dan de Geforce GTX 1080. Op Guru3D testen ze een stuk meer DX12 spellen (7 stuks), en daarbij is de RX Vega 64 gelijk aan de Geforce GTX 1080, een paar fps (2 a 5 fps) sneller soms En Guru3D had geen problemen om de RX Vega 64 goed te laten draaien op GTA 5: Edit spelfouten.
RX Vega 64;2;0.43568259477615356;Ligt eraan wat je een teleurstelling noemt, ik zie het al een aantal keer voorbij komen. Wel heel makkelijk om de vinger op de slome plekken te leggen. Maar vergeet niet dat AMD een veel lagere omzet heeft, en heel veel minder aan GPU research te besteden heeft dan Nvidia. Het is een enorme prestatie van AMD om Nvidia uberhaupt bij te kunnen benen. Laat staan een sprint voor hun geld te geven.
RX Vega 64;3;0.4004303514957428;Ik verwachte geen wonder wel een wildcard wat Goflo vs TMSC zou kunnen betekenen. Dit is AMD GF vs nV TMSC. Allemaal op verschillende flavor 14nm. Dan kan geen wonderen verwachten. Daarnaast ondanks kleinere R&D innveert AMD wel. Nadeel van innoveren is dat je later televant wordt. Gezien kip en ei probleem. Wat ik mij ook afvraag als die architectuur zo drastisch op de schop is. Zou de DX12 & Vulkan developers guideline dan ook niet veranderen. Denk dat ik voor een Vega 56 ga. Misschien is Vega betere mach met iNtel gezien Optane SSD voor helemaal diep en geavanceerd te gaan met zware caching en streaming open werelden.
RX Vega 64;1;0.6020915508270264;"""Denk dat ik voor een Vega 56 ga."" Tjah, daar zou ik nu niet voor gaan in ieder geval. Custom versies met goede koeling zijn er nog niet. Wat krijg je nu dan voor je geld: - heel veel lawaai - heel veel warmteproductie - duur in gebruik door heel hoog energieverbruik - hogere CPU-overhead in DX11 titels i.v.m. Nvidia - AMD drivers in plaats van Nvidia-drivers. Vaak geen verbetering. AMD zet de ondersteuning op een laag pitje door bijvoorbeeld W8.1. maar helemaal niet meer te ondersteunen. Is geen geld voor. - wordt waarschijnlijk duur in aanschaf. In de Benelux is de kaart nog helemaal niet te krijgen. Zodra die wel beschikbaar wordt, misschien wel woekerprijzen. Wat heeft de concurrentie te bieden? Nou, bijvoorbeeld de MSI GeForce GTX 1070 GAMING X 8G. Wat krijg je dan? - Gemiddeld gezien ongeveer Vega 56 performance (dit is een custom 1070 met hogere clocks dan referentie) - óntzettend stil. Idle onhoorbaar. Load: vanaf 50 cm onder de 30 dB(A). - heel weinig warmteproductie. Valt gemakkelijk te koelen in vrijwel iedere behuizing - goedkoop in gebruik dankzij lage energieproductie - lagere CPU-overhead in DX11 titels i.v.m. AMD - Nvidia drivers: naar mijn mening zijn deze van erg goede kwaliteit. En bij Nvidia is er ook meer dan voldoende geld om bijvoorbeeld W8.1 te ondersteunen. Nvidia maakt daadwerkelijk winst (en veel!) en er is geen enkele reden om aan te nemen dat de drivers slechter gaan worden. Nvidia heeft een héél groot team werken aan driverontwikkeling. - de kaart is goed verkrijgbaar. In vrijwel alle shops op voorraad. Prijzen schommelen wel een beetje. Enkele dagen terug nog voor 499 euro op voorraad verkrijgbaar, nu enkele tientjes duurder. Wie weet volgende week voor 489."
RX Vega 64;2;0.3291064500808716;Ze testen allemaal basis 1070 kaartjes. deze zijn in de praktijk 400mhz langzamer dan de AIB versies. Oftewel in dit geval ben je alsnog beter af met een 1070 op prestatie gebied. De prijs kan het nog niet op beoordeeld worden. De 1070GTX zou oorspronkelijk ook 399 euro kosten. We weten allemaal hoe dat afgelopen is.
RX Vega 64;3;0.3222670257091522;ja dat is waar, je hebt een grote kans dat bitmining er voor gaat zorgen dat ze duur woorden of niet te kaap zien, maar misschien ook wel niet, omdat ze veel meer stroom gebruiken dan de Geforce GTX 1060 en 1070 kaarten.
RX Vega 64;2;0.4269033372402191;Helaas heeft overclockers UK al aangegeven dat alleen de eerste batch kaarten zo goedkoop was. Was maar een tijdelijke RRP, de nieuwe boards zullen duurder zijn. Helaas
RX Vega 64;1;0.5259864926338196;Ja klopt had ik gezien, erg triest, nou woorden ze waarschijnlijk zelfs duurder dan de goedkoopste Geforce GTX 1070 en 1080.
RX Vega 64;2;0.43599647283554077;Helaas voor amd, als ze tussen 400-450 zijn, zouden ze nog mss nog intressant zijn.
RX Vega 64;2;0.5057462453842163;"Naar mijn inzien niet, voor dat geld (zelf iets minder) heb je een 1070. Vega56 heeft vrijwel dezelfde prestaties tegen een veel hoger stroomverbruik. Vergeet niet dat die stroom als hitte wordt afgevoerd en je dus ook meer lawaai van zowel je kaart, als voeding krijgt. Zelfde geldt voor Vega64. Vermoed, helaas, dat AMD weer het pad volgt van high end chips tegen mid/low range prijzen en we dit Vega spul nog wel drie ""generaties"" gaan zien. Nvidia zou dit hele product weg kunnen vagen door freesync te ondersteunen."
RX Vega 64;3;0.4260849952697754;Het probleem voor AMD is meer dat als ze het prijstechnisch heel interessant maken, Nvidia gewoon ook met hun prijs daalt. Dus ze zullen er echt wel over hebben nagedacht wat de beste prijs is, waarbij ze zichzelf niet uit de markt prijzen maar wel ook nog winstmarge hebben. En als je Freesync/G-sync wil dan is AMD nog altijd wel een goede keus. Lawaai ligt wel eraan wat voor een koeler erop zit. Uiteraard, alle extra vermogen moet ook weer gekoeld worden. Echter ik heb zelf een R9 Fury (Waarom? Was goedkoop tijdje geleden, en al helemaal significant goedkoper dan een 1070 + G-sync monitor). Kaart verbruikt een hoop, maar zit geleverd goede koeling (weegt ook een hele hoop), dus passief gekoeld in idle, en prima stil onder load. Het veranderd uiteraard niet dat minder vermogen altijd stiller is onder gelijke condities dan meer vermogen. Maar tegelijk blijft lawaai ook sterk afhankelijk van de koeler.
RX Vega 64;3;0.29549452662467957;"Hier houdt zich ook een R9 Fury koest in mijn kast. Nog nooit zo een ""premium"" product voor in m'n PC gehad wat dat betreft. Zoals je stelt weegt het apparaat een hoop, maar de koelprestaties zijn top of the line. Indien Sapphire met een dergelijke koeler voor op de Vega kaarten komt zou ik hem ook direct aanbevelen, mits je goede koeling in je kast hebt om de hitte er uit te werken. Een stofzuiger zou ik nooit meer willen, en ik ben ook bang dat na deze kaart het alleen nog maar tegen kan vallen in de toekomst."
RX Vega 64;2;0.41538137197494507;Ik vind het vooral gek dat de rx 64 met 12 terraflops de 1080 niet vloert. Misschien is er nog ernstige marge voor verbetering met driver updates?
RX Vega 64;1;0.45513492822647095;Pixel rate van de VEGA is 86.4 GPixel/s en van de GTX1080FE is 102.8 GPixel/s. Dit is deels een verklaring. Verder zijn de drivers van NVidia een stuk volwassener.
RX Vega 64;2;0.3894832730293274;"Pixelrate is idd een zeer goede graadmeter om te weten hoe je kaart presteert. (Kijk bijv. maar naar die tabel-benchmarksites). Als je daar de pixelrate pakt en naar de ""echte"" benchmarksites toe gaat en vergelijkt zie je dat de prestaties aardig kloppen tov de pixelrate. Kortom; zeer teleurstellend dat AMD met pakweg 70% méér transistors niet eens de GTX1080 kan verslaan."
RX Vega 64;1;0.43592604994773865;Wat een enorm stroomverbruik zeg in verhouding tot de GTX serie. Ik vind dat Tweakers in de conclusie vermelding had moeten maken van het enorme stroomverbruik van deze kaarten ingame. Het verschil tussen de AMD Radeon RX Vega 64 8GB en de Nvidia GeForce GTX 1080 is ruim 120 watt. Als je 5 uur per dag gamed is dat al gauw €35 per jaar. Dat doet een groot deel van het prijsverschil teniet zeker als je de videokaart meerdere jaren gebruikt.
RX Vega 64;2;0.4448911249637604;Het stroomverbruik is minder een issue. Je merkt dat niet zoveel dat beetje extra geld. De hitte dat die stroomverbruik veroorzaakt is meer een issue. Want dat merk je wel iedere dag 5x365 in de vorm van sneller draaiende fans... We spreken over 50 a 60% meer hitte productie en bijhorend geluid tegenover de concurrentie. De dagen dat een goede en snelle GPU klinkt zoals een jet zijn al lang gedaan en dat is AMD hun grootste fout vind ik.
RX Vega 64;2;0.5705220699310303;Het klopt dat het een relatief klein bedrag is op je stroomrekening. Wat AMD doet is twee kaarten in de markt zetten die gewaagd zijn aan die van Nvidia maar tegen een stuk lagere prijs. Dat klinkt als een voordeel maar als dan het stroomverbruik bij intensief gamen tientjes meer kost op jaarbasis dan valt dat gewoon weg. Afhankelijk van hoe intensief je de videokaart gebruikt valt het voordeel in aanschaf steeds meer weg. Dat geluid en hitte een groter probleem zijn heb je groot gelijk in. AMD scoort niet goed op geluid en hitteproductie. Dit is al jaren zo en ze zouden er goed aan doen het een keer op te lossen. Ik moet wel erbij vermelden dat basiskaarten zelden goed scoren in geluidtesten en koeling. Kaarten van fabrikanten als Asus/MSI/etc zijn een stuk stiller en koeler. Mijn conclusie is dat Nvidia de zaken op gebied van energieverbruik vs geluidsproductie vs snelheid gewoon beter voor elkaar heeft. AMD heeft naar mijn mening een lange weg te gaan.
RX Vega 64;1;0.5481764674186707;Een gtx kaart verbruikt ook wel meer dan er op de doos staat hoor... Iig heb genoeg mensen erover gehoord.
RX Vega 64;2;0.3484223783016205;Nee, tenzij je de powerlimit verhoogd, anders blijven ze netjes binnen opgegeven TDP. Zie ook:
RX Vega 64;3;0.6051778197288513;Ben redelijk pro-AMD, maar vind het lastig om hier (zoals sommige reviewers doen) een positieve draai aan te geven. Deze kaarten komen 2 jaar na de Fury X en bijna 1.5 jaar na de 1070 en 1080. De chips zijn veel groter en het verbruik ook. De prijs is daarnaast ook niet heel spectaculair. Deze kaarten zouden voor 300 en 400 euro MAX in de winkel moeten liggen om nog enigzins interessant te zijn. Er is maar één reden om Vega aan te schaffen, en dat is omdat je een freesync monitor hebt. Ik zie er zelf nu misschien wel van af, er zijn nu 3440x1440 monitoren met gsync die net zo duur zijn als hun freesync tegenhanger. Misschien dat AMD nog wat winst kan behalen met hun drivers. Ik weet niet wat ze anders hebben gedaan met de hele nieuwe architechture.
RX Vega 64;3;0.2735064625740051;Er is een duidelijke positieve (Freesync) en negatieve (verbruik) draai aan dit hele Vega verhaal. De afweging ertussen moet je zelf maken, maar die lijn lijkt vrij duidelijk in elke review die ik tot nu toe heb gezien. De uiteindelijke prijs kan vervolgens de zegen of de genadeklap zijn, maar dat is voor iedereen koffie dik kijken, persoonlijk laat ik de prijs liever zo veel mogelijk buiten de details, want achteraf corrigeren is dan een draak. En wat optimalisaties betreft, die lijken vanzelfsprekend zoals in elke eerdere release, zowel Nvidia als AMD. AMD is weliswaar al even bezig met de kaart, maar vooruitgang zien we meestal toch pas wanneer een bepaalde chip echt breed verspreid wordt. Maar ook dat : beetje koffie dik kijken.
RX Vega 64;2;0.43092963099479675;"We hebben NiceHash Miner (2.0.0.12 beta) gebruikt maar de meeste algoritmes in de benchmark werken nog niet op de Vega-kaarten. Claymore en Optiminer worden niet gedraaid met de melding ""Needs Benchmark"", van DaggerHashimoto heb ik de prestaties hier gemeld, zo'n 30-34GH/s. Gezien de tijd die we met de kaarten hadden (drie dagen, give or take) en het feit dat miningsoftware nog nauwelijks werkt (en vast niet optimaal) gaven we prioriteit aan de game-benches voor de review."
RX Vega 64;4;0.4204646646976471;Fair enough, bedankt voor je antwoord!
RX Vega 64;1;0.7130488753318787;En hoe gaat het minen met deze kaarten? Spelletjes spelen via een desktop interesseert me helaas helemaal niks..
RX Vega 64;5;0.31625843048095703;Zie mijn reactie hier
RX Vega 64;2;0.4203258156776428;Nja maar dat is nicehash haha Claymore heeft ondertussen al een ''tijdje'' de VEGA ondersteuning ingebouwd. Nicehash is in ieder geval niet de tool die je gebruikt om te minen. Meer een gemak voor mensen die niet meer kunnen dan 2 muisklikken maar toch willen minen, daar is nicehash goed voor. Niet voor mining (snelheid) resultaten. Oftewel: claymore testen, zcash of ethereum, miner tunen, clocks tunen etc etc, allemaal onderdeel van minen.
RX Vega 64;5;0.4379597306251526;Nee, maar nicehash was zo vriendelijk om snel even verschillende algoritmes te testen Ik heb net claymore op het testsysteem gezet, morgen even kijken wat ie doet. Ik heb in het verleden litecoins geminded, tunen duurde lang Nu nog geen tijd voor gehad, morgen even verder spelen
RX Vega 64;3;0.44124993681907654;Er komen wekelijks wel coins uit die zeer goed te minen zijn, wat de toekomst wordt weet niemand (zelfde zoals bij Bitcoin, Ethereum, Dash, Zcash etc etc). Ethereum levert ook wel wat op, persoonlijk mine ik al maanden geen Ethereum meer, wel andere coins met meer profit. @willemdemoor Gelukkig valt er niet heel veel te tunen bij Claymore, is qua basic al 1 van de snelste. Wel zijn er enkele instellingen die je kan toevoegen / aanpassen (moet binnen 30 minuten wel klaar zijn), clocks zit ook niet veel tijd in, zodra je ziet dat je TEveel overclockt terwijl de prestaties uit blijven /minimaal verbeteren weet je genoeg.
RX Vega 64;2;0.5069082975387573;Levert steeds minder op door de Difficulty bommen en natuurlijk afhankelijk van de valuta prijs. Tenzij je stroom gratis is is het nog steeds beter je geld in crypto te stoppen ipv mine apparatuur. Ga maar eens kijken wat een beetje rig je gaat kosten, stroomkosten en de stijgende moeilijkheidsgraad. Nee, ETH is alleen nog leuk voor de mensen met veel hashing power en goedkope of gratis stroom. Andere coins zijn er wel alhoewel je bij sommige nooit weet wat die je later gaan opleveren. Ik voorzie door deze downtrend dat de Gfx cards goedkoper gaan worden en op MP belanden, voor de hobbyist als zakcentje erbij is het met de dag minder interessant.
RX Vega 64;2;0.3832831084728241;Had die conclusie inmiddels ook getrokken na wat googlen idd. Makkelijker om in te spelen op een wisselkoers, wanneer het niet rendabel is blijf je er simpelweg uit de buurt. Met een mining-rig heb je al behoorlijk wat uitgegeven en dan nog maar zien of je het terug verdiend. Ik hou het wel bij een zelf-studie cryptocurrency . Heb wel wat 'ripple' gekocht, maar dat is weer een heel ander principe.
RX Vega 64;3;0.4507538378238678;Ripple is mischien op de lange termijn wel wat waard maar door de enorme aantallen coins valt het nog te bezien. Mooiste is coins uit te zoeken die daadwerkelijk een ICO overleven en vlak voordat ze echt gaan stijgen kopen. Je kunt dan makkelijk een 10x-1000x pakken en dus wat verdienen zonder enorme bedragen in te leggen. Het wordt met de dag moeilijker omdat er veels te veel coins bijkomen en zie in die jungle dan maar iets te vinden wat geen scam is, geen hype is, of geen pump coin. Al met al blijft het een crypto casino.
RX Vega 64;1;0.23689031600952148;Dat was me inmiddels wel duidelijk ja, met het recente nieuws. Wat een chaos! Toch wonderbaarlijk dat de koerzen maar blijven klimmen. Het vertrouwen is er nog ruimschoots begrijp ik. Het is nog een beetje volwassen aan het worden. Zo als vroeger de handel in grondstoffen ook chaotische tijden kenden, en nog moest stabiliseren.
RX Vega 64;1;0.5720072388648987;Crypto staat in weze in het jaar 1994 nog voor de .com boom ... Zo moet je het zien. De enige die nu zich bezig houden met crypto zijn wat geeks in weze want de reguliere bevolking weet niet eens wat crypto currency is. Het woord Bitcoin hebben sommige wel van gehoord maar meer ook niet. Het is absoluut geen mainstream, net zoals internet in 1994 onbekend was voor de 'gewone' mensen. Pas vanaf 1996 kwam het een beetje op en 2 jaar later gingen de mensen een beetje online. Het was rond 1999 dat internet echt mainstream werd voor iedereen. Kortom, er is ruimte zat voor groei. De vraag is waar zit de groei ? Welke coins ? Kijk, Bitcoin is de Grootvader der Crypto maar het is absoluut een Bullshit coin technisch gezien. Zelfs nu na de Softfork (Segwit) is het niet merkbaar sneller en in de huidige configuratie is het ONMOGELIJK om het ook maar enigzins zinvol te gebruiken als betaalmiddel voor het dagelijks leven. Je kunt niet bij de kassa staan en 20 minuten moeten wachten op confirmations, en dan is dat nog snel en dan praat je op dit moment maar over een kleine groep mensen (relatief gezien) die het gebruiken. Als het massaal gebruikt wordt zoals wij nu pinnen dan duurt het dagen tot weken. Kortom, BTC is de benchmark en prima voor investering op de lange termijn en als spaarrekening zeg maar. Als betaalrekening voor regulier gebruik is het totaal kansloos. Zelfs het vergroten naae 2MB van de Block (nu 1MB) zal daar niks aan veranderen. Voor het betaalverkeer van de toekomst zal echt een andere crypto de boventoon voeren, mischien wel XRP (Ripple) want dat is wel erg snel.
RX Vega 64;2;0.3336322605609894;Je stuurt deze leek iig de goeie kant op, in mijn drang naar informatie omtrent dit onderwerp. Het hele betalen met bitcoin gebeuren kan ik dus nog met een korreltje zout nemen. Je kunt bijvoorbeeld bij 'thuisbezorgd.nl' betalen met bitcoin. Je kan dan eerst een half uur wachten tot de betaling rond is, en dan nog een uurtje op je eten. Erg praktisch . Bitcoin heeft dus een hoop in gang gezet, vooral de afgelopen jaren. Echter is het het meer een opzet naar een volwassen cryptocurrency, dan dat bitcoin dat zelf is begrijp ik. Morgen in de pauze weer verder neuzen.
RX Vega 64;2;0.32065409421920776;Zoals het er nu naar uit ziet zal de Vega 56 met een andere koeler (en pcb ontwerp) de enige slimme keuze zijn. Al zou het me niet verbazen moesten ook die kaarten belachelijk duur zijn vanwege miners.
RX Vega 64;3;0.525895893573761;"Beetje to little, to late gevoel bij deze release. Qua prestaties doet de kaart het niet verkeerd (tussen 1070 en 1080 in) maar het stroomverbruik is mega in verhouding met een GTX1070 of 1080 (120 watt meer!). Zie het nut van HBM2 geheugen ook niet helemaal; technisch mooi maar Nvidia levert meer prestaties met (veel goedkoper??) DDR5X?. Prijs en verkrijgbaarheid zijn belangrijk voor deze kaart, ben bang dat het minen daar erg veel invloed op gaat hebben. Het is helaas (voor AMD en ons consumenten) geen klapper om de concurrentie echt serieus wakker te schudden, wat Ryzen en Threadripper zeker wel is."
RX Vega 64;2;0.6358044743537903;Wat ik heb kunnen zien is het minen helemaal NIET interessant met dit ding. Dure kaart vergeleken met andere kaarten die ongeveer hetzelfde presteren voor duidelijk minder geld en ook duidelijk minder verbruiken. Minen met kaarten die stroom vreten is niet echt een geweldig idee.
RX Vega 64;3;0.6843534111976624;Vega scoort wel beter bij mining dan het scoort bij gaming, maar idd, het stroomverbruik maakt ze voor mining wellicht minder interessant. ( misschien is dat wel de redding voor de gamers onder ons, minder vraag door miners is mogelijks minder schaarste op de markt )
RX Vega 64;3;0.40775418281555176;Wat is beter met mining ? Een paar MH/S ??? Het moet echt een flink verschil zijn voordat men dus aan de Vega zou gaan. Zeker als het verbruik hoger ligt dan ben je je winst voordeel qua snelheid weer weg door extra stroomkosten. Ik denk dat de populaire kaarten NU nog steeds schaars zullen blijven want daar weet men wat ze eraan hebben en het verbruik is sowieso beter. Voor miners zijn die onder de streep effcienter.
RX Vega 64;2;0.5052624940872192;Dus net iets goedkoper dan de 1080, net iets langzamer en bijna 2x zo hoog stroomverbruik. Teleurstellend. Enige winst die je hebt is dat je geen gsync belasting hoeft te betalen bij een freesync monitor
RX Vega 64;4;0.5896603465080261;Leuke kaarten. Nu nog goede drivers en dan kan performance nog 10-15% omhoog. Alleen stroomverbruik valt me tegen. Maar dat is green bezwaar. Als prijs 50-100 euro onder de 1080 ligt is het een aanschaffertje.
RX Vega 64;2;0.5082950592041016;Prestaties an sich zijn wel prima. Maar een jaar te laat, te hoog verbruik en ook nog eens een paper launch.
RX Vega 64;4;0.5279316902160645;Als ze er een goede prijs aan kunnen binden zijn het mooie kaarten. Was natuurlijk nog mooier geweest als er ook echt een concurrent voor de 1080ti zou komen. Maar als concurrenten van de 1070/1080 zijn het leuke kaarten. Vooral als er goede drivers uitkomen, die de prestaties nog net even wat omhoog helpen.
RX Vega 64;3;0.6416676044464111;Het verbruik is wel flink hoger . Opzich wel te verklaren met 12,5 miljard transistors tov van de 7.2 miljard van de gtx1080. Maar het is wel jammer dat de performance dan niet eens hoger is met zoveel extra transistors. Ben benieuwd of driver updates nog veranderingen kunnen brengen, je zou zeggen dat er in elk geval genoeg hardwarematige horsepower aanwezig zou moeten zijn.
RX Vega 64;3;0.5028964877128601;Gelukkig duurt het niet zo heel lang meer dan GloFo 14 nm+ (voor AMD) uitkomt. Dit gezegd hebbende duurt het ook niet heel lang meer dat TSMC 12 nm (voor NVidia) uitkomt. AMD is goed bezig maar ze zijn er nog niet op GPU gebied.
RX Vega 64;2;0.4689546227455139;"Zucht... AMD heeft echt de boel goed verkloot met Vega. De prestaties zijn ondermaats, het stroomverbruik te hoog, de koeler is nog steeds ruk en ze hebben het te hard lopen hypen. ""Poor Volta""? Poor Radeon afdeling eerder. 15 Maanden achterlopen, aantal keer uitstellen, beloftes doen en uiteindelijk de standaard ""too little, too late"" doen waar we AMD van kennen. De R9 290 was een super interessante kaart, maar ook niet zonder een enorm hoog energieverbruik en een super luidruchtige koeler. Als Nvidia ooit Adaptive Sync activeert op z'n kaarten dan stap ik over. De enige reden dat ik nu nog een AMD-kaart heb is vanwege m'n Freesync monitor. Ohja, en qua prijs: ze zullen de Vega 64 voor ~500,- in de markt moeten leggen wil het interessant zijn. Vega 56 voor ~400,-. Ze zullen het echt van de prijs moeten gaan hebben."
RX Vega 64;1;0.350058376789093;5u/d....365d/j gamen. Dan heb je een ander probleem dan een hoog stroomverbruik.
RX Vega 64;3;0.34267574548721313;Wat voor probleem zou dit dan zijn? Ben ik wel benieuwd naar
RX Vega 64;5;0.41269728541374207;versleten muismatten enzo
RX Vega 64;5;0.33458781242370605;Zie die 390 nog gaan dan, wat is die 290 een legendarische kaart zeg!
RX Vega 64;2;0.40992629528045654;Ik mis de benchmarks voor Optiminer, Claymore miners, e.a. Wat zijn de haalbare hash rates voor Ethereum en Zcash? Hashrate - Power grafiek bij diverse undervolt settings? Vergelijking met andere kaarten? Gezien de run op GPUs door miners lijkt me dit ook interessante aspecten in een review.
RX Vega 64;2;0.46756187081336975;Kunnen de gamers eindelijk weer een kaart kopen. de 570 en 580 zijn echt veel te hoog geprijsd door de miners, en als je het ervoor wilt betalen dan zijn ze niet op voorraad. Hopen dat de mingsoftware een leuke optimalistatie krijgt, dan hebben we weer videokaarten met een echte 2ehandse prijs.
RX Vega 64;4;0.2744351923465729;Ben vooral benieuwd naar het verschil qua hashrates (mining) tussen 56/64 en de 1080(TI). Een idee om dat ook te gaan testen?
RX Vega 64;1;0.5688161849975586;"Promotie filmpje van half jaar geleden ""Warning poor Volta"" kan niet eens Pascal aan... geweldige marketing weer AMD, hoe goed ze het met Ryzen en Threadripper deden hebben ze hier toch echt een fail product, heb al meer dan een jaar een 1080 en die is beter als je verbruik ook rekent dan de snelste Vega helaas blijft de 1080 dan naast de Threadripper, volgende keer beter mag ik hopen."
RX Vega 64;3;0.40541872382164;Doe eens een gooi naar perfomance per € - geeft een iets frisser beeld Net als Ryzen reviews overal het vingertje naar intel cpus die 3x de prijs zijn en dan zeggen die is toch wel beter heh XD Qua prestaties zal er zeer waarschijnlijk wel behoorlijk winst behaalt worden met driver verbeteringen - en die winsten tov nvidia zijn vallen meestal wat beter uit ook historisch gezien. bijvoorbeeld van een paar jaar geleden.
RX Vega 64;2;0.5332320928573608;Maar de performance per watt is dan weer rampzalig. Vooral rekening houdende dat ze op 14nm zitten en dus veel zuiniger zouden moeten zijn.
RX Vega 64;2;0.5244312882423401;Had Tweakers gesierd ook niet basis modellen 1070s en 1080s te testen. 1070s bijvoorbeeld die betere koeling hebben tikken al tegen de 2ghz aan zonder over te clocken door GPU boost in plaats van 1600+ mhz. Dan is de Vega 56 in eens helemaal niet zo geweldig meer een jaar na de release van de 1070. In da topzicht is deze launch dus voor mij een teleurstelling. Soortgelijke prestaties ruim een jaar na de launch van wat het moet verslaan maar dan zeer minimaal. Ik wens AMD het beste maar hierbij hebben ze echt te hoog van de toren geblazen en het is maar afwachten wat er met de prijs gebeurt dankzij de miners. Het enige waar deze kaarten leuk voor zijn, zijn compute toepassingen waar Nvidia de laatste jaren enigzins schijt aan heeft en duurdere kaarten pushed.
RX Vega 64;3;0.5507100224494934;tja er zijn nog geen custom modellen van Vega dus niet echt een eerlijke vergelijking dan.
RX Vega 64;3;0.34489908814430237;Met de powerdraw die de kaarten nu al hebben verwacht ik geen mega oc potentie.
RX Vega 64;5;0.29661038517951965;Waarom testen jullie niet met PU:B als game? Is nu de populairste game zo'n beetje op de markt. Had me leuk geleken om ook daar wat van te zien.
RX Vega 64;1;0.46363240480422974;omdat AMD daar slecht in presteerd en het een kleine developer is die de game niet geoptimaliseerd heeft voor AMD kaarten.
RX Vega 64;2;0.4968079924583435;omdat elke ronde anders verloopt en dus niet consistent onderling getest kan worden. Daarnaast zijn er ook geen benchmark tools in de game aanwezig.
RX Vega 64;2;0.41435888409614563;wie kan mij helpen. ik moet een videokaart hebben, en wilde een 580 hebben, maar die prijzen vind ik te belachelijk voor woorden. nu kan ik via een medetweaker een r9 390x krijgen voor een nette prijs maar zit ik tegen het verbruik aan te hikken van deze kaart. blijf wel het liefst bij amd vanwege het feit dat ik al een ryzen 1700x set heb, en het dus wel leuk vind om alles onder amd te houden (nee, geen fan, maar vind het een lekker idee om alle drivers onder 1 merk te houden) de vega 64 word m zo ie zo niet aangezien ik die bij Caseking voorbij zie komen voor 600+ als inloop prijs, vega 56 zou optie zijn, maar hoeveel verschil maakt het nu werkelijk tussen deze 3. de 390x, 580 en de vega 56 qua verbruik?
RX Vega 64;2;0.39975032210350037;Je geluids, netwerk, chipset drivers zijn toch ook niet van AMD? Sorry, maar je maakt het jezelf wel onnodig lastig met zulke voorwaarden. Er is geen enkele goede reden te verzinnen waarom je een GPU van hetzelfde merk als de CPU moet hebben, behalve in je hoofd
RX Vega 64;3;0.3790535628795624;is dat waar? volgens mij zijn er genoeg geruchten dat nvidia op ryzen systemen niet optimaal presteren (of het waar is of niet weet ik niet, dus ga ik ook geen uitspraak over doen, maar volgens mij is daar wel degelijk sprake van toch? ) daarnaast is een free sync monitor aanschaffen een stuk goedkoper dan de nvidia tegenhanger, dus volgens mij is die reden niet zo gek als jij laat voorkomen volgens mij
RX Vega 64;2;0.51807701587677;Denk dat je de geruchten niet goed leest en/of ze zitten er naast: Nvidia's drivers doen het niet lekker met 6 of meer cores, bij beide CPU kampen. Daar heeft Nvidia idd nog werk te verrichten, maar het is dus een algemeen driver probleem en niet toegespitst op alleen Ryzen. De monitor is een ander verhaal, die is niet afhankelijk van cpu die er gebruikt word. Voor freesync heb je uiteraard wél een AMD GPU nodig. Dat is idd een valide keus, al kies ik persoonlijk eerder voor minder stroomverbruik. Als het nou om 10~30 watt ging, ala. Hier praat je over 100+watts, dat is toch geen kattepis. Je hoeft natuurlijk geen sync te nemen (van wie dan ook), het kan ook prima zonder. Heb er zelf nog geen behoeft aan, maar dat kan voor een ander anders zijn natuurlijk.
RX Vega 64;4;0.42853760719299316;Vega 56 heeft een hardlock van 300watt, meer verbruikt hij niet. Als je een power supply van 550watt of hoger hebt, zit je goed. Het HBM overclocken werkt trouwens bijna net zo goed als de Core overclocken, en zorgt voor een stuk minder extra verbruik. Source:
RX Vega 64;5;0.3107718527317047;Ik raad je de volgende kaart aan: pricewatch: MSI GeForce GTX 1070 GAMING X 8G De MSI Geforce GTX 1070 Gaming X 8G. Werkt zonder problemen samen met je Ryzen 1700X processor, dat gaat nooit tot issues leiden. De kaart is ONTZETTEND stil, vanaf 50 cm afstand gemeten zit de kaart onder load onder de 30 decibel. Idle is de kaart onhoorbaar, dan staan de fans stil. Deze videokaart kun je alleen bij de allerzwaarste games heel zachtjes horen. Het stroomverbruik is ook ontzettend gunstig. Meer dan 150W verbruikt de kaart zelden. De Nvidia drivers voor deze kaart zijn ontzettend stabiel en betrouwbaar, Nvidia heeft een héél erg groot team werken aan driverontwikkeling. Vrijwel altijd is er op of voor de release-dag van een nieuw spel al een game ready driver uit. De feature-set van de kaart is gewoon compleet. Op het gebied van hardware-acceleratie voor video bijvoorbeeld ondersteunt deze kaart letterlijk alles. Een 8K HEVC video kan die zelfs accelereren. Op de 4K resolutie kan die uiteraard alles accelereren. Alle features die je wilt zitten erop. En met Displayport x3, HDMI en DVI-D kun je ook alles erop aansluiten. Voor 500 euro krijg je nu deze kaart, op voorraad. En dat is een prima prijs voor een prima kaart. Deze GPU is zeer goed ontvangen met alleen maar positieve reviews. Het is een high-end GPU, en één van de stilste high-end GPU's ooit. En de performance is soooo nice. Je kunt letterlijk tientallen reviews over deze kaart lezen online, en allemaal waren ze zeer te spreken over de performance. Indien je kiest voor de GTX 1070, ben je zeker niet de enige. Neem maar eens een kijkje naar de statistieken hier:
RX Vega 64;1;0.4236633777618408;Voor de mensen die net als ik de hele dag hebben liggen zoeken naar prijzen en beschikbaarheid... enkele pre-order prijzen gevonden die ook naar België en Nederland verzenden: €676.90 voor de Wave (watercooled) versie van MSI en €580,90 voor de aircooled black editie (ook MSI), zonder game pack...exclusief verzendkosten en betrouwbare? Tjechische webshop moet je er even wel bijnemen !
RX Vega 64;3;0.5587982535362244;Had niks anders verwacht Lisa su is een pr vrouw en dat zag je al met rx 480 die als vr kaart werd bestempeld wat echt niet zo is. Maar het is wel mooi dat er een beetje concurrentie is.
RX Vega 64;3;0.27769172191619873;Is het al bekend wanneer de AIB partners hun eigen varianten van de 56 en 64 mogen uitbrengen? Vooral de 56 lijkt me erg interessant als een AIB als MSI of Gigabyte de factory clocks flink opvoeren (wat als ik naar de review kijk goed mogelijk zal zijn), er een goede koeler op zetten en ~75 EUR goedkoper dan de GTX1070 verkopen (wat nu zeer goed te doen is vanwege de mining hype) dan zal de 56 een zeer goede koop zijn.
RX Vega 64;1;0.40552642941474915;Laatste wat ik gehoord heb is eind Q3 / begin Q4, wat eind september begin oktober is. Maar het schijnt dat Nederland in ieder geval geen belangijke launch partner is, dus beschikbaarheid van reference modellen is schaars... Hopen dat AIB-kaarten wel direct verkrijgbaar worden....
RX Vega 64;4;0.47449132800102234;"Top review Jelle. Zoals @Frozen hierboven ook al aangeeft, lekker uitgebreid en met erg veel testjes. Topie dus. Zo zien we het graag. (althans ik dan) Ontopic: Ik vind de RX Vega 64 wel erg veel vermogen (Watt) verbruiken, wanneer deze aanmaal goed aan het werk wordt gezet zeg. -Nvidia GeForce GTX 1080: 181 Watt ingame -AMD RX Vega 64: 301 Watt ingame Nu zijn dit twee totaal verschillende kaarten, en dus GPU's natuurlijk, maar deamn. Stond er zelf toch wel even van te kijken, en dit metname, aangezien er eerder in het artikel regelmatig ""minder energiegebruik"" voorbij komt. Wat maakt dat je verwacht dat het verbruik dan ook daadwerkelijk minder is. Al is de kans groot dat dit laatste nog wel gaat gebeuren, aangezien er, voor zover ik weet, nog niet specifiek voor deze kaart ontwikkeld wordt (niet vreemd ook, hij is immers net gelanceerd), en dat een hoop games hoor voor wellicht later nog updates gaan/zullen krijgen. Maar dat blijft altijd afwachten. Zoals reeds gezegd, een top review Jelle."
RX Vega 64;3;0.49209773540496826;In het geval van het TDP is het eerder rampzalig qua vooruitgang. De trend was meer performance ten opzichte van TDP, niet performance ten opzichte van de besteden euro's. Die €100 die je met freesync bespaard praat heel wat jaartjes hoger verbruik goed, dat is zeer zeker waar. Maar dit is niet echt een technisch hoogstandje zo.. jammer.. Het zijn geen slechte kaarten, de driver updates zullen nog wat extra performance bieden. Maar het loopt net wat achter ten opzichte van Nvidia. Terwijl er ten opzichte van Intel toch aardig wat gewonnen is. Met een beetje geluk verlaagd Nvidia hun prijzen, dat zou iig al prima zijn.
RX Vega 64;2;0.381308913230896;Waarom verbaasd iedereen zich zo over de TDP/Powerconsumptie? De Vega 64 heeft 4k cores en de 1080 ti heeft er 2.5k. Het is toch logish dat de 4k kaart meer power nodig heeft dan een kaart met 2.5k cores? En de Vega heeft een betere Watt per core dan de 1080, hier hoor ik niemand over. Wat betreft de prestaties, heb het idee dat drivers ook nog niet perfect zijn. Want een kaart die op papier 2x sneller is dan zijn nvidia variant dan zou je dat moeten terug zien in de Benchmarks. Dit zien we niet. Waar dat aan licht is mij een raadsel.
RX Vega 64;1;0.45466873049736023;Staar je jezelf niet blind op cores? Als je kijkt naar tflops, dan ligt het nagenoeg gelijk. edit: volgens mij ben je de enige die zich überhaupt met de cores bezig houd, watt per core is meer iets voor CPU's niet?
RX Vega 64;1;0.4283440113067627;Conclusie: de Gtx 1070 en 1080 blijven geen gekke kaarten!
RX Vega 64;1;0.8450313210487366;Helemaal nie tals je erbij bedenkt dat vrijwel alle non reference Nvidia kaarten fors hoger boosten. Basis 1070 boost rond de 1600, een EVGA FTW zit tegen de 2000 aan zonder een overclock en met overclock kan je vaak tegen de 2100mhz aan zitten. Dat is doodleuk 25% meer performance.
RX Vega 64;1;0.5396353006362915;Wanneer zijn de kaarten beschikbaar in europa? Ik heb nu spijt van mijn freesync display...
RX Vega 64;2;0.3166235685348511;"Nou, als ik het zo zie, ben ik blij dat ik 9 maanden geleden een GTX 1070 heb aangeschaft. Ik heb een MSI GTX 1070 GamingX 8GB, waar ik via Coolblue destijds €507 voor heb betaald. Nu is hij bij twee winkels nog te krijgen voor rond de €500, maar bij Coolblue kost hij nu €649 (!). Ik vraag me af of die cryptocurrency ooit iets gaat worden; de hoop is natuurlijk om één of als het kan 2-3 munten te vinden, daarmee de kaarten eruit te halen, en dan binnen te lopen via prijsstijgingen. Gelukkig speel ik nauwelijks nog spellen. Ik speel zonder problemen een 10-15 jaar oude RPG op 800x600 op mijn 1920x1200 monitor, en het zwaarste spel dat ik speel is met grote afstand The Witcher 3, op 1920x1200/Ultra. De 1070 voldoet daar prima voor. Alle andere spellen, en de twee die ik nog ga spelen waarschijnlijk (Torment: Tides of Numenera an Divinity Original Sin 2) gaan daar makkelijk op draaien. Vanwege mijn grote backlog aan spellen ben ik nog makkelijk 4-5 voorzien. Tegen die tijd wissel ik de grafische kaart uit voor een nieuw model indien nodig. Dat moet prima kunnen met een 6700K en 32GB RAM, en dan kan die computer nog eens 4 jaar mee. Dat is ook zo gegaan bij de vorige, omdat toen met The Witcher 3 een spel langs kwam dat ik niet meer kon spelen op de gewenste instellingen."
RX Vega 64;2;0.6474776268005371;Jammer, en dat meen ik oprecht. Persoonlijk had ik meer verwacht van AMD na zo lang afwezig te zijn geweest in het topsegment. Gezien de ontwikkeltijd had ik op z'n minst een zuinigere kaart verwacht tegen een redelijke prijs. Uiteraard is het altijd wachten op driveroptimalisaties maar de eerste indruk is voor mij niet positief.
RX Vega 64;4;0.27516958117485046;Als ik de reviews een beetje her en der zie dan is het niet eens zo verkeerd. In de nieuwe API benchmarks heeft de 64 nog best een voorsprong op de 1080 en dan ben ik heel benieuwd naar de fp16(rapid packed math) games die eraan komen zoals Far Cry 5 en Wolfenstein, dat moet de gat alleen maar groter maken.
RX Vega 64;2;0.43567773699760437;"Tja, de prijs is nog niet definitief en laten we ook niet vergeten dat je bij Nvidia nog een premium betaald voor gsync. Sowieso gaat AMD goed omzet uit deze kaart kunnen halen indien het de markt van miners gaat bedienen. Voor AMD een prima kaart, maar volgens mij ook niet hetgeen het bedrijf graag naar voren had willen brengen. Uitstel na uitstel, om een reden die iedereen al vermoedde en nu wordt bevestigd. De prangende vraag vanuit mij persoonlijk is nu wel; wat gaat Nvidia doen? Zit het bedrijf ook aan de max, of heeft het ondertussen alweer een nieuwe architectuur met HBM2 gereed die Vega wegvaagt? Zal het een kleine update doen om volop de leider te blijven? Ik ben wel benieuwd, dat maakt voor een doorslag definitief. Op dit moment heb ik nog altijd een freesync scherm staan die ik met een AMD kaart wil bedienen. Maar als Nvidia nog een genadeslag heeft die de meerprijs waarmaakt, dan koop ik wel een scherm met Gsync."
RX Vega 64;3;0.3571319878101349;Er wordt hier iets over het hoofd gezien en dat zijn de power modes dat Vega aanbiedt. Als je naar de TPU review kijkt, dan zie je dat PwrSave mode 96% van de performance geeft van Std mode, terwijl Vega64 dan 78W minder verbruikt. Vega64 PwrSave <2% trager dan GTX1080 terwijl die maar 48W meer verbruikt.
RX Vega 64;2;0.35395583510398865;Het prijsbeleid is vrij duidelijk:RX Vega 56 komt op $399 MSRPRX Vega 64 komt op $499 stock, $599 Limited Edition (looks) en $699 voor de Liquid Edition.GTX 1070 MSRP zit officieel op $379GTX 1080 verlaagd van $599 naar $499.Dan de realiteit bij Nvidia. Om even de euro/dollar problematiek en de import heffing en BTW buiten beschouwing te laten kan men het beste naar newegg kijken. Daar is de goedkoopste 1070 momenteel $429 dollar. De goedkoopste GTX 1080 zit inderdaad op $499. Gemiddeld zijn de kaarten hier een stuk duurder, dat heeft hier niet alleen te maken met de zwakke euro, de btw en de import heffing maar ook met de mining hype. Daar zit voor AMD nu net het pijnpunt. Immers blijkt dat de RX Vega 56 met 31 MT/s al bijna het niveau van een veel duurdere GTX 1080 Ti haalt. Voor miners dus heel erg interessant, AMD is nu niet het bepaald het bedrijf met een overschot aan zelfvertrouwen dus traditioneel zie je die erg voorzichtige productie orders plaatsen. Combineer dat met de gunstige mining performance en ik zou er niet op rekenen dat je, op een paar launch kaarten na, de komende 6 maanden voor een normale prijs een RX Vega 56 kunt aanschaffen. Verder werd er door The Source (MSI) al gemeld dat AMD de marge van de hele keten onder druk zet om gunstige launch prijzen bij de reviews te kunnen laten zien. De custom modellen zouden duurder worden.
RX Vega 64;3;0.5350952744483948;Mooi staaltje techniek deze nieuwe kaarten. De benchmarks zullen vele tegen vallen, die baseren hoe goed iets is puur op hoe het nu in directe vorm presteert tegen over iets wat er al is op een gebied van een 2 jaar oude game. Op zich logisch maar ook een beetje scheef. Ik kijk erg uit naar de benchmark van Wolfenstein. Men klaagt wel vaak op AMD maar ze leveren wel vaak iets innovatief en wanneer de opties die deze kaart heeft worden benut dan kan 64 zo maar boven aan het lijstje komen te staan en dan is het de vraag of je oude games wilt spelen met meer fps of de nieuwe games? Om enig succesvol te zijn moet de prijs dan wel onder de die van de concurrentie zitten, ook al is de kaart nieuwer en over een jaar misschien wel 20% sneller. Daar heeft de consument nu niet zoveel aan.
RX Vega 64;2;0.4174826443195343;hoe kun je nou zo'n kaart testen maar hem niet testen op mining speeds? jammer hoor
RX Vega 64;5;0.3351079821586609;Wat ik van de vega zie is dit. Miners heaven, gamers hell. Vega 56 zal goed verkopen onder de miners door veel lager stroomverbruik dan zijn grote broer en minimale prestatie verschil. Move a long, nothing to see here.
RX Vega 64;3;0.43560490012168884;Ik zie bredere support van Feature level 12_1 en SM6.0+ met veel meer Tier3 level. Voor stoeien met DX12 en Vulkan SDK de betere keuze
RX Vega 64;2;0.36051610112190247;"dat is ook wat ik zie... waar dx12/vulcan gebruikt wordt begint de kracht van vega zichtbaar te worden... hoewel AMD zich met de korte tijd die ze de reviewers gaven zichzelf wel in de voeten heeft geschoten. je ziet tussen de verschillende gebruikte drivers behoorlijk grote verschillen en mining lijkt zelfs terug geschroefd te zijn. waar ander opencl test juist sporen van vuur achter laten... (een paar standaard test lieten 30% (ongeveer) hogere scores zien als de 1080ti... ""Somethings weird"""
RX Vega 64;1;0.40620583295822144;nVidia chokehold op de GPU markt haha. Ze lopen effectief gewoon een generatie voor. Volta is gonna crush AMD so hard. Had ik mijn aandelen nVidia maar gehouden...
RX Vega 64;3;0.6103509068489075;Mooi review maar mis wel wat dingetjes.. O.a. de minimum framerates, deze blijken voor de rx64 in bv GTA V hoger te zijn dan de 1080 ... wat opmerkelijk te noemen is gezien de maximage fps. Dit is wel op basis van andere reviews overigens. Ook zie ik in jullie eigen benchmarks de rx56 sneller zijn dan de rx64 bij enkel tests, en kan er overheen gelezen hebben maar zover ik zag is dit verder onopgemerkt gebleven. Dit is natuurlijk ook opmerkelijk. Ergens denk ik dat het ook voornamelijk problemen met de drivers zijn die gaming parten spelen, ik neem aan dat jullie oude drivers hebben gebruikt vanwege de korte tijd na uitkomen van de 17.8.1 bèta 6? Ik heb zelf gerucht meegekregen dat er alsnog veel moeite gedaan om de hashrates voor miners toch omlaag te halen zodat niet alle kaarten meteen weg zouden zijn door miners die bitcoin-tekens in hun ogen hadden na eerdere berichten over zeer hoge hashrates. Alle reviews geven eigenlijk weer dat de kaart in ieder geval tegen bijzondere driver problemen aanloopt... in enkele test fietst de rx64 namelijk ook de 1080ti voorbij en ook in enkele opencl test gaat ie er vrij ruim voorbij waarbij de ethereum hash rate juist omlaag gegaan is. Ik denk dat we in de komende weken alsnog wel redelijk wat verbetering te zien krijgen... Wat AMD voornamelijk te verwijten is dat ondanks alle vertragingen zo dit toch gehaast vrijgegeven hebben, met ZEER onvolwassen drivers. Crypto-mining-gekte of niet, als het daarom als is, de drivers lijken in enkele gevallen zelf veel trager dan eerder gelekte testen en benchmarks.
RX Vega 64;4;0.6409690976142883;Mooie review en leuke kaarten tenminste een beetje concurrentie voor nvidia nu, wel jammer is dat het stroom verbruik onder load nog een beetje aan de hoge kant is, De vraag is nu natuurlijk wat de echte prijzen gaan zijn in Nederland en zeker met die mining hype ... zucht.
RX Vega 64;1;0.8252773880958557;Mininghype is al voorbij. Je verdiend niks meer mee en moet juist inleveren ( in nl tenminste ). Ik zal die vega een dragon punch geven waar hij nog vele jaren pijn van zal hebben. Troep blijft troep. Al die hype voor dit, pffff. Ik word echt niet vrolijk van als amd zo zit te verneuken.
RX Vega 64;2;0.3393908143043518;Ik ben misschien een leek hierin over de nieuwe Vega Kaarten. Wat ik zie is een intel systeem, hoe zou de kaart draaien als de computer volledig Ryzen processor in zat? Zou de kaart misschien beter draaien omdat alles AMD is? Heb zelf nu een AMD phenom x6 met een AMD R9 280x Dual kaart en het draait echt stabiel. Of is dit een rare vraag? is er iemand die hier over kan uitleggen of er nou verschil in zit ja of nee?
RX Vega 64;2;0.4288676381111145;Nou de genoemde prijzen zijn alweer achterhaald. De nieuwe lichting kaarten gaan duurder worden, oftewel nog minder competitief tegenover Nvidia.
RX Vega 64;1;0.4770747721195221;Een teleurstelling dus, helaas. AMD komt anderhalf jaar na Nvidia met 'concurrenerde' kaarten die het niet alleen qua performance af moeten leggen tegen de concurrent, maar ook nog eens gemiddeld zo'n 15-20% meer stroom verbuiken en meer herrie produceren. Daarbij heeft Nvidia nu na al die tijd zijn drivers op orde. Bij AMD zal dat nog wel even gaan duren. Ik kan geen enkele reden bedenken om nu voor één van AMD's nieuwelingen te kiezen...
RX Vega 64;2;0.3508242070674896;"Ik wel, en dat is dat ze sinds de laatste drivers ""enhanced sync"" hebben bij AMD. Een beetje zoals freesync of gsync , maar dan op software niveau. En het werkt verdomd goed. Als ik nu van mijn RX480 ( die net wat te weinig power heeft om Quake Champions te spelen ) en LG 21/*9 scherm, zou willen upgraden naar een Nvidia GPU en gsync scherm, dan ben ik veel meer geld kwijt dat dat ik nu simpelweg een upgrade uitvoer naar vega. Dat die kaart dan 100W meer verbruikt zal mij een worst wezen."
RX Vega 64;3;0.34050366282463074;hier ga ik wel een paar minertjes van bouwen, 70 megahash per second
RX Vega 64;1;0.7821722626686096;650 euro bij alternate.de er is niet eens de Bundel erbij! Dus middelmatige performance, slechte preis, hoog stroomverbruik. yay Well al meer dan een paar verkocht
RX Vega 64;4;0.4247188866138458;"Prijs schrijven met ""ei"" , wel met ""ll"" ... sterk bezig."
RX Vega 64;1;0.6542509198188782;Juist, mijn argument is niet geldig door een spellingfout... ik kan jouw onfaalbare logica niet aan.
RX Vega 64;1;0.6909023523330688;Jij ziet ook nog woorden staan die ik niet typ. Waar zeg ik dat ik jouw argument niet snap ? Meer dan een opmerking op slechte spelling was het niet hoor.
RX Vega 64;1;0.25640004873275757;"Spreek ""ego-boost"""
RX Vega 64;3;0.5056076645851135;Het valt mij op hoe goed de Fury X presteert tegenover de rest. Ik meen mij te herinneren dat de Fury X een paar jaar terug tegenover de 980ti werd geplaatst. Met andere woorden, driver-optimalisaties zullen over een aantal maanden wat meer duidelijkheid scheppen. Ook als het gaat om verbruik. Momenteel is het goed om te zien dat ze hun mannetje staan tegenover de 1080 en de 1070 en prijsconcurrent kunnen zijn.
RX Vega 64;3;0.4044285714626312;Vergeet niet dat de Vega 64 tussen 1080 en 1080 ti performance zit wanneer alle kaarten ZONDER MSAA draaien. Er is een kans dat dit met driver goed gepatched kan worden waardoor de Vega kaarten nog wat aantrekkelijker gaan lijken.
RX Vega 64;3;0.39543065428733826;"Had graag ook nog een warmtecamerafoto gezien zoals die op Tomshardware. De warmteonwikkeling valt mee. Tweakers geluidsmetingen zijn nogal aan de optimistische kant. Geluidsmeter (welke?) is niet echt accuraat genoeg. Ze hebben het bij Toms over een geluidsproductie in turbomode van 50 dB(A) of 48,2 dB(A) in de ""normale modus"". Dit is gemeten vanaf 50 cm!!!! Dit is voor mij het geluid van een stofzuiger cq tornado. Tomshardware heeft dan ook de beschikking over een ""bijna"" stille kamer en NTI Audio M2211 meetmicrofoon voor het zachte prijsje van € 2.535 Bron:"
RX Vega 64;1;0.498355507850647;Waarom zou je in godsnaam deze kaarten kopen als je geen specifieke use case hebt (Blender, minen)? Kosten praktisch hetzelfde als je het verbruik meerekent, en komen nog maar net in de buurt van de groene jongens. Dan kan je het wel over overklokken hebben, maar een 1070, 1080 en 1080ti kun je ook overklokken. Hell, ik zou m'n PSU moeten upgraden om een AMD kaart erin te zetten .
RX Vega 64;1;0.5013313889503479;En is er al iemand die deze kaart gekocht heeft?
RX Vega 64;3;0.37482914328575134;Als je deftige 144fps wil en budget niet voor overprice Ti top end hebt , kan je de game setting tweaken om 144 te halen. Als dat belangrijkste reden is hoog fps voor bunny jumpen in frantic fastpace shooters. Dan gaat dat boven ultra setting. Medium mix kan ook zonder je blauw te betalen. Kan er ook nog goed uitzien. Aangezien Ultra setting vaker de meeste subtiele gfx verbeteringen zijn maar compute intensief. Freesync is handig voor 60hz stabiler te maken ipv 30hz frame time stutter door iets meer tijd nodig hebben voor 1/60 frame time. Gezien 1/30 vs 1/60 een flinke stutter is tegen een minderstabile 1/144. Freesync is belangrijker voor hoge resoluties en 60 fps Tja als game tussen de 55 en 100 fps schommeld bij tweaked setting dan past die setting goed bij een 60 sync. En free sync stijkt de dips stuk strakker Als 50 tot 200fps haald en minimaal strakke 90fps moet hebben dan stel je setting terug zodat min FPS omhoog gaat. Meestal gebeurt er niet veel bij hoge fps. Kijkt tegen muur aan upclose. Bij bunny hoppen doet animatie character logic voor weergeven van die andere foo bunnyhopper flink wat werk en grafische wodt het aanzienlijk dynamischer load in de render pipeline. Het Ultra setting gamen op 144 is luxe probleem geen verplichtng om in ultra settings te gamen maar optie van hoeveel geld je ertegen aan wilt gooien. Een luxe probleem dus. Er zijn games die er duidelijk slecht in low setting uitzien zo een game is crysis op 8600GT waar medium to much is. Een HD2900 met medium high mix zag er heel goed uit. Gezien de open wereld slow pace speelde ik met te lage FPS liever op medium setting denk dat 30fps niet goed gehaald werd. Far cry2 op low met 8600GT zag er goed uit en liep ook soepel. Ja zijn meer de open wereld games.
RX Vega 64;3;0.47715476155281067;Tja lage settings etc.. kan een R9 290x/ FURY X ook wel aan je 1080p freesync...
RX Vega 56;2;0.349263072013855;Bij andere reviews (Rx570 of Rx580) lees ik veel reacties waarin wordt aangegeven dat de review 'oneerlijk' is omdat Intel+Nvidia wordt vergeleken met Intel+AMD. Wat als deze tests worden uitgevoerd met een Ryzen CPU (1800X bijvoorbeeld), zouden de Vega's dan beter uit de test komen?
RX Vega 56;3;0.5728334784507751;Ben bang dat dat nogal tegenvalt:
RX Vega 56;4;0.46647879481315613;Interessant om te lezen! Bedankt voor de tip!
RX Vega 56;5;0.6314813494682312;Wat een prachtige review Tweakers, mooi uitgebreid met erg veel testjes. Dat is de kwaliteit die we graag zien! Ook erg handig vind ik de geluidsproductie testen. Immers kan je daardoor nu al zeggen dat, tenzij je een stofzuiger naast je voor lief neemt, je beter kunt wachten op aftermarket coolers van MSI, Asus, Sapphire, et cetera. Daarbij gebruikt een vega 64 zo'n 120W meer onder load dan een vergelijkbaar presterende 1080. Als je ervan uitgaat dat een computer 3 uur per dag onder load staat dan kost je dat per jaar 29 euro aan elektriciteit meer. Als je dus deze kaart zou willen vergelijken met de 1080 dan zou ik 60 euro bij de prijs van de vega 64 optellen. In het geval dat je de vega 56 wilt vergelijken met de 1070 moet je 36 euro bij de prijs van de vega 56 optellen. Omdat Tweakers niet alleen bestaat uit de reactie, maar ook uit de community, is hier nog wat leesvoer vanuit onze geweldige community. Ik hoop dat er veel reviews van deze kaartenserie (en dan vooral die met aftermarketcoolers) mogen volgen. Eerlijk is eerlijk, iedereen is gebaat bij concurrentie tussen het groene en het rode team. We zien hoe erg veel prestatie we er bij Intel hebben bijgekregen de afgelopen jaren door gebrek aan concurrentie. Okee, we zijn bij de desktop-CPU's er wat Watt ( ) aan efficiëntie op vooruit gegaan, maar in wezen kan een 5 jaar oude overgelockte i5 2600k nog goed meekomen. productreview: AMD Radeon RX Vega 64 8GB review door Foritain Fortain schrijft in zijn conclusie:
RX Vega 56;1;0.5272456407546997;de vega 64 liquid (met een meer dan 15% hogere clock) wordt niet gereviewed, wat dé tegenstander is van de 1080ti, de specificaties zijn fout vermeld (zie de rekenkracht), vreemd op volgorde gezet en een prijs/prestaties vergelijking ontbreekt (ja zelfs al is het maar een paper-launch met richtprijzen), je kan dit dan ook moeilijk een uitgebreide review noemen.
RX Vega 56;1;0.3612971901893616;We hebben de RX Vega 64 op woensdag gekregen, de RX Vega 56 op vrijdag en de Liquid nooit. Dan wordt het lastig reviewen
RX Vega 56;2;0.48584800958633423;Tel 15% op bij de prestaties en een prijs die hoger ligt dan een 1080ti (die aanzienlijk sneller blijft) heb je alleen nog maar een grotere teleurstelling, de AIO versie is een 345watt monster die de vega niet veel interessanter maakt. Vergelijk je een aio 1080ti met een aio vega64 is het huilen met de pet op. Voor de liefhebbers, in NL is hij niet te koop, in DE hebben ze hem wel. Mindfactory hebben ze hem in het systeem staan voor 509€ (vega 64) 609€ (vega64 LI) en 715€ de AIO. Ter referentie: je hebt daar voor 509€ een 1080 van zotac met custom cooler. (in het systeem staan en bestelbaar) EDIT: ik denk dat ze van elke kaart maar 1 hadden wat ze zijn op er blijven nu twee vega64 staan beiden voor 649, aanzienlijk duurder dan de AMD kaart was.
RX Vega 56;2;0.5087007284164429;Als je ziet dat er met de overclock van deze vega 64 meer dan 15 % wordt overgeclocked en de prestaties met maar 9% toeneemt dan raakt je eerste zin kant nog wal. Echter als de prijs hoger ligt dan de 1080ti heb je juist veel meer gelijk...
RX Vega 56;4;0.2645828127861023;De gpu throttled, kan niet anders.
RX Vega 56;2;0.5300534963607788;Ik heb gekeken naar wat de 56 deed(clock) is 200mhz langzamer, de vega 64 deed ~15% beter, de 64 AIO is 200mhz sneller dan de stock 64 dus deed ik daarboven 15% gem. Het zal altijd lager zijn aangezien de gelijk gebleven streamproc. De prijs in DE, 715€, dat is vergelijkbaar met de 1080ti die net even eronder zit en toch echt nu beter is, misschien dat de vega in de toekomst beter doet maar, maar er is natuurlijk geen garantie. Zeker niet als vega een slechte zaak blijkt voor AMD en dat ze het roer compleet omgooien. Dan zal de driver ontwikkeling ook stoppen voor vega, want dat de drivers in het verleden beter werden kwam simpelweg omdat ze GCN gaande weg beter begrepen en optimaliseerde.
RX Vega 56;2;0.45238491892814636;Ik heb geen idee waar je het over hebt? 'het zijn kaarten wat maximaal 200eurie kosten', wat kost 200€ welke kaarten. Ik heb het alleen over de vega64 aio kaart, die kost 700+ euro vergelijkbaar met de 1080ti die gewoon sneller is en bijna 100watt minder verbruikt. De vega 64 kost op zijn minst 500€ als je hem voor die prijs kan vinden, de 1080 paar tientjes meer, heb je een betere koeler dan de standaard (reference) heatsink & fan van AMD en standaar minder vermogen verbruikt en dus stiller is en makkelijker overclockt (nvidia 180watt vs AMD 295 watt), na verloop van tijd wordt de vega misschien sneller maar dat is nooit extreem en onzeker.
RX Vega 56;2;0.41248902678489685;Moet ik nog duidelijker aangeven wat we testen? Er staat bij de specs welke kaarten we getest hebben. We wisten tot vrijdag niet eens dat we de Vega 56 zouden testen. Ik zie de meerwaarde van ons beklagen dat we de Liquid niet getest hebben, terwijl dat evident is, niet. Bovendien is de Liquid niet eens los te koop, alleen als onderdeel van een pack. Lastig ook om daar een prijs/prestatie aan te hangen...
RX Vega 56;3;0.31314244866371155;Hoe werkt het stroomverbruik meten precies? Ik lees dat jullie een nieuwe manier van meten hebben d.m.v. een pci-e riser. Zit er ook nog meetapparatuur tussen de psu 2x8pin aansluitingen? Het lijkt me sterk dat je het verbruik kan meten met alleen de pcie port.
RX Vega 56;5;0.587992250919342;We meten zowel stroom en spanning aan het pci-e-slot als van de peg-connectors, alle stroom die naar de kaart gaat dus. Die riser zit er niet alleen om de slot-power te meten, maar de twee peg-connectors lopen ook via dat pcb om in 1 keer pci-e-slotpower, peg1 (6/8pin) en peg2(6/8pin) te meten en via usb uit te lezen. We kunnen dan netjes slotpower en per peg-connector de stroom/spanning/vermogen zien en uiteraard combined.
RX Vega 56;1;0.4630998969078064;Resultaten 1 op 1 overgenomen van HWI. Die van AMD TR ook al, dus de vraag is wie doet de review jullie of HWI?
RX Vega 56;3;0.46155643463134766;De reviews doen we los van elkaar Ze zijn ook door verschillende auteurs geschreven en daarom komen er verschillende conclusies uit. Wel werken we sinds we onder een dak zitten samen op het gebied van tests, waarbij data gedeeld wordt. Zeker bij producten als deze, waar samples enorm kort voor de deadline aangeleverd worden, kunnen we daardoor meer en beter testen.
RX Vega 56;2;0.4757937788963318;Uitgelekte benchmarks toonden gisteren aan dat de liquid versie niet veel beter presteert dan de lucht gekoelde versie. Zeer zeker geen concurrentie voor de 1080ti iig. Mogelijk als de drivers in orde zijn dat er weer een procent of 10 gepakt kan worden. De liquid versie is ongetwijfeld een stuk koeler, en overclocked nog een stukje verder. Maar dat zie je dan ook wel terug in het verbruikte vermogen. En de positieve overclock resultaten zorgen volgens deze review maar voor een relatief kleine winst begrijp ik.
RX Vega 56;5;0.30760493874549866;Ik kan niet wachten op de vega 64 liquid aftermarket kaarten. Ik ben heel trots op AMD ze hebben laten zien dat met wat ze hebben en kunnen toch wel competitief genoeg is om interessant te zijn om voor AMD ipv NVidia te kiezen en als we de aftermarket kaarten hebben hoop ik dat we gezamenlijk nog trotser kunnen zijn
RX Vega 56;3;0.4578961431980133;"Die voorbij 1080p wens is een beetje dubbel. Ik gebruik een steeds luidere GTX780Ti GHz edition en behoudens de games die struikelen over 3GB geheugen kan ik met redelijke kwaliteitsinstellingen de meeste games op 1440p goed spelen. (En die geheugen problemen kan je vaak ook voorkomen door de textures wat verder terug te draaien. Kijk maar naar de test resultaten: op ultra zit een (naar ik aanneem) stock 780Ti overal al boven de 30FPS. Een RX580 of een 390X presteren al beter dan de 780Ti en hebben geen geheugen tekort ,deze zijn voor 1440p voor de meeste gamers dan ook voldoende. Met een 1070 is het gros van de spelers op 1440p ruim voorzien van FPS. Dan blijft over de groep top gamers en 4k gamers, die eerste wil 144Hz monitoren aansturen en die laatste wil op zijn minst fatsoenlijke FPS halen. De 1080 en daarmee dus ook Vega64 vallen dan een beetje buiten de boot. Te snel voor de 1440p crowd, te langzaam voor de top spelers/4k crowd; die grijpen naar de GTX1080Ti of een Titan (als ze van hun geld af willen). Ik verwacht dan ook dat AMD het van Vega56 zal moeten hebben, die pakt net een mooie sweetspot, maar dan moet de prijs wel kloppen."
RX Vega 56;2;0.49763888120651245;4K met een 1080 is prima te doen hoor. Je moet alleen geen ultra en AA verwachten. Dit test tweakers wel, maar er zijn genoeg instellingen die je niet direct nodig hebt om je game er goed uit te laten zien. En eenmaal gewend aan 4K wil je niet echt meer terug naar 1080p of 1440p. Maybe ultrawide maar dat heeft weer haar eigen problemen. Gewoon wat spelen met de instellingen en je zit zo boven de 60fps. Behalve de titels die sowieso al voor geen meter lopen. Het jammere van deze kaart is dat gamers die op dit prestatie punt (vega64) willen instappen zijn of al een jaar op de 1080 (zoals ik) of verstokte AMD liefhebbers.
RX Vega 56;3;0.7146250605583191;Ik vind dat anti-aliasing met een wat lagere resolutie er soms beter uit kan zien dan een wat hogere zonder. Maargoed, ik game niet zoveel en mijn games zijn al wat ouder dus dat is misschien ook niet meer te vergelijken.
RX Vega 56;3;0.6675093770027161;Tja, ik houd het vooralsnog even op 1440p, visual studio werkt lekkerder op 2x 1440 dan op 1x 4k. Voor gaming ben ik echter een beetje teleurgesteld in Vega, ik denk dat ik de Freesync maar ongebruikt laat en even door spaar voor een 1080Ti.
RX Vega 56;1;0.3034309148788452;"Is AA nog echt nodig als je op een sub 32"" scherm 4k draait?"
RX Vega 56;5;0.6077457666397095;"Ik zit met 28"" en zet het altijd gelijk uit als ik meer fps nodig heb. Je kan het zien, zeker als je gaat zoeken in de kleine dingen en op de plekken waar je standaard die trappetjes kan verwachten. Je moet echt naar 8 of 16K schermen voordat we daar vanaf zijn denk ik. Maar het is echt meer dan acceptabel om zonder AA te draaien"
RX Vega 56;1;0.7107934355735779;Vergeet de VR-gamers niet. Dan is het met een 1080 al schipperen met instellingen
RX Vega 56;5;0.46506601572036743;Dank je (Vanuit het hiernamaals)
RX Vega 56;3;0.435801237821579;Echter, als je in de markt bent voor een nieuwe monitor, dan ligt het erg voor de hand dat er een beeldscherm komt met Adaptive vSync. In dat geval heeft AMD weer een duidelijk prijsvoordeel omdat Gsync schermen een stuk duurder zijn dan vergelijkbare FreeSync schermen. In dat geval kan je dus weer makkelijk 100 Euro aftrekken van de Vega videokaarten en brengt het het nodige weer in balans. Zeker voor iemand die minder dan 3 uur per dag gemiddeld bezig is met gaming. Voor de volledigheid zou dit eigenlijk in jouw post ook vermeld mogen worden
RX Vega 56;3;0.4736166000366211;"Dit is wel een goed punt.. ik heb 2 weken terug een 24"" IIyama scherm gekocht voor 183 euro (nieuw met 1 dode pixel bij Coolblue, normaal 260 euro). Heeft 1080p, 144hz en Freesync. Voor het Gsync alternatief zit je al snel op 350 euro voor het instapmodel. Als je in de markt bent voor een GPU en een monitor dan heeft AMD vrij snel de betereprijs/kwaliteit verhouding"
RX Vega 56;2;0.33951544761657715;het geluid van de koeler zal meer decibels geven dan een strix gtx1080 zoals ik die heb. mijn kaart gebruikt 180 watt op load en de vega 64 gaat daar met 120 watt erbij ruim over heen ( 180 + 120 = 300 watt!! ). dat betekend dat die ene koeler echt heel hard moet werken. die warmte zal toch afgevoerd moeten worden en ik betwijfel het gameplezier als je systeem een stofzuiger is qua geluid. mijn kaart laat al licht van zich horen als ik hem langdurig op 80 % + load zet. dan zitten er op mijn gtx1080 nog 3 grote koelers. de kosten van het extra energieverbruik liggen hoger dan die 120 watt extra alleen. je moet ook de efficientie van de voeding hierin meenemen. het kan waarschijnlijk zijn dat je een andere voeding nodig gaat hebben. het gaat er mij niet om om amd in een negatief daglicht te zetten, maar de wens om de 1080 familie van nvidia voorbij te gaan kost zoveel extra vermogen dat er echt een betere koeling nodig is. die zullen een stuk stiller zijn, maar je hebt dan een behoorlijk raster nodig met minimaal 3 fans om die warmte je kast uit te krijgen. en dan het laatste. je zegt zoveel extra stroom te moeten betalen door een heel jaar heen.... deel je uitkomst door de helft. in de winter is de extra afgegeven warmte van je kast nuttig omdat je cv dan minder hard aan het werk moet. alleen in de zomer op je zolderkamer kan het een probleem worden. hoe gaat de vega zich gedragen bij een omgevingstemperatuur van 35 graden op je zolderkamer?? die mooie vega kaarten lijden heel veel door het enorme vermogen wat door die print heen moet en de daarbij vrijkomende hitte. iemand een youtube filmpje gezien van een vega 64 op 100 % load? maar we mogen niet vergeten dat zonder de innovatie van amd de prijzen van nvidia kaarten omhoog schieten. een monopolist kan vragen wat ie wil. amd blijft een prima competitie in de midrange - markt. ik vind de vormgeving van de vega ronduit prachtig te noemen. laten we wel wezen.
RX Vega 56;1;0.594199538230896;"Vers van de pers: Prijs AMD Vega 64 wordt mogelijk 100 euro duurder. Prijs at launch = ""introductie prijs"". En dan hebben we het over de stand alone black editie zonder extra's. Sources: Als dit waar is dan ben ik persoonlijk erg teleurgesteld, vind het ook erg misleidend om kaarten te laten reviewen en dan een maand later de prijs omhoog te gooien als alle reviews al gemaakt zijn."
RX Vega 56;1;0.35413527488708496;Ik las dus ergens dat het niet eens een maand was maar zelfs alleen het release weekend
RX Vega 56;3;0.39682066440582275;Ik weet niet of die kortings actie die AMD bij de aankondiging noemde in NL ook werkt, maar dat zou het enige interessante scenario zijn, als je een Ryzen systeem wilde bouwen en toevallig een Ultrawide monitor ging kopen. Dan is het een mooie aanbieding, anders niet IMO.
RX Vega 56;2;0.5759281516075134;"1. Niet geldig in Europa: 2. En zo mooi is de actie niet. Je moet meer betalen voor wat korting op producten die ze kwijt moeten. Ze bieden een 1800X in de bundels en dat is juist de minst hardlopende Ryzen 7. En het scherm is nu niet echt een geweldig Freesync scherm. Er zijn nogal wat issues mee; o.a. flikkeren van het scherm als je Freesync gebruikt. Het is overigens een Freesync 1 scherm en niet een of ander nieuw Freesync 2 scherm."
RX Vega 56;3;0.42659828066825867;Dan is het wachten op de volgende generatie van Nvidia. AMD zal wel wat meer budget voor R&D hebben voor de opvolger. ASSP is goed en alles wordt verkocht voor het GPU segment en CPU's verkopen ze ook weer sinds kort...
RX Vega 56;2;0.45270049571990967;Niet als ik al iets vergelijkbaar heb.
RX Vega 56;2;0.3903331756591797;^ Goed punt. Mogelijk cruciaal punt zelfs. Maar de informatie eromheen is erg onduidelijk, het lijkt er zelfs op dat AMD gewoon maling heeft aan Nederland. Hopelijk de komende dagen wat duidelijkheid erover, want gratis games en dikke kortingen op schermen zijn toch sterke argumenten.
RX Vega 56;3;0.44023409485816956;Ondanks dat ik waarschijnlijk het antwoord al wel ongeveer weet ben ik wel benieuwd hoe deze kaarten presteren op het gebied van OpenCL rendering met bijv. Blender.
RX Vega 56;5;0.5475815534591675;Best goed
RX Vega 56;3;0.6375735998153687;Dat is inderdaad niet slecht. Zo te zien valt hier wel een mooie renderbak van te maken zolang je er maar een flinke PSU in zet.
RX Vega 56;2;0.3995489776134491;En dat is exact het probleem met de arch van AMD. GCN is gemaakt voor parallellisme wat je dus nu in die grafiek uitstekend goed kunt zien. Als het komt op games is het een compleet ander plaatje. Zelfs de Fury X is sneller dan de 1080Ti terwijl die kaart momenteel weer op games vlak goed scoord. Ik heb je foto even gejat ter verduidelijking van mijn uitleg op Als ik de review zo lees is het geen slechte kaart. Het presteert, weliswaar tegen een ietwat hogere wattage, maar niet iedereen MOET 144FPS hebben of MOET de laagste latency hebben. De games die ik speel (sporadisch) heb ik helemaal geen fikse eisen verder voor. Ik vindt het wel passen in mijn straks te bouwen HEDT systeem omdat ik vaak met video's ook werk. De Vega maakt gehakt van Nvidia in dat opzicht. De bruteforce power die die kaarten brengen is gewoon ongelofelijk hoog. Check de mining scores maar eens.
RX Vega 56;2;0.38948532938957214;De enige goede keuze is eigenlijk de RX vega nano want de rest ga je nooit terugverdienen. vega 64 liquid heeft verbruik van 350 watt maar de 1080 TI verbruikt 50 watt minder en is krachtiger.
RX Vega 56;2;0.4648289382457733;Maak daar maar 100 watt minder van. En ik lees hier in posts dat de liquid versie zelfs ruim 400 watt trok wanneer overclocked. En dan sporadisch in de buurt komen van de 1080ti. Er komen kleine versie's van deze kaarten iig ja, ik zag dat de PCB van de vega 56 maar de helft van de kaart bezet. De rest is voor het koelblok! Wat ook weer een hoop zegt natuurlijk. De nano versie zal dus behoorlijk lager geklokt zijn ook nog eens. En dan is een 1070 mini toch een stuk aantrekkelijker.
RX Vega 56;3;0.32878953218460083;Nou nee als TDP geen probleem vind dan klok je over. Ik draai liever op stock. Hoge TDP kan ool vetekenen dat fabrikant klok te hoog gekozen heeft. Dan klok je wat onder beetje Vcore minder en je draaid het als een nano.
RX Vega 56;3;0.5962677001953125;De linux prestaties zijn volgens phoronix wel goed. Met als interrant weetje dat de open-source opengl driver sneller is dan de closed-source. Daarbij zouden nog optimalisaties komen die het nog beter maakt.
RX Vega 56;2;0.4491075575351715;"Je moet wat duidelijker zijn want beide Mesa en AMDGPU (van AMD) zijn open-source. Er is geen ""de open-source driver"" aangezien AMD drivers nu ook in de kernel zitten. AMDGPU-PRO drivers zijn wel closed source maar zelfs AMD raadt het niet aan om die te gebruiken als je er geen specifieke nut voor hebt."
RX Vega 56;3;0.44540154933929443;Jammer dat het toch niet de kaart is geworden zoals ik had verwacht, de grootste teleurstelling zit hem toch in het stroom verbruik, De AMD Radeon RX Vega 64 8GB gebruikt bijna 50 Watt meer dat GTX1080 TI. Prestaties zijn opzich wel op orde, vind ik zelf dan. ik ben erg benieuwd hoe de prestatie per watt gaat zijn voor de refresh van deze architectuur. Zijn ze net terug op CPU gebied, lijken ze het een beetje te laten vallen op GPU gebied
RX Vega 56;2;0.39242202043533325;is 50W nu echt zo doorslag gevend? Na 20 uur kost het je 0.20 cent meer. Ik vind dat nog wel meevallen. AMD is de laatsten jaren op GPU gebied minder zuinig geweest. Voornamelijk in het High segment heeft AMD gewoon niet de middelen om zuinig chips te ontwikkelen en bakken. Maar deze zou wel erg interessant zijn voor miners. En dus met een beetje geluk komt de rx 570/rx580 weer een beetje op prijs. Wat wel weer goed is voor de consument.
RX Vega 56;3;0.3833271265029907;Wel als je je realiseert dat het appels met peren vergelijken is als je 'em vergelijkt met een GTX1080 Ti. Het daadwerkelijke verhaal is dat hij 120 watt meer stroom trekt dan een GTX1080 en niet beter presteert. Het gaat niet alleen om de kilowatturen, maar bij zulke verschillen komen ook een aantal andere zaken de hoek om kom kijken: de warmte die afgevoerd moet worden, de herrie tijdens gamen (60 dB is aardig wat), en de voeding die weer een stuk sterker moet zijn. Bovendien is nVidia's refresh ook niet heel ver weg meer, en daar mag je van verwachten dat de performance per watt weer gaat verbeteren. Onder de streep geef ik dan toch wederom liever mijn geld uit aan kamp groen.
RX Vega 56;2;0.4080345332622528;"De warmte en herrie is ook weer afhankelijk van de gebruikte koeler. AMD heeft niet voor niks een watergekoelde Vega 64 beschikbaar, die je qua adviesprijs voor hetzelfde geld haalt als een luchtgekoelde GTX 1080. Een budget-versie van een GTX 1080 met blower-koeler (laat staan de 1080Ti, die bestaan ook...) tikt net zo goed ongezonde geluidswaarden aan. Daarbij verwacht ik dat er heel veel gamers nog een flinke voeding hebben overgehouden aan de GTX400-serie of de R9 200-serie die ze ook nog hebben liggen. Terwijl CPU's alleen maar zuiniger worden. Een Vega 64 + een i7 op Z270 of een Ryzen 5/7 CPU moet prima lukken op een 500w tot 600w voeding van een fatsoenlijk merk, en dat is echt niks bijzonders. Waarop baseer je dat Nvidia's refresh dichtbij is? Er zijn geruchten geweest over een Pascal refresh, maar daar hoor je helemaal niks meer over (terwijl Nvidia normaliter wel wat lekt tijdens releases van de concurrent). En waarom zou Nvidia ook? De marges op Pascal zijn gigantisch; ze gaan denk ik liever met de prijs dalen dan een nieuw product ontwikkelen en lanceren. Sowieso, wat zou een refresh voor de consument opleveren? En Volta gaat echt niet eerder komen dan 2018, daar hoeven we ook niet bang voor te zijn."
RX Vega 56;1;0.3667930066585541;"Op de doos van de Vega 64 LC staat ""Minimum 1000W PSU"". Deze heeft ook 2x 8-pin power connector. 0:22"
RX Vega 56;1;0.40280935168266296;Op de doos van een GTX1070 staat ook dat je een 600w voeding nodig hebt terwijl het vaak wel lukt met een 350watter. Die getallen zijn enorm overdreven voor het geval iemand m aan een Trust voeding hangt.
RX Vega 56;2;0.34234195947647095;Een kwaliteitsvoeding zou er totaal geen problemen mee mogen hebben op 80% van het opgegeven vermogen te draaien: Het verschil in efficiëntie tussen 50% en 100% load is volgens de certificering slechts 3%. Op 600W zou dat dus een rendementsverlies van 18W betekenen. In jouw geval lijkt me dan dat onder normale omstandigheden (lees: alles behalve Furmark) een goede 500W voldoende zou moeten zijn, en 600W al een veilige marge heeft.
RX Vega 56;1;0.5435614585876465;Het gros van de geklommen marge's ivm mining-trend gaat naar de retailer, niet Nvidia zelf.
RX Vega 56;1;0.7252950072288513;Dat klopt. En dan nog weet Nvidia met bizar dikke marges weg te komen. Reken zelf maar eens met de kwartaalcijfers van Nvidia.
RX Vega 56;2;0.26766616106033325;Dat komt niet alleen door het succes van hun GPU's. *edit, hoewel dit natuurlijk hun brood blijft. Zet de koers maar eens op 5 jaar of max, bijna 8 keer in waarden gestegen de afgelopen 2 jaar. Heeft te maken met het uitbreiden van hun bedrijfsportfolio naar *mobiele chips, deep learning, chips voor auto's/autonoom rijden ect. Die zijn voorlopig nog niet klaar met groeien. AMD had zich op CPU's moeten richten, daar al in energie in stoppen. Laat de gpu's lekker gaan zou ik zeggen. Waar Intel schijnbaar concurrentie nodig heeft omdat ze anders lui worden, lijkt Nvidia vrolijk door te ontwikkelen. De gtx1000 serie is een gigantische vooruitgang ten opzichte van de vorige generatie, terwijl er amper concurrentie was. Nvidia heeft letterlijk volledig 'schijt' aan AMD. De verhoogde prijzen van de gtx1000 serie destijds, blijken nu kijkend naar vega, gerechtvaardigd te zijn. Want AMD heeft ongetwijfeld deze kaarten voor de laagste prijs mogelijk in de markt gezet. Ik werk uiteraard niet bij Nvidia, maar ik vermoed dat ze gekozen hebben voor juist een redelijk prijs en een véél groter marktaandeel (en dus meer verkochte kaarten tegen wat kleinere marge's). Tevens kijkend naar de 'performance' toename ten opzichte van vorige generatie. Aangezien een gtx1070 de 980ti met gemak verslaat met een veel lager verbruik.
RX Vega 56;1;0.4622740149497986;Het gros komt echt wel van de GPU's. Er zijn gedetailleerde cijfers te vinden daarover. Zoals je zelf al zegt: het brood komt echt van de GPU's af. Juist door die enorme winstmarges waar Nvidia mee werkt als praktisch monopolist hebben ze ook de ruimte om enorm te investeren in die groeimarkten. Dat maakt Nvidia een ontzettend welvarend bedrijf. AMD zou RTG inderdaad goed kunnen laten vallen. Door gebrek aan investeringsruimte is het zo goed als onmogelijk om Nvidia bij te benen, zeker als je gokt op het verkeerde HBM2-paard. Al moet je er als consument toch niet aan denken dat AMD dat zou doen... Dat zou echt een regelrechte ramp zijn voor videokaart prijzen. Edit: wij verschillen totaal van mening als we het hebben over hoe 'gigantisch innovatief' Pascal was (namelijk, Maxwell op 14nm, that's it) en hoe gerechtvaardigd de bijbehorende prijsstijging was (de consument betaalt gewoon net zoveel voor dezelfde prestaties. Enkel de Ti- en Titan kaarten kan je nu qua specs makkelijk inhalen voor minder geld. Het gros van de consument koopt in €200-€300 segment en die krijgen nu alleen een lager energieverbruik). 14nm zorgt voor een dermate kleinere oppervlakte van de GPU dat de kostprijs voor Nvidia fors lager ligt. Zie kwartaalcijfers. In plaats daarvan heeft Nvidia de prijzen verhoogd. De marges zijn niet verkleind maar fors verhoogd. En tuurlijk is dat super logisch vanuit Nvidia gezien, dat kan niemand ze kwalijk nemen. Maar als je stelt dat Nvidia geen concurrentie nodig heeft omdat ze toch wel 'doorontwikkelen' (Pascal refresh says hi), dan sta je heel ver van de industrie af en heb je geen idee waar we naar toe gaan.
RX Vega 56;3;0.3060751259326935;Ik vind eerlijk gezegd de stap van volwaardige desktop-GPU's naar laptops, met behoud van prestaties (toegegeven, enigszins afhankelijk van de implementatie), een van de grootste ontwikkelingen (lichtelijk overdreven zelfs revolutie) op de GPU-markt van de laatste jaren: voor het eerst is 1080p met alle toeters en bellen geen enkel obstakel meer voor een laptop (vanaf GTX1060), en zelfs 4K gaming is met terugdraaien van enkele instellingen prima mogelijk, ook voor actuele titels (vanaf GTX1070). OP desktop-gebied hoeft nVidia niet per se, aangezien de concurrentie momenteel ver achterloopt. Het feit dat ze zich op de mobiele markt hebben georienteerd geeft in mijn optiek toch zeker wel aan de nVidia een bedrijf is dat volop doorontwikkeld - de focus ligt alleen wat anders.
RX Vega 56;2;0.2316981852054596;Dat is volledig te danken aan de transitie naar 14nm, gecombineerd met hoe zuinig Maxwell standaard al is. Polaris zou ook prima in laptops kunnen eigelijk. Tuurlijk innoveert Nvidia wel. Het punt is dat AMD dat niet kan door gebrek aan geld. Het is een neerwaartse spiraal voor AMD als je Nvidia in die monopoliepositie houdt.
RX Vega 56;2;0.527820348739624;Amd heeft anders die stap niet laten zien naar 14nm. Om het alleen daar op te gooien is naief. En polaris vreet te veel voor wat het biedt in een laptop.
RX Vega 56;3;0.3127771317958832;Het probleem met amd hun ontwerp en videokaarten is dat Nvidia 2 snelwegen gebruikt met ieder 300kmph, en amd 4 snelwegen met ieder een maximum van 75kmph. Nvidia is hierdoor in de regel sneller en gebruikt veel minder brandstof. Echter op flinke workloads zoals bijv te zien is met video encoderen en alles wat baat heeft bij de gcn architectuur is amd weer heer en meester. Opzich geen verkeerde kaart, de prijs lijkt ook goed. Drivers zullen uitblijken of er nog winst te behalen is of niet.
RX Vega 56;2;0.46305373311042786;"Dit slaat nergens op Op welke snelwegen doel je? Het is namelijk overduidelijk dat AMD meer bandbreedte biedt over een bredere geheugenbus, mocht je daarop doelen... AMD biedt al jaren meer theoretische performance dan Nvidia op hetzelfde prijsniveau. Dat zie je in statische benchmarks en coin mining. AMD kaarten hebben al jaren meer tflop's door het gebruik van een grotere chip met meer shaders, waardoor AMD kaarten ook duurder zijn om te maken (Nvidia draait gigantische marges, bij Vega vraagt men zich af of AMD überhaupt verdient aan de kaart) en natuurlijk meer energieverbruik. Waar het verschil vandaan komt is eerder de softwarekant; Nvidia's architectuur wordt beter gebruikt dan die van AMD zo lijkt het. Wat natuurlijk een vicieuze cirkel is: meer Nvidia kaarten in de wereld = betere optimalisatie voor Nvidia = meer Nvidia kaarten in de wereld."
RX Vega 56;2;0.5218390822410583;Ja dat schrijf ik ook. De arch van AMD maakt het harstikke leuk in intensieve berekeningen zoals mining, video encoderen en noem maar op, maar het legt het af tegen het ontwerp van Nvidia. Nvidia daarintegen scoort weer slechter in juist intensieve berekeningen. Zie m'n eerdere bericht. AMD is goed in het bieden van een 4 way snelweg met daarop iedere baan 75kmph. Die van Nvdidia biedt echter maar 2 way met een maximum van 150kmph. Hierdoor is Nvidia met kleinere chunks van data sneller dan die van AMD. Echter veranderd dat plaatje zodra er grote datasets worden verwerkt.
RX Vega 56;3;0.5051873922348022;Maar wat als de AMD dadelijk 450 euro kost? Met 40 uur in de week kom je (met een kWh-prijs van 30 ct) uit op 12,48 euro per jaar meer. Ik denk dat je de aanschafprijs dus zeker in ogenschouw moet nemen. Je hebt dan wat mindere prestaties, en een hoger stroomverbruik dan een 1080, maar ook een lagere aanschafprijs, dat kan een drempel zijn.
RX Vega 56;1;0.7002155184745789;120 watt, oftewel 0,12 kilowatt, a 40 uur per week, 52 weken, tegen 30 cent per KWh... Dat is dus 0,12 * 40 * 52 * 0,3 is zo'n 75 euro. Per jaar... Of verreken ik me hier nou ergens gigantisch?
RX Vega 56;1;0.38307175040245056;30 cent is dan wel een dure leverancier betaal hier bij eneco al een stuk minder 17cent per Kwh
RX Vega 56;2;0.5017858147621155;Dat idee had ik ook al, maar heb de energieprijzen niet echt meer gevolgd dus gewoon gebruikt wat hierboven gezegd werd. Dan is het echter nog steeds een significant bedrag als je redelijk wat gamet dat je eigenlijk wel mee moet nemen als je de prijzen van de kaarten gaat vergelijken. En dat dan los van de hogere lawaaiproductie, of straks bij custom coolers (die niet de warmte naar buiten blazen) dat die warmte ook allemaal je kast in gedumpt wordt waardoor je potentieel op andere vlakken ook moet investeren in betere koeling.
RX Vega 56;2;0.35147857666015625;Dat verbruik -kan- de doorslag geven. Even bij @A Lurker aansluiten dat je appels en peren aan het vergelijken bent, maar we doen even een vlot rekensommetje op basis van de ~100W die ik tussen de Vega en de GTX 1080 zie: Vuistregelje, 100W bij 24/7 is ~200 euro, uitgaande dat je gewoon een normaal particulier tarief betaald voor je stroom (en we gaan even niet mekkeren over een paar procent, we rekenen makkelijk). Speel je gemiddeld elke dag een uur games, dan kost het je grofweg een tientje meer per jaar. In dat geval zijn de pro-Vega argumenten, eigenlijk vooral Freesync, meer dan genoeg. Game je elke dag tig uur.. tja. Het is niet gezond, maar er zijn genoeg gamers die dagelijks 8 uur aan het gamen zijn. Of je GPU dan full load draait terzijde, en AMD kan met hun Radeon Chill oplossing om de FPS te beperken wmb ophoepelen als argument, maar het is niet ondenkbaar dat er gamers zijn die dan letterlijk richting de 100 eur per jaar extra aftikken op hun stroomrekening. Nogmaals: gezond nee, maar het gebeurd wel. Ik heb ook een Freesync scherm, en met een paar uurtjes in de week (als het mee zit), is het verbruik voor mij geen zwaar wegend argument. Maar dat neemt niet weg dat het wel -het- blok aan het been van Vega is.
RX Vega 56;1;0.6218954920768738;"Ik ging even snel van die 50w die vertelt werd... dat was stom... 100 veranderd de boel al. En doet de ""gratis"" freesync teniet helaas."
RX Vega 56;3;0.291875422000885;Nou ja, dat hangt dus af van hoe veel games je speelt. Als je echt uren maakt, jazeker. Ik ben de laatste 3 jaar blij als ik 5 uur in de week haal
RX Vega 56;2;0.37244680523872375;ja 5 uur per week is niet veel.
RX Vega 56;3;0.562999963760376;Wel interessant onderzoek: hoeveel uren gamed de gemiddelde Tweaker nu echt? Ik denk dat ik geen uitzondering ben. Toegegeven: In mn school- en studietijd was het aantal uurtjes ieeeeetsjes anders.
RX Vega 56;1;0.4109719693660736;laten we maar niet doen, ben bang dat het heel confronterend word
RX Vega 56;3;0.3457232117652893;Ik schrok wel eens in games waarin je /played kon doen
RX Vega 56;1;0.6569101214408875;ik schrok pas toen ik op steam keek hoeveel speeltijd ik had in rift. dat is belachelijk. en ik speel het al heel tijdje niet meer via steam zelfs wist ook niet dat het zo hard kon oplopen qua speeltijd
RX Vega 56;5;0.4294995069503784;Ik speel online op de PS4. PC exclusives en singleplayer op PC. Online games nemen de meeste uren. Voor mij is Vega 56 niet een probleem.
RX Vega 56;2;0.38214147090911865;Dat zal van de leeftijd afhangen, toen ik nog een schoolkiddo was haalde ik uren per dag... Nu ben ik inderdaad ook blij als ik een avond even een spel kan opstarten. Echter is mijn beschikbare budget ook veranderd, waar ik destijds blij mocht zijn als ik na veel sparen een X700 Pro kon kopen (voor heul veul geld, en het ding was toen al wat ouder) kan ik nu als ik dat wil morgen de Vega betalen.
RX Vega 56;1;0.5105271339416504;Precies dat, plus de vraag of je er toen minder plezier van had dat je niet het duurste had maar af en toe genoegen moest nemen met wat minder wellicht. Enorm luxeprobleem nu met hardware, maar de tijd... PUBG gekregen van de dame een week of 2 terug, welgeteld 2 potjes kunnen spelen :-/ Komt AMD weer met Vega kaartje aanzetten enzo he, heel vervelend.
RX Vega 56;1;0.2859456539154053;Toen ik nog schoolkiddo was hadden we Pong...
RX Vega 56;5;0.2522166669368744;Ik zat met mijn famicom
RX Vega 56;3;0.41604408621788025;Volgens mij kan je een grafiek maken met de hoeveelheid funds tot je beschikking, en de hoeveelheid tijd die je hebt.... Hoe meer tijd, hoe minder funds. Hoe meer funds, des te minder tijd. Ik denk dat de Tweakers met een dikke 1080ti en een 16-core CPU juist degene zijn die het niet aan tijd hebben om 8 uur per dag te gamen. En ik denk dat de Tweakers die 8 uur per dag gamen, het toch moeten doen met een i5 + GTX 1060/1070.... En als je dan kijkt, dan maakt het stroomverbruik een stuk minder uit... Misschien zit ik er naast. Maar als ik naar mezelf kijk... Mijn PC is krachtiger dan ooit (en duurdere, meer high-end componenten) maar ik heb ook minder tijd dan ooit. 5 uur per week is hier ook een luxe
RX Vega 56;2;0.5314415097236633;Je vergeet de bespaarde kosten van de verwarming eraf te trekken, als je daadwerkelijk meerdere uren per dag speelt kan de verwarming sowieso een graden lager. Scheelt ook in de kosten Voor de casual gamer die een uurtje per dag speelt niet interessant maar de die hard 5+ uur per dag gamer kan het een verschil maken.
RX Vega 56;3;0.41730985045433044;AMD zou ook hier eens serieus op moeten focussen met hun nieuwe architectuur naar mijn mening. Het verschil wordt toch wel pijnlijk als je kijkt naar de huidige generatie GPU's van het groene kamp. *edit* De vergelijking is performance per Watt.
RX Vega 56;5;0.28707942366600037;"Dit maal ook letterlijk het ""groene"" kamp"
RX Vega 56;2;0.42164987325668335;Je bedoelt de NCU architectuur die ze nu gebruiken? Die is nieuw en support meer DX12 features dan Nvidia's Pascal. Het verschil zit 'm meer in softwareoptimalisatie dan in dat de architectuur van AMD slechter zou zijn, ofzoiets...
RX Vega 56;3;0.4166332185268402;Ik bedoel dat AMD rekening had moeten houden met stroomconsumptie met de Vega architectuur. Hopelijk doen ze dit wel met de volgende architectuur. Ik doel op performance per Watt, als je puur en alleen naar performance kijkt dan zijn het gewoon goede kaarten.
RX Vega 56;2;0.3577190637588501;Rekening houden? Je doet nu net alsof het een soort 'keuze' is Je moet bedenken dat door de huidige marktverdeling AMD amper geld heeft voor R&D. Nvidia investeert veel meer en maakt daardoor grotere stappen. AMD heeft simpelweg niet het budget om én zuiniger te zijn én sneller. En als je dan als AMD zijnde dan toch een keuze zou moeten maken tussen specs en performance per watt (waar de gemiddelde gamer die 10 uur per week speelt in de praktijk niks van merkt) dan snap ik heel goed dat ze voor specs kiezen.
RX Vega 56;4;0.4302313029766083;Of ze moeten Net als Ryzen even terug naar de tekentafel. En eens goed kijken hoe ze de High end GPU markt kunnen veroveren. Met CPU is het ook gelukt. Al heeft het ze wel 5 jaar gekost. Maar uiteindelijk is het ze gelukt.
RX Vega 56;2;0.3887574374675751;Met cpu is het AMD gelukt om de markt te veroveren...? Ryzen is nét uit en de enige reden dat ze zoveel hype hebben gecreëerd is omdat Intel sinds Sandy Bridge (2011!!) geen concurrentie van AMD heeft gehad en dus op deze manier 6 jaar lang heeft kunnen melken. Kijk maar eens naar de single core performance tussen een gelijkgeklokte Sandy Bridge (2011) en een Kaby Lake (2017). Het enige dat Intel hoeft te doen is elke serie iets zuiniger maken en iets hoger te klokken. AMD heeft in 6 jaar geen fatsoenlijk antwoord gehad en heeft het op single core performance nog steeds niet, dus gooien ze het over de multi core boeg. Helaas voor AMD heeft Intel daar een eenvoudig antwoord op: Coffee Lake i5 en i7 zijn hexacores en de i3 worden quadcores. Begrijp me niet verkeerd, ik wil niets liever dan AMD weer terug in de strijd bij cpu en gpu, maar zoals AMD de afgelopen 5 jaar is bezig geweest duurt het nog minstens 5 jaar voordat ze weer gezond mee kunnen strijden, tenzij ze natuurlijk in die 5 jaar weer stomme fouten gaan maken.
RX Vega 56;2;0.4895821809768677;Ik doelde meer dat Ryzen een goede concurrent is voor Kaby lake. En uiteraard is 5 jaar zonder zitten geen pretje. Maar door gaan met zulke power hunger GPU's is ook geen oplossing.
RX Vega 56;2;0.41039803624153137;Het probleem is geluid ... Als iemand dat een AMD 290X had en licht gedowngrade heeft naar een GTX 970, puur en gewoon wegens het geluid. Je hoorde de 290X veel meer omspinnen en op hogere toeren draaien, tot vervelend toe. Dat was ook mijn vrees doen de eerste Vega FF resultaten bekend werden op gebied van stroom verbruik. Ja, men zal er een aftermarkt cooler kunnen op smijten dat stiller is maar als we dezelfde cooler op bijvoorbeeld: 56 vs 1070. Welke zal meer lawaai maken? Die dat het meeste verbruikt. Je gebruik 50W als voorbeeld en ja, dat klinkt niet zoveel. Maar je vergelijk de TI en niet de 1080, waar de 64 tegen concurreert. Maar als je het in percentages uitdrukt: 1070 vs 56: 51% meer stroomverbruik bij AMD ... 1080 vs 64: 66% meer stroomverbruik bij AMD ... Dat betekend dat je GPU cooler, 51 tot 66% meer warmte moet verplaatsen. Aangezien beide dezelfde GPU coolers zullen gebruiken ( er is geen magische technologie dat AMD of Nvidia krijgt ), betekend het dat de coolers op de AMD kaarten meer wind ( = geluid ) moeten generen voor te koelen. Sommige mensen hebben geen last van fans met kop telefoons op maar niet alles dat je doet is gaming, wat je GPU kan doen opwarmen En dan nog het feit dat deze kaarten met Nvidia hun 1 jaar oude oplossing concurreren. Ik zie gerust een 1070Ti op 1 van deze dagen
RX Vega 56;3;0.26983121037483215;Als je kijkt wat sommige koelers met een Ti doen hoeft dat zeker geen issue te zijn. Neemt natuurlijk niet weg dat het niet ideaal is, en eerst zien, dan geloven
RX Vega 56;3;0.4573417007923126;Ik kwam er ook achter dat het niet 50w was maar 120w. Lang leven lui zijn en uitgaan van andere... en ja geluid productie kan een issue zijn. Maar voor de meeste after marker cooler zal het geen probleem zijn. Mijn r9 290 doet kwa geluid productie niet echt onder van de MSI 970 die mijn vrouw heeft. Ja in de Zomer is de extra wat niet fijn nee. In de winkel scheelt het wel in stook kosten😀. Nu maar hopen dat ze met drivers iets extra uit krijgen. Of dat miners er zo blij mee zijn dat ze de 570/580/1070/1080 links laten liggen. En er voor ieder wat te koop is.
RX Vega 56;4;0.4400290250778198;Ook geen probleem voor mij al ben ik wel benieuwd. PS4 en PC in de woonkamer dus er zit vaak iemand TV te kijken. PC Geluid staat voor gamen zacht.
RX Vega 56;3;0.4576805531978607;Voor miners niet echt gezien de niet overweldigende hashrates en het hoge wattage wat gezien de Nederlandse prijzen een aardige impact heeft op je winstgevendheid. Al zullen er genoeg miners zijn die wel een aantal van deze kaarten laten draaien doordat de GTX1070 amper te verkrijgen is. Hiernaast zijn er genoeg landen waar stroom amper iets kost en zullen de kaarten hoe dan ook winstgevend zijn. zie de review bij de buren:
RX Vega 56;2;0.5624951124191284;Dat is te simpel gesteld. Vega is zo'n stroomvreter omdat de GPU op hoge clocks word gezet zodat de 1070/1080 bij te benen is. Maar als je gaat minen zal je die kaart flink undervolten en de GPU enorm downclocken. Praktijktests hebben we nog niet gezien, maar in theorie is HBM2 (door veel minder, kleinere chips met minder afstand tot de GPU) zuiniger dan GDDR5. Tot slot lijkt de hashrate van Ethereum juist flink hoog te liggen - dit is vergelijkbaar met twee GTX 1070's voor Vega 64. Mocht je Vega 64 voor de adviesprijs kunnen krijgen, dan is dat een prima deal... HWI kijkt nu enkel naar Nicehash en concludeert niet voor niks dat hier softwarematig nog winsten te halen zouden zijn.
RX Vega 56;1;0.49779197573661804;De nederlandse prijzen van stroom hebben weinig invloed want die kaarten worden al verkocht voordat ze uberhaupt Europa binnen komen.
RX Vega 56;2;0.5510191917419434;"Ik denk dat je naar de totale package moet kijken, 50W in een vacuum is natuurlijk niet doorslaggevend. Maar 50W meer verbruik en ook nog +- 30% mindere prestaties is gewoon niet goed, een GTX 1080 Ti tegenhanger zal het iig niet worden (maar werd eigenlijk gezien de geruchten ook niet meer verwacht). Net als 120W meer verbruik en +- gelijke prestaties en nog eens een jaar ""te laat"" op het feestje komen is ook gewoon niet goed t.o.v. de GTX1080. Als je een jaar te laat bent moet je in ieder geval zorgen dat je een flinke klap sneller bent, dan zal er namelijk door de meesten ook niet naar stroomverbruik gekeken worden. Prijs en FreeSync v.s. G-Sync kan hier echter nog wel wat doen. Ik heb nu een 10tal reviews doorgenomen sinds 15:00 en de Vega RX64 lijkt gewoon een hard sell te worden, tenzij toekomstige verbeteringen de performance nog flink weten op te krikken. De Vega RX56 lijkt echter een stuk beter voor de dag te komen en mits verkrijgbaar (dit schijnt mogelijk nogal een probleem te gaan worden) een aardige kaart te gaan worden in zijn prijssegment t.o.v. de GTX1070"
RX Vega 56;1;0.457448273897171;"Sorry, maar AMD had een tijdje terug met de ""Cyprus?"" volgens mij een zuiniger ontwerp dan Nvidia, met de GTX280 ofzo..."
RX Vega 56;3;0.47223857045173645;Mogelijk is die tijd wel ja. Bedoelde meer afgelopen jaren. Zal het even aanpassen.
RX Vega 56;1;0.7908822298049927;Ik vind het meer uit principe een zware teleurstelling. Eén jaar en drie maanden na de 1080 komt AMD met een kaart die de 1080 nauwelijks kan bijbenen en ook nog eens meer verbruikt om dat magere resultaat te halen. Zoveel hype, zo'n enorme teleurstelling.
RX Vega 56;3;0.43615347146987915;Er komen nog driver improvements aan dus ik denk dat vega zeker nog wat beter gaat worden.
RX Vega 56;3;0.6623430848121643;De performance is in lijn van verwachting en optimalizaties zullen zeker nog positief effect hebben echter verwacht ik niet dat ze de 1080 TI van de troon kunnen stoten. Het zijn ook gewoon goede kaarten echter is het stroomverbruik gewoon hoog.
RX Vega 56;3;0.5290748476982117;Al is dat wel mogelijkheid. Drivers hebben nogal tijd nodig dus 3 a 7 drivers verder. NV haald uiteraad nu tegen maximale eruit om de release benches van Vega onderuit te halen. Voordeel van extra refresh die al langere tijd uit is. Deze vers architectuur anders dus kan het ook wat extremer zijn.
RX Vega 56;4;0.5221655368804932;Precies. Een 290x doet het hedendaags nog erg goed. Dat is bij de tegenhanger van nvidia van die tijd wel anders.
RX Vega 56;5;0.5943765640258789;50 watt meer dan een 1080Ti die ~30% sneller is. 120 watt meer dan een GTX1080 welke grofweg gelijk presteert. Je kunt 2 GTX1070's in SLI laten draaien op de powerconsumption van een RX64.
RX Vega 56;1;0.49660101532936096;Wilde net de andere der bij zetten maar je was me voor. Het is inderdaad een bittere pil die stroomconsumptie
RX Vega 56;2;0.46977153420448303;Mwah. Ik zie iets andere waarden: 1080 premium lucht: 188W gemiddeld, 235W top. (2X8p) 1080 premium water: 216W voor 2089mhz. (*Dit is sneller dan de Vega 64 op 1645mhz.) 1080 Ti middeklasser lucht: 246w gemiddeld (lastig om em echt >98% te krijgen) 298w top. 1080 Ti water: 267W gemiddeld (2030mhz) 312W top. (enkel in Synth) - hoewel na veel optimalisatie. Vega64 stock, aftermarket kaart: 288W peak 295W Vega64 OC: 312W, peak 345W (1645mhz) Hij gaat dus 100W over het verbruik van de 1080 (2e versie) héén, voor 5-8% tragere results buiten een aantal titels om. Hij gaat dus nog over een niet-geoptimaliseerde 1080Ti OC heen, voor 30% minder performance. Maar veel 1080Ti's hebben wel ongeveer dezeflde peak power use als een stock Vega64. Maar daar concurreert deze kaart helemaal niet mee. In een systeem met forse air cooler, scheelt een Vega64 vs 1080 (1e versie) al snel 5 graden op de CPU in idle na 30 min runtime, bij matige acceleratie. In games is het ook een goede 3-10 graden warmer. Hitman 2016 spelen op Vega loopt het hele systeem echt een heel stuk heter...
RX Vega 56;2;0.36189454793930054;"Het valt me op dat AMD deze kaarten heel stilletjes gelanceerd hebben invergelijking met de rx470/80. Ze wisten goed genoeg dat deze kaart geen potten ging breken. Amd is niet volledig kansloos. Apple maakt trouwens gretig gebruik van midrange kaarten in hun Macbook pro 15"" en Imac's en Mac pro's. Dat zijn geen game stations maar ze sturen wel als enige 5K schermen aan, zijn stil en zuinig. Heel wat anders dan deze Vega. Nvidia verdient goed zijn boterham met de gtx 1060/70/80 mede dankzij de mining hype. Zolang dag doorgaat is het niet ondenkbaar dat ze lang gaan teren op deze architectuur. Ze hebben helemaal geen haast, zeker nu AMD niet meteen met een ijzersterk alternatief op de proppen komt. In CPU-land liggen de kaarten anders. Intel moet een stevig tandje bijsteken door extra cores toe te voegen aan de volgende 'generatie' i5 en i7's. Maar ook hier scheert AMD spijtig genoeg geen hoge toppen in de gamingmarkt omdat de meeste games geoptimaliseerd zijn voor intel al is het verschil daar wel veel kleiner en soms is AMD zelf zuiniger en ook goedkoper. Maar canon lake ligt op de loer waardoor AMD het weer en zwaar weer kan komen als ze niet in de intel-pace kunnen lopen. Besef dat intel de ontwikkelingskost al lang heeft kunnen spreiden. Al bij al de de Vega 56 geen slechte kaart. Ze verbruikt maar 40 a 70 watt meer dan de gtx1070 in game en de kans is groot dat ze de gtx 1070 met gemak kloppen wat prestatie/prijs betreft. Mede omdat Nvidia gebukt gaat onder de miners Hype. Dat Amd iets minder zuinig is kan voor de consument nog wel eens positief uitdraaien in de huidige marktevolutie."
RX Vega 56;1;0.6565204858779907;Helaas is de Vega een flop gebleken, kaart kwam een jaar te laat op de markt en dat stroomverbruik kan hedendaags gewoon echt niet meer. Met overklokken kom je tegen de 400W aan, bij de LC FE Vega bleek het zelfs 440W te zijn. Dit is eigenlijk een kaart die het op had moeten nemen tegen de grote Maxwells, nu blijkt dus de, meer dan een jaar, oude Pascal buiten bereik terwijl Volta al om de hoek staat.
RX Vega 56;2;0.44819778203964233;Vega is geen flop, maar de prijs wel, knap wat AMD gedaan heeft met zo weinig uitgaven aan r&d, maar het is niet voldoende gebleken. Was de prijs laag genoeg geweest was het een mooie kaart en meer keuze voor budget en midrange. Maar nu blijft nvidia dominant en de prijzen zullen niet snel veranderen en heeft nvidia weinig haast met volta (tel daarbij op de mining hype= inflatie prijs en late verkoop van vega in b.v. NL). Alle hoop is nu voor AMD fanatici op navi gericht, maar 7nm... intel heeft een budget waar amd van kan dromen(r&d budget intel is 8 miljard meer p/j dan AMD aan inkomsten genereert..) Ik zie voorlopig intel nog niet komen met 7nm(3 tot 4 jaar nog) en AMD denkt in 2018 navi op de markt te hebben met 7nm(global foundries dan)? Ik gok wederom op uitstel. Helaas maar het ziet er nu uit dat AMD uit de race is bij gpu's, gelukkig hebben we ryzen/threadripper en is die markt wel weer open, maar gpu markt blijft zo stilstaan. no bueno voor consument.
RX Vega 56;2;0.4086465537548065;Vega is wel een flop, als je de prestaties afmeet tegenover het stroomverbruik. De prijs daarbij kan alleen nog maar meer pijn doen.
RX Vega 56;2;0.46671217679977417;De prijs is zeer pijnlijk en dit is nog maar reference, boardpartner prijs schijnt nog hoger te liggen. Nog pijnlijker is de hashrates, deze zijn niet zoals de gefantaseerde 70/100MH/s maar gewoon wat te verwachten was op basis van GCN met hogere klock. Mining hype zal ook wel mee vallen.
RX Vega 56;3;0.5764896869659424;Doet wel pijn btw. Heb altijd Radeons gehad maar als ze op deze lijn blijven zitten, ga ik serieus nadenken om over te stappen. De RX470 doet het voorlopig nog goed hier, dus er is geen haast.
RX Vega 56;1;0.5869601368904114;Waarom kijk je altijd dan naar het groene kamp, het maakt mij niks uit, gewoon welke het beste is?
RX Vega 56;4;0.4198865294456482;Natuurlijk, altijd de beste ongeacht de kleur. Voor mij niet perse de snelste, maar die kaart met de betere prijs/prestatie verhouding. Echter ben ik nogal gewend geraakt aan AMD's Control Center en alleen daarom al valt de keus toch telkens op AMD. Als ze nu qua stroomverbruik ver achterblijven heb ik echter een heel goede reden erbij om wél over te stappen.
RX Vega 56;1;0.2821822762489319;Jij en velen met jouw, maar de schuld ligt bij AMD en hun marketing.
RX Vega 56;1;0.4416520297527313;Neen, je vergeet een heel belangrijk ding - STROOMVERBRUIK. Ook al zou de Vega exact op het niveau van een 1080Ti presteren en net zo veel kosten, dan was deze alsnog niet interessant geweest, behalve voor de echte AMD fanboys. Ik zie al professionele reviews voorbij komen waarbij de non-FE Vega LC tegen de 500W kruipt na overklokken. Dat is 2x een GTX1080 terwijl de kaart in de meeste gevallen langzamer is dan een enkele 1080 (en dan ook nog eens de 1080FE, wie koopt die nog? Een grof meerderheid van de 1080's zijn allemaal custom PCB's die stock tegen de 1950-2000MHz klokken).
RX Vega 56;2;0.32877078652381897;Nee dat vergeet ik niet, AMD kan het gewoon niet winnen van nvidia met efficiency, het verbruik ligt hoger, de kaart was geen flop geweest als de kaart deze prestaties had en verbruik, maar simpelweg goedkoper was. Jij had het over of de kaart een flop was, dat is hij inderdaad met deze prijs, niet of hij goed overclocked, als je graag een spelletje speelt en goede gpu wil is het vermogen niet zo ernstig. Mensen kijken als eerste naar de prijs, als prestaties gelijk zijn en een aantal ook naar vermogen. Ik kijk er ook naar en ik weet wat het verbruik is, niet alleen is de kaart aan de prijs, met zo'n vermogen mag je ook meer uitgeven aan een aanzienlijk sterkere voeding, wat de prijs nog meer omhoog gooit (en daarnaast betaal je ook meer aan energie kosten per jaar, wat je tco ook omhoog gooit). Als je flink wat achter je pc zit en gamed kan dat flink oplopen. (gem 4 uur per dag tav 1080, p/j 35€ meer, of als je seti@home/boinc gebruikt 24/7 aan is het 210€ meer per jaar, dat is na 3 jaar al meer dan de kaart heeft gekost. Met mijn verbruik kom ik per jaar op ~100€ meer per jaar, meestal doe ik 3 a 4 jaar met een kaart, is toch een aardige meerprijs dan)
RX Vega 56;1;0.7838942408561707;Waarom krijgt deze post een -1? Het gaat niet offtopic toch, ik probeer het weer naar 0 te krijgen iig.
RX Vega 56;1;0.4888496994972229;Ik zou het geen flop noemen naar idd een jaar te laat op de markt. Het is eigenlijk niets dat de markt pushed. De Ryzen deed iets wat vele niet mogelijk acht. Echte 8/16 CPUs voor een schappelijk prijs. Ja, AMD kon niet winnen op Single thread maar ze gaven wel iets interessant terug op een ander gebied. Dat is wat er ontbreekt bij de Vega ... Eindelijk terug concurrentie JA! Maar tegen een kost. Die verbruik cijfers zijn pijnlijk. En het geeft ook aan dat AMD in de toekomst nog altijd problemen gaat hebben met verbruik/hitte op de hogere markt segment. Je zal maar weinig mensen een Vega in een SSF zien steken ( of AMD moet die grondig downclocken ).
RX Vega 56;2;0.4653911888599396;Nou, als je na een hoop bombarie ruim een jaar na de release van de 1080 van Nvidia zelfs die kaart niet weet te verslaan met je topproduct en ook nog eens behoorlijk meer verstookt, mag je toch wel van een flop spreken lijkt me. Daar weegt zelfs een aantrekkelijke prijs niet tegenop, en afgaande op de schattingen gaat die prijs ook niet echt aantrekkelijk worden.
RX Vega 56;2;0.4709688425064087;Nou vriendelijk prijsje zou fijn zijn maar ja HBM2 en mining gooit wat roet in. Denk dat volgend jaar de driver toppie zullen zijn en performance sowieso bump krijgt.
RX Vega 56;3;0.47639012336730957;Mooie kaartjes. Alleen dat stroomverbruik, dat blijft toch wel een dingetje hoor. Nog steeds te twijfelen tussen 1070, vega 56 of zelfs wachten tot 20/11 refresh van Nvidia.
RX Vega 56;2;0.4875134527683258;Zou voor de AMD RX Vega 56 gaan als je alleen kijkt naar de snelheid, de AMD RX Vega 56 is sneller in meeste spellen, vooral in DX12 spellen, maar hij gebruikt wel 75w meer dan de Geforce GTX 1070. Wel een grote teleur stelling de AMD RX Vega 56 en vooral de RX Vega 64, na 20 maanden na dat de Geforce GTX 1080 uit kwam hij nog steeds iets langzamer in de meeste spellen, ik had dus gelijk dat hij bijna gelijk zou zijn qua snelheid als de Radeon Vega Frontier Edition, klopt ook wel ze hebben de zelfde specs, zelfde GPU en geheugen, erg jammer voor AMD, en erg triest dat de RX Vega 64 wel tot 130w meer gebruikt dan de Geforce GTX 1080. Maar de drivers zijn wel beter geoptimaliseerd nu met de AMD RX Vega 64, en het scheeld maar een paar fps in de meeste spellen, en en ook anders om is de AMD RX Vega 64 een paar fps sneller dan de Geforce GTX 1080. Op Guru3D testen ze een stuk meer DX12 spellen (7 stuks), en daarbij is de RX Vega 64 gelijk aan de Geforce GTX 1080, een paar fps (2 a 5 fps) sneller soms En Guru3D had geen problemen om de RX Vega 64 goed te laten draaien op GTA 5: Edit spelfouten.
RX Vega 56;2;0.43568259477615356;Ligt eraan wat je een teleurstelling noemt, ik zie het al een aantal keer voorbij komen. Wel heel makkelijk om de vinger op de slome plekken te leggen. Maar vergeet niet dat AMD een veel lagere omzet heeft, en heel veel minder aan GPU research te besteden heeft dan Nvidia. Het is een enorme prestatie van AMD om Nvidia uberhaupt bij te kunnen benen. Laat staan een sprint voor hun geld te geven.
RX Vega 56;3;0.4004303514957428;Ik verwachte geen wonder wel een wildcard wat Goflo vs TMSC zou kunnen betekenen. Dit is AMD GF vs nV TMSC. Allemaal op verschillende flavor 14nm. Dan kan geen wonderen verwachten. Daarnaast ondanks kleinere R&D innveert AMD wel. Nadeel van innoveren is dat je later televant wordt. Gezien kip en ei probleem. Wat ik mij ook afvraag als die architectuur zo drastisch op de schop is. Zou de DX12 & Vulkan developers guideline dan ook niet veranderen. Denk dat ik voor een Vega 56 ga. Misschien is Vega betere mach met iNtel gezien Optane SSD voor helemaal diep en geavanceerd te gaan met zware caching en streaming open werelden.
RX Vega 56;1;0.6020915508270264;"""Denk dat ik voor een Vega 56 ga."" Tjah, daar zou ik nu niet voor gaan in ieder geval. Custom versies met goede koeling zijn er nog niet. Wat krijg je nu dan voor je geld: - heel veel lawaai - heel veel warmteproductie - duur in gebruik door heel hoog energieverbruik - hogere CPU-overhead in DX11 titels i.v.m. Nvidia - AMD drivers in plaats van Nvidia-drivers. Vaak geen verbetering. AMD zet de ondersteuning op een laag pitje door bijvoorbeeld W8.1. maar helemaal niet meer te ondersteunen. Is geen geld voor. - wordt waarschijnlijk duur in aanschaf. In de Benelux is de kaart nog helemaal niet te krijgen. Zodra die wel beschikbaar wordt, misschien wel woekerprijzen. Wat heeft de concurrentie te bieden? Nou, bijvoorbeeld de MSI GeForce GTX 1070 GAMING X 8G. Wat krijg je dan? - Gemiddeld gezien ongeveer Vega 56 performance (dit is een custom 1070 met hogere clocks dan referentie) - óntzettend stil. Idle onhoorbaar. Load: vanaf 50 cm onder de 30 dB(A). - heel weinig warmteproductie. Valt gemakkelijk te koelen in vrijwel iedere behuizing - goedkoop in gebruik dankzij lage energieproductie - lagere CPU-overhead in DX11 titels i.v.m. AMD - Nvidia drivers: naar mijn mening zijn deze van erg goede kwaliteit. En bij Nvidia is er ook meer dan voldoende geld om bijvoorbeeld W8.1 te ondersteunen. Nvidia maakt daadwerkelijk winst (en veel!) en er is geen enkele reden om aan te nemen dat de drivers slechter gaan worden. Nvidia heeft een héél groot team werken aan driverontwikkeling. - de kaart is goed verkrijgbaar. In vrijwel alle shops op voorraad. Prijzen schommelen wel een beetje. Enkele dagen terug nog voor 499 euro op voorraad verkrijgbaar, nu enkele tientjes duurder. Wie weet volgende week voor 489."
RX Vega 56;2;0.3291064500808716;Ze testen allemaal basis 1070 kaartjes. deze zijn in de praktijk 400mhz langzamer dan de AIB versies. Oftewel in dit geval ben je alsnog beter af met een 1070 op prestatie gebied. De prijs kan het nog niet op beoordeeld worden. De 1070GTX zou oorspronkelijk ook 399 euro kosten. We weten allemaal hoe dat afgelopen is.
RX Vega 56;3;0.3222670257091522;ja dat is waar, je hebt een grote kans dat bitmining er voor gaat zorgen dat ze duur woorden of niet te kaap zien, maar misschien ook wel niet, omdat ze veel meer stroom gebruiken dan de Geforce GTX 1060 en 1070 kaarten.
RX Vega 56;2;0.4269033372402191;Helaas heeft overclockers UK al aangegeven dat alleen de eerste batch kaarten zo goedkoop was. Was maar een tijdelijke RRP, de nieuwe boards zullen duurder zijn. Helaas
RX Vega 56;1;0.5259864926338196;Ja klopt had ik gezien, erg triest, nou woorden ze waarschijnlijk zelfs duurder dan de goedkoopste Geforce GTX 1070 en 1080.
RX Vega 56;2;0.43599647283554077;Helaas voor amd, als ze tussen 400-450 zijn, zouden ze nog mss nog intressant zijn.
RX Vega 56;2;0.5057462453842163;"Naar mijn inzien niet, voor dat geld (zelf iets minder) heb je een 1070. Vega56 heeft vrijwel dezelfde prestaties tegen een veel hoger stroomverbruik. Vergeet niet dat die stroom als hitte wordt afgevoerd en je dus ook meer lawaai van zowel je kaart, als voeding krijgt. Zelfde geldt voor Vega64. Vermoed, helaas, dat AMD weer het pad volgt van high end chips tegen mid/low range prijzen en we dit Vega spul nog wel drie ""generaties"" gaan zien. Nvidia zou dit hele product weg kunnen vagen door freesync te ondersteunen."
RX Vega 56;3;0.4260849952697754;Het probleem voor AMD is meer dat als ze het prijstechnisch heel interessant maken, Nvidia gewoon ook met hun prijs daalt. Dus ze zullen er echt wel over hebben nagedacht wat de beste prijs is, waarbij ze zichzelf niet uit de markt prijzen maar wel ook nog winstmarge hebben. En als je Freesync/G-sync wil dan is AMD nog altijd wel een goede keus. Lawaai ligt wel eraan wat voor een koeler erop zit. Uiteraard, alle extra vermogen moet ook weer gekoeld worden. Echter ik heb zelf een R9 Fury (Waarom? Was goedkoop tijdje geleden, en al helemaal significant goedkoper dan een 1070 + G-sync monitor). Kaart verbruikt een hoop, maar zit geleverd goede koeling (weegt ook een hele hoop), dus passief gekoeld in idle, en prima stil onder load. Het veranderd uiteraard niet dat minder vermogen altijd stiller is onder gelijke condities dan meer vermogen. Maar tegelijk blijft lawaai ook sterk afhankelijk van de koeler.
RX Vega 56;3;0.29549452662467957;"Hier houdt zich ook een R9 Fury koest in mijn kast. Nog nooit zo een ""premium"" product voor in m'n PC gehad wat dat betreft. Zoals je stelt weegt het apparaat een hoop, maar de koelprestaties zijn top of the line. Indien Sapphire met een dergelijke koeler voor op de Vega kaarten komt zou ik hem ook direct aanbevelen, mits je goede koeling in je kast hebt om de hitte er uit te werken. Een stofzuiger zou ik nooit meer willen, en ik ben ook bang dat na deze kaart het alleen nog maar tegen kan vallen in de toekomst."
RX Vega 56;2;0.41538137197494507;Ik vind het vooral gek dat de rx 64 met 12 terraflops de 1080 niet vloert. Misschien is er nog ernstige marge voor verbetering met driver updates?
RX Vega 56;1;0.45513492822647095;Pixel rate van de VEGA is 86.4 GPixel/s en van de GTX1080FE is 102.8 GPixel/s. Dit is deels een verklaring. Verder zijn de drivers van NVidia een stuk volwassener.
RX Vega 56;2;0.3894832730293274;"Pixelrate is idd een zeer goede graadmeter om te weten hoe je kaart presteert. (Kijk bijv. maar naar die tabel-benchmarksites). Als je daar de pixelrate pakt en naar de ""echte"" benchmarksites toe gaat en vergelijkt zie je dat de prestaties aardig kloppen tov de pixelrate. Kortom; zeer teleurstellend dat AMD met pakweg 70% méér transistors niet eens de GTX1080 kan verslaan."
RX Vega 56;1;0.43592604994773865;Wat een enorm stroomverbruik zeg in verhouding tot de GTX serie. Ik vind dat Tweakers in de conclusie vermelding had moeten maken van het enorme stroomverbruik van deze kaarten ingame. Het verschil tussen de AMD Radeon RX Vega 64 8GB en de Nvidia GeForce GTX 1080 is ruim 120 watt. Als je 5 uur per dag gamed is dat al gauw €35 per jaar. Dat doet een groot deel van het prijsverschil teniet zeker als je de videokaart meerdere jaren gebruikt.
RX Vega 56;2;0.4448911249637604;Het stroomverbruik is minder een issue. Je merkt dat niet zoveel dat beetje extra geld. De hitte dat die stroomverbruik veroorzaakt is meer een issue. Want dat merk je wel iedere dag 5x365 in de vorm van sneller draaiende fans... We spreken over 50 a 60% meer hitte productie en bijhorend geluid tegenover de concurrentie. De dagen dat een goede en snelle GPU klinkt zoals een jet zijn al lang gedaan en dat is AMD hun grootste fout vind ik.
RX Vega 56;2;0.5705220699310303;Het klopt dat het een relatief klein bedrag is op je stroomrekening. Wat AMD doet is twee kaarten in de markt zetten die gewaagd zijn aan die van Nvidia maar tegen een stuk lagere prijs. Dat klinkt als een voordeel maar als dan het stroomverbruik bij intensief gamen tientjes meer kost op jaarbasis dan valt dat gewoon weg. Afhankelijk van hoe intensief je de videokaart gebruikt valt het voordeel in aanschaf steeds meer weg. Dat geluid en hitte een groter probleem zijn heb je groot gelijk in. AMD scoort niet goed op geluid en hitteproductie. Dit is al jaren zo en ze zouden er goed aan doen het een keer op te lossen. Ik moet wel erbij vermelden dat basiskaarten zelden goed scoren in geluidtesten en koeling. Kaarten van fabrikanten als Asus/MSI/etc zijn een stuk stiller en koeler. Mijn conclusie is dat Nvidia de zaken op gebied van energieverbruik vs geluidsproductie vs snelheid gewoon beter voor elkaar heeft. AMD heeft naar mijn mening een lange weg te gaan.
RX Vega 56;1;0.5481764674186707;Een gtx kaart verbruikt ook wel meer dan er op de doos staat hoor... Iig heb genoeg mensen erover gehoord.
RX Vega 56;2;0.3484223783016205;Nee, tenzij je de powerlimit verhoogd, anders blijven ze netjes binnen opgegeven TDP. Zie ook:
RX Vega 56;3;0.6051778197288513;Ben redelijk pro-AMD, maar vind het lastig om hier (zoals sommige reviewers doen) een positieve draai aan te geven. Deze kaarten komen 2 jaar na de Fury X en bijna 1.5 jaar na de 1070 en 1080. De chips zijn veel groter en het verbruik ook. De prijs is daarnaast ook niet heel spectaculair. Deze kaarten zouden voor 300 en 400 euro MAX in de winkel moeten liggen om nog enigzins interessant te zijn. Er is maar één reden om Vega aan te schaffen, en dat is omdat je een freesync monitor hebt. Ik zie er zelf nu misschien wel van af, er zijn nu 3440x1440 monitoren met gsync die net zo duur zijn als hun freesync tegenhanger. Misschien dat AMD nog wat winst kan behalen met hun drivers. Ik weet niet wat ze anders hebben gedaan met de hele nieuwe architechture.
RX Vega 56;3;0.2735064625740051;Er is een duidelijke positieve (Freesync) en negatieve (verbruik) draai aan dit hele Vega verhaal. De afweging ertussen moet je zelf maken, maar die lijn lijkt vrij duidelijk in elke review die ik tot nu toe heb gezien. De uiteindelijke prijs kan vervolgens de zegen of de genadeklap zijn, maar dat is voor iedereen koffie dik kijken, persoonlijk laat ik de prijs liever zo veel mogelijk buiten de details, want achteraf corrigeren is dan een draak. En wat optimalisaties betreft, die lijken vanzelfsprekend zoals in elke eerdere release, zowel Nvidia als AMD. AMD is weliswaar al even bezig met de kaart, maar vooruitgang zien we meestal toch pas wanneer een bepaalde chip echt breed verspreid wordt. Maar ook dat : beetje koffie dik kijken.
RX Vega 56;2;0.43092963099479675;"We hebben NiceHash Miner (2.0.0.12 beta) gebruikt maar de meeste algoritmes in de benchmark werken nog niet op de Vega-kaarten. Claymore en Optiminer worden niet gedraaid met de melding ""Needs Benchmark"", van DaggerHashimoto heb ik de prestaties hier gemeld, zo'n 30-34GH/s. Gezien de tijd die we met de kaarten hadden (drie dagen, give or take) en het feit dat miningsoftware nog nauwelijks werkt (en vast niet optimaal) gaven we prioriteit aan de game-benches voor de review."
RX Vega 56;4;0.4204646646976471;Fair enough, bedankt voor je antwoord!
RX Vega 56;1;0.7130488753318787;En hoe gaat het minen met deze kaarten? Spelletjes spelen via een desktop interesseert me helaas helemaal niks..
RX Vega 56;5;0.31625843048095703;Zie mijn reactie hier
RX Vega 56;2;0.4203258156776428;Nja maar dat is nicehash haha Claymore heeft ondertussen al een ''tijdje'' de VEGA ondersteuning ingebouwd. Nicehash is in ieder geval niet de tool die je gebruikt om te minen. Meer een gemak voor mensen die niet meer kunnen dan 2 muisklikken maar toch willen minen, daar is nicehash goed voor. Niet voor mining (snelheid) resultaten. Oftewel: claymore testen, zcash of ethereum, miner tunen, clocks tunen etc etc, allemaal onderdeel van minen.
RX Vega 56;5;0.4379597306251526;Nee, maar nicehash was zo vriendelijk om snel even verschillende algoritmes te testen Ik heb net claymore op het testsysteem gezet, morgen even kijken wat ie doet. Ik heb in het verleden litecoins geminded, tunen duurde lang Nu nog geen tijd voor gehad, morgen even verder spelen
RX Vega 56;3;0.44124993681907654;Er komen wekelijks wel coins uit die zeer goed te minen zijn, wat de toekomst wordt weet niemand (zelfde zoals bij Bitcoin, Ethereum, Dash, Zcash etc etc). Ethereum levert ook wel wat op, persoonlijk mine ik al maanden geen Ethereum meer, wel andere coins met meer profit. @willemdemoor Gelukkig valt er niet heel veel te tunen bij Claymore, is qua basic al 1 van de snelste. Wel zijn er enkele instellingen die je kan toevoegen / aanpassen (moet binnen 30 minuten wel klaar zijn), clocks zit ook niet veel tijd in, zodra je ziet dat je TEveel overclockt terwijl de prestaties uit blijven /minimaal verbeteren weet je genoeg.
RX Vega 56;2;0.5069082975387573;Levert steeds minder op door de Difficulty bommen en natuurlijk afhankelijk van de valuta prijs. Tenzij je stroom gratis is is het nog steeds beter je geld in crypto te stoppen ipv mine apparatuur. Ga maar eens kijken wat een beetje rig je gaat kosten, stroomkosten en de stijgende moeilijkheidsgraad. Nee, ETH is alleen nog leuk voor de mensen met veel hashing power en goedkope of gratis stroom. Andere coins zijn er wel alhoewel je bij sommige nooit weet wat die je later gaan opleveren. Ik voorzie door deze downtrend dat de Gfx cards goedkoper gaan worden en op MP belanden, voor de hobbyist als zakcentje erbij is het met de dag minder interessant.
RX Vega 56;2;0.3832831084728241;Had die conclusie inmiddels ook getrokken na wat googlen idd. Makkelijker om in te spelen op een wisselkoers, wanneer het niet rendabel is blijf je er simpelweg uit de buurt. Met een mining-rig heb je al behoorlijk wat uitgegeven en dan nog maar zien of je het terug verdiend. Ik hou het wel bij een zelf-studie cryptocurrency . Heb wel wat 'ripple' gekocht, maar dat is weer een heel ander principe.
RX Vega 56;3;0.4507538378238678;Ripple is mischien op de lange termijn wel wat waard maar door de enorme aantallen coins valt het nog te bezien. Mooiste is coins uit te zoeken die daadwerkelijk een ICO overleven en vlak voordat ze echt gaan stijgen kopen. Je kunt dan makkelijk een 10x-1000x pakken en dus wat verdienen zonder enorme bedragen in te leggen. Het wordt met de dag moeilijker omdat er veels te veel coins bijkomen en zie in die jungle dan maar iets te vinden wat geen scam is, geen hype is, of geen pump coin. Al met al blijft het een crypto casino.
RX Vega 56;1;0.23689031600952148;Dat was me inmiddels wel duidelijk ja, met het recente nieuws. Wat een chaos! Toch wonderbaarlijk dat de koerzen maar blijven klimmen. Het vertrouwen is er nog ruimschoots begrijp ik. Het is nog een beetje volwassen aan het worden. Zo als vroeger de handel in grondstoffen ook chaotische tijden kenden, en nog moest stabiliseren.
RX Vega 56;1;0.5720072388648987;Crypto staat in weze in het jaar 1994 nog voor de .com boom ... Zo moet je het zien. De enige die nu zich bezig houden met crypto zijn wat geeks in weze want de reguliere bevolking weet niet eens wat crypto currency is. Het woord Bitcoin hebben sommige wel van gehoord maar meer ook niet. Het is absoluut geen mainstream, net zoals internet in 1994 onbekend was voor de 'gewone' mensen. Pas vanaf 1996 kwam het een beetje op en 2 jaar later gingen de mensen een beetje online. Het was rond 1999 dat internet echt mainstream werd voor iedereen. Kortom, er is ruimte zat voor groei. De vraag is waar zit de groei ? Welke coins ? Kijk, Bitcoin is de Grootvader der Crypto maar het is absoluut een Bullshit coin technisch gezien. Zelfs nu na de Softfork (Segwit) is het niet merkbaar sneller en in de huidige configuratie is het ONMOGELIJK om het ook maar enigzins zinvol te gebruiken als betaalmiddel voor het dagelijks leven. Je kunt niet bij de kassa staan en 20 minuten moeten wachten op confirmations, en dan is dat nog snel en dan praat je op dit moment maar over een kleine groep mensen (relatief gezien) die het gebruiken. Als het massaal gebruikt wordt zoals wij nu pinnen dan duurt het dagen tot weken. Kortom, BTC is de benchmark en prima voor investering op de lange termijn en als spaarrekening zeg maar. Als betaalrekening voor regulier gebruik is het totaal kansloos. Zelfs het vergroten naae 2MB van de Block (nu 1MB) zal daar niks aan veranderen. Voor het betaalverkeer van de toekomst zal echt een andere crypto de boventoon voeren, mischien wel XRP (Ripple) want dat is wel erg snel.
RX Vega 56;2;0.3336322605609894;Je stuurt deze leek iig de goeie kant op, in mijn drang naar informatie omtrent dit onderwerp. Het hele betalen met bitcoin gebeuren kan ik dus nog met een korreltje zout nemen. Je kunt bijvoorbeeld bij 'thuisbezorgd.nl' betalen met bitcoin. Je kan dan eerst een half uur wachten tot de betaling rond is, en dan nog een uurtje op je eten. Erg praktisch . Bitcoin heeft dus een hoop in gang gezet, vooral de afgelopen jaren. Echter is het het meer een opzet naar een volwassen cryptocurrency, dan dat bitcoin dat zelf is begrijp ik. Morgen in de pauze weer verder neuzen.
RX Vega 56;2;0.32065409421920776;Zoals het er nu naar uit ziet zal de Vega 56 met een andere koeler (en pcb ontwerp) de enige slimme keuze zijn. Al zou het me niet verbazen moesten ook die kaarten belachelijk duur zijn vanwege miners.
RX Vega 56;3;0.525895893573761;"Beetje to little, to late gevoel bij deze release. Qua prestaties doet de kaart het niet verkeerd (tussen 1070 en 1080 in) maar het stroomverbruik is mega in verhouding met een GTX1070 of 1080 (120 watt meer!). Zie het nut van HBM2 geheugen ook niet helemaal; technisch mooi maar Nvidia levert meer prestaties met (veel goedkoper??) DDR5X?. Prijs en verkrijgbaarheid zijn belangrijk voor deze kaart, ben bang dat het minen daar erg veel invloed op gaat hebben. Het is helaas (voor AMD en ons consumenten) geen klapper om de concurrentie echt serieus wakker te schudden, wat Ryzen en Threadripper zeker wel is."
RX Vega 56;2;0.6358044743537903;Wat ik heb kunnen zien is het minen helemaal NIET interessant met dit ding. Dure kaart vergeleken met andere kaarten die ongeveer hetzelfde presteren voor duidelijk minder geld en ook duidelijk minder verbruiken. Minen met kaarten die stroom vreten is niet echt een geweldig idee.
RX Vega 56;3;0.6843534111976624;Vega scoort wel beter bij mining dan het scoort bij gaming, maar idd, het stroomverbruik maakt ze voor mining wellicht minder interessant. ( misschien is dat wel de redding voor de gamers onder ons, minder vraag door miners is mogelijks minder schaarste op de markt )
RX Vega 56;3;0.40775418281555176;Wat is beter met mining ? Een paar MH/S ??? Het moet echt een flink verschil zijn voordat men dus aan de Vega zou gaan. Zeker als het verbruik hoger ligt dan ben je je winst voordeel qua snelheid weer weg door extra stroomkosten. Ik denk dat de populaire kaarten NU nog steeds schaars zullen blijven want daar weet men wat ze eraan hebben en het verbruik is sowieso beter. Voor miners zijn die onder de streep effcienter.
RX Vega 56;2;0.5052624940872192;Dus net iets goedkoper dan de 1080, net iets langzamer en bijna 2x zo hoog stroomverbruik. Teleurstellend. Enige winst die je hebt is dat je geen gsync belasting hoeft te betalen bij een freesync monitor
RX Vega 56;4;0.5896603465080261;Leuke kaarten. Nu nog goede drivers en dan kan performance nog 10-15% omhoog. Alleen stroomverbruik valt me tegen. Maar dat is green bezwaar. Als prijs 50-100 euro onder de 1080 ligt is het een aanschaffertje.
RX Vega 56;2;0.5082950592041016;Prestaties an sich zijn wel prima. Maar een jaar te laat, te hoog verbruik en ook nog eens een paper launch.
RX Vega 56;4;0.5279316902160645;Als ze er een goede prijs aan kunnen binden zijn het mooie kaarten. Was natuurlijk nog mooier geweest als er ook echt een concurrent voor de 1080ti zou komen. Maar als concurrenten van de 1070/1080 zijn het leuke kaarten. Vooral als er goede drivers uitkomen, die de prestaties nog net even wat omhoog helpen.
RX Vega 56;3;0.6416676044464111;Het verbruik is wel flink hoger . Opzich wel te verklaren met 12,5 miljard transistors tov van de 7.2 miljard van de gtx1080. Maar het is wel jammer dat de performance dan niet eens hoger is met zoveel extra transistors. Ben benieuwd of driver updates nog veranderingen kunnen brengen, je zou zeggen dat er in elk geval genoeg hardwarematige horsepower aanwezig zou moeten zijn.
RX Vega 56;3;0.5028964877128601;Gelukkig duurt het niet zo heel lang meer dan GloFo 14 nm+ (voor AMD) uitkomt. Dit gezegd hebbende duurt het ook niet heel lang meer dat TSMC 12 nm (voor NVidia) uitkomt. AMD is goed bezig maar ze zijn er nog niet op GPU gebied.
RX Vega 56;2;0.4689546227455139;"Zucht... AMD heeft echt de boel goed verkloot met Vega. De prestaties zijn ondermaats, het stroomverbruik te hoog, de koeler is nog steeds ruk en ze hebben het te hard lopen hypen. ""Poor Volta""? Poor Radeon afdeling eerder. 15 Maanden achterlopen, aantal keer uitstellen, beloftes doen en uiteindelijk de standaard ""too little, too late"" doen waar we AMD van kennen. De R9 290 was een super interessante kaart, maar ook niet zonder een enorm hoog energieverbruik en een super luidruchtige koeler. Als Nvidia ooit Adaptive Sync activeert op z'n kaarten dan stap ik over. De enige reden dat ik nu nog een AMD-kaart heb is vanwege m'n Freesync monitor. Ohja, en qua prijs: ze zullen de Vega 64 voor ~500,- in de markt moeten leggen wil het interessant zijn. Vega 56 voor ~400,-. Ze zullen het echt van de prijs moeten gaan hebben."
RX Vega 56;1;0.350058376789093;5u/d....365d/j gamen. Dan heb je een ander probleem dan een hoog stroomverbruik.
RX Vega 56;3;0.34267574548721313;Wat voor probleem zou dit dan zijn? Ben ik wel benieuwd naar
RX Vega 56;5;0.41269728541374207;versleten muismatten enzo
RX Vega 56;5;0.33458781242370605;Zie die 390 nog gaan dan, wat is die 290 een legendarische kaart zeg!
RX Vega 56;2;0.40992629528045654;Ik mis de benchmarks voor Optiminer, Claymore miners, e.a. Wat zijn de haalbare hash rates voor Ethereum en Zcash? Hashrate - Power grafiek bij diverse undervolt settings? Vergelijking met andere kaarten? Gezien de run op GPUs door miners lijkt me dit ook interessante aspecten in een review.
RX Vega 56;2;0.46756187081336975;Kunnen de gamers eindelijk weer een kaart kopen. de 570 en 580 zijn echt veel te hoog geprijsd door de miners, en als je het ervoor wilt betalen dan zijn ze niet op voorraad. Hopen dat de mingsoftware een leuke optimalistatie krijgt, dan hebben we weer videokaarten met een echte 2ehandse prijs.
RX Vega 56;4;0.2744351923465729;Ben vooral benieuwd naar het verschil qua hashrates (mining) tussen 56/64 en de 1080(TI). Een idee om dat ook te gaan testen?
RX Vega 56;1;0.5688161849975586;"Promotie filmpje van half jaar geleden ""Warning poor Volta"" kan niet eens Pascal aan... geweldige marketing weer AMD, hoe goed ze het met Ryzen en Threadripper deden hebben ze hier toch echt een fail product, heb al meer dan een jaar een 1080 en die is beter als je verbruik ook rekent dan de snelste Vega helaas blijft de 1080 dan naast de Threadripper, volgende keer beter mag ik hopen."
RX Vega 56;3;0.40541872382164;Doe eens een gooi naar perfomance per € - geeft een iets frisser beeld Net als Ryzen reviews overal het vingertje naar intel cpus die 3x de prijs zijn en dan zeggen die is toch wel beter heh XD Qua prestaties zal er zeer waarschijnlijk wel behoorlijk winst behaalt worden met driver verbeteringen - en die winsten tov nvidia zijn vallen meestal wat beter uit ook historisch gezien. bijvoorbeeld van een paar jaar geleden.
RX Vega 56;2;0.5332320928573608;Maar de performance per watt is dan weer rampzalig. Vooral rekening houdende dat ze op 14nm zitten en dus veel zuiniger zouden moeten zijn.
RX Vega 56;2;0.5244312882423401;Had Tweakers gesierd ook niet basis modellen 1070s en 1080s te testen. 1070s bijvoorbeeld die betere koeling hebben tikken al tegen de 2ghz aan zonder over te clocken door GPU boost in plaats van 1600+ mhz. Dan is de Vega 56 in eens helemaal niet zo geweldig meer een jaar na de release van de 1070. In da topzicht is deze launch dus voor mij een teleurstelling. Soortgelijke prestaties ruim een jaar na de launch van wat het moet verslaan maar dan zeer minimaal. Ik wens AMD het beste maar hierbij hebben ze echt te hoog van de toren geblazen en het is maar afwachten wat er met de prijs gebeurt dankzij de miners. Het enige waar deze kaarten leuk voor zijn, zijn compute toepassingen waar Nvidia de laatste jaren enigzins schijt aan heeft en duurdere kaarten pushed.
RX Vega 56;3;0.5507100224494934;tja er zijn nog geen custom modellen van Vega dus niet echt een eerlijke vergelijking dan.
RX Vega 56;3;0.34489908814430237;Met de powerdraw die de kaarten nu al hebben verwacht ik geen mega oc potentie.
RX Vega 56;5;0.29661038517951965;Waarom testen jullie niet met PU:B als game? Is nu de populairste game zo'n beetje op de markt. Had me leuk geleken om ook daar wat van te zien.
RX Vega 56;1;0.46363240480422974;omdat AMD daar slecht in presteerd en het een kleine developer is die de game niet geoptimaliseerd heeft voor AMD kaarten.
RX Vega 56;2;0.4968079924583435;omdat elke ronde anders verloopt en dus niet consistent onderling getest kan worden. Daarnaast zijn er ook geen benchmark tools in de game aanwezig.
RX Vega 56;2;0.41435888409614563;wie kan mij helpen. ik moet een videokaart hebben, en wilde een 580 hebben, maar die prijzen vind ik te belachelijk voor woorden. nu kan ik via een medetweaker een r9 390x krijgen voor een nette prijs maar zit ik tegen het verbruik aan te hikken van deze kaart. blijf wel het liefst bij amd vanwege het feit dat ik al een ryzen 1700x set heb, en het dus wel leuk vind om alles onder amd te houden (nee, geen fan, maar vind het een lekker idee om alle drivers onder 1 merk te houden) de vega 64 word m zo ie zo niet aangezien ik die bij Caseking voorbij zie komen voor 600+ als inloop prijs, vega 56 zou optie zijn, maar hoeveel verschil maakt het nu werkelijk tussen deze 3. de 390x, 580 en de vega 56 qua verbruik?
RX Vega 56;2;0.39975032210350037;Je geluids, netwerk, chipset drivers zijn toch ook niet van AMD? Sorry, maar je maakt het jezelf wel onnodig lastig met zulke voorwaarden. Er is geen enkele goede reden te verzinnen waarom je een GPU van hetzelfde merk als de CPU moet hebben, behalve in je hoofd
RX Vega 56;3;0.3790535628795624;is dat waar? volgens mij zijn er genoeg geruchten dat nvidia op ryzen systemen niet optimaal presteren (of het waar is of niet weet ik niet, dus ga ik ook geen uitspraak over doen, maar volgens mij is daar wel degelijk sprake van toch? ) daarnaast is een free sync monitor aanschaffen een stuk goedkoper dan de nvidia tegenhanger, dus volgens mij is die reden niet zo gek als jij laat voorkomen volgens mij
RX Vega 56;2;0.51807701587677;Denk dat je de geruchten niet goed leest en/of ze zitten er naast: Nvidia's drivers doen het niet lekker met 6 of meer cores, bij beide CPU kampen. Daar heeft Nvidia idd nog werk te verrichten, maar het is dus een algemeen driver probleem en niet toegespitst op alleen Ryzen. De monitor is een ander verhaal, die is niet afhankelijk van cpu die er gebruikt word. Voor freesync heb je uiteraard wél een AMD GPU nodig. Dat is idd een valide keus, al kies ik persoonlijk eerder voor minder stroomverbruik. Als het nou om 10~30 watt ging, ala. Hier praat je over 100+watts, dat is toch geen kattepis. Je hoeft natuurlijk geen sync te nemen (van wie dan ook), het kan ook prima zonder. Heb er zelf nog geen behoeft aan, maar dat kan voor een ander anders zijn natuurlijk.
RX Vega 56;4;0.42853760719299316;Vega 56 heeft een hardlock van 300watt, meer verbruikt hij niet. Als je een power supply van 550watt of hoger hebt, zit je goed. Het HBM overclocken werkt trouwens bijna net zo goed als de Core overclocken, en zorgt voor een stuk minder extra verbruik. Source:
RX Vega 56;5;0.3107718527317047;Ik raad je de volgende kaart aan: pricewatch: MSI GeForce GTX 1070 GAMING X 8G De MSI Geforce GTX 1070 Gaming X 8G. Werkt zonder problemen samen met je Ryzen 1700X processor, dat gaat nooit tot issues leiden. De kaart is ONTZETTEND stil, vanaf 50 cm afstand gemeten zit de kaart onder load onder de 30 decibel. Idle is de kaart onhoorbaar, dan staan de fans stil. Deze videokaart kun je alleen bij de allerzwaarste games heel zachtjes horen. Het stroomverbruik is ook ontzettend gunstig. Meer dan 150W verbruikt de kaart zelden. De Nvidia drivers voor deze kaart zijn ontzettend stabiel en betrouwbaar, Nvidia heeft een héél erg groot team werken aan driverontwikkeling. Vrijwel altijd is er op of voor de release-dag van een nieuw spel al een game ready driver uit. De feature-set van de kaart is gewoon compleet. Op het gebied van hardware-acceleratie voor video bijvoorbeeld ondersteunt deze kaart letterlijk alles. Een 8K HEVC video kan die zelfs accelereren. Op de 4K resolutie kan die uiteraard alles accelereren. Alle features die je wilt zitten erop. En met Displayport x3, HDMI en DVI-D kun je ook alles erop aansluiten. Voor 500 euro krijg je nu deze kaart, op voorraad. En dat is een prima prijs voor een prima kaart. Deze GPU is zeer goed ontvangen met alleen maar positieve reviews. Het is een high-end GPU, en één van de stilste high-end GPU's ooit. En de performance is soooo nice. Je kunt letterlijk tientallen reviews over deze kaart lezen online, en allemaal waren ze zeer te spreken over de performance. Indien je kiest voor de GTX 1070, ben je zeker niet de enige. Neem maar eens een kijkje naar de statistieken hier:
RX Vega 56;1;0.4236633777618408;Voor de mensen die net als ik de hele dag hebben liggen zoeken naar prijzen en beschikbaarheid... enkele pre-order prijzen gevonden die ook naar België en Nederland verzenden: €676.90 voor de Wave (watercooled) versie van MSI en €580,90 voor de aircooled black editie (ook MSI), zonder game pack...exclusief verzendkosten en betrouwbare? Tjechische webshop moet je er even wel bijnemen !
RX Vega 56;3;0.5587982535362244;Had niks anders verwacht Lisa su is een pr vrouw en dat zag je al met rx 480 die als vr kaart werd bestempeld wat echt niet zo is. Maar het is wel mooi dat er een beetje concurrentie is.
RX Vega 56;3;0.27769172191619873;Is het al bekend wanneer de AIB partners hun eigen varianten van de 56 en 64 mogen uitbrengen? Vooral de 56 lijkt me erg interessant als een AIB als MSI of Gigabyte de factory clocks flink opvoeren (wat als ik naar de review kijk goed mogelijk zal zijn), er een goede koeler op zetten en ~75 EUR goedkoper dan de GTX1070 verkopen (wat nu zeer goed te doen is vanwege de mining hype) dan zal de 56 een zeer goede koop zijn.
RX Vega 56;1;0.40552642941474915;Laatste wat ik gehoord heb is eind Q3 / begin Q4, wat eind september begin oktober is. Maar het schijnt dat Nederland in ieder geval geen belangijke launch partner is, dus beschikbaarheid van reference modellen is schaars... Hopen dat AIB-kaarten wel direct verkrijgbaar worden....
RX Vega 56;4;0.47449132800102234;"Top review Jelle. Zoals @Frozen hierboven ook al aangeeft, lekker uitgebreid en met erg veel testjes. Topie dus. Zo zien we het graag. (althans ik dan) Ontopic: Ik vind de RX Vega 64 wel erg veel vermogen (Watt) verbruiken, wanneer deze aanmaal goed aan het werk wordt gezet zeg. -Nvidia GeForce GTX 1080: 181 Watt ingame -AMD RX Vega 64: 301 Watt ingame Nu zijn dit twee totaal verschillende kaarten, en dus GPU's natuurlijk, maar deamn. Stond er zelf toch wel even van te kijken, en dit metname, aangezien er eerder in het artikel regelmatig ""minder energiegebruik"" voorbij komt. Wat maakt dat je verwacht dat het verbruik dan ook daadwerkelijk minder is. Al is de kans groot dat dit laatste nog wel gaat gebeuren, aangezien er, voor zover ik weet, nog niet specifiek voor deze kaart ontwikkeld wordt (niet vreemd ook, hij is immers net gelanceerd), en dat een hoop games hoor voor wellicht later nog updates gaan/zullen krijgen. Maar dat blijft altijd afwachten. Zoals reeds gezegd, een top review Jelle."
RX Vega 56;3;0.49209773540496826;In het geval van het TDP is het eerder rampzalig qua vooruitgang. De trend was meer performance ten opzichte van TDP, niet performance ten opzichte van de besteden euro's. Die €100 die je met freesync bespaard praat heel wat jaartjes hoger verbruik goed, dat is zeer zeker waar. Maar dit is niet echt een technisch hoogstandje zo.. jammer.. Het zijn geen slechte kaarten, de driver updates zullen nog wat extra performance bieden. Maar het loopt net wat achter ten opzichte van Nvidia. Terwijl er ten opzichte van Intel toch aardig wat gewonnen is. Met een beetje geluk verlaagd Nvidia hun prijzen, dat zou iig al prima zijn.
RX Vega 56;2;0.381308913230896;Waarom verbaasd iedereen zich zo over de TDP/Powerconsumptie? De Vega 64 heeft 4k cores en de 1080 ti heeft er 2.5k. Het is toch logish dat de 4k kaart meer power nodig heeft dan een kaart met 2.5k cores? En de Vega heeft een betere Watt per core dan de 1080, hier hoor ik niemand over. Wat betreft de prestaties, heb het idee dat drivers ook nog niet perfect zijn. Want een kaart die op papier 2x sneller is dan zijn nvidia variant dan zou je dat moeten terug zien in de Benchmarks. Dit zien we niet. Waar dat aan licht is mij een raadsel.
RX Vega 56;1;0.45466873049736023;Staar je jezelf niet blind op cores? Als je kijkt naar tflops, dan ligt het nagenoeg gelijk. edit: volgens mij ben je de enige die zich überhaupt met de cores bezig houd, watt per core is meer iets voor CPU's niet?
RX Vega 56;1;0.4283440113067627;Conclusie: de Gtx 1070 en 1080 blijven geen gekke kaarten!
RX Vega 56;1;0.8450313210487366;Helemaal nie tals je erbij bedenkt dat vrijwel alle non reference Nvidia kaarten fors hoger boosten. Basis 1070 boost rond de 1600, een EVGA FTW zit tegen de 2000 aan zonder een overclock en met overclock kan je vaak tegen de 2100mhz aan zitten. Dat is doodleuk 25% meer performance.
RX Vega 56;1;0.5396353006362915;Wanneer zijn de kaarten beschikbaar in europa? Ik heb nu spijt van mijn freesync display...
RX Vega 56;2;0.3166235685348511;"Nou, als ik het zo zie, ben ik blij dat ik 9 maanden geleden een GTX 1070 heb aangeschaft. Ik heb een MSI GTX 1070 GamingX 8GB, waar ik via Coolblue destijds €507 voor heb betaald. Nu is hij bij twee winkels nog te krijgen voor rond de €500, maar bij Coolblue kost hij nu €649 (!). Ik vraag me af of die cryptocurrency ooit iets gaat worden; de hoop is natuurlijk om één of als het kan 2-3 munten te vinden, daarmee de kaarten eruit te halen, en dan binnen te lopen via prijsstijgingen. Gelukkig speel ik nauwelijks nog spellen. Ik speel zonder problemen een 10-15 jaar oude RPG op 800x600 op mijn 1920x1200 monitor, en het zwaarste spel dat ik speel is met grote afstand The Witcher 3, op 1920x1200/Ultra. De 1070 voldoet daar prima voor. Alle andere spellen, en de twee die ik nog ga spelen waarschijnlijk (Torment: Tides of Numenera an Divinity Original Sin 2) gaan daar makkelijk op draaien. Vanwege mijn grote backlog aan spellen ben ik nog makkelijk 4-5 voorzien. Tegen die tijd wissel ik de grafische kaart uit voor een nieuw model indien nodig. Dat moet prima kunnen met een 6700K en 32GB RAM, en dan kan die computer nog eens 4 jaar mee. Dat is ook zo gegaan bij de vorige, omdat toen met The Witcher 3 een spel langs kwam dat ik niet meer kon spelen op de gewenste instellingen."
RX Vega 56;2;0.6474776268005371;Jammer, en dat meen ik oprecht. Persoonlijk had ik meer verwacht van AMD na zo lang afwezig te zijn geweest in het topsegment. Gezien de ontwikkeltijd had ik op z'n minst een zuinigere kaart verwacht tegen een redelijke prijs. Uiteraard is het altijd wachten op driveroptimalisaties maar de eerste indruk is voor mij niet positief.
RX Vega 56;4;0.27516958117485046;Als ik de reviews een beetje her en der zie dan is het niet eens zo verkeerd. In de nieuwe API benchmarks heeft de 64 nog best een voorsprong op de 1080 en dan ben ik heel benieuwd naar de fp16(rapid packed math) games die eraan komen zoals Far Cry 5 en Wolfenstein, dat moet de gat alleen maar groter maken.
RX Vega 56;2;0.43567773699760437;"Tja, de prijs is nog niet definitief en laten we ook niet vergeten dat je bij Nvidia nog een premium betaald voor gsync. Sowieso gaat AMD goed omzet uit deze kaart kunnen halen indien het de markt van miners gaat bedienen. Voor AMD een prima kaart, maar volgens mij ook niet hetgeen het bedrijf graag naar voren had willen brengen. Uitstel na uitstel, om een reden die iedereen al vermoedde en nu wordt bevestigd. De prangende vraag vanuit mij persoonlijk is nu wel; wat gaat Nvidia doen? Zit het bedrijf ook aan de max, of heeft het ondertussen alweer een nieuwe architectuur met HBM2 gereed die Vega wegvaagt? Zal het een kleine update doen om volop de leider te blijven? Ik ben wel benieuwd, dat maakt voor een doorslag definitief. Op dit moment heb ik nog altijd een freesync scherm staan die ik met een AMD kaart wil bedienen. Maar als Nvidia nog een genadeslag heeft die de meerprijs waarmaakt, dan koop ik wel een scherm met Gsync."
RX Vega 56;3;0.3571319878101349;Er wordt hier iets over het hoofd gezien en dat zijn de power modes dat Vega aanbiedt. Als je naar de TPU review kijkt, dan zie je dat PwrSave mode 96% van de performance geeft van Std mode, terwijl Vega64 dan 78W minder verbruikt. Vega64 PwrSave <2% trager dan GTX1080 terwijl die maar 48W meer verbruikt.
RX Vega 56;2;0.35395583510398865;Het prijsbeleid is vrij duidelijk:RX Vega 56 komt op $399 MSRPRX Vega 64 komt op $499 stock, $599 Limited Edition (looks) en $699 voor de Liquid Edition.GTX 1070 MSRP zit officieel op $379GTX 1080 verlaagd van $599 naar $499.Dan de realiteit bij Nvidia. Om even de euro/dollar problematiek en de import heffing en BTW buiten beschouwing te laten kan men het beste naar newegg kijken. Daar is de goedkoopste 1070 momenteel $429 dollar. De goedkoopste GTX 1080 zit inderdaad op $499. Gemiddeld zijn de kaarten hier een stuk duurder, dat heeft hier niet alleen te maken met de zwakke euro, de btw en de import heffing maar ook met de mining hype. Daar zit voor AMD nu net het pijnpunt. Immers blijkt dat de RX Vega 56 met 31 MT/s al bijna het niveau van een veel duurdere GTX 1080 Ti haalt. Voor miners dus heel erg interessant, AMD is nu niet het bepaald het bedrijf met een overschot aan zelfvertrouwen dus traditioneel zie je die erg voorzichtige productie orders plaatsen. Combineer dat met de gunstige mining performance en ik zou er niet op rekenen dat je, op een paar launch kaarten na, de komende 6 maanden voor een normale prijs een RX Vega 56 kunt aanschaffen. Verder werd er door The Source (MSI) al gemeld dat AMD de marge van de hele keten onder druk zet om gunstige launch prijzen bij de reviews te kunnen laten zien. De custom modellen zouden duurder worden.
RX Vega 56;3;0.5350952744483948;Mooi staaltje techniek deze nieuwe kaarten. De benchmarks zullen vele tegen vallen, die baseren hoe goed iets is puur op hoe het nu in directe vorm presteert tegen over iets wat er al is op een gebied van een 2 jaar oude game. Op zich logisch maar ook een beetje scheef. Ik kijk erg uit naar de benchmark van Wolfenstein. Men klaagt wel vaak op AMD maar ze leveren wel vaak iets innovatief en wanneer de opties die deze kaart heeft worden benut dan kan 64 zo maar boven aan het lijstje komen te staan en dan is het de vraag of je oude games wilt spelen met meer fps of de nieuwe games? Om enig succesvol te zijn moet de prijs dan wel onder de die van de concurrentie zitten, ook al is de kaart nieuwer en over een jaar misschien wel 20% sneller. Daar heeft de consument nu niet zoveel aan.
RX Vega 56;2;0.4174826443195343;hoe kun je nou zo'n kaart testen maar hem niet testen op mining speeds? jammer hoor
RX Vega 56;5;0.3351079821586609;Wat ik van de vega zie is dit. Miners heaven, gamers hell. Vega 56 zal goed verkopen onder de miners door veel lager stroomverbruik dan zijn grote broer en minimale prestatie verschil. Move a long, nothing to see here.
RX Vega 56;3;0.43560490012168884;Ik zie bredere support van Feature level 12_1 en SM6.0+ met veel meer Tier3 level. Voor stoeien met DX12 en Vulkan SDK de betere keuze
RX Vega 56;2;0.36051610112190247;"dat is ook wat ik zie... waar dx12/vulcan gebruikt wordt begint de kracht van vega zichtbaar te worden... hoewel AMD zich met de korte tijd die ze de reviewers gaven zichzelf wel in de voeten heeft geschoten. je ziet tussen de verschillende gebruikte drivers behoorlijk grote verschillen en mining lijkt zelfs terug geschroefd te zijn. waar ander opencl test juist sporen van vuur achter laten... (een paar standaard test lieten 30% (ongeveer) hogere scores zien als de 1080ti... ""Somethings weird"""
RX Vega 56;1;0.40620583295822144;nVidia chokehold op de GPU markt haha. Ze lopen effectief gewoon een generatie voor. Volta is gonna crush AMD so hard. Had ik mijn aandelen nVidia maar gehouden...
RX Vega 56;3;0.6103509068489075;Mooi review maar mis wel wat dingetjes.. O.a. de minimum framerates, deze blijken voor de rx64 in bv GTA V hoger te zijn dan de 1080 ... wat opmerkelijk te noemen is gezien de maximage fps. Dit is wel op basis van andere reviews overigens. Ook zie ik in jullie eigen benchmarks de rx56 sneller zijn dan de rx64 bij enkel tests, en kan er overheen gelezen hebben maar zover ik zag is dit verder onopgemerkt gebleven. Dit is natuurlijk ook opmerkelijk. Ergens denk ik dat het ook voornamelijk problemen met de drivers zijn die gaming parten spelen, ik neem aan dat jullie oude drivers hebben gebruikt vanwege de korte tijd na uitkomen van de 17.8.1 bèta 6? Ik heb zelf gerucht meegekregen dat er alsnog veel moeite gedaan om de hashrates voor miners toch omlaag te halen zodat niet alle kaarten meteen weg zouden zijn door miners die bitcoin-tekens in hun ogen hadden na eerdere berichten over zeer hoge hashrates. Alle reviews geven eigenlijk weer dat de kaart in ieder geval tegen bijzondere driver problemen aanloopt... in enkele test fietst de rx64 namelijk ook de 1080ti voorbij en ook in enkele opencl test gaat ie er vrij ruim voorbij waarbij de ethereum hash rate juist omlaag gegaan is. Ik denk dat we in de komende weken alsnog wel redelijk wat verbetering te zien krijgen... Wat AMD voornamelijk te verwijten is dat ondanks alle vertragingen zo dit toch gehaast vrijgegeven hebben, met ZEER onvolwassen drivers. Crypto-mining-gekte of niet, als het daarom als is, de drivers lijken in enkele gevallen zelf veel trager dan eerder gelekte testen en benchmarks.
RX Vega 56;4;0.6409690976142883;Mooie review en leuke kaarten tenminste een beetje concurrentie voor nvidia nu, wel jammer is dat het stroom verbruik onder load nog een beetje aan de hoge kant is, De vraag is nu natuurlijk wat de echte prijzen gaan zijn in Nederland en zeker met die mining hype ... zucht.
RX Vega 56;1;0.8252773880958557;Mininghype is al voorbij. Je verdiend niks meer mee en moet juist inleveren ( in nl tenminste ). Ik zal die vega een dragon punch geven waar hij nog vele jaren pijn van zal hebben. Troep blijft troep. Al die hype voor dit, pffff. Ik word echt niet vrolijk van als amd zo zit te verneuken.
RX Vega 56;2;0.3393908143043518;Ik ben misschien een leek hierin over de nieuwe Vega Kaarten. Wat ik zie is een intel systeem, hoe zou de kaart draaien als de computer volledig Ryzen processor in zat? Zou de kaart misschien beter draaien omdat alles AMD is? Heb zelf nu een AMD phenom x6 met een AMD R9 280x Dual kaart en het draait echt stabiel. Of is dit een rare vraag? is er iemand die hier over kan uitleggen of er nou verschil in zit ja of nee?
RX Vega 56;2;0.4288676381111145;Nou de genoemde prijzen zijn alweer achterhaald. De nieuwe lichting kaarten gaan duurder worden, oftewel nog minder competitief tegenover Nvidia.
RX Vega 56;1;0.4770747721195221;Een teleurstelling dus, helaas. AMD komt anderhalf jaar na Nvidia met 'concurrenerde' kaarten die het niet alleen qua performance af moeten leggen tegen de concurrent, maar ook nog eens gemiddeld zo'n 15-20% meer stroom verbuiken en meer herrie produceren. Daarbij heeft Nvidia nu na al die tijd zijn drivers op orde. Bij AMD zal dat nog wel even gaan duren. Ik kan geen enkele reden bedenken om nu voor één van AMD's nieuwelingen te kiezen...
RX Vega 56;2;0.3508242070674896;"Ik wel, en dat is dat ze sinds de laatste drivers ""enhanced sync"" hebben bij AMD. Een beetje zoals freesync of gsync , maar dan op software niveau. En het werkt verdomd goed. Als ik nu van mijn RX480 ( die net wat te weinig power heeft om Quake Champions te spelen ) en LG 21/*9 scherm, zou willen upgraden naar een Nvidia GPU en gsync scherm, dan ben ik veel meer geld kwijt dat dat ik nu simpelweg een upgrade uitvoer naar vega. Dat die kaart dan 100W meer verbruikt zal mij een worst wezen."
RX Vega 56;3;0.34050366282463074;hier ga ik wel een paar minertjes van bouwen, 70 megahash per second
RX Vega 56;1;0.7821722626686096;650 euro bij alternate.de er is niet eens de Bundel erbij! Dus middelmatige performance, slechte preis, hoog stroomverbruik. yay Well al meer dan een paar verkocht
RX Vega 56;4;0.4247188866138458;"Prijs schrijven met ""ei"" , wel met ""ll"" ... sterk bezig."
RX Vega 56;1;0.6542509198188782;Juist, mijn argument is niet geldig door een spellingfout... ik kan jouw onfaalbare logica niet aan.
RX Vega 56;1;0.6909023523330688;Jij ziet ook nog woorden staan die ik niet typ. Waar zeg ik dat ik jouw argument niet snap ? Meer dan een opmerking op slechte spelling was het niet hoor.
RX Vega 56;1;0.25640004873275757;"Spreek ""ego-boost"""
RX Vega 56;3;0.5056076645851135;Het valt mij op hoe goed de Fury X presteert tegenover de rest. Ik meen mij te herinneren dat de Fury X een paar jaar terug tegenover de 980ti werd geplaatst. Met andere woorden, driver-optimalisaties zullen over een aantal maanden wat meer duidelijkheid scheppen. Ook als het gaat om verbruik. Momenteel is het goed om te zien dat ze hun mannetje staan tegenover de 1080 en de 1070 en prijsconcurrent kunnen zijn.
RX Vega 56;3;0.4044285714626312;Vergeet niet dat de Vega 64 tussen 1080 en 1080 ti performance zit wanneer alle kaarten ZONDER MSAA draaien. Er is een kans dat dit met driver goed gepatched kan worden waardoor de Vega kaarten nog wat aantrekkelijker gaan lijken.
RX Vega 56;3;0.39543065428733826;"Had graag ook nog een warmtecamerafoto gezien zoals die op Tomshardware. De warmteonwikkeling valt mee. Tweakers geluidsmetingen zijn nogal aan de optimistische kant. Geluidsmeter (welke?) is niet echt accuraat genoeg. Ze hebben het bij Toms over een geluidsproductie in turbomode van 50 dB(A) of 48,2 dB(A) in de ""normale modus"". Dit is gemeten vanaf 50 cm!!!! Dit is voor mij het geluid van een stofzuiger cq tornado. Tomshardware heeft dan ook de beschikking over een ""bijna"" stille kamer en NTI Audio M2211 meetmicrofoon voor het zachte prijsje van € 2.535 Bron:"
RX Vega 56;1;0.498355507850647;Waarom zou je in godsnaam deze kaarten kopen als je geen specifieke use case hebt (Blender, minen)? Kosten praktisch hetzelfde als je het verbruik meerekent, en komen nog maar net in de buurt van de groene jongens. Dan kan je het wel over overklokken hebben, maar een 1070, 1080 en 1080ti kun je ook overklokken. Hell, ik zou m'n PSU moeten upgraden om een AMD kaart erin te zetten .
RX Vega 56;1;0.5013313889503479;En is er al iemand die deze kaart gekocht heeft?
RX Vega 56;3;0.37482914328575134;Als je deftige 144fps wil en budget niet voor overprice Ti top end hebt , kan je de game setting tweaken om 144 te halen. Als dat belangrijkste reden is hoog fps voor bunny jumpen in frantic fastpace shooters. Dan gaat dat boven ultra setting. Medium mix kan ook zonder je blauw te betalen. Kan er ook nog goed uitzien. Aangezien Ultra setting vaker de meeste subtiele gfx verbeteringen zijn maar compute intensief. Freesync is handig voor 60hz stabiler te maken ipv 30hz frame time stutter door iets meer tijd nodig hebben voor 1/60 frame time. Gezien 1/30 vs 1/60 een flinke stutter is tegen een minderstabile 1/144. Freesync is belangrijker voor hoge resoluties en 60 fps Tja als game tussen de 55 en 100 fps schommeld bij tweaked setting dan past die setting goed bij een 60 sync. En free sync stijkt de dips stuk strakker Als 50 tot 200fps haald en minimaal strakke 90fps moet hebben dan stel je setting terug zodat min FPS omhoog gaat. Meestal gebeurt er niet veel bij hoge fps. Kijkt tegen muur aan upclose. Bij bunny hoppen doet animatie character logic voor weergeven van die andere foo bunnyhopper flink wat werk en grafische wodt het aanzienlijk dynamischer load in de render pipeline. Het Ultra setting gamen op 144 is luxe probleem geen verplichtng om in ultra settings te gamen maar optie van hoeveel geld je ertegen aan wilt gooien. Een luxe probleem dus. Er zijn games die er duidelijk slecht in low setting uitzien zo een game is crysis op 8600GT waar medium to much is. Een HD2900 met medium high mix zag er heel goed uit. Gezien de open wereld slow pace speelde ik met te lage FPS liever op medium setting denk dat 30fps niet goed gehaald werd. Far cry2 op low met 8600GT zag er goed uit en liep ook soepel. Ja zijn meer de open wereld games.
RX Vega 56;3;0.47715476155281067;Tja lage settings etc.. kan een R9 290x/ FURY X ook wel aan je 1080p freesync...
